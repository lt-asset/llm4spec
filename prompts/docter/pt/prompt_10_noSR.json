{
    "data": [
        {
            "fname": "torch.nn.functional.interpolate.yaml",
            "arg": "align_corners",
            "prompt": "Signature: torch.distributed.recv(tensor,src=None,group=<objectobject>,tag=0)\nParameter description: tensor(Tensor) - Tensor to fill with received data.\nConstraints:\ndtype:Null\nstructure:tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.div(input,other,out=None)\nParameter description: out(Tensor, optional) - the output tensor.\nConstraints:\ndtype:Null\nstructure:tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.nn.TransformerDecoder(decoder_layer,num_layers,norm=None)\nParameter description: num_layers - the number of sub-decoder-layers in the decoder (required).\nConstraints:\ndtype:int\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:[0,inf)\n\n###\n\nSignature: torch.nn.quantized.functional.interpolate(input,size=None,scale_factor=None,mode=nearest,align_corners=None)\nParameter description: align_corners(bool, optional) - Geometrically, we consider the pixels of the input and output as squares rather than points. If set to `True`, the input and output tensors are aligned by the center points of their corner pixels, preserving the values at the corner pixels. If set to `False`, the input and output tensors are aligned by the corner points of their corner pixels, and the interpolation uses edge value padding for out-of-boundary values, making this operation independent of input size when `scale_factor` is kept the same. This only has an effect when `mode` is `'bilinear'`. Default: `False`\nConstraints:\ndtype:bool\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.gt(input,other,out=None)\nParameter description: out(Tensor, optional) - the output tensor that must be a BoolTensor\nConstraints:\ndtype:booltensor\nstructure:tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.empty_like(input,dtype=None,layout=None,device=None,requires_grad=False,memory_format=torch.preserve_format)\nParameter description: dtype(`torch.dtype`, optional) - the desired data type of returned Tensor. Default: if `None`, defaults to the dtype of `input`.\nConstraints:\ndtype:torch.dtype\nstructure:Null\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.nn.RNN(*args,**kwargs,input_size=None,hidden_size=None,num_layers=None,nonlinearity=None,bias=None,batch_first=None,dropout=None,bidirectional=None)\nParameter description: hidden_size - The number of features in the hidden state h\nConstraints:\ndtype:int\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:[0,inf)\n\n###\n\nSignature: torch.nn.functional.avg_pool1d(input,kernel_size,stride=None,padding=0,ceil_mode=False,count_include_pad=True)\nParameter description: padding - implicit zero paddings on both sides of the input. Can be a single number or a tuple (padW,). Default: 0\nConstraints:\ndtype:bool;int\nstructure:tuple\nshape:Null\nndim:0;1\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.autograd.grad(outputs,inputs,grad_outputs=None,retain_graph=None,create_graph=False,only_inputs=True,allow_unused=False)\nParameter description: outputs(sequence of Tensor) - outputs of the differentiated function.\nConstraints:\ndtype:Null\nstructure:sequence;tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.repeat_interleave(input,repeats,dim=None)\nParameter description: dim(int, optional) - The dimension along which to repeat values.By default, use the flattened input array, and return a flat outputarray.\nConstraints:\ndtype:int\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.nn.functional.interpolate(input,size=None,scale_factor=None,mode=nearest,align_corners=None,recompute_scale_factor=None)\nParameter description: align_corners(bool, optional) - Geometrically, we consider the pixels of the input and output as squares rather than points. If set to `True`, the input and output tensors are aligned by the center points of their corner pixels, preserving the values at the corner pixels. If set to `False`, the input and output tensors are aligned by the corner points of their corner pixels, and the interpolation uses edge value padding for out-of-boundary values, making this operation independent of input size when `scale_factor` is kept the same. This only has an effect when `mode` is `'linear'`, `'bilinear'`, `'bicubic'` or `'trilinear'`. Default: `False`\nConstraints:",
            "constr": {
                "dtype": [
                    "bool"
                ],
                "structure": [],
                "shape": [],
                "ndim": [
                    "0"
                ],
                "enum": [],
                "range": []
            },
            "signature": "torch.nn.functional.interpolate(input,size=None,scale_factor=None,mode=nearest,align_corners=None,recompute_scale_factor=None)",
            "descp": "align_corners(bool, optional) - Geometrically, we consider the pixels of the input and output as squares rather than points. If set to `True`, the input and output tensors are aligned by the center points of their corner pixels, preserving the values at the corner pixels. If set to `False`, the input and output tensors are aligned by the corner points of their corner pixels, and the interpolation uses edge value padding for out-of-boundary values, making this operation independent of input size when `scale_factor` is kept the same. This only has an effect when `mode` is `'linear'`, `'bilinear'`, `'bicubic'` or `'trilinear'`. Default: `False`"
        },
        {
            "fname": "torch.pow.yaml",
            "arg": "out",
            "prompt": "Signature: torch.distributed.recv(tensor,src=None,group=<objectobject>,tag=0)\nParameter description: tensor(Tensor) - Tensor to fill with received data.\nConstraints:\ndtype:Null\nstructure:tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.div(input,other,out=None)\nParameter description: out(Tensor, optional) - the output tensor.\nConstraints:\ndtype:Null\nstructure:tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.nn.TransformerDecoder(decoder_layer,num_layers,norm=None)\nParameter description: num_layers - the number of sub-decoder-layers in the decoder (required).\nConstraints:\ndtype:int\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:[0,inf)\n\n###\n\nSignature: torch.nn.quantized.functional.interpolate(input,size=None,scale_factor=None,mode=nearest,align_corners=None)\nParameter description: align_corners(bool, optional) - Geometrically, we consider the pixels of the input and output as squares rather than points. If set to `True`, the input and output tensors are aligned by the center points of their corner pixels, preserving the values at the corner pixels. If set to `False`, the input and output tensors are aligned by the corner points of their corner pixels, and the interpolation uses edge value padding for out-of-boundary values, making this operation independent of input size when `scale_factor` is kept the same. This only has an effect when `mode` is `'bilinear'`. Default: `False`\nConstraints:\ndtype:bool\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.gt(input,other,out=None)\nParameter description: out(Tensor, optional) - the output tensor that must be a BoolTensor\nConstraints:\ndtype:booltensor\nstructure:tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.empty_like(input,dtype=None,layout=None,device=None,requires_grad=False,memory_format=torch.preserve_format)\nParameter description: dtype(`torch.dtype`, optional) - the desired data type of returned Tensor. Default: if `None`, defaults to the dtype of `input`.\nConstraints:\ndtype:torch.dtype\nstructure:Null\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.nn.RNN(*args,**kwargs,input_size=None,hidden_size=None,num_layers=None,nonlinearity=None,bias=None,batch_first=None,dropout=None,bidirectional=None)\nParameter description: hidden_size - The number of features in the hidden state h\nConstraints:\ndtype:int\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:[0,inf)\n\n###\n\nSignature: torch.nn.functional.avg_pool1d(input,kernel_size,stride=None,padding=0,ceil_mode=False,count_include_pad=True)\nParameter description: padding - implicit zero paddings on both sides of the input. Can be a single number or a tuple (padW,). Default: 0\nConstraints:\ndtype:bool;int\nstructure:tuple\nshape:Null\nndim:0;1\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.autograd.grad(outputs,inputs,grad_outputs=None,retain_graph=None,create_graph=False,only_inputs=True,allow_unused=False)\nParameter description: outputs(sequence of Tensor) - outputs of the differentiated function.\nConstraints:\ndtype:Null\nstructure:sequence;tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.repeat_interleave(input,repeats,dim=None)\nParameter description: dim(int, optional) - The dimension along which to repeat values.By default, use the flattened input array, and return a flat outputarray.\nConstraints:\ndtype:int\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.pow(input,exponent,out=None)\nParameter description: out(Tensor, optional) - the output tensor.\nConstraints:",
            "constr": {
                "dtype": [],
                "structure": [
                    "tensor"
                ],
                "shape": [],
                "ndim": [],
                "enum": [],
                "range": []
            },
            "signature": "torch.pow(input,exponent,out=None)",
            "descp": "out(Tensor, optional) - the output tensor."
        },
        {
            "fname": "torch.nn.fold.yaml",
            "arg": "padding",
            "prompt": "Signature: torch.distributed.recv(tensor,src=None,group=<objectobject>,tag=0)\nParameter description: tensor(Tensor) - Tensor to fill with received data.\nConstraints:\ndtype:Null\nstructure:tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.div(input,other,out=None)\nParameter description: out(Tensor, optional) - the output tensor.\nConstraints:\ndtype:Null\nstructure:tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.nn.TransformerDecoder(decoder_layer,num_layers,norm=None)\nParameter description: num_layers - the number of sub-decoder-layers in the decoder (required).\nConstraints:\ndtype:int\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:[0,inf)\n\n###\n\nSignature: torch.nn.quantized.functional.interpolate(input,size=None,scale_factor=None,mode=nearest,align_corners=None)\nParameter description: align_corners(bool, optional) - Geometrically, we consider the pixels of the input and output as squares rather than points. If set to `True`, the input and output tensors are aligned by the center points of their corner pixels, preserving the values at the corner pixels. If set to `False`, the input and output tensors are aligned by the corner points of their corner pixels, and the interpolation uses edge value padding for out-of-boundary values, making this operation independent of input size when `scale_factor` is kept the same. This only has an effect when `mode` is `'bilinear'`. Default: `False`\nConstraints:\ndtype:bool\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.gt(input,other,out=None)\nParameter description: out(Tensor, optional) - the output tensor that must be a BoolTensor\nConstraints:\ndtype:booltensor\nstructure:tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.empty_like(input,dtype=None,layout=None,device=None,requires_grad=False,memory_format=torch.preserve_format)\nParameter description: dtype(`torch.dtype`, optional) - the desired data type of returned Tensor. Default: if `None`, defaults to the dtype of `input`.\nConstraints:\ndtype:torch.dtype\nstructure:Null\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.nn.RNN(*args,**kwargs,input_size=None,hidden_size=None,num_layers=None,nonlinearity=None,bias=None,batch_first=None,dropout=None,bidirectional=None)\nParameter description: hidden_size - The number of features in the hidden state h\nConstraints:\ndtype:int\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:[0,inf)\n\n###\n\nSignature: torch.nn.functional.avg_pool1d(input,kernel_size,stride=None,padding=0,ceil_mode=False,count_include_pad=True)\nParameter description: padding - implicit zero paddings on both sides of the input. Can be a single number or a tuple (padW,). Default: 0\nConstraints:\ndtype:bool;int\nstructure:tuple\nshape:Null\nndim:0;1\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.autograd.grad(outputs,inputs,grad_outputs=None,retain_graph=None,create_graph=False,only_inputs=True,allow_unused=False)\nParameter description: outputs(sequence of Tensor) - outputs of the differentiated function.\nConstraints:\ndtype:Null\nstructure:sequence;tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.repeat_interleave(input,repeats,dim=None)\nParameter description: dim(int, optional) - The dimension along which to repeat values.By default, use the flattened input array, and return a flat outputarray.\nConstraints:\ndtype:int\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.nn.Fold(output_size,kernel_size,dilation=1,padding=0,stride=1)\nParameter description: padding(int or tuple, optional) - implicit zero padding to be added on both sides of input. Default: 0\nConstraints:",
            "constr": {
                "dtype": [
                    "int"
                ],
                "structure": [
                    "tuple"
                ],
                "shape": [],
                "ndim": [
                    "0",
                    "1"
                ],
                "enum": [],
                "range": []
            },
            "signature": "torch.nn.Fold(output_size,kernel_size,dilation=1,padding=0,stride=1)",
            "descp": "padding(int or tuple, optional) - implicit zero padding to be added on both sides of input. Default: 0"
        },
        {
            "fname": "torch.autograd.functional.vhp.yaml",
            "arg": "strict",
            "prompt": "Signature: torch.distributed.recv(tensor,src=None,group=<objectobject>,tag=0)\nParameter description: tensor(Tensor) - Tensor to fill with received data.\nConstraints:\ndtype:Null\nstructure:tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.div(input,other,out=None)\nParameter description: out(Tensor, optional) - the output tensor.\nConstraints:\ndtype:Null\nstructure:tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.nn.TransformerDecoder(decoder_layer,num_layers,norm=None)\nParameter description: num_layers - the number of sub-decoder-layers in the decoder (required).\nConstraints:\ndtype:int\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:[0,inf)\n\n###\n\nSignature: torch.nn.quantized.functional.interpolate(input,size=None,scale_factor=None,mode=nearest,align_corners=None)\nParameter description: align_corners(bool, optional) - Geometrically, we consider the pixels of the input and output as squares rather than points. If set to `True`, the input and output tensors are aligned by the center points of their corner pixels, preserving the values at the corner pixels. If set to `False`, the input and output tensors are aligned by the corner points of their corner pixels, and the interpolation uses edge value padding for out-of-boundary values, making this operation independent of input size when `scale_factor` is kept the same. This only has an effect when `mode` is `'bilinear'`. Default: `False`\nConstraints:\ndtype:bool\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.gt(input,other,out=None)\nParameter description: out(Tensor, optional) - the output tensor that must be a BoolTensor\nConstraints:\ndtype:booltensor\nstructure:tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.empty_like(input,dtype=None,layout=None,device=None,requires_grad=False,memory_format=torch.preserve_format)\nParameter description: dtype(`torch.dtype`, optional) - the desired data type of returned Tensor. Default: if `None`, defaults to the dtype of `input`.\nConstraints:\ndtype:torch.dtype\nstructure:Null\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.nn.RNN(*args,**kwargs,input_size=None,hidden_size=None,num_layers=None,nonlinearity=None,bias=None,batch_first=None,dropout=None,bidirectional=None)\nParameter description: hidden_size - The number of features in the hidden state h\nConstraints:\ndtype:int\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:[0,inf)\n\n###\n\nSignature: torch.nn.functional.avg_pool1d(input,kernel_size,stride=None,padding=0,ceil_mode=False,count_include_pad=True)\nParameter description: padding - implicit zero paddings on both sides of the input. Can be a single number or a tuple (padW,). Default: 0\nConstraints:\ndtype:bool;int\nstructure:tuple\nshape:Null\nndim:0;1\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.autograd.grad(outputs,inputs,grad_outputs=None,retain_graph=None,create_graph=False,only_inputs=True,allow_unused=False)\nParameter description: outputs(sequence of Tensor) - outputs of the differentiated function.\nConstraints:\ndtype:Null\nstructure:sequence;tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.repeat_interleave(input,repeats,dim=None)\nParameter description: dim(int, optional) - The dimension along which to repeat values.By default, use the flattened input array, and return a flat outputarray.\nConstraints:\ndtype:int\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.autograd.functional.vhp(func,inputs,v=None,create_graph=False,strict=False)\nParameter description: strict(bool, optional) - If `True`, an error will be raised when we detect that there exists an input such that all the outputs are independent of it. If `False`, we return a Tensor of zeros as the vhp for said inputs, which is the expected mathematical value. Defaults to `False`.\nConstraints:",
            "constr": {
                "dtype": [
                    "bool"
                ],
                "structure": [],
                "shape": [],
                "ndim": [
                    "0"
                ],
                "enum": [],
                "range": []
            },
            "signature": "torch.autograd.functional.vhp(func,inputs,v=None,create_graph=False,strict=False)",
            "descp": "strict(bool, optional) - If `True`, an error will be raised when we detect that there exists an input such that all the outputs are independent of it. If `False`, we return a Tensor of zeros as the vhp for said inputs, which is the expected mathematical value. Defaults to `False`."
        },
        {
            "fname": "torch.remainder.yaml",
            "arg": "input",
            "prompt": "Signature: torch.distributed.recv(tensor,src=None,group=<objectobject>,tag=0)\nParameter description: tensor(Tensor) - Tensor to fill with received data.\nConstraints:\ndtype:Null\nstructure:tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.div(input,other,out=None)\nParameter description: out(Tensor, optional) - the output tensor.\nConstraints:\ndtype:Null\nstructure:tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.nn.TransformerDecoder(decoder_layer,num_layers,norm=None)\nParameter description: num_layers - the number of sub-decoder-layers in the decoder (required).\nConstraints:\ndtype:int\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:[0,inf)\n\n###\n\nSignature: torch.nn.quantized.functional.interpolate(input,size=None,scale_factor=None,mode=nearest,align_corners=None)\nParameter description: align_corners(bool, optional) - Geometrically, we consider the pixels of the input and output as squares rather than points. If set to `True`, the input and output tensors are aligned by the center points of their corner pixels, preserving the values at the corner pixels. If set to `False`, the input and output tensors are aligned by the corner points of their corner pixels, and the interpolation uses edge value padding for out-of-boundary values, making this operation independent of input size when `scale_factor` is kept the same. This only has an effect when `mode` is `'bilinear'`. Default: `False`\nConstraints:\ndtype:bool\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.gt(input,other,out=None)\nParameter description: out(Tensor, optional) - the output tensor that must be a BoolTensor\nConstraints:\ndtype:booltensor\nstructure:tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.empty_like(input,dtype=None,layout=None,device=None,requires_grad=False,memory_format=torch.preserve_format)\nParameter description: dtype(`torch.dtype`, optional) - the desired data type of returned Tensor. Default: if `None`, defaults to the dtype of `input`.\nConstraints:\ndtype:torch.dtype\nstructure:Null\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.nn.RNN(*args,**kwargs,input_size=None,hidden_size=None,num_layers=None,nonlinearity=None,bias=None,batch_first=None,dropout=None,bidirectional=None)\nParameter description: hidden_size - The number of features in the hidden state h\nConstraints:\ndtype:int\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:[0,inf)\n\n###\n\nSignature: torch.nn.functional.avg_pool1d(input,kernel_size,stride=None,padding=0,ceil_mode=False,count_include_pad=True)\nParameter description: padding - implicit zero paddings on both sides of the input. Can be a single number or a tuple (padW,). Default: 0\nConstraints:\ndtype:bool;int\nstructure:tuple\nshape:Null\nndim:0;1\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.autograd.grad(outputs,inputs,grad_outputs=None,retain_graph=None,create_graph=False,only_inputs=True,allow_unused=False)\nParameter description: outputs(sequence of Tensor) - outputs of the differentiated function.\nConstraints:\ndtype:Null\nstructure:sequence;tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.repeat_interleave(input,repeats,dim=None)\nParameter description: dim(int, optional) - The dimension along which to repeat values.By default, use the flattened input array, and return a flat outputarray.\nConstraints:\ndtype:int\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.remainder(input,other,out=None)\nParameter description: input(Tensor) - the dividend\nConstraints:",
            "constr": {
                "dtype": [],
                "structure": [
                    "tensor"
                ],
                "shape": [],
                "ndim": [],
                "enum": [],
                "range": []
            },
            "signature": "torch.remainder(input,other,out=None)",
            "descp": "input(Tensor) - the dividend"
        },
        {
            "fname": "torch.sparse_coo_tensor.yaml",
            "arg": "values",
            "prompt": "Signature: torch.distributed.recv(tensor,src=None,group=<objectobject>,tag=0)\nParameter description: tensor(Tensor) - Tensor to fill with received data.\nConstraints:\ndtype:Null\nstructure:tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.div(input,other,out=None)\nParameter description: out(Tensor, optional) - the output tensor.\nConstraints:\ndtype:Null\nstructure:tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.nn.TransformerDecoder(decoder_layer,num_layers,norm=None)\nParameter description: num_layers - the number of sub-decoder-layers in the decoder (required).\nConstraints:\ndtype:int\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:[0,inf)\n\n###\n\nSignature: torch.nn.quantized.functional.interpolate(input,size=None,scale_factor=None,mode=nearest,align_corners=None)\nParameter description: align_corners(bool, optional) - Geometrically, we consider the pixels of the input and output as squares rather than points. If set to `True`, the input and output tensors are aligned by the center points of their corner pixels, preserving the values at the corner pixels. If set to `False`, the input and output tensors are aligned by the corner points of their corner pixels, and the interpolation uses edge value padding for out-of-boundary values, making this operation independent of input size when `scale_factor` is kept the same. This only has an effect when `mode` is `'bilinear'`. Default: `False`\nConstraints:\ndtype:bool\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.gt(input,other,out=None)\nParameter description: out(Tensor, optional) - the output tensor that must be a BoolTensor\nConstraints:\ndtype:booltensor\nstructure:tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.empty_like(input,dtype=None,layout=None,device=None,requires_grad=False,memory_format=torch.preserve_format)\nParameter description: dtype(`torch.dtype`, optional) - the desired data type of returned Tensor. Default: if `None`, defaults to the dtype of `input`.\nConstraints:\ndtype:torch.dtype\nstructure:Null\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.nn.RNN(*args,**kwargs,input_size=None,hidden_size=None,num_layers=None,nonlinearity=None,bias=None,batch_first=None,dropout=None,bidirectional=None)\nParameter description: hidden_size - The number of features in the hidden state h\nConstraints:\ndtype:int\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:[0,inf)\n\n###\n\nSignature: torch.nn.functional.avg_pool1d(input,kernel_size,stride=None,padding=0,ceil_mode=False,count_include_pad=True)\nParameter description: padding - implicit zero paddings on both sides of the input. Can be a single number or a tuple (padW,). Default: 0\nConstraints:\ndtype:bool;int\nstructure:tuple\nshape:Null\nndim:0;1\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.autograd.grad(outputs,inputs,grad_outputs=None,retain_graph=None,create_graph=False,only_inputs=True,allow_unused=False)\nParameter description: outputs(sequence of Tensor) - outputs of the differentiated function.\nConstraints:\ndtype:Null\nstructure:sequence;tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.repeat_interleave(input,repeats,dim=None)\nParameter description: dim(int, optional) - The dimension along which to repeat values.By default, use the flattened input array, and return a flat outputarray.\nConstraints:\ndtype:int\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.sparse_coo_tensor(indices,values,size=None,dtype=None,device=None,requires_grad=False)\nParameter description: values(array_like) - Initial values for the tensor. Can be a list, tuple, NumPy `ndarray`, scalar, and other types.\nConstraints:",
            "constr": {
                "dtype": [],
                "structure": [
                    "array_like",
                    "list",
                    "tuple",
                    "ndarray"
                ],
                "shape": [],
                "ndim": [
                    "0",
                    "1"
                ],
                "enum": [],
                "range": []
            },
            "signature": "torch.sparse_coo_tensor(indices,values,size=None,dtype=None,device=None,requires_grad=False)",
            "descp": "values(array_like) - Initial values for the tensor. Can be a list, tuple, NumPy `ndarray`, scalar, and other types."
        },
        {
            "fname": "torch.rfft.yaml",
            "arg": "input",
            "prompt": "Signature: torch.distributed.recv(tensor,src=None,group=<objectobject>,tag=0)\nParameter description: tensor(Tensor) - Tensor to fill with received data.\nConstraints:\ndtype:Null\nstructure:tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.div(input,other,out=None)\nParameter description: out(Tensor, optional) - the output tensor.\nConstraints:\ndtype:Null\nstructure:tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.nn.TransformerDecoder(decoder_layer,num_layers,norm=None)\nParameter description: num_layers - the number of sub-decoder-layers in the decoder (required).\nConstraints:\ndtype:int\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:[0,inf)\n\n###\n\nSignature: torch.nn.quantized.functional.interpolate(input,size=None,scale_factor=None,mode=nearest,align_corners=None)\nParameter description: align_corners(bool, optional) - Geometrically, we consider the pixels of the input and output as squares rather than points. If set to `True`, the input and output tensors are aligned by the center points of their corner pixels, preserving the values at the corner pixels. If set to `False`, the input and output tensors are aligned by the corner points of their corner pixels, and the interpolation uses edge value padding for out-of-boundary values, making this operation independent of input size when `scale_factor` is kept the same. This only has an effect when `mode` is `'bilinear'`. Default: `False`\nConstraints:\ndtype:bool\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.gt(input,other,out=None)\nParameter description: out(Tensor, optional) - the output tensor that must be a BoolTensor\nConstraints:\ndtype:booltensor\nstructure:tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.empty_like(input,dtype=None,layout=None,device=None,requires_grad=False,memory_format=torch.preserve_format)\nParameter description: dtype(`torch.dtype`, optional) - the desired data type of returned Tensor. Default: if `None`, defaults to the dtype of `input`.\nConstraints:\ndtype:torch.dtype\nstructure:Null\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.nn.RNN(*args,**kwargs,input_size=None,hidden_size=None,num_layers=None,nonlinearity=None,bias=None,batch_first=None,dropout=None,bidirectional=None)\nParameter description: hidden_size - The number of features in the hidden state h\nConstraints:\ndtype:int\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:[0,inf)\n\n###\n\nSignature: torch.nn.functional.avg_pool1d(input,kernel_size,stride=None,padding=0,ceil_mode=False,count_include_pad=True)\nParameter description: padding - implicit zero paddings on both sides of the input. Can be a single number or a tuple (padW,). Default: 0\nConstraints:\ndtype:bool;int\nstructure:tuple\nshape:Null\nndim:0;1\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.autograd.grad(outputs,inputs,grad_outputs=None,retain_graph=None,create_graph=False,only_inputs=True,allow_unused=False)\nParameter description: outputs(sequence of Tensor) - outputs of the differentiated function.\nConstraints:\ndtype:Null\nstructure:sequence;tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.repeat_interleave(input,repeats,dim=None)\nParameter description: dim(int, optional) - The dimension along which to repeat values.By default, use the flattened input array, and return a flat outputarray.\nConstraints:\ndtype:int\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.rfft(input,signal_ndim,normalized=False,onesided=True)\nParameter description: input(Tensor) - the input tensor of at least `signal_ndim` dimensions\nConstraints:",
            "constr": {
                "dtype": [],
                "structure": [
                    "tensor"
                ],
                "shape": [],
                "ndim": [
                    ">=signal_ndim"
                ],
                "enum": [],
                "range": []
            },
            "signature": "torch.rfft(input,signal_ndim,normalized=False,onesided=True)",
            "descp": "input(Tensor) - the input tensor of at least `signal_ndim` dimensions"
        },
        {
            "fname": "torch.eye.yaml",
            "arg": "m",
            "prompt": "Signature: torch.distributed.recv(tensor,src=None,group=<objectobject>,tag=0)\nParameter description: tensor(Tensor) - Tensor to fill with received data.\nConstraints:\ndtype:Null\nstructure:tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.div(input,other,out=None)\nParameter description: out(Tensor, optional) - the output tensor.\nConstraints:\ndtype:Null\nstructure:tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.nn.TransformerDecoder(decoder_layer,num_layers,norm=None)\nParameter description: num_layers - the number of sub-decoder-layers in the decoder (required).\nConstraints:\ndtype:int\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:[0,inf)\n\n###\n\nSignature: torch.nn.quantized.functional.interpolate(input,size=None,scale_factor=None,mode=nearest,align_corners=None)\nParameter description: align_corners(bool, optional) - Geometrically, we consider the pixels of the input and output as squares rather than points. If set to `True`, the input and output tensors are aligned by the center points of their corner pixels, preserving the values at the corner pixels. If set to `False`, the input and output tensors are aligned by the corner points of their corner pixels, and the interpolation uses edge value padding for out-of-boundary values, making this operation independent of input size when `scale_factor` is kept the same. This only has an effect when `mode` is `'bilinear'`. Default: `False`\nConstraints:\ndtype:bool\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.gt(input,other,out=None)\nParameter description: out(Tensor, optional) - the output tensor that must be a BoolTensor\nConstraints:\ndtype:booltensor\nstructure:tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.empty_like(input,dtype=None,layout=None,device=None,requires_grad=False,memory_format=torch.preserve_format)\nParameter description: dtype(`torch.dtype`, optional) - the desired data type of returned Tensor. Default: if `None`, defaults to the dtype of `input`.\nConstraints:\ndtype:torch.dtype\nstructure:Null\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.nn.RNN(*args,**kwargs,input_size=None,hidden_size=None,num_layers=None,nonlinearity=None,bias=None,batch_first=None,dropout=None,bidirectional=None)\nParameter description: hidden_size - The number of features in the hidden state h\nConstraints:\ndtype:int\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:[0,inf)\n\n###\n\nSignature: torch.nn.functional.avg_pool1d(input,kernel_size,stride=None,padding=0,ceil_mode=False,count_include_pad=True)\nParameter description: padding - implicit zero paddings on both sides of the input. Can be a single number or a tuple (padW,). Default: 0\nConstraints:\ndtype:bool;int\nstructure:tuple\nshape:Null\nndim:0;1\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.autograd.grad(outputs,inputs,grad_outputs=None,retain_graph=None,create_graph=False,only_inputs=True,allow_unused=False)\nParameter description: outputs(sequence of Tensor) - outputs of the differentiated function.\nConstraints:\ndtype:Null\nstructure:sequence;tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.repeat_interleave(input,repeats,dim=None)\nParameter description: dim(int, optional) - The dimension along which to repeat values.By default, use the flattened input array, and return a flat outputarray.\nConstraints:\ndtype:int\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.eye(n,m=None,out=None,dtype=None,layout=torch.strided,device=None,requires_grad=False)\nParameter description: m(int, optional) - the number of columns with default being `n`\nConstraints:",
            "constr": {
                "dtype": [
                    "int"
                ],
                "structure": [],
                "shape": [],
                "ndim": [
                    "0"
                ],
                "enum": [],
                "range": [
                    "[0,inf)"
                ]
            },
            "signature": "torch.eye(n,m=None,out=None,dtype=None,layout=torch.strided,device=None,requires_grad=False)",
            "descp": "m(int, optional) - the number of columns with default being `n`"
        },
        {
            "fname": "torch.where.yaml",
            "arg": "y",
            "prompt": "Signature: torch.distributed.recv(tensor,src=None,group=<objectobject>,tag=0)\nParameter description: tensor(Tensor) - Tensor to fill with received data.\nConstraints:\ndtype:Null\nstructure:tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.div(input,other,out=None)\nParameter description: out(Tensor, optional) - the output tensor.\nConstraints:\ndtype:Null\nstructure:tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.nn.TransformerDecoder(decoder_layer,num_layers,norm=None)\nParameter description: num_layers - the number of sub-decoder-layers in the decoder (required).\nConstraints:\ndtype:int\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:[0,inf)\n\n###\n\nSignature: torch.nn.quantized.functional.interpolate(input,size=None,scale_factor=None,mode=nearest,align_corners=None)\nParameter description: align_corners(bool, optional) - Geometrically, we consider the pixels of the input and output as squares rather than points. If set to `True`, the input and output tensors are aligned by the center points of their corner pixels, preserving the values at the corner pixels. If set to `False`, the input and output tensors are aligned by the corner points of their corner pixels, and the interpolation uses edge value padding for out-of-boundary values, making this operation independent of input size when `scale_factor` is kept the same. This only has an effect when `mode` is `'bilinear'`. Default: `False`\nConstraints:\ndtype:bool\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.gt(input,other,out=None)\nParameter description: out(Tensor, optional) - the output tensor that must be a BoolTensor\nConstraints:\ndtype:booltensor\nstructure:tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.empty_like(input,dtype=None,layout=None,device=None,requires_grad=False,memory_format=torch.preserve_format)\nParameter description: dtype(`torch.dtype`, optional) - the desired data type of returned Tensor. Default: if `None`, defaults to the dtype of `input`.\nConstraints:\ndtype:torch.dtype\nstructure:Null\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.nn.RNN(*args,**kwargs,input_size=None,hidden_size=None,num_layers=None,nonlinearity=None,bias=None,batch_first=None,dropout=None,bidirectional=None)\nParameter description: hidden_size - The number of features in the hidden state h\nConstraints:\ndtype:int\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:[0,inf)\n\n###\n\nSignature: torch.nn.functional.avg_pool1d(input,kernel_size,stride=None,padding=0,ceil_mode=False,count_include_pad=True)\nParameter description: padding - implicit zero paddings on both sides of the input. Can be a single number or a tuple (padW,). Default: 0\nConstraints:\ndtype:bool;int\nstructure:tuple\nshape:Null\nndim:0;1\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.autograd.grad(outputs,inputs,grad_outputs=None,retain_graph=None,create_graph=False,only_inputs=True,allow_unused=False)\nParameter description: outputs(sequence of Tensor) - outputs of the differentiated function.\nConstraints:\ndtype:Null\nstructure:sequence;tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.repeat_interleave(input,repeats,dim=None)\nParameter description: dim(int, optional) - The dimension along which to repeat values.By default, use the flattened input array, and return a flat outputarray.\nConstraints:\ndtype:int\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.where(condition,x,y)\nParameter description: y(Tensor) - values selected at indices where `condition` is `False`\nConstraints:",
            "constr": {
                "dtype": [],
                "structure": [
                    "tensor"
                ],
                "shape": [],
                "ndim": [],
                "enum": [],
                "range": []
            },
            "signature": "torch.where(condition,x,y)",
            "descp": "y(Tensor) - values selected at indices where `condition` is `False`"
        },
        {
            "fname": "torch.nn.functional.ctc_loss.yaml",
            "arg": "target_lengths",
            "prompt": "Signature: torch.distributed.recv(tensor,src=None,group=<objectobject>,tag=0)\nParameter description: tensor(Tensor) - Tensor to fill with received data.\nConstraints:\ndtype:Null\nstructure:tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.div(input,other,out=None)\nParameter description: out(Tensor, optional) - the output tensor.\nConstraints:\ndtype:Null\nstructure:tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.nn.TransformerDecoder(decoder_layer,num_layers,norm=None)\nParameter description: num_layers - the number of sub-decoder-layers in the decoder (required).\nConstraints:\ndtype:int\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:[0,inf)\n\n###\n\nSignature: torch.nn.quantized.functional.interpolate(input,size=None,scale_factor=None,mode=nearest,align_corners=None)\nParameter description: align_corners(bool, optional) - Geometrically, we consider the pixels of the input and output as squares rather than points. If set to `True`, the input and output tensors are aligned by the center points of their corner pixels, preserving the values at the corner pixels. If set to `False`, the input and output tensors are aligned by the corner points of their corner pixels, and the interpolation uses edge value padding for out-of-boundary values, making this operation independent of input size when `scale_factor` is kept the same. This only has an effect when `mode` is `'bilinear'`. Default: `False`\nConstraints:\ndtype:bool\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.gt(input,other,out=None)\nParameter description: out(Tensor, optional) - the output tensor that must be a BoolTensor\nConstraints:\ndtype:booltensor\nstructure:tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.empty_like(input,dtype=None,layout=None,device=None,requires_grad=False,memory_format=torch.preserve_format)\nParameter description: dtype(`torch.dtype`, optional) - the desired data type of returned Tensor. Default: if `None`, defaults to the dtype of `input`.\nConstraints:\ndtype:torch.dtype\nstructure:Null\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.nn.RNN(*args,**kwargs,input_size=None,hidden_size=None,num_layers=None,nonlinearity=None,bias=None,batch_first=None,dropout=None,bidirectional=None)\nParameter description: hidden_size - The number of features in the hidden state h\nConstraints:\ndtype:int\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:[0,inf)\n\n###\n\nSignature: torch.nn.functional.avg_pool1d(input,kernel_size,stride=None,padding=0,ceil_mode=False,count_include_pad=True)\nParameter description: padding - implicit zero paddings on both sides of the input. Can be a single number or a tuple (padW,). Default: 0\nConstraints:\ndtype:bool;int\nstructure:tuple\nshape:Null\nndim:0;1\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.autograd.grad(outputs,inputs,grad_outputs=None,retain_graph=None,create_graph=False,only_inputs=True,allow_unused=False)\nParameter description: outputs(sequence of Tensor) - outputs of the differentiated function.\nConstraints:\ndtype:Null\nstructure:sequence;tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.repeat_interleave(input,repeats,dim=None)\nParameter description: dim(int, optional) - The dimension along which to repeat values.By default, use the flattened input array, and return a flat outputarray.\nConstraints:\ndtype:int\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.nn.functional.ctc_loss(log_probs,targets,input_lengths,target_lengths,blank=0,reduction=mean,zero_infinity=False)\nParameter description: target_lengths - (N) . Lengths of the targets\nConstraints:",
            "constr": {
                "dtype": [
                    "int"
                ],
                "structure": [],
                "shape": [],
                "ndim": [
                    "1"
                ],
                "enum": [],
                "range": [
                    "[0,inf)"
                ]
            },
            "signature": "torch.nn.functional.ctc_loss(log_probs,targets,input_lengths,target_lengths,blank=0,reduction=mean,zero_infinity=False)",
            "descp": "target_lengths - (N) . Lengths of the targets"
        },
        {
            "fname": "torch.nn.constantpad3d.yaml",
            "arg": "padding",
            "prompt": "Signature: torch.distributed.recv(tensor,src=None,group=<objectobject>,tag=0)\nParameter description: tensor(Tensor) - Tensor to fill with received data.\nConstraints:\ndtype:Null\nstructure:tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.div(input,other,out=None)\nParameter description: out(Tensor, optional) - the output tensor.\nConstraints:\ndtype:Null\nstructure:tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.nn.TransformerDecoder(decoder_layer,num_layers,norm=None)\nParameter description: num_layers - the number of sub-decoder-layers in the decoder (required).\nConstraints:\ndtype:int\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:[0,inf)\n\n###\n\nSignature: torch.nn.quantized.functional.interpolate(input,size=None,scale_factor=None,mode=nearest,align_corners=None)\nParameter description: align_corners(bool, optional) - Geometrically, we consider the pixels of the input and output as squares rather than points. If set to `True`, the input and output tensors are aligned by the center points of their corner pixels, preserving the values at the corner pixels. If set to `False`, the input and output tensors are aligned by the corner points of their corner pixels, and the interpolation uses edge value padding for out-of-boundary values, making this operation independent of input size when `scale_factor` is kept the same. This only has an effect when `mode` is `'bilinear'`. Default: `False`\nConstraints:\ndtype:bool\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.gt(input,other,out=None)\nParameter description: out(Tensor, optional) - the output tensor that must be a BoolTensor\nConstraints:\ndtype:booltensor\nstructure:tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.empty_like(input,dtype=None,layout=None,device=None,requires_grad=False,memory_format=torch.preserve_format)\nParameter description: dtype(`torch.dtype`, optional) - the desired data type of returned Tensor. Default: if `None`, defaults to the dtype of `input`.\nConstraints:\ndtype:torch.dtype\nstructure:Null\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.nn.RNN(*args,**kwargs,input_size=None,hidden_size=None,num_layers=None,nonlinearity=None,bias=None,batch_first=None,dropout=None,bidirectional=None)\nParameter description: hidden_size - The number of features in the hidden state h\nConstraints:\ndtype:int\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:[0,inf)\n\n###\n\nSignature: torch.nn.functional.avg_pool1d(input,kernel_size,stride=None,padding=0,ceil_mode=False,count_include_pad=True)\nParameter description: padding - implicit zero paddings on both sides of the input. Can be a single number or a tuple (padW,). Default: 0\nConstraints:\ndtype:bool;int\nstructure:tuple\nshape:Null\nndim:0;1\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.autograd.grad(outputs,inputs,grad_outputs=None,retain_graph=None,create_graph=False,only_inputs=True,allow_unused=False)\nParameter description: outputs(sequence of Tensor) - outputs of the differentiated function.\nConstraints:\ndtype:Null\nstructure:sequence;tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.repeat_interleave(input,repeats,dim=None)\nParameter description: dim(int, optional) - The dimension along which to repeat values.By default, use the flattened input array, and return a flat outputarray.\nConstraints:\ndtype:int\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.nn.ConstantPad3d(padding,value)\nParameter description: padding(int, tuple) - the size of the padding. If is int, uses the same padding in all boundaries. If a 6-tuple, uses (padding _left , padding _right , padding _top , padding _bottom , padding _front , padding _back )\nConstraints:",
            "constr": {
                "dtype": [
                    "int"
                ],
                "structure": [
                    "tuple"
                ],
                "shape": [],
                "ndim": [
                    "1"
                ],
                "enum": [],
                "range": [
                    "[0,inf)"
                ]
            },
            "signature": "torch.nn.ConstantPad3d(padding,value)",
            "descp": "padding(int, tuple) - the size of the padding. If is int, uses the same padding in all boundaries. If a 6-tuple, uses (padding _left , padding _right , padding _top , padding _bottom , padding _front , padding _back )"
        },
        {
            "fname": "torch.nn.utils.prune.custom_from_mask.yaml",
            "arg": "mask",
            "prompt": "Signature: torch.distributed.recv(tensor,src=None,group=<objectobject>,tag=0)\nParameter description: tensor(Tensor) - Tensor to fill with received data.\nConstraints:\ndtype:Null\nstructure:tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.div(input,other,out=None)\nParameter description: out(Tensor, optional) - the output tensor.\nConstraints:\ndtype:Null\nstructure:tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.nn.TransformerDecoder(decoder_layer,num_layers,norm=None)\nParameter description: num_layers - the number of sub-decoder-layers in the decoder (required).\nConstraints:\ndtype:int\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:[0,inf)\n\n###\n\nSignature: torch.nn.quantized.functional.interpolate(input,size=None,scale_factor=None,mode=nearest,align_corners=None)\nParameter description: align_corners(bool, optional) - Geometrically, we consider the pixels of the input and output as squares rather than points. If set to `True`, the input and output tensors are aligned by the center points of their corner pixels, preserving the values at the corner pixels. If set to `False`, the input and output tensors are aligned by the corner points of their corner pixels, and the interpolation uses edge value padding for out-of-boundary values, making this operation independent of input size when `scale_factor` is kept the same. This only has an effect when `mode` is `'bilinear'`. Default: `False`\nConstraints:\ndtype:bool\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.gt(input,other,out=None)\nParameter description: out(Tensor, optional) - the output tensor that must be a BoolTensor\nConstraints:\ndtype:booltensor\nstructure:tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.empty_like(input,dtype=None,layout=None,device=None,requires_grad=False,memory_format=torch.preserve_format)\nParameter description: dtype(`torch.dtype`, optional) - the desired data type of returned Tensor. Default: if `None`, defaults to the dtype of `input`.\nConstraints:\ndtype:torch.dtype\nstructure:Null\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.nn.RNN(*args,**kwargs,input_size=None,hidden_size=None,num_layers=None,nonlinearity=None,bias=None,batch_first=None,dropout=None,bidirectional=None)\nParameter description: hidden_size - The number of features in the hidden state h\nConstraints:\ndtype:int\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:[0,inf)\n\n###\n\nSignature: torch.nn.functional.avg_pool1d(input,kernel_size,stride=None,padding=0,ceil_mode=False,count_include_pad=True)\nParameter description: padding - implicit zero paddings on both sides of the input. Can be a single number or a tuple (padW,). Default: 0\nConstraints:\ndtype:bool;int\nstructure:tuple\nshape:Null\nndim:0;1\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.autograd.grad(outputs,inputs,grad_outputs=None,retain_graph=None,create_graph=False,only_inputs=True,allow_unused=False)\nParameter description: outputs(sequence of Tensor) - outputs of the differentiated function.\nConstraints:\ndtype:Null\nstructure:sequence;tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.repeat_interleave(input,repeats,dim=None)\nParameter description: dim(int, optional) - The dimension along which to repeat values.By default, use the flattened input array, and return a flat outputarray.\nConstraints:\ndtype:int\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.nn.utils.prune.custom_from_mask(module,name,mask)\nParameter description: mask(Tensor) - binary mask to be applied to the parameter.\nConstraints:",
            "constr": {
                "dtype": [],
                "structure": [
                    "tensor"
                ],
                "shape": [],
                "ndim": [],
                "enum": [],
                "range": []
            },
            "signature": "torch.nn.utils.prune.custom_from_mask(module,name,mask)",
            "descp": "mask(Tensor) - binary mask to be applied to the parameter."
        },
        {
            "fname": "torch.le.yaml",
            "arg": "input",
            "prompt": "Signature: torch.distributed.recv(tensor,src=None,group=<objectobject>,tag=0)\nParameter description: tensor(Tensor) - Tensor to fill with received data.\nConstraints:\ndtype:Null\nstructure:tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.div(input,other,out=None)\nParameter description: out(Tensor, optional) - the output tensor.\nConstraints:\ndtype:Null\nstructure:tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.nn.TransformerDecoder(decoder_layer,num_layers,norm=None)\nParameter description: num_layers - the number of sub-decoder-layers in the decoder (required).\nConstraints:\ndtype:int\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:[0,inf)\n\n###\n\nSignature: torch.nn.quantized.functional.interpolate(input,size=None,scale_factor=None,mode=nearest,align_corners=None)\nParameter description: align_corners(bool, optional) - Geometrically, we consider the pixels of the input and output as squares rather than points. If set to `True`, the input and output tensors are aligned by the center points of their corner pixels, preserving the values at the corner pixels. If set to `False`, the input and output tensors are aligned by the corner points of their corner pixels, and the interpolation uses edge value padding for out-of-boundary values, making this operation independent of input size when `scale_factor` is kept the same. This only has an effect when `mode` is `'bilinear'`. Default: `False`\nConstraints:\ndtype:bool\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.gt(input,other,out=None)\nParameter description: out(Tensor, optional) - the output tensor that must be a BoolTensor\nConstraints:\ndtype:booltensor\nstructure:tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.empty_like(input,dtype=None,layout=None,device=None,requires_grad=False,memory_format=torch.preserve_format)\nParameter description: dtype(`torch.dtype`, optional) - the desired data type of returned Tensor. Default: if `None`, defaults to the dtype of `input`.\nConstraints:\ndtype:torch.dtype\nstructure:Null\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.nn.RNN(*args,**kwargs,input_size=None,hidden_size=None,num_layers=None,nonlinearity=None,bias=None,batch_first=None,dropout=None,bidirectional=None)\nParameter description: hidden_size - The number of features in the hidden state h\nConstraints:\ndtype:int\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:[0,inf)\n\n###\n\nSignature: torch.nn.functional.avg_pool1d(input,kernel_size,stride=None,padding=0,ceil_mode=False,count_include_pad=True)\nParameter description: padding - implicit zero paddings on both sides of the input. Can be a single number or a tuple (padW,). Default: 0\nConstraints:\ndtype:bool;int\nstructure:tuple\nshape:Null\nndim:0;1\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.autograd.grad(outputs,inputs,grad_outputs=None,retain_graph=None,create_graph=False,only_inputs=True,allow_unused=False)\nParameter description: outputs(sequence of Tensor) - outputs of the differentiated function.\nConstraints:\ndtype:Null\nstructure:sequence;tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.repeat_interleave(input,repeats,dim=None)\nParameter description: dim(int, optional) - The dimension along which to repeat values.By default, use the flattened input array, and return a flat outputarray.\nConstraints:\ndtype:int\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.le(input,other,out=None)\nParameter description: input(Tensor) - the tensor to compare\nConstraints:",
            "constr": {
                "dtype": [],
                "structure": [
                    "tensor"
                ],
                "shape": [],
                "ndim": [],
                "enum": [],
                "range": []
            },
            "signature": "torch.le(input,other,out=None)",
            "descp": "input(Tensor) - the tensor to compare"
        },
        {
            "fname": "torch.numel.yaml",
            "arg": "input",
            "prompt": "Signature: torch.distributed.recv(tensor,src=None,group=<objectobject>,tag=0)\nParameter description: tensor(Tensor) - Tensor to fill with received data.\nConstraints:\ndtype:Null\nstructure:tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.div(input,other,out=None)\nParameter description: out(Tensor, optional) - the output tensor.\nConstraints:\ndtype:Null\nstructure:tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.nn.TransformerDecoder(decoder_layer,num_layers,norm=None)\nParameter description: num_layers - the number of sub-decoder-layers in the decoder (required).\nConstraints:\ndtype:int\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:[0,inf)\n\n###\n\nSignature: torch.nn.quantized.functional.interpolate(input,size=None,scale_factor=None,mode=nearest,align_corners=None)\nParameter description: align_corners(bool, optional) - Geometrically, we consider the pixels of the input and output as squares rather than points. If set to `True`, the input and output tensors are aligned by the center points of their corner pixels, preserving the values at the corner pixels. If set to `False`, the input and output tensors are aligned by the corner points of their corner pixels, and the interpolation uses edge value padding for out-of-boundary values, making this operation independent of input size when `scale_factor` is kept the same. This only has an effect when `mode` is `'bilinear'`. Default: `False`\nConstraints:\ndtype:bool\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.gt(input,other,out=None)\nParameter description: out(Tensor, optional) - the output tensor that must be a BoolTensor\nConstraints:\ndtype:booltensor\nstructure:tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.empty_like(input,dtype=None,layout=None,device=None,requires_grad=False,memory_format=torch.preserve_format)\nParameter description: dtype(`torch.dtype`, optional) - the desired data type of returned Tensor. Default: if `None`, defaults to the dtype of `input`.\nConstraints:\ndtype:torch.dtype\nstructure:Null\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.nn.RNN(*args,**kwargs,input_size=None,hidden_size=None,num_layers=None,nonlinearity=None,bias=None,batch_first=None,dropout=None,bidirectional=None)\nParameter description: hidden_size - The number of features in the hidden state h\nConstraints:\ndtype:int\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:[0,inf)\n\n###\n\nSignature: torch.nn.functional.avg_pool1d(input,kernel_size,stride=None,padding=0,ceil_mode=False,count_include_pad=True)\nParameter description: padding - implicit zero paddings on both sides of the input. Can be a single number or a tuple (padW,). Default: 0\nConstraints:\ndtype:bool;int\nstructure:tuple\nshape:Null\nndim:0;1\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.autograd.grad(outputs,inputs,grad_outputs=None,retain_graph=None,create_graph=False,only_inputs=True,allow_unused=False)\nParameter description: outputs(sequence of Tensor) - outputs of the differentiated function.\nConstraints:\ndtype:Null\nstructure:sequence;tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.repeat_interleave(input,repeats,dim=None)\nParameter description: dim(int, optional) - The dimension along which to repeat values.By default, use the flattened input array, and return a flat outputarray.\nConstraints:\ndtype:int\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.numel(input)\nParameter description: input(Tensor) - the input tensor.\nConstraints:",
            "constr": {
                "dtype": [],
                "structure": [
                    "tensor"
                ],
                "shape": [],
                "ndim": [],
                "enum": [],
                "range": []
            },
            "signature": "torch.numel(input)",
            "descp": "input(Tensor) - the input tensor."
        },
        {
            "fname": "torch.nn.unfold.yaml",
            "arg": "stride",
            "prompt": "Signature: torch.distributed.recv(tensor,src=None,group=<objectobject>,tag=0)\nParameter description: tensor(Tensor) - Tensor to fill with received data.\nConstraints:\ndtype:Null\nstructure:tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.div(input,other,out=None)\nParameter description: out(Tensor, optional) - the output tensor.\nConstraints:\ndtype:Null\nstructure:tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.nn.TransformerDecoder(decoder_layer,num_layers,norm=None)\nParameter description: num_layers - the number of sub-decoder-layers in the decoder (required).\nConstraints:\ndtype:int\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:[0,inf)\n\n###\n\nSignature: torch.nn.quantized.functional.interpolate(input,size=None,scale_factor=None,mode=nearest,align_corners=None)\nParameter description: align_corners(bool, optional) - Geometrically, we consider the pixels of the input and output as squares rather than points. If set to `True`, the input and output tensors are aligned by the center points of their corner pixels, preserving the values at the corner pixels. If set to `False`, the input and output tensors are aligned by the corner points of their corner pixels, and the interpolation uses edge value padding for out-of-boundary values, making this operation independent of input size when `scale_factor` is kept the same. This only has an effect when `mode` is `'bilinear'`. Default: `False`\nConstraints:\ndtype:bool\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.gt(input,other,out=None)\nParameter description: out(Tensor, optional) - the output tensor that must be a BoolTensor\nConstraints:\ndtype:booltensor\nstructure:tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.empty_like(input,dtype=None,layout=None,device=None,requires_grad=False,memory_format=torch.preserve_format)\nParameter description: dtype(`torch.dtype`, optional) - the desired data type of returned Tensor. Default: if `None`, defaults to the dtype of `input`.\nConstraints:\ndtype:torch.dtype\nstructure:Null\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.nn.RNN(*args,**kwargs,input_size=None,hidden_size=None,num_layers=None,nonlinearity=None,bias=None,batch_first=None,dropout=None,bidirectional=None)\nParameter description: hidden_size - The number of features in the hidden state h\nConstraints:\ndtype:int\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:[0,inf)\n\n###\n\nSignature: torch.nn.functional.avg_pool1d(input,kernel_size,stride=None,padding=0,ceil_mode=False,count_include_pad=True)\nParameter description: padding - implicit zero paddings on both sides of the input. Can be a single number or a tuple (padW,). Default: 0\nConstraints:\ndtype:bool;int\nstructure:tuple\nshape:Null\nndim:0;1\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.autograd.grad(outputs,inputs,grad_outputs=None,retain_graph=None,create_graph=False,only_inputs=True,allow_unused=False)\nParameter description: outputs(sequence of Tensor) - outputs of the differentiated function.\nConstraints:\ndtype:Null\nstructure:sequence;tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.repeat_interleave(input,repeats,dim=None)\nParameter description: dim(int, optional) - The dimension along which to repeat values.By default, use the flattened input array, and return a flat outputarray.\nConstraints:\ndtype:int\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.nn.Unfold(kernel_size,dilation=1,padding=0,stride=1)\nParameter description: stride(int or tuple, optional) - the stride of the sliding blocks in the input spatial dimensions. Default: 1\nConstraints:",
            "constr": {
                "dtype": [
                    "bool",
                    "int"
                ],
                "structure": [
                    "tuple"
                ],
                "shape": [],
                "ndim": [
                    "0"
                ],
                "enum": [],
                "range": [
                    "[0,inf)"
                ]
            },
            "signature": "torch.nn.Unfold(kernel_size,dilation=1,padding=0,stride=1)",
            "descp": "stride(int or tuple, optional) - the stride of the sliding blocks in the input spatial dimensions. Default: 1"
        },
        {
            "fname": "torch.cummax.yaml",
            "arg": "out",
            "prompt": "Signature: torch.distributed.recv(tensor,src=None,group=<objectobject>,tag=0)\nParameter description: tensor(Tensor) - Tensor to fill with received data.\nConstraints:\ndtype:Null\nstructure:tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.div(input,other,out=None)\nParameter description: out(Tensor, optional) - the output tensor.\nConstraints:\ndtype:Null\nstructure:tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.nn.TransformerDecoder(decoder_layer,num_layers,norm=None)\nParameter description: num_layers - the number of sub-decoder-layers in the decoder (required).\nConstraints:\ndtype:int\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:[0,inf)\n\n###\n\nSignature: torch.nn.quantized.functional.interpolate(input,size=None,scale_factor=None,mode=nearest,align_corners=None)\nParameter description: align_corners(bool, optional) - Geometrically, we consider the pixels of the input and output as squares rather than points. If set to `True`, the input and output tensors are aligned by the center points of their corner pixels, preserving the values at the corner pixels. If set to `False`, the input and output tensors are aligned by the corner points of their corner pixels, and the interpolation uses edge value padding for out-of-boundary values, making this operation independent of input size when `scale_factor` is kept the same. This only has an effect when `mode` is `'bilinear'`. Default: `False`\nConstraints:\ndtype:bool\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.gt(input,other,out=None)\nParameter description: out(Tensor, optional) - the output tensor that must be a BoolTensor\nConstraints:\ndtype:booltensor\nstructure:tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.empty_like(input,dtype=None,layout=None,device=None,requires_grad=False,memory_format=torch.preserve_format)\nParameter description: dtype(`torch.dtype`, optional) - the desired data type of returned Tensor. Default: if `None`, defaults to the dtype of `input`.\nConstraints:\ndtype:torch.dtype\nstructure:Null\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.nn.RNN(*args,**kwargs,input_size=None,hidden_size=None,num_layers=None,nonlinearity=None,bias=None,batch_first=None,dropout=None,bidirectional=None)\nParameter description: hidden_size - The number of features in the hidden state h\nConstraints:\ndtype:int\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:[0,inf)\n\n###\n\nSignature: torch.nn.functional.avg_pool1d(input,kernel_size,stride=None,padding=0,ceil_mode=False,count_include_pad=True)\nParameter description: padding - implicit zero paddings on both sides of the input. Can be a single number or a tuple (padW,). Default: 0\nConstraints:\ndtype:bool;int\nstructure:tuple\nshape:Null\nndim:0;1\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.autograd.grad(outputs,inputs,grad_outputs=None,retain_graph=None,create_graph=False,only_inputs=True,allow_unused=False)\nParameter description: outputs(sequence of Tensor) - outputs of the differentiated function.\nConstraints:\ndtype:Null\nstructure:sequence;tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.repeat_interleave(input,repeats,dim=None)\nParameter description: dim(int, optional) - The dimension along which to repeat values.By default, use the flattened input array, and return a flat outputarray.\nConstraints:\ndtype:int\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.cummax(input,dim,out=None)\nParameter description: out(tuple, optional) - the result tuple of two output tensors (values, indices)\nConstraints:",
            "constr": {
                "dtype": [],
                "structure": [
                    "tuple",
                    "tensor"
                ],
                "shape": [],
                "ndim": [],
                "enum": [],
                "range": []
            },
            "signature": "torch.cummax(input,dim,out=None)",
            "descp": "out(tuple, optional) - the result tuple of two output tensors (values, indices)"
        },
        {
            "fname": "torch.cummax.yaml",
            "arg": "input",
            "prompt": "Signature: torch.distributed.recv(tensor,src=None,group=<objectobject>,tag=0)\nParameter description: tensor(Tensor) - Tensor to fill with received data.\nConstraints:\ndtype:Null\nstructure:tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.div(input,other,out=None)\nParameter description: out(Tensor, optional) - the output tensor.\nConstraints:\ndtype:Null\nstructure:tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.nn.TransformerDecoder(decoder_layer,num_layers,norm=None)\nParameter description: num_layers - the number of sub-decoder-layers in the decoder (required).\nConstraints:\ndtype:int\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:[0,inf)\n\n###\n\nSignature: torch.nn.quantized.functional.interpolate(input,size=None,scale_factor=None,mode=nearest,align_corners=None)\nParameter description: align_corners(bool, optional) - Geometrically, we consider the pixels of the input and output as squares rather than points. If set to `True`, the input and output tensors are aligned by the center points of their corner pixels, preserving the values at the corner pixels. If set to `False`, the input and output tensors are aligned by the corner points of their corner pixels, and the interpolation uses edge value padding for out-of-boundary values, making this operation independent of input size when `scale_factor` is kept the same. This only has an effect when `mode` is `'bilinear'`. Default: `False`\nConstraints:\ndtype:bool\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.gt(input,other,out=None)\nParameter description: out(Tensor, optional) - the output tensor that must be a BoolTensor\nConstraints:\ndtype:booltensor\nstructure:tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.empty_like(input,dtype=None,layout=None,device=None,requires_grad=False,memory_format=torch.preserve_format)\nParameter description: dtype(`torch.dtype`, optional) - the desired data type of returned Tensor. Default: if `None`, defaults to the dtype of `input`.\nConstraints:\ndtype:torch.dtype\nstructure:Null\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.nn.RNN(*args,**kwargs,input_size=None,hidden_size=None,num_layers=None,nonlinearity=None,bias=None,batch_first=None,dropout=None,bidirectional=None)\nParameter description: hidden_size - The number of features in the hidden state h\nConstraints:\ndtype:int\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:[0,inf)\n\n###\n\nSignature: torch.nn.functional.avg_pool1d(input,kernel_size,stride=None,padding=0,ceil_mode=False,count_include_pad=True)\nParameter description: padding - implicit zero paddings on both sides of the input. Can be a single number or a tuple (padW,). Default: 0\nConstraints:\ndtype:bool;int\nstructure:tuple\nshape:Null\nndim:0;1\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.autograd.grad(outputs,inputs,grad_outputs=None,retain_graph=None,create_graph=False,only_inputs=True,allow_unused=False)\nParameter description: outputs(sequence of Tensor) - outputs of the differentiated function.\nConstraints:\ndtype:Null\nstructure:sequence;tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.repeat_interleave(input,repeats,dim=None)\nParameter description: dim(int, optional) - The dimension along which to repeat values.By default, use the flattened input array, and return a flat outputarray.\nConstraints:\ndtype:int\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.cummax(input,dim,out=None)\nParameter description: input(Tensor) - the input tensor.\nConstraints:",
            "constr": {
                "dtype": [],
                "structure": [
                    "tensor"
                ],
                "shape": [],
                "ndim": [],
                "enum": [],
                "range": []
            },
            "signature": "torch.cummax(input,dim,out=None)",
            "descp": "input(Tensor) - the input tensor."
        },
        {
            "fname": "torch.autograd.functional.vjp.yaml",
            "arg": "create_graph",
            "prompt": "Signature: torch.distributed.recv(tensor,src=None,group=<objectobject>,tag=0)\nParameter description: tensor(Tensor) - Tensor to fill with received data.\nConstraints:\ndtype:Null\nstructure:tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.div(input,other,out=None)\nParameter description: out(Tensor, optional) - the output tensor.\nConstraints:\ndtype:Null\nstructure:tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.nn.TransformerDecoder(decoder_layer,num_layers,norm=None)\nParameter description: num_layers - the number of sub-decoder-layers in the decoder (required).\nConstraints:\ndtype:int\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:[0,inf)\n\n###\n\nSignature: torch.nn.quantized.functional.interpolate(input,size=None,scale_factor=None,mode=nearest,align_corners=None)\nParameter description: align_corners(bool, optional) - Geometrically, we consider the pixels of the input and output as squares rather than points. If set to `True`, the input and output tensors are aligned by the center points of their corner pixels, preserving the values at the corner pixels. If set to `False`, the input and output tensors are aligned by the corner points of their corner pixels, and the interpolation uses edge value padding for out-of-boundary values, making this operation independent of input size when `scale_factor` is kept the same. This only has an effect when `mode` is `'bilinear'`. Default: `False`\nConstraints:\ndtype:bool\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.gt(input,other,out=None)\nParameter description: out(Tensor, optional) - the output tensor that must be a BoolTensor\nConstraints:\ndtype:booltensor\nstructure:tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.empty_like(input,dtype=None,layout=None,device=None,requires_grad=False,memory_format=torch.preserve_format)\nParameter description: dtype(`torch.dtype`, optional) - the desired data type of returned Tensor. Default: if `None`, defaults to the dtype of `input`.\nConstraints:\ndtype:torch.dtype\nstructure:Null\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.nn.RNN(*args,**kwargs,input_size=None,hidden_size=None,num_layers=None,nonlinearity=None,bias=None,batch_first=None,dropout=None,bidirectional=None)\nParameter description: hidden_size - The number of features in the hidden state h\nConstraints:\ndtype:int\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:[0,inf)\n\n###\n\nSignature: torch.nn.functional.avg_pool1d(input,kernel_size,stride=None,padding=0,ceil_mode=False,count_include_pad=True)\nParameter description: padding - implicit zero paddings on both sides of the input. Can be a single number or a tuple (padW,). Default: 0\nConstraints:\ndtype:bool;int\nstructure:tuple\nshape:Null\nndim:0;1\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.autograd.grad(outputs,inputs,grad_outputs=None,retain_graph=None,create_graph=False,only_inputs=True,allow_unused=False)\nParameter description: outputs(sequence of Tensor) - outputs of the differentiated function.\nConstraints:\ndtype:Null\nstructure:sequence;tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.repeat_interleave(input,repeats,dim=None)\nParameter description: dim(int, optional) - The dimension along which to repeat values.By default, use the flattened input array, and return a flat outputarray.\nConstraints:\ndtype:int\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.autograd.functional.vjp(func,inputs,v=None,create_graph=False,strict=False)\nParameter description: create_graph(bool, optional) - If `True`, both the output and result will be computed in a differentiable way. Note that when `strict` is `False`, the result can not require gradients or be disconnected from the inputs. Defaults to `False`.\nConstraints:",
            "constr": {
                "dtype": [
                    "bool"
                ],
                "structure": [],
                "shape": [],
                "ndim": [
                    "0"
                ],
                "enum": [],
                "range": []
            },
            "signature": "torch.autograd.functional.vjp(func,inputs,v=None,create_graph=False,strict=False)",
            "descp": "create_graph(bool, optional) - If `True`, both the output and result will be computed in a differentiable way. Note that when `strict` is `False`, the result can not require gradients or be disconnected from the inputs. Defaults to `False`."
        },
        {
            "fname": "torch.nn.transformerencoderlayer.yaml",
            "arg": "dropout",
            "prompt": "Signature: torch.distributed.recv(tensor,src=None,group=<objectobject>,tag=0)\nParameter description: tensor(Tensor) - Tensor to fill with received data.\nConstraints:\ndtype:Null\nstructure:tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.div(input,other,out=None)\nParameter description: out(Tensor, optional) - the output tensor.\nConstraints:\ndtype:Null\nstructure:tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.nn.TransformerDecoder(decoder_layer,num_layers,norm=None)\nParameter description: num_layers - the number of sub-decoder-layers in the decoder (required).\nConstraints:\ndtype:int\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:[0,inf)\n\n###\n\nSignature: torch.nn.quantized.functional.interpolate(input,size=None,scale_factor=None,mode=nearest,align_corners=None)\nParameter description: align_corners(bool, optional) - Geometrically, we consider the pixels of the input and output as squares rather than points. If set to `True`, the input and output tensors are aligned by the center points of their corner pixels, preserving the values at the corner pixels. If set to `False`, the input and output tensors are aligned by the corner points of their corner pixels, and the interpolation uses edge value padding for out-of-boundary values, making this operation independent of input size when `scale_factor` is kept the same. This only has an effect when `mode` is `'bilinear'`. Default: `False`\nConstraints:\ndtype:bool\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.gt(input,other,out=None)\nParameter description: out(Tensor, optional) - the output tensor that must be a BoolTensor\nConstraints:\ndtype:booltensor\nstructure:tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.empty_like(input,dtype=None,layout=None,device=None,requires_grad=False,memory_format=torch.preserve_format)\nParameter description: dtype(`torch.dtype`, optional) - the desired data type of returned Tensor. Default: if `None`, defaults to the dtype of `input`.\nConstraints:\ndtype:torch.dtype\nstructure:Null\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.nn.RNN(*args,**kwargs,input_size=None,hidden_size=None,num_layers=None,nonlinearity=None,bias=None,batch_first=None,dropout=None,bidirectional=None)\nParameter description: hidden_size - The number of features in the hidden state h\nConstraints:\ndtype:int\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:[0,inf)\n\n###\n\nSignature: torch.nn.functional.avg_pool1d(input,kernel_size,stride=None,padding=0,ceil_mode=False,count_include_pad=True)\nParameter description: padding - implicit zero paddings on both sides of the input. Can be a single number or a tuple (padW,). Default: 0\nConstraints:\ndtype:bool;int\nstructure:tuple\nshape:Null\nndim:0;1\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.autograd.grad(outputs,inputs,grad_outputs=None,retain_graph=None,create_graph=False,only_inputs=True,allow_unused=False)\nParameter description: outputs(sequence of Tensor) - outputs of the differentiated function.\nConstraints:\ndtype:Null\nstructure:sequence;tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.repeat_interleave(input,repeats,dim=None)\nParameter description: dim(int, optional) - The dimension along which to repeat values.By default, use the flattened input array, and return a flat outputarray.\nConstraints:\ndtype:int\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.nn.TransformerEncoderLayer(d_model,nhead,dim_feedforward=2048,dropout=0.1,activation=relu)\nParameter description: dropout - the dropout value (default=0.1).\nConstraints:",
            "constr": {
                "dtype": [
                    "float"
                ],
                "structure": [],
                "shape": [],
                "ndim": [
                    "0"
                ],
                "enum": [],
                "range": []
            },
            "signature": "torch.nn.TransformerEncoderLayer(d_model,nhead,dim_feedforward=2048,dropout=0.1,activation=relu)",
            "descp": "dropout - the dropout value (default=0.1)."
        },
        {
            "fname": "torch.rot90.yaml",
            "arg": "dims",
            "prompt": "Signature: torch.distributed.recv(tensor,src=None,group=<objectobject>,tag=0)\nParameter description: tensor(Tensor) - Tensor to fill with received data.\nConstraints:\ndtype:Null\nstructure:tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.div(input,other,out=None)\nParameter description: out(Tensor, optional) - the output tensor.\nConstraints:\ndtype:Null\nstructure:tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.nn.TransformerDecoder(decoder_layer,num_layers,norm=None)\nParameter description: num_layers - the number of sub-decoder-layers in the decoder (required).\nConstraints:\ndtype:int\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:[0,inf)\n\n###\n\nSignature: torch.nn.quantized.functional.interpolate(input,size=None,scale_factor=None,mode=nearest,align_corners=None)\nParameter description: align_corners(bool, optional) - Geometrically, we consider the pixels of the input and output as squares rather than points. If set to `True`, the input and output tensors are aligned by the center points of their corner pixels, preserving the values at the corner pixels. If set to `False`, the input and output tensors are aligned by the corner points of their corner pixels, and the interpolation uses edge value padding for out-of-boundary values, making this operation independent of input size when `scale_factor` is kept the same. This only has an effect when `mode` is `'bilinear'`. Default: `False`\nConstraints:\ndtype:bool\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.gt(input,other,out=None)\nParameter description: out(Tensor, optional) - the output tensor that must be a BoolTensor\nConstraints:\ndtype:booltensor\nstructure:tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.empty_like(input,dtype=None,layout=None,device=None,requires_grad=False,memory_format=torch.preserve_format)\nParameter description: dtype(`torch.dtype`, optional) - the desired data type of returned Tensor. Default: if `None`, defaults to the dtype of `input`.\nConstraints:\ndtype:torch.dtype\nstructure:Null\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.nn.RNN(*args,**kwargs,input_size=None,hidden_size=None,num_layers=None,nonlinearity=None,bias=None,batch_first=None,dropout=None,bidirectional=None)\nParameter description: hidden_size - The number of features in the hidden state h\nConstraints:\ndtype:int\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:[0,inf)\n\n###\n\nSignature: torch.nn.functional.avg_pool1d(input,kernel_size,stride=None,padding=0,ceil_mode=False,count_include_pad=True)\nParameter description: padding - implicit zero paddings on both sides of the input. Can be a single number or a tuple (padW,). Default: 0\nConstraints:\ndtype:bool;int\nstructure:tuple\nshape:Null\nndim:0;1\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.autograd.grad(outputs,inputs,grad_outputs=None,retain_graph=None,create_graph=False,only_inputs=True,allow_unused=False)\nParameter description: outputs(sequence of Tensor) - outputs of the differentiated function.\nConstraints:\ndtype:Null\nstructure:sequence;tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.repeat_interleave(input,repeats,dim=None)\nParameter description: dim(int, optional) - The dimension along which to repeat values.By default, use the flattened input array, and return a flat outputarray.\nConstraints:\ndtype:int\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.rot90(input,k,dims)\nParameter description: dims(a list or tuple) - axis to rotate\nConstraints:",
            "constr": {
                "dtype": [
                    "int"
                ],
                "structure": [
                    "list",
                    "tuple"
                ],
                "shape": [],
                "ndim": [],
                "enum": [],
                "range": []
            },
            "signature": "torch.rot90(input,k,dims)",
            "descp": "dims(a list or tuple) - axis to rotate"
        },
        {
            "fname": "torch.nn.init.dirac_.yaml",
            "arg": "groups",
            "prompt": "Signature: torch.distributed.recv(tensor,src=None,group=<objectobject>,tag=0)\nParameter description: tensor(Tensor) - Tensor to fill with received data.\nConstraints:\ndtype:Null\nstructure:tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.div(input,other,out=None)\nParameter description: out(Tensor, optional) - the output tensor.\nConstraints:\ndtype:Null\nstructure:tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.nn.TransformerDecoder(decoder_layer,num_layers,norm=None)\nParameter description: num_layers - the number of sub-decoder-layers in the decoder (required).\nConstraints:\ndtype:int\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:[0,inf)\n\n###\n\nSignature: torch.nn.quantized.functional.interpolate(input,size=None,scale_factor=None,mode=nearest,align_corners=None)\nParameter description: align_corners(bool, optional) - Geometrically, we consider the pixels of the input and output as squares rather than points. If set to `True`, the input and output tensors are aligned by the center points of their corner pixels, preserving the values at the corner pixels. If set to `False`, the input and output tensors are aligned by the corner points of their corner pixels, and the interpolation uses edge value padding for out-of-boundary values, making this operation independent of input size when `scale_factor` is kept the same. This only has an effect when `mode` is `'bilinear'`. Default: `False`\nConstraints:\ndtype:bool\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.gt(input,other,out=None)\nParameter description: out(Tensor, optional) - the output tensor that must be a BoolTensor\nConstraints:\ndtype:booltensor\nstructure:tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.empty_like(input,dtype=None,layout=None,device=None,requires_grad=False,memory_format=torch.preserve_format)\nParameter description: dtype(`torch.dtype`, optional) - the desired data type of returned Tensor. Default: if `None`, defaults to the dtype of `input`.\nConstraints:\ndtype:torch.dtype\nstructure:Null\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.nn.RNN(*args,**kwargs,input_size=None,hidden_size=None,num_layers=None,nonlinearity=None,bias=None,batch_first=None,dropout=None,bidirectional=None)\nParameter description: hidden_size - The number of features in the hidden state h\nConstraints:\ndtype:int\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:[0,inf)\n\n###\n\nSignature: torch.nn.functional.avg_pool1d(input,kernel_size,stride=None,padding=0,ceil_mode=False,count_include_pad=True)\nParameter description: padding - implicit zero paddings on both sides of the input. Can be a single number or a tuple (padW,). Default: 0\nConstraints:\ndtype:bool;int\nstructure:tuple\nshape:Null\nndim:0;1\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.autograd.grad(outputs,inputs,grad_outputs=None,retain_graph=None,create_graph=False,only_inputs=True,allow_unused=False)\nParameter description: outputs(sequence of Tensor) - outputs of the differentiated function.\nConstraints:\ndtype:Null\nstructure:sequence;tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.repeat_interleave(input,repeats,dim=None)\nParameter description: dim(int, optional) - The dimension along which to repeat values.By default, use the flattened input array, and return a flat outputarray.\nConstraints:\ndtype:int\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.nn.init.dirac_(tensor,groups=1)\nParameter description: groups(optional) - number of groups in the conv layer (default: 1)\nConstraints:",
            "constr": {
                "dtype": [
                    "int"
                ],
                "structure": [],
                "shape": [],
                "ndim": [
                    "0"
                ],
                "enum": [],
                "range": [
                    "[0,inf)"
                ]
            },
            "signature": "torch.nn.init.dirac_(tensor,groups=1)",
            "descp": "groups(optional) - number of groups in the conv layer (default: 1)"
        },
        {
            "fname": "torch.nn.rnncell.yaml",
            "arg": "bias",
            "prompt": "Signature: torch.distributed.recv(tensor,src=None,group=<objectobject>,tag=0)\nParameter description: tensor(Tensor) - Tensor to fill with received data.\nConstraints:\ndtype:Null\nstructure:tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.div(input,other,out=None)\nParameter description: out(Tensor, optional) - the output tensor.\nConstraints:\ndtype:Null\nstructure:tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.nn.TransformerDecoder(decoder_layer,num_layers,norm=None)\nParameter description: num_layers - the number of sub-decoder-layers in the decoder (required).\nConstraints:\ndtype:int\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:[0,inf)\n\n###\n\nSignature: torch.nn.quantized.functional.interpolate(input,size=None,scale_factor=None,mode=nearest,align_corners=None)\nParameter description: align_corners(bool, optional) - Geometrically, we consider the pixels of the input and output as squares rather than points. If set to `True`, the input and output tensors are aligned by the center points of their corner pixels, preserving the values at the corner pixels. If set to `False`, the input and output tensors are aligned by the corner points of their corner pixels, and the interpolation uses edge value padding for out-of-boundary values, making this operation independent of input size when `scale_factor` is kept the same. This only has an effect when `mode` is `'bilinear'`. Default: `False`\nConstraints:\ndtype:bool\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.gt(input,other,out=None)\nParameter description: out(Tensor, optional) - the output tensor that must be a BoolTensor\nConstraints:\ndtype:booltensor\nstructure:tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.empty_like(input,dtype=None,layout=None,device=None,requires_grad=False,memory_format=torch.preserve_format)\nParameter description: dtype(`torch.dtype`, optional) - the desired data type of returned Tensor. Default: if `None`, defaults to the dtype of `input`.\nConstraints:\ndtype:torch.dtype\nstructure:Null\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.nn.RNN(*args,**kwargs,input_size=None,hidden_size=None,num_layers=None,nonlinearity=None,bias=None,batch_first=None,dropout=None,bidirectional=None)\nParameter description: hidden_size - The number of features in the hidden state h\nConstraints:\ndtype:int\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:[0,inf)\n\n###\n\nSignature: torch.nn.functional.avg_pool1d(input,kernel_size,stride=None,padding=0,ceil_mode=False,count_include_pad=True)\nParameter description: padding - implicit zero paddings on both sides of the input. Can be a single number or a tuple (padW,). Default: 0\nConstraints:\ndtype:bool;int\nstructure:tuple\nshape:Null\nndim:0;1\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.autograd.grad(outputs,inputs,grad_outputs=None,retain_graph=None,create_graph=False,only_inputs=True,allow_unused=False)\nParameter description: outputs(sequence of Tensor) - outputs of the differentiated function.\nConstraints:\ndtype:Null\nstructure:sequence;tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.repeat_interleave(input,repeats,dim=None)\nParameter description: dim(int, optional) - The dimension along which to repeat values.By default, use the flattened input array, and return a flat outputarray.\nConstraints:\ndtype:int\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.nn.RNNCell(input_size,hidden_size,bias=True,nonlinearity=tanh)\nParameter description: bias - If `False`, then the layer does not use bias weights b_ih and b_hh. Default: `True`\nConstraints:",
            "constr": {
                "dtype": [
                    "bool"
                ],
                "structure": [],
                "shape": [],
                "ndim": [
                    "0"
                ],
                "enum": [],
                "range": []
            },
            "signature": "torch.nn.RNNCell(input_size,hidden_size,bias=True,nonlinearity=tanh)",
            "descp": "bias - If `False`, then the layer does not use bias weights b_ih and b_hh. Default: `True`"
        },
        {
            "fname": "torch.distributed.all_gather.yaml",
            "arg": "tensor",
            "prompt": "Signature: torch.distributed.recv(tensor,src=None,group=<objectobject>,tag=0)\nParameter description: tensor(Tensor) - Tensor to fill with received data.\nConstraints:\ndtype:Null\nstructure:tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.div(input,other,out=None)\nParameter description: out(Tensor, optional) - the output tensor.\nConstraints:\ndtype:Null\nstructure:tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.nn.TransformerDecoder(decoder_layer,num_layers,norm=None)\nParameter description: num_layers - the number of sub-decoder-layers in the decoder (required).\nConstraints:\ndtype:int\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:[0,inf)\n\n###\n\nSignature: torch.nn.quantized.functional.interpolate(input,size=None,scale_factor=None,mode=nearest,align_corners=None)\nParameter description: align_corners(bool, optional) - Geometrically, we consider the pixels of the input and output as squares rather than points. If set to `True`, the input and output tensors are aligned by the center points of their corner pixels, preserving the values at the corner pixels. If set to `False`, the input and output tensors are aligned by the corner points of their corner pixels, and the interpolation uses edge value padding for out-of-boundary values, making this operation independent of input size when `scale_factor` is kept the same. This only has an effect when `mode` is `'bilinear'`. Default: `False`\nConstraints:\ndtype:bool\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.gt(input,other,out=None)\nParameter description: out(Tensor, optional) - the output tensor that must be a BoolTensor\nConstraints:\ndtype:booltensor\nstructure:tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.empty_like(input,dtype=None,layout=None,device=None,requires_grad=False,memory_format=torch.preserve_format)\nParameter description: dtype(`torch.dtype`, optional) - the desired data type of returned Tensor. Default: if `None`, defaults to the dtype of `input`.\nConstraints:\ndtype:torch.dtype\nstructure:Null\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.nn.RNN(*args,**kwargs,input_size=None,hidden_size=None,num_layers=None,nonlinearity=None,bias=None,batch_first=None,dropout=None,bidirectional=None)\nParameter description: hidden_size - The number of features in the hidden state h\nConstraints:\ndtype:int\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:[0,inf)\n\n###\n\nSignature: torch.nn.functional.avg_pool1d(input,kernel_size,stride=None,padding=0,ceil_mode=False,count_include_pad=True)\nParameter description: padding - implicit zero paddings on both sides of the input. Can be a single number or a tuple (padW,). Default: 0\nConstraints:\ndtype:bool;int\nstructure:tuple\nshape:Null\nndim:0;1\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.autograd.grad(outputs,inputs,grad_outputs=None,retain_graph=None,create_graph=False,only_inputs=True,allow_unused=False)\nParameter description: outputs(sequence of Tensor) - outputs of the differentiated function.\nConstraints:\ndtype:Null\nstructure:sequence;tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.repeat_interleave(input,repeats,dim=None)\nParameter description: dim(int, optional) - The dimension along which to repeat values.By default, use the flattened input array, and return a flat outputarray.\nConstraints:\ndtype:int\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.distributed.all_gather(tensor_list,tensor,group=<objectobject>,async_op=False)\nParameter description: tensor(Tensor) - Tensor to be broadcast from current process.\nConstraints:",
            "constr": {
                "dtype": [],
                "structure": [
                    "tensor"
                ],
                "shape": [],
                "ndim": [],
                "enum": [],
                "range": []
            },
            "signature": "torch.distributed.all_gather(tensor_list,tensor,group=<objectobject>,async_op=False)",
            "descp": "tensor(Tensor) - Tensor to be broadcast from current process."
        },
        {
            "fname": "torch.masked_select.yaml",
            "arg": "input",
            "prompt": "Signature: torch.distributed.recv(tensor,src=None,group=<objectobject>,tag=0)\nParameter description: tensor(Tensor) - Tensor to fill with received data.\nConstraints:\ndtype:Null\nstructure:tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.div(input,other,out=None)\nParameter description: out(Tensor, optional) - the output tensor.\nConstraints:\ndtype:Null\nstructure:tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.nn.TransformerDecoder(decoder_layer,num_layers,norm=None)\nParameter description: num_layers - the number of sub-decoder-layers in the decoder (required).\nConstraints:\ndtype:int\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:[0,inf)\n\n###\n\nSignature: torch.nn.quantized.functional.interpolate(input,size=None,scale_factor=None,mode=nearest,align_corners=None)\nParameter description: align_corners(bool, optional) - Geometrically, we consider the pixels of the input and output as squares rather than points. If set to `True`, the input and output tensors are aligned by the center points of their corner pixels, preserving the values at the corner pixels. If set to `False`, the input and output tensors are aligned by the corner points of their corner pixels, and the interpolation uses edge value padding for out-of-boundary values, making this operation independent of input size when `scale_factor` is kept the same. This only has an effect when `mode` is `'bilinear'`. Default: `False`\nConstraints:\ndtype:bool\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.gt(input,other,out=None)\nParameter description: out(Tensor, optional) - the output tensor that must be a BoolTensor\nConstraints:\ndtype:booltensor\nstructure:tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.empty_like(input,dtype=None,layout=None,device=None,requires_grad=False,memory_format=torch.preserve_format)\nParameter description: dtype(`torch.dtype`, optional) - the desired data type of returned Tensor. Default: if `None`, defaults to the dtype of `input`.\nConstraints:\ndtype:torch.dtype\nstructure:Null\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.nn.RNN(*args,**kwargs,input_size=None,hidden_size=None,num_layers=None,nonlinearity=None,bias=None,batch_first=None,dropout=None,bidirectional=None)\nParameter description: hidden_size - The number of features in the hidden state h\nConstraints:\ndtype:int\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:[0,inf)\n\n###\n\nSignature: torch.nn.functional.avg_pool1d(input,kernel_size,stride=None,padding=0,ceil_mode=False,count_include_pad=True)\nParameter description: padding - implicit zero paddings on both sides of the input. Can be a single number or a tuple (padW,). Default: 0\nConstraints:\ndtype:bool;int\nstructure:tuple\nshape:Null\nndim:0;1\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.autograd.grad(outputs,inputs,grad_outputs=None,retain_graph=None,create_graph=False,only_inputs=True,allow_unused=False)\nParameter description: outputs(sequence of Tensor) - outputs of the differentiated function.\nConstraints:\ndtype:Null\nstructure:sequence;tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.repeat_interleave(input,repeats,dim=None)\nParameter description: dim(int, optional) - The dimension along which to repeat values.By default, use the flattened input array, and return a flat outputarray.\nConstraints:\ndtype:int\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.masked_select(input,mask,out=None)\nParameter description: input(Tensor) - the input tensor.\nConstraints:",
            "constr": {
                "dtype": [],
                "structure": [
                    "tensor"
                ],
                "shape": [],
                "ndim": [],
                "enum": [],
                "range": []
            },
            "signature": "torch.masked_select(input,mask,out=None)",
            "descp": "input(Tensor) - the input tensor."
        },
        {
            "fname": "torch.acos.yaml",
            "arg": "out",
            "prompt": "Signature: torch.distributed.recv(tensor,src=None,group=<objectobject>,tag=0)\nParameter description: tensor(Tensor) - Tensor to fill with received data.\nConstraints:\ndtype:Null\nstructure:tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.div(input,other,out=None)\nParameter description: out(Tensor, optional) - the output tensor.\nConstraints:\ndtype:Null\nstructure:tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.nn.TransformerDecoder(decoder_layer,num_layers,norm=None)\nParameter description: num_layers - the number of sub-decoder-layers in the decoder (required).\nConstraints:\ndtype:int\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:[0,inf)\n\n###\n\nSignature: torch.nn.quantized.functional.interpolate(input,size=None,scale_factor=None,mode=nearest,align_corners=None)\nParameter description: align_corners(bool, optional) - Geometrically, we consider the pixels of the input and output as squares rather than points. If set to `True`, the input and output tensors are aligned by the center points of their corner pixels, preserving the values at the corner pixels. If set to `False`, the input and output tensors are aligned by the corner points of their corner pixels, and the interpolation uses edge value padding for out-of-boundary values, making this operation independent of input size when `scale_factor` is kept the same. This only has an effect when `mode` is `'bilinear'`. Default: `False`\nConstraints:\ndtype:bool\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.gt(input,other,out=None)\nParameter description: out(Tensor, optional) - the output tensor that must be a BoolTensor\nConstraints:\ndtype:booltensor\nstructure:tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.empty_like(input,dtype=None,layout=None,device=None,requires_grad=False,memory_format=torch.preserve_format)\nParameter description: dtype(`torch.dtype`, optional) - the desired data type of returned Tensor. Default: if `None`, defaults to the dtype of `input`.\nConstraints:\ndtype:torch.dtype\nstructure:Null\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.nn.RNN(*args,**kwargs,input_size=None,hidden_size=None,num_layers=None,nonlinearity=None,bias=None,batch_first=None,dropout=None,bidirectional=None)\nParameter description: hidden_size - The number of features in the hidden state h\nConstraints:\ndtype:int\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:[0,inf)\n\n###\n\nSignature: torch.nn.functional.avg_pool1d(input,kernel_size,stride=None,padding=0,ceil_mode=False,count_include_pad=True)\nParameter description: padding - implicit zero paddings on both sides of the input. Can be a single number or a tuple (padW,). Default: 0\nConstraints:\ndtype:bool;int\nstructure:tuple\nshape:Null\nndim:0;1\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.autograd.grad(outputs,inputs,grad_outputs=None,retain_graph=None,create_graph=False,only_inputs=True,allow_unused=False)\nParameter description: outputs(sequence of Tensor) - outputs of the differentiated function.\nConstraints:\ndtype:Null\nstructure:sequence;tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.repeat_interleave(input,repeats,dim=None)\nParameter description: dim(int, optional) - The dimension along which to repeat values.By default, use the flattened input array, and return a flat outputarray.\nConstraints:\ndtype:int\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.acos(input,out=None)\nParameter description: out(Tensor, optional) - the output tensor.\nConstraints:",
            "constr": {
                "dtype": [],
                "structure": [
                    "tensor"
                ],
                "shape": [],
                "ndim": [],
                "enum": [],
                "range": []
            },
            "signature": "torch.acos(input,out=None)",
            "descp": "out(Tensor, optional) - the output tensor."
        },
        {
            "fname": "torch.nn.functional.binary_cross_entropy_with_logits.yaml",
            "arg": "reduction",
            "prompt": "Signature: torch.distributed.recv(tensor,src=None,group=<objectobject>,tag=0)\nParameter description: tensor(Tensor) - Tensor to fill with received data.\nConstraints:\ndtype:Null\nstructure:tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.div(input,other,out=None)\nParameter description: out(Tensor, optional) - the output tensor.\nConstraints:\ndtype:Null\nstructure:tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.nn.TransformerDecoder(decoder_layer,num_layers,norm=None)\nParameter description: num_layers - the number of sub-decoder-layers in the decoder (required).\nConstraints:\ndtype:int\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:[0,inf)\n\n###\n\nSignature: torch.nn.quantized.functional.interpolate(input,size=None,scale_factor=None,mode=nearest,align_corners=None)\nParameter description: align_corners(bool, optional) - Geometrically, we consider the pixels of the input and output as squares rather than points. If set to `True`, the input and output tensors are aligned by the center points of their corner pixels, preserving the values at the corner pixels. If set to `False`, the input and output tensors are aligned by the corner points of their corner pixels, and the interpolation uses edge value padding for out-of-boundary values, making this operation independent of input size when `scale_factor` is kept the same. This only has an effect when `mode` is `'bilinear'`. Default: `False`\nConstraints:\ndtype:bool\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.gt(input,other,out=None)\nParameter description: out(Tensor, optional) - the output tensor that must be a BoolTensor\nConstraints:\ndtype:booltensor\nstructure:tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.empty_like(input,dtype=None,layout=None,device=None,requires_grad=False,memory_format=torch.preserve_format)\nParameter description: dtype(`torch.dtype`, optional) - the desired data type of returned Tensor. Default: if `None`, defaults to the dtype of `input`.\nConstraints:\ndtype:torch.dtype\nstructure:Null\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.nn.RNN(*args,**kwargs,input_size=None,hidden_size=None,num_layers=None,nonlinearity=None,bias=None,batch_first=None,dropout=None,bidirectional=None)\nParameter description: hidden_size - The number of features in the hidden state h\nConstraints:\ndtype:int\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:[0,inf)\n\n###\n\nSignature: torch.nn.functional.avg_pool1d(input,kernel_size,stride=None,padding=0,ceil_mode=False,count_include_pad=True)\nParameter description: padding - implicit zero paddings on both sides of the input. Can be a single number or a tuple (padW,). Default: 0\nConstraints:\ndtype:bool;int\nstructure:tuple\nshape:Null\nndim:0;1\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.autograd.grad(outputs,inputs,grad_outputs=None,retain_graph=None,create_graph=False,only_inputs=True,allow_unused=False)\nParameter description: outputs(sequence of Tensor) - outputs of the differentiated function.\nConstraints:\ndtype:Null\nstructure:sequence;tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.repeat_interleave(input,repeats,dim=None)\nParameter description: dim(int, optional) - The dimension along which to repeat values.By default, use the flattened input array, and return a flat outputarray.\nConstraints:\ndtype:int\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.nn.functional.binary_cross_entropy_with_logits(input,target,weight=None,size_average=None,reduce=None,reduction=mean,pos_weight=None)\nParameter description: reduction(string, optional) - Specifies the reduction to apply to the output: `'none'` | `'mean'` | `'sum'`. `'none'`: no reduction will be applied, `'mean'`: the sum of the output will be divided by the number of elements in the output, `'sum'`: the output will be summed. Note: `size_average` and `reduce` are in the process of being deprecated, and in the meantime, specifying either of those two args will override `reduction`. Default: `'mean'`\nConstraints:",
            "constr": {
                "dtype": [
                    "string"
                ],
                "structure": [],
                "shape": [],
                "ndim": [],
                "enum": [
                    "sum",
                    "mean",
                    "none"
                ],
                "range": []
            },
            "signature": "torch.nn.functional.binary_cross_entropy_with_logits(input,target,weight=None,size_average=None,reduce=None,reduction=mean,pos_weight=None)",
            "descp": "reduction(string, optional) - Specifies the reduction to apply to the output: `'none'` | `'mean'` | `'sum'`. `'none'`: no reduction will be applied, `'mean'`: the sum of the output will be divided by the number of elements in the output, `'sum'`: the output will be summed. Note: `size_average` and `reduce` are in the process of being deprecated, and in the meantime, specifying either of those two args will override `reduction`. Default: `'mean'`"
        },
        {
            "fname": "torch.norm.yaml",
            "arg": "dim",
            "prompt": "Signature: torch.distributed.recv(tensor,src=None,group=<objectobject>,tag=0)\nParameter description: tensor(Tensor) - Tensor to fill with received data.\nConstraints:\ndtype:Null\nstructure:tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.div(input,other,out=None)\nParameter description: out(Tensor, optional) - the output tensor.\nConstraints:\ndtype:Null\nstructure:tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.nn.TransformerDecoder(decoder_layer,num_layers,norm=None)\nParameter description: num_layers - the number of sub-decoder-layers in the decoder (required).\nConstraints:\ndtype:int\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:[0,inf)\n\n###\n\nSignature: torch.nn.quantized.functional.interpolate(input,size=None,scale_factor=None,mode=nearest,align_corners=None)\nParameter description: align_corners(bool, optional) - Geometrically, we consider the pixels of the input and output as squares rather than points. If set to `True`, the input and output tensors are aligned by the center points of their corner pixels, preserving the values at the corner pixels. If set to `False`, the input and output tensors are aligned by the corner points of their corner pixels, and the interpolation uses edge value padding for out-of-boundary values, making this operation independent of input size when `scale_factor` is kept the same. This only has an effect when `mode` is `'bilinear'`. Default: `False`\nConstraints:\ndtype:bool\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.gt(input,other,out=None)\nParameter description: out(Tensor, optional) - the output tensor that must be a BoolTensor\nConstraints:\ndtype:booltensor\nstructure:tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.empty_like(input,dtype=None,layout=None,device=None,requires_grad=False,memory_format=torch.preserve_format)\nParameter description: dtype(`torch.dtype`, optional) - the desired data type of returned Tensor. Default: if `None`, defaults to the dtype of `input`.\nConstraints:\ndtype:torch.dtype\nstructure:Null\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.nn.RNN(*args,**kwargs,input_size=None,hidden_size=None,num_layers=None,nonlinearity=None,bias=None,batch_first=None,dropout=None,bidirectional=None)\nParameter description: hidden_size - The number of features in the hidden state h\nConstraints:\ndtype:int\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:[0,inf)\n\n###\n\nSignature: torch.nn.functional.avg_pool1d(input,kernel_size,stride=None,padding=0,ceil_mode=False,count_include_pad=True)\nParameter description: padding - implicit zero paddings on both sides of the input. Can be a single number or a tuple (padW,). Default: 0\nConstraints:\ndtype:bool;int\nstructure:tuple\nshape:Null\nndim:0;1\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.autograd.grad(outputs,inputs,grad_outputs=None,retain_graph=None,create_graph=False,only_inputs=True,allow_unused=False)\nParameter description: outputs(sequence of Tensor) - outputs of the differentiated function.\nConstraints:\ndtype:Null\nstructure:sequence;tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.repeat_interleave(input,repeats,dim=None)\nParameter description: dim(int, optional) - The dimension along which to repeat values.By default, use the flattened input array, and return a flat outputarray.\nConstraints:\ndtype:int\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.norm(input,p=fro,dim=None,keepdim=False,out=None,dtype=None)\nParameter description: dim(int, 2-tuple of python:ints, 2-list of python:ints, optional) - If it is an int, vector norm will be calculated, if it is 2-tuple of ints, matrix norm will be calculated. If the value is None, matrix norm will be calculated when the input tensor only has two dimensions, vector norm will be calculated when the input tensor only has one dimension. If the input tensor has more than two dimensions, the vector norm will be applied to last dimension.\nConstraints:",
            "constr": {
                "dtype": [
                    "int"
                ],
                "structure": [
                    "list",
                    "tuple"
                ],
                "shape": [
                    "[2]"
                ],
                "ndim": [
                    "0",
                    "1"
                ],
                "enum": [],
                "range": []
            },
            "signature": "torch.norm(input,p=fro,dim=None,keepdim=False,out=None,dtype=None)",
            "descp": "dim(int, 2-tuple of python:ints, 2-list of python:ints, optional) - If it is an int, vector norm will be calculated, if it is 2-tuple of ints, matrix norm will be calculated. If the value is None, matrix norm will be calculated when the input tensor only has two dimensions, vector norm will be calculated when the input tensor only has one dimension. If the input tensor has more than two dimensions, the vector norm will be applied to last dimension."
        },
        {
            "fname": "torch.nn.kldivloss.yaml",
            "arg": "reduction",
            "prompt": "Signature: torch.distributed.recv(tensor,src=None,group=<objectobject>,tag=0)\nParameter description: tensor(Tensor) - Tensor to fill with received data.\nConstraints:\ndtype:Null\nstructure:tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.div(input,other,out=None)\nParameter description: out(Tensor, optional) - the output tensor.\nConstraints:\ndtype:Null\nstructure:tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.nn.TransformerDecoder(decoder_layer,num_layers,norm=None)\nParameter description: num_layers - the number of sub-decoder-layers in the decoder (required).\nConstraints:\ndtype:int\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:[0,inf)\n\n###\n\nSignature: torch.nn.quantized.functional.interpolate(input,size=None,scale_factor=None,mode=nearest,align_corners=None)\nParameter description: align_corners(bool, optional) - Geometrically, we consider the pixels of the input and output as squares rather than points. If set to `True`, the input and output tensors are aligned by the center points of their corner pixels, preserving the values at the corner pixels. If set to `False`, the input and output tensors are aligned by the corner points of their corner pixels, and the interpolation uses edge value padding for out-of-boundary values, making this operation independent of input size when `scale_factor` is kept the same. This only has an effect when `mode` is `'bilinear'`. Default: `False`\nConstraints:\ndtype:bool\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.gt(input,other,out=None)\nParameter description: out(Tensor, optional) - the output tensor that must be a BoolTensor\nConstraints:\ndtype:booltensor\nstructure:tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.empty_like(input,dtype=None,layout=None,device=None,requires_grad=False,memory_format=torch.preserve_format)\nParameter description: dtype(`torch.dtype`, optional) - the desired data type of returned Tensor. Default: if `None`, defaults to the dtype of `input`.\nConstraints:\ndtype:torch.dtype\nstructure:Null\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.nn.RNN(*args,**kwargs,input_size=None,hidden_size=None,num_layers=None,nonlinearity=None,bias=None,batch_first=None,dropout=None,bidirectional=None)\nParameter description: hidden_size - The number of features in the hidden state h\nConstraints:\ndtype:int\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:[0,inf)\n\n###\n\nSignature: torch.nn.functional.avg_pool1d(input,kernel_size,stride=None,padding=0,ceil_mode=False,count_include_pad=True)\nParameter description: padding - implicit zero paddings on both sides of the input. Can be a single number or a tuple (padW,). Default: 0\nConstraints:\ndtype:bool;int\nstructure:tuple\nshape:Null\nndim:0;1\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.autograd.grad(outputs,inputs,grad_outputs=None,retain_graph=None,create_graph=False,only_inputs=True,allow_unused=False)\nParameter description: outputs(sequence of Tensor) - outputs of the differentiated function.\nConstraints:\ndtype:Null\nstructure:sequence;tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.repeat_interleave(input,repeats,dim=None)\nParameter description: dim(int, optional) - The dimension along which to repeat values.By default, use the flattened input array, and return a flat outputarray.\nConstraints:\ndtype:int\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.nn.KLDivLoss(size_average=None,reduce=None,reduction=mean)\nParameter description: reduction(string, optional) - Specifies the reduction to apply to the output: `'none'` | `'batchmean'` | `'sum'` | `'mean'`. `'none'`: no reduction will be applied. `'batchmean'`: the sum of the output will be divided by batchsize. `'sum'`: the output will be summed. `'mean'`: the output will be divided by the number of elements in the output. Default: `'mean'`\nConstraints:",
            "constr": {
                "dtype": [
                    "string"
                ],
                "structure": [],
                "shape": [],
                "ndim": [],
                "enum": [
                    "none",
                    "batchmean",
                    "sum",
                    "mean"
                ],
                "range": []
            },
            "signature": "torch.nn.KLDivLoss(size_average=None,reduce=None,reduction=mean)",
            "descp": "reduction(string, optional) - Specifies the reduction to apply to the output: `'none'` | `'batchmean'` | `'sum'` | `'mean'`. `'none'`: no reduction will be applied. `'batchmean'`: the sum of the output will be divided by batchsize. `'sum'`: the output will be summed. `'mean'`: the output will be divided by the number of elements in the output. Default: `'mean'`"
        },
        {
            "fname": "torch.nn.functional.pad.yaml",
            "arg": "value",
            "prompt": "Signature: torch.distributed.recv(tensor,src=None,group=<objectobject>,tag=0)\nParameter description: tensor(Tensor) - Tensor to fill with received data.\nConstraints:\ndtype:Null\nstructure:tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.div(input,other,out=None)\nParameter description: out(Tensor, optional) - the output tensor.\nConstraints:\ndtype:Null\nstructure:tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.nn.TransformerDecoder(decoder_layer,num_layers,norm=None)\nParameter description: num_layers - the number of sub-decoder-layers in the decoder (required).\nConstraints:\ndtype:int\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:[0,inf)\n\n###\n\nSignature: torch.nn.quantized.functional.interpolate(input,size=None,scale_factor=None,mode=nearest,align_corners=None)\nParameter description: align_corners(bool, optional) - Geometrically, we consider the pixels of the input and output as squares rather than points. If set to `True`, the input and output tensors are aligned by the center points of their corner pixels, preserving the values at the corner pixels. If set to `False`, the input and output tensors are aligned by the corner points of their corner pixels, and the interpolation uses edge value padding for out-of-boundary values, making this operation independent of input size when `scale_factor` is kept the same. This only has an effect when `mode` is `'bilinear'`. Default: `False`\nConstraints:\ndtype:bool\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.gt(input,other,out=None)\nParameter description: out(Tensor, optional) - the output tensor that must be a BoolTensor\nConstraints:\ndtype:booltensor\nstructure:tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.empty_like(input,dtype=None,layout=None,device=None,requires_grad=False,memory_format=torch.preserve_format)\nParameter description: dtype(`torch.dtype`, optional) - the desired data type of returned Tensor. Default: if `None`, defaults to the dtype of `input`.\nConstraints:\ndtype:torch.dtype\nstructure:Null\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.nn.RNN(*args,**kwargs,input_size=None,hidden_size=None,num_layers=None,nonlinearity=None,bias=None,batch_first=None,dropout=None,bidirectional=None)\nParameter description: hidden_size - The number of features in the hidden state h\nConstraints:\ndtype:int\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:[0,inf)\n\n###\n\nSignature: torch.nn.functional.avg_pool1d(input,kernel_size,stride=None,padding=0,ceil_mode=False,count_include_pad=True)\nParameter description: padding - implicit zero paddings on both sides of the input. Can be a single number or a tuple (padW,). Default: 0\nConstraints:\ndtype:bool;int\nstructure:tuple\nshape:Null\nndim:0;1\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.autograd.grad(outputs,inputs,grad_outputs=None,retain_graph=None,create_graph=False,only_inputs=True,allow_unused=False)\nParameter description: outputs(sequence of Tensor) - outputs of the differentiated function.\nConstraints:\ndtype:Null\nstructure:sequence;tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.repeat_interleave(input,repeats,dim=None)\nParameter description: dim(int, optional) - The dimension along which to repeat values.By default, use the flattened input array, and return a flat outputarray.\nConstraints:\ndtype:int\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.nn.functional.pad(input,pad,mode=constant,value=0)\nParameter description: value - fill value for `'constant'` padding. Default: `0`\nConstraints:",
            "constr": {
                "dtype": [
                    "int"
                ],
                "structure": [],
                "shape": [],
                "ndim": [
                    "0"
                ],
                "enum": [],
                "range": []
            },
            "signature": "torch.nn.functional.pad(input,pad,mode=constant,value=0)",
            "descp": "value - fill value for `'constant'` padding. Default: `0`"
        },
        {
            "fname": "torch.onnx.export.yaml",
            "arg": "aten",
            "prompt": "Signature: torch.distributed.recv(tensor,src=None,group=<objectobject>,tag=0)\nParameter description: tensor(Tensor) - Tensor to fill with received data.\nConstraints:\ndtype:Null\nstructure:tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.div(input,other,out=None)\nParameter description: out(Tensor, optional) - the output tensor.\nConstraints:\ndtype:Null\nstructure:tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.nn.TransformerDecoder(decoder_layer,num_layers,norm=None)\nParameter description: num_layers - the number of sub-decoder-layers in the decoder (required).\nConstraints:\ndtype:int\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:[0,inf)\n\n###\n\nSignature: torch.nn.quantized.functional.interpolate(input,size=None,scale_factor=None,mode=nearest,align_corners=None)\nParameter description: align_corners(bool, optional) - Geometrically, we consider the pixels of the input and output as squares rather than points. If set to `True`, the input and output tensors are aligned by the center points of their corner pixels, preserving the values at the corner pixels. If set to `False`, the input and output tensors are aligned by the corner points of their corner pixels, and the interpolation uses edge value padding for out-of-boundary values, making this operation independent of input size when `scale_factor` is kept the same. This only has an effect when `mode` is `'bilinear'`. Default: `False`\nConstraints:\ndtype:bool\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.gt(input,other,out=None)\nParameter description: out(Tensor, optional) - the output tensor that must be a BoolTensor\nConstraints:\ndtype:booltensor\nstructure:tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.empty_like(input,dtype=None,layout=None,device=None,requires_grad=False,memory_format=torch.preserve_format)\nParameter description: dtype(`torch.dtype`, optional) - the desired data type of returned Tensor. Default: if `None`, defaults to the dtype of `input`.\nConstraints:\ndtype:torch.dtype\nstructure:Null\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.nn.RNN(*args,**kwargs,input_size=None,hidden_size=None,num_layers=None,nonlinearity=None,bias=None,batch_first=None,dropout=None,bidirectional=None)\nParameter description: hidden_size - The number of features in the hidden state h\nConstraints:\ndtype:int\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:[0,inf)\n\n###\n\nSignature: torch.nn.functional.avg_pool1d(input,kernel_size,stride=None,padding=0,ceil_mode=False,count_include_pad=True)\nParameter description: padding - implicit zero paddings on both sides of the input. Can be a single number or a tuple (padW,). Default: 0\nConstraints:\ndtype:bool;int\nstructure:tuple\nshape:Null\nndim:0;1\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.autograd.grad(outputs,inputs,grad_outputs=None,retain_graph=None,create_graph=False,only_inputs=True,allow_unused=False)\nParameter description: outputs(sequence of Tensor) - outputs of the differentiated function.\nConstraints:\ndtype:Null\nstructure:sequence;tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.repeat_interleave(input,repeats,dim=None)\nParameter description: dim(int, optional) - The dimension along which to repeat values.By default, use the flattened input array, and return a flat outputarray.\nConstraints:\ndtype:int\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.onnx.export(model,args,f,export_params=True,verbose=False,training=False,input_names=None,output_names=None,aten=False,export_raw_ir=False,operator_export_type=None,opset_version=None,_retain_param_name=True,do_constant_folding=True,example_outputs=None,strip_doc_string=True,dynamic_axes=None,keep_initializers_as_inputs=None,custom_opsets=None,enable_onnx_checker=True,use_external_data_format=False)\nParameter description: aten(bool, default False) - [DEPRECATED. use operator_export_type] export the model in aten mode. If using aten mode, all the ops original exported by the functions in symbolic_opset<version>.py are exported as ATen ops.\nConstraints:",
            "constr": {
                "dtype": [
                    "bool"
                ],
                "structure": [],
                "shape": [],
                "ndim": [
                    "0"
                ],
                "enum": [],
                "range": []
            },
            "signature": "torch.onnx.export(model,args,f,export_params=True,verbose=False,training=False,input_names=None,output_names=None,aten=False,export_raw_ir=False,operator_export_type=None,opset_version=None,_retain_param_name=True,do_constant_folding=True,example_outputs=None,strip_doc_string=True,dynamic_axes=None,keep_initializers_as_inputs=None,custom_opsets=None,enable_onnx_checker=True,use_external_data_format=False)",
            "descp": "aten(bool, default False) - [DEPRECATED. use operator_export_type] export the model in aten mode. If using aten mode, all the ops original exported by the functions in symbolic_opset<version>.py are exported as ATen ops."
        },
        {
            "fname": "torch.gt.yaml",
            "arg": "input",
            "prompt": "Signature: torch.distributed.recv(tensor,src=None,group=<objectobject>,tag=0)\nParameter description: tensor(Tensor) - Tensor to fill with received data.\nConstraints:\ndtype:Null\nstructure:tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.div(input,other,out=None)\nParameter description: out(Tensor, optional) - the output tensor.\nConstraints:\ndtype:Null\nstructure:tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.nn.TransformerDecoder(decoder_layer,num_layers,norm=None)\nParameter description: num_layers - the number of sub-decoder-layers in the decoder (required).\nConstraints:\ndtype:int\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:[0,inf)\n\n###\n\nSignature: torch.nn.quantized.functional.interpolate(input,size=None,scale_factor=None,mode=nearest,align_corners=None)\nParameter description: align_corners(bool, optional) - Geometrically, we consider the pixels of the input and output as squares rather than points. If set to `True`, the input and output tensors are aligned by the center points of their corner pixels, preserving the values at the corner pixels. If set to `False`, the input and output tensors are aligned by the corner points of their corner pixels, and the interpolation uses edge value padding for out-of-boundary values, making this operation independent of input size when `scale_factor` is kept the same. This only has an effect when `mode` is `'bilinear'`. Default: `False`\nConstraints:\ndtype:bool\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.gt(input,other,out=None)\nParameter description: out(Tensor, optional) - the output tensor that must be a BoolTensor\nConstraints:\ndtype:booltensor\nstructure:tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.empty_like(input,dtype=None,layout=None,device=None,requires_grad=False,memory_format=torch.preserve_format)\nParameter description: dtype(`torch.dtype`, optional) - the desired data type of returned Tensor. Default: if `None`, defaults to the dtype of `input`.\nConstraints:\ndtype:torch.dtype\nstructure:Null\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.nn.RNN(*args,**kwargs,input_size=None,hidden_size=None,num_layers=None,nonlinearity=None,bias=None,batch_first=None,dropout=None,bidirectional=None)\nParameter description: hidden_size - The number of features in the hidden state h\nConstraints:\ndtype:int\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:[0,inf)\n\n###\n\nSignature: torch.nn.functional.avg_pool1d(input,kernel_size,stride=None,padding=0,ceil_mode=False,count_include_pad=True)\nParameter description: padding - implicit zero paddings on both sides of the input. Can be a single number or a tuple (padW,). Default: 0\nConstraints:\ndtype:bool;int\nstructure:tuple\nshape:Null\nndim:0;1\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.autograd.grad(outputs,inputs,grad_outputs=None,retain_graph=None,create_graph=False,only_inputs=True,allow_unused=False)\nParameter description: outputs(sequence of Tensor) - outputs of the differentiated function.\nConstraints:\ndtype:Null\nstructure:sequence;tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.repeat_interleave(input,repeats,dim=None)\nParameter description: dim(int, optional) - The dimension along which to repeat values.By default, use the flattened input array, and return a flat outputarray.\nConstraints:\ndtype:int\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.gt(input,other,out=None)\nParameter description: input(Tensor) - the tensor to compare\nConstraints:",
            "constr": {
                "dtype": [],
                "structure": [
                    "tensor"
                ],
                "shape": [],
                "ndim": [],
                "enum": [],
                "range": []
            },
            "signature": "torch.gt(input,other,out=None)",
            "descp": "input(Tensor) - the tensor to compare"
        },
        {
            "fname": "torch.rfft.yaml",
            "arg": "signal_ndim",
            "prompt": "Signature: torch.distributed.recv(tensor,src=None,group=<objectobject>,tag=0)\nParameter description: tensor(Tensor) - Tensor to fill with received data.\nConstraints:\ndtype:Null\nstructure:tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.div(input,other,out=None)\nParameter description: out(Tensor, optional) - the output tensor.\nConstraints:\ndtype:Null\nstructure:tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.nn.TransformerDecoder(decoder_layer,num_layers,norm=None)\nParameter description: num_layers - the number of sub-decoder-layers in the decoder (required).\nConstraints:\ndtype:int\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:[0,inf)\n\n###\n\nSignature: torch.nn.quantized.functional.interpolate(input,size=None,scale_factor=None,mode=nearest,align_corners=None)\nParameter description: align_corners(bool, optional) - Geometrically, we consider the pixels of the input and output as squares rather than points. If set to `True`, the input and output tensors are aligned by the center points of their corner pixels, preserving the values at the corner pixels. If set to `False`, the input and output tensors are aligned by the corner points of their corner pixels, and the interpolation uses edge value padding for out-of-boundary values, making this operation independent of input size when `scale_factor` is kept the same. This only has an effect when `mode` is `'bilinear'`. Default: `False`\nConstraints:\ndtype:bool\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.gt(input,other,out=None)\nParameter description: out(Tensor, optional) - the output tensor that must be a BoolTensor\nConstraints:\ndtype:booltensor\nstructure:tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.empty_like(input,dtype=None,layout=None,device=None,requires_grad=False,memory_format=torch.preserve_format)\nParameter description: dtype(`torch.dtype`, optional) - the desired data type of returned Tensor. Default: if `None`, defaults to the dtype of `input`.\nConstraints:\ndtype:torch.dtype\nstructure:Null\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.nn.RNN(*args,**kwargs,input_size=None,hidden_size=None,num_layers=None,nonlinearity=None,bias=None,batch_first=None,dropout=None,bidirectional=None)\nParameter description: hidden_size - The number of features in the hidden state h\nConstraints:\ndtype:int\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:[0,inf)\n\n###\n\nSignature: torch.nn.functional.avg_pool1d(input,kernel_size,stride=None,padding=0,ceil_mode=False,count_include_pad=True)\nParameter description: padding - implicit zero paddings on both sides of the input. Can be a single number or a tuple (padW,). Default: 0\nConstraints:\ndtype:bool;int\nstructure:tuple\nshape:Null\nndim:0;1\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.autograd.grad(outputs,inputs,grad_outputs=None,retain_graph=None,create_graph=False,only_inputs=True,allow_unused=False)\nParameter description: outputs(sequence of Tensor) - outputs of the differentiated function.\nConstraints:\ndtype:Null\nstructure:sequence;tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.repeat_interleave(input,repeats,dim=None)\nParameter description: dim(int, optional) - The dimension along which to repeat values.By default, use the flattened input array, and return a flat outputarray.\nConstraints:\ndtype:int\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.rfft(input,signal_ndim,normalized=False,onesided=True)\nParameter description: signal_ndim(int) - the number of dimensions in each signal. `signal_ndim` can only be 1, 2 or 3\nConstraints:",
            "constr": {
                "dtype": [
                    "int"
                ],
                "structure": [],
                "shape": [],
                "ndim": [
                    "0"
                ],
                "enum": [
                    "1",
                    "2",
                    "3"
                ],
                "range": [
                    "[0,inf)"
                ]
            },
            "signature": "torch.rfft(input,signal_ndim,normalized=False,onesided=True)",
            "descp": "signal_ndim(int) - the number of dimensions in each signal. `signal_ndim` can only be 1, 2 or 3"
        },
        {
            "fname": "torch.nn.quantized.functional.conv2d.yaml",
            "arg": "bias",
            "prompt": "Signature: torch.distributed.recv(tensor,src=None,group=<objectobject>,tag=0)\nParameter description: tensor(Tensor) - Tensor to fill with received data.\nConstraints:\ndtype:Null\nstructure:tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.div(input,other,out=None)\nParameter description: out(Tensor, optional) - the output tensor.\nConstraints:\ndtype:Null\nstructure:tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.nn.TransformerDecoder(decoder_layer,num_layers,norm=None)\nParameter description: num_layers - the number of sub-decoder-layers in the decoder (required).\nConstraints:\ndtype:int\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:[0,inf)\n\n###\n\nSignature: torch.nn.quantized.functional.interpolate(input,size=None,scale_factor=None,mode=nearest,align_corners=None)\nParameter description: align_corners(bool, optional) - Geometrically, we consider the pixels of the input and output as squares rather than points. If set to `True`, the input and output tensors are aligned by the center points of their corner pixels, preserving the values at the corner pixels. If set to `False`, the input and output tensors are aligned by the corner points of their corner pixels, and the interpolation uses edge value padding for out-of-boundary values, making this operation independent of input size when `scale_factor` is kept the same. This only has an effect when `mode` is `'bilinear'`. Default: `False`\nConstraints:\ndtype:bool\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.gt(input,other,out=None)\nParameter description: out(Tensor, optional) - the output tensor that must be a BoolTensor\nConstraints:\ndtype:booltensor\nstructure:tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.empty_like(input,dtype=None,layout=None,device=None,requires_grad=False,memory_format=torch.preserve_format)\nParameter description: dtype(`torch.dtype`, optional) - the desired data type of returned Tensor. Default: if `None`, defaults to the dtype of `input`.\nConstraints:\ndtype:torch.dtype\nstructure:Null\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.nn.RNN(*args,**kwargs,input_size=None,hidden_size=None,num_layers=None,nonlinearity=None,bias=None,batch_first=None,dropout=None,bidirectional=None)\nParameter description: hidden_size - The number of features in the hidden state h\nConstraints:\ndtype:int\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:[0,inf)\n\n###\n\nSignature: torch.nn.functional.avg_pool1d(input,kernel_size,stride=None,padding=0,ceil_mode=False,count_include_pad=True)\nParameter description: padding - implicit zero paddings on both sides of the input. Can be a single number or a tuple (padW,). Default: 0\nConstraints:\ndtype:bool;int\nstructure:tuple\nshape:Null\nndim:0;1\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.autograd.grad(outputs,inputs,grad_outputs=None,retain_graph=None,create_graph=False,only_inputs=True,allow_unused=False)\nParameter description: outputs(sequence of Tensor) - outputs of the differentiated function.\nConstraints:\ndtype:Null\nstructure:sequence;tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.repeat_interleave(input,repeats,dim=None)\nParameter description: dim(int, optional) - The dimension along which to repeat values.By default, use the flattened input array, and return a flat outputarray.\nConstraints:\ndtype:int\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.nn.quantized.functional.conv2d(input,weight,bias,stride=1,padding=0,dilation=1,groups=1,padding_mode=zeros,scale=1.0,zero_point=0,dtype=torch.quint8)\nParameter description: bias - non-quantized bias tensor of shape (out _channels) . The tensor type must be torch.float.\nConstraints:",
            "constr": {
                "dtype": [
                    "torch.float"
                ],
                "structure": [
                    "tensor"
                ],
                "shape": [
                    "(out _channels)"
                ],
                "ndim": [],
                "enum": [],
                "range": []
            },
            "signature": "torch.nn.quantized.functional.conv2d(input,weight,bias,stride=1,padding=0,dilation=1,groups=1,padding_mode=zeros,scale=1.0,zero_point=0,dtype=torch.quint8)",
            "descp": "bias - non-quantized bias tensor of shape (out _channels) . The tensor type must be torch.float."
        },
        {
            "fname": "torch.nn.utils.clip_grad_norm_.yaml",
            "arg": "max_norm",
            "prompt": "Signature: torch.distributed.recv(tensor,src=None,group=<objectobject>,tag=0)\nParameter description: tensor(Tensor) - Tensor to fill with received data.\nConstraints:\ndtype:Null\nstructure:tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.div(input,other,out=None)\nParameter description: out(Tensor, optional) - the output tensor.\nConstraints:\ndtype:Null\nstructure:tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.nn.TransformerDecoder(decoder_layer,num_layers,norm=None)\nParameter description: num_layers - the number of sub-decoder-layers in the decoder (required).\nConstraints:\ndtype:int\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:[0,inf)\n\n###\n\nSignature: torch.nn.quantized.functional.interpolate(input,size=None,scale_factor=None,mode=nearest,align_corners=None)\nParameter description: align_corners(bool, optional) - Geometrically, we consider the pixels of the input and output as squares rather than points. If set to `True`, the input and output tensors are aligned by the center points of their corner pixels, preserving the values at the corner pixels. If set to `False`, the input and output tensors are aligned by the corner points of their corner pixels, and the interpolation uses edge value padding for out-of-boundary values, making this operation independent of input size when `scale_factor` is kept the same. This only has an effect when `mode` is `'bilinear'`. Default: `False`\nConstraints:\ndtype:bool\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.gt(input,other,out=None)\nParameter description: out(Tensor, optional) - the output tensor that must be a BoolTensor\nConstraints:\ndtype:booltensor\nstructure:tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.empty_like(input,dtype=None,layout=None,device=None,requires_grad=False,memory_format=torch.preserve_format)\nParameter description: dtype(`torch.dtype`, optional) - the desired data type of returned Tensor. Default: if `None`, defaults to the dtype of `input`.\nConstraints:\ndtype:torch.dtype\nstructure:Null\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.nn.RNN(*args,**kwargs,input_size=None,hidden_size=None,num_layers=None,nonlinearity=None,bias=None,batch_first=None,dropout=None,bidirectional=None)\nParameter description: hidden_size - The number of features in the hidden state h\nConstraints:\ndtype:int\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:[0,inf)\n\n###\n\nSignature: torch.nn.functional.avg_pool1d(input,kernel_size,stride=None,padding=0,ceil_mode=False,count_include_pad=True)\nParameter description: padding - implicit zero paddings on both sides of the input. Can be a single number or a tuple (padW,). Default: 0\nConstraints:\ndtype:bool;int\nstructure:tuple\nshape:Null\nndim:0;1\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.autograd.grad(outputs,inputs,grad_outputs=None,retain_graph=None,create_graph=False,only_inputs=True,allow_unused=False)\nParameter description: outputs(sequence of Tensor) - outputs of the differentiated function.\nConstraints:\ndtype:Null\nstructure:sequence;tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.repeat_interleave(input,repeats,dim=None)\nParameter description: dim(int, optional) - The dimension along which to repeat values.By default, use the flattened input array, and return a flat outputarray.\nConstraints:\ndtype:int\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.nn.utils.clip_grad_norm_(parameters,max_norm,norm_type=2)\nParameter description: max_norm(float or int) - max norm of the gradients\nConstraints:",
            "constr": {
                "dtype": [
                    "numeric",
                    "float",
                    "int"
                ],
                "structure": [],
                "shape": [],
                "ndim": [],
                "enum": [],
                "range": []
            },
            "signature": "torch.nn.utils.clip_grad_norm_(parameters,max_norm,norm_type=2)",
            "descp": "max_norm(float or int) - max norm of the gradients"
        },
        {
            "fname": "torch.nn.fold.yaml",
            "arg": "dilation",
            "prompt": "Signature: torch.distributed.recv(tensor,src=None,group=<objectobject>,tag=0)\nParameter description: tensor(Tensor) - Tensor to fill with received data.\nConstraints:\ndtype:Null\nstructure:tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.div(input,other,out=None)\nParameter description: out(Tensor, optional) - the output tensor.\nConstraints:\ndtype:Null\nstructure:tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.nn.TransformerDecoder(decoder_layer,num_layers,norm=None)\nParameter description: num_layers - the number of sub-decoder-layers in the decoder (required).\nConstraints:\ndtype:int\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:[0,inf)\n\n###\n\nSignature: torch.nn.quantized.functional.interpolate(input,size=None,scale_factor=None,mode=nearest,align_corners=None)\nParameter description: align_corners(bool, optional) - Geometrically, we consider the pixels of the input and output as squares rather than points. If set to `True`, the input and output tensors are aligned by the center points of their corner pixels, preserving the values at the corner pixels. If set to `False`, the input and output tensors are aligned by the corner points of their corner pixels, and the interpolation uses edge value padding for out-of-boundary values, making this operation independent of input size when `scale_factor` is kept the same. This only has an effect when `mode` is `'bilinear'`. Default: `False`\nConstraints:\ndtype:bool\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.gt(input,other,out=None)\nParameter description: out(Tensor, optional) - the output tensor that must be a BoolTensor\nConstraints:\ndtype:booltensor\nstructure:tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.empty_like(input,dtype=None,layout=None,device=None,requires_grad=False,memory_format=torch.preserve_format)\nParameter description: dtype(`torch.dtype`, optional) - the desired data type of returned Tensor. Default: if `None`, defaults to the dtype of `input`.\nConstraints:\ndtype:torch.dtype\nstructure:Null\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.nn.RNN(*args,**kwargs,input_size=None,hidden_size=None,num_layers=None,nonlinearity=None,bias=None,batch_first=None,dropout=None,bidirectional=None)\nParameter description: hidden_size - The number of features in the hidden state h\nConstraints:\ndtype:int\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:[0,inf)\n\n###\n\nSignature: torch.nn.functional.avg_pool1d(input,kernel_size,stride=None,padding=0,ceil_mode=False,count_include_pad=True)\nParameter description: padding - implicit zero paddings on both sides of the input. Can be a single number or a tuple (padW,). Default: 0\nConstraints:\ndtype:bool;int\nstructure:tuple\nshape:Null\nndim:0;1\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.autograd.grad(outputs,inputs,grad_outputs=None,retain_graph=None,create_graph=False,only_inputs=True,allow_unused=False)\nParameter description: outputs(sequence of Tensor) - outputs of the differentiated function.\nConstraints:\ndtype:Null\nstructure:sequence;tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.repeat_interleave(input,repeats,dim=None)\nParameter description: dim(int, optional) - The dimension along which to repeat values.By default, use the flattened input array, and return a flat outputarray.\nConstraints:\ndtype:int\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.nn.Fold(output_size,kernel_size,dilation=1,padding=0,stride=1)\nParameter description: dilation(int or tuple, optional) - a parameter that controls the stride of elements within the neighborhood. Default: 1\nConstraints:",
            "constr": {
                "dtype": [
                    "int"
                ],
                "structure": [
                    "tuple"
                ],
                "shape": [],
                "ndim": [
                    "0",
                    "1"
                ],
                "enum": [],
                "range": []
            },
            "signature": "torch.nn.Fold(output_size,kernel_size,dilation=1,padding=0,stride=1)",
            "descp": "dilation(int or tuple, optional) - a parameter that controls the stride of elements within the neighborhood. Default: 1"
        },
        {
            "fname": "torch.addmm.yaml",
            "arg": "mat1",
            "prompt": "Signature: torch.distributed.recv(tensor,src=None,group=<objectobject>,tag=0)\nParameter description: tensor(Tensor) - Tensor to fill with received data.\nConstraints:\ndtype:Null\nstructure:tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.div(input,other,out=None)\nParameter description: out(Tensor, optional) - the output tensor.\nConstraints:\ndtype:Null\nstructure:tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.nn.TransformerDecoder(decoder_layer,num_layers,norm=None)\nParameter description: num_layers - the number of sub-decoder-layers in the decoder (required).\nConstraints:\ndtype:int\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:[0,inf)\n\n###\n\nSignature: torch.nn.quantized.functional.interpolate(input,size=None,scale_factor=None,mode=nearest,align_corners=None)\nParameter description: align_corners(bool, optional) - Geometrically, we consider the pixels of the input and output as squares rather than points. If set to `True`, the input and output tensors are aligned by the center points of their corner pixels, preserving the values at the corner pixels. If set to `False`, the input and output tensors are aligned by the corner points of their corner pixels, and the interpolation uses edge value padding for out-of-boundary values, making this operation independent of input size when `scale_factor` is kept the same. This only has an effect when `mode` is `'bilinear'`. Default: `False`\nConstraints:\ndtype:bool\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.gt(input,other,out=None)\nParameter description: out(Tensor, optional) - the output tensor that must be a BoolTensor\nConstraints:\ndtype:booltensor\nstructure:tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.empty_like(input,dtype=None,layout=None,device=None,requires_grad=False,memory_format=torch.preserve_format)\nParameter description: dtype(`torch.dtype`, optional) - the desired data type of returned Tensor. Default: if `None`, defaults to the dtype of `input`.\nConstraints:\ndtype:torch.dtype\nstructure:Null\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.nn.RNN(*args,**kwargs,input_size=None,hidden_size=None,num_layers=None,nonlinearity=None,bias=None,batch_first=None,dropout=None,bidirectional=None)\nParameter description: hidden_size - The number of features in the hidden state h\nConstraints:\ndtype:int\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:[0,inf)\n\n###\n\nSignature: torch.nn.functional.avg_pool1d(input,kernel_size,stride=None,padding=0,ceil_mode=False,count_include_pad=True)\nParameter description: padding - implicit zero paddings on both sides of the input. Can be a single number or a tuple (padW,). Default: 0\nConstraints:\ndtype:bool;int\nstructure:tuple\nshape:Null\nndim:0;1\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.autograd.grad(outputs,inputs,grad_outputs=None,retain_graph=None,create_graph=False,only_inputs=True,allow_unused=False)\nParameter description: outputs(sequence of Tensor) - outputs of the differentiated function.\nConstraints:\ndtype:Null\nstructure:sequence;tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.repeat_interleave(input,repeats,dim=None)\nParameter description: dim(int, optional) - The dimension along which to repeat values.By default, use the flattened input array, and return a flat outputarray.\nConstraints:\ndtype:int\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.addmm(input,mat1,mat2,beta=1,alpha=1,out=None)\nParameter description: mat1(Tensor) - the first matrix to be multiplied\nConstraints:",
            "constr": {
                "dtype": [
                    "numeric"
                ],
                "structure": [
                    "tensor"
                ],
                "shape": [],
                "ndim": [],
                "enum": [],
                "range": []
            },
            "signature": "torch.addmm(input,mat1,mat2,beta=1,alpha=1,out=None)",
            "descp": "mat1(Tensor) - the first matrix to be multiplied"
        },
        {
            "fname": "torch.nn.nllloss.yaml",
            "arg": "reduction",
            "prompt": "Signature: torch.distributed.recv(tensor,src=None,group=<objectobject>,tag=0)\nParameter description: tensor(Tensor) - Tensor to fill with received data.\nConstraints:\ndtype:Null\nstructure:tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.div(input,other,out=None)\nParameter description: out(Tensor, optional) - the output tensor.\nConstraints:\ndtype:Null\nstructure:tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.nn.TransformerDecoder(decoder_layer,num_layers,norm=None)\nParameter description: num_layers - the number of sub-decoder-layers in the decoder (required).\nConstraints:\ndtype:int\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:[0,inf)\n\n###\n\nSignature: torch.nn.quantized.functional.interpolate(input,size=None,scale_factor=None,mode=nearest,align_corners=None)\nParameter description: align_corners(bool, optional) - Geometrically, we consider the pixels of the input and output as squares rather than points. If set to `True`, the input and output tensors are aligned by the center points of their corner pixels, preserving the values at the corner pixels. If set to `False`, the input and output tensors are aligned by the corner points of their corner pixels, and the interpolation uses edge value padding for out-of-boundary values, making this operation independent of input size when `scale_factor` is kept the same. This only has an effect when `mode` is `'bilinear'`. Default: `False`\nConstraints:\ndtype:bool\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.gt(input,other,out=None)\nParameter description: out(Tensor, optional) - the output tensor that must be a BoolTensor\nConstraints:\ndtype:booltensor\nstructure:tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.empty_like(input,dtype=None,layout=None,device=None,requires_grad=False,memory_format=torch.preserve_format)\nParameter description: dtype(`torch.dtype`, optional) - the desired data type of returned Tensor. Default: if `None`, defaults to the dtype of `input`.\nConstraints:\ndtype:torch.dtype\nstructure:Null\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.nn.RNN(*args,**kwargs,input_size=None,hidden_size=None,num_layers=None,nonlinearity=None,bias=None,batch_first=None,dropout=None,bidirectional=None)\nParameter description: hidden_size - The number of features in the hidden state h\nConstraints:\ndtype:int\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:[0,inf)\n\n###\n\nSignature: torch.nn.functional.avg_pool1d(input,kernel_size,stride=None,padding=0,ceil_mode=False,count_include_pad=True)\nParameter description: padding - implicit zero paddings on both sides of the input. Can be a single number or a tuple (padW,). Default: 0\nConstraints:\ndtype:bool;int\nstructure:tuple\nshape:Null\nndim:0;1\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.autograd.grad(outputs,inputs,grad_outputs=None,retain_graph=None,create_graph=False,only_inputs=True,allow_unused=False)\nParameter description: outputs(sequence of Tensor) - outputs of the differentiated function.\nConstraints:\ndtype:Null\nstructure:sequence;tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.repeat_interleave(input,repeats,dim=None)\nParameter description: dim(int, optional) - The dimension along which to repeat values.By default, use the flattened input array, and return a flat outputarray.\nConstraints:\ndtype:int\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.nn.NLLLoss(weight=None,size_average=None,ignore_index=-100,reduce=None,reduction=mean)\nParameter description: reduction(string, optional) - Specifies the reduction to apply to the output: `'none'` | `'mean'` | `'sum'`. `'none'`: no reduction will be applied, `'mean'`: the sum of the output will be divided by the number of elements in the output, `'sum'`: the output will be summed. Note: `size_average` and `reduce` are in the process of being deprecated, and in the meantime, specifying either of those two args will override `reduction`. Default: `'mean'`\nConstraints:",
            "constr": {
                "dtype": [
                    "string"
                ],
                "structure": [],
                "shape": [],
                "ndim": [],
                "enum": [
                    "none",
                    "mean",
                    "sum"
                ],
                "range": []
            },
            "signature": "torch.nn.NLLLoss(weight=None,size_average=None,ignore_index=-100,reduce=None,reduction=mean)",
            "descp": "reduction(string, optional) - Specifies the reduction to apply to the output: `'none'` | `'mean'` | `'sum'`. `'none'`: no reduction will be applied, `'mean'`: the sum of the output will be divided by the number of elements in the output, `'sum'`: the output will be summed. Note: `size_average` and `reduce` are in the process of being deprecated, and in the meantime, specifying either of those two args will override `reduction`. Default: `'mean'`"
        },
        {
            "fname": "torch.triu.yaml",
            "arg": "diagonal",
            "prompt": "Signature: torch.distributed.recv(tensor,src=None,group=<objectobject>,tag=0)\nParameter description: tensor(Tensor) - Tensor to fill with received data.\nConstraints:\ndtype:Null\nstructure:tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.div(input,other,out=None)\nParameter description: out(Tensor, optional) - the output tensor.\nConstraints:\ndtype:Null\nstructure:tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.nn.TransformerDecoder(decoder_layer,num_layers,norm=None)\nParameter description: num_layers - the number of sub-decoder-layers in the decoder (required).\nConstraints:\ndtype:int\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:[0,inf)\n\n###\n\nSignature: torch.nn.quantized.functional.interpolate(input,size=None,scale_factor=None,mode=nearest,align_corners=None)\nParameter description: align_corners(bool, optional) - Geometrically, we consider the pixels of the input and output as squares rather than points. If set to `True`, the input and output tensors are aligned by the center points of their corner pixels, preserving the values at the corner pixels. If set to `False`, the input and output tensors are aligned by the corner points of their corner pixels, and the interpolation uses edge value padding for out-of-boundary values, making this operation independent of input size when `scale_factor` is kept the same. This only has an effect when `mode` is `'bilinear'`. Default: `False`\nConstraints:\ndtype:bool\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.gt(input,other,out=None)\nParameter description: out(Tensor, optional) - the output tensor that must be a BoolTensor\nConstraints:\ndtype:booltensor\nstructure:tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.empty_like(input,dtype=None,layout=None,device=None,requires_grad=False,memory_format=torch.preserve_format)\nParameter description: dtype(`torch.dtype`, optional) - the desired data type of returned Tensor. Default: if `None`, defaults to the dtype of `input`.\nConstraints:\ndtype:torch.dtype\nstructure:Null\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.nn.RNN(*args,**kwargs,input_size=None,hidden_size=None,num_layers=None,nonlinearity=None,bias=None,batch_first=None,dropout=None,bidirectional=None)\nParameter description: hidden_size - The number of features in the hidden state h\nConstraints:\ndtype:int\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:[0,inf)\n\n###\n\nSignature: torch.nn.functional.avg_pool1d(input,kernel_size,stride=None,padding=0,ceil_mode=False,count_include_pad=True)\nParameter description: padding - implicit zero paddings on both sides of the input. Can be a single number or a tuple (padW,). Default: 0\nConstraints:\ndtype:bool;int\nstructure:tuple\nshape:Null\nndim:0;1\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.autograd.grad(outputs,inputs,grad_outputs=None,retain_graph=None,create_graph=False,only_inputs=True,allow_unused=False)\nParameter description: outputs(sequence of Tensor) - outputs of the differentiated function.\nConstraints:\ndtype:Null\nstructure:sequence;tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.repeat_interleave(input,repeats,dim=None)\nParameter description: dim(int, optional) - The dimension along which to repeat values.By default, use the flattened input array, and return a flat outputarray.\nConstraints:\ndtype:int\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.triu(input,diagonal=0,out=None)\nParameter description: diagonal(int, optional) - the diagonal to consider\nConstraints:",
            "constr": {
                "dtype": [
                    "int"
                ],
                "structure": [],
                "shape": [],
                "ndim": [
                    "0"
                ],
                "enum": [],
                "range": []
            },
            "signature": "torch.triu(input,diagonal=0,out=None)",
            "descp": "diagonal(int, optional) - the diagonal to consider"
        },
        {
            "fname": "torch.zeros.yaml",
            "arg": "*size",
            "prompt": "Signature: torch.distributed.recv(tensor,src=None,group=<objectobject>,tag=0)\nParameter description: tensor(Tensor) - Tensor to fill with received data.\nConstraints:\ndtype:Null\nstructure:tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.div(input,other,out=None)\nParameter description: out(Tensor, optional) - the output tensor.\nConstraints:\ndtype:Null\nstructure:tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.nn.TransformerDecoder(decoder_layer,num_layers,norm=None)\nParameter description: num_layers - the number of sub-decoder-layers in the decoder (required).\nConstraints:\ndtype:int\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:[0,inf)\n\n###\n\nSignature: torch.nn.quantized.functional.interpolate(input,size=None,scale_factor=None,mode=nearest,align_corners=None)\nParameter description: align_corners(bool, optional) - Geometrically, we consider the pixels of the input and output as squares rather than points. If set to `True`, the input and output tensors are aligned by the center points of their corner pixels, preserving the values at the corner pixels. If set to `False`, the input and output tensors are aligned by the corner points of their corner pixels, and the interpolation uses edge value padding for out-of-boundary values, making this operation independent of input size when `scale_factor` is kept the same. This only has an effect when `mode` is `'bilinear'`. Default: `False`\nConstraints:\ndtype:bool\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.gt(input,other,out=None)\nParameter description: out(Tensor, optional) - the output tensor that must be a BoolTensor\nConstraints:\ndtype:booltensor\nstructure:tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.empty_like(input,dtype=None,layout=None,device=None,requires_grad=False,memory_format=torch.preserve_format)\nParameter description: dtype(`torch.dtype`, optional) - the desired data type of returned Tensor. Default: if `None`, defaults to the dtype of `input`.\nConstraints:\ndtype:torch.dtype\nstructure:Null\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.nn.RNN(*args,**kwargs,input_size=None,hidden_size=None,num_layers=None,nonlinearity=None,bias=None,batch_first=None,dropout=None,bidirectional=None)\nParameter description: hidden_size - The number of features in the hidden state h\nConstraints:\ndtype:int\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:[0,inf)\n\n###\n\nSignature: torch.nn.functional.avg_pool1d(input,kernel_size,stride=None,padding=0,ceil_mode=False,count_include_pad=True)\nParameter description: padding - implicit zero paddings on both sides of the input. Can be a single number or a tuple (padW,). Default: 0\nConstraints:\ndtype:bool;int\nstructure:tuple\nshape:Null\nndim:0;1\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.autograd.grad(outputs,inputs,grad_outputs=None,retain_graph=None,create_graph=False,only_inputs=True,allow_unused=False)\nParameter description: outputs(sequence of Tensor) - outputs of the differentiated function.\nConstraints:\ndtype:Null\nstructure:sequence;tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.repeat_interleave(input,repeats,dim=None)\nParameter description: dim(int, optional) - The dimension along which to repeat values.By default, use the flattened input array, and return a flat outputarray.\nConstraints:\ndtype:int\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.zeros(*size,out=None,dtype=None,layout=torch.strided,device=None,requires_grad=False)\nParameter description: *size(int...) - a sequence of integers defining the shape of the output tensor. Can be a variable number of arguments or a collection like a list or tuple.\nConstraints:",
            "constr": {
                "dtype": [
                    "integer",
                    "int"
                ],
                "structure": [
                    "list",
                    "sequence",
                    "tuple"
                ],
                "shape": [],
                "ndim": [
                    "0",
                    "1"
                ],
                "enum": [],
                "range": [
                    "[0,inf)"
                ]
            },
            "signature": "torch.zeros(*size,out=None,dtype=None,layout=torch.strided,device=None,requires_grad=False)",
            "descp": "*size(int...) - a sequence of integers defining the shape of the output tensor. Can be a variable number of arguments or a collection like a list or tuple."
        },
        {
            "fname": "torch.ormqr.yaml",
            "arg": "input2",
            "prompt": "Signature: torch.distributed.recv(tensor,src=None,group=<objectobject>,tag=0)\nParameter description: tensor(Tensor) - Tensor to fill with received data.\nConstraints:\ndtype:Null\nstructure:tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.div(input,other,out=None)\nParameter description: out(Tensor, optional) - the output tensor.\nConstraints:\ndtype:Null\nstructure:tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.nn.TransformerDecoder(decoder_layer,num_layers,norm=None)\nParameter description: num_layers - the number of sub-decoder-layers in the decoder (required).\nConstraints:\ndtype:int\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:[0,inf)\n\n###\n\nSignature: torch.nn.quantized.functional.interpolate(input,size=None,scale_factor=None,mode=nearest,align_corners=None)\nParameter description: align_corners(bool, optional) - Geometrically, we consider the pixels of the input and output as squares rather than points. If set to `True`, the input and output tensors are aligned by the center points of their corner pixels, preserving the values at the corner pixels. If set to `False`, the input and output tensors are aligned by the corner points of their corner pixels, and the interpolation uses edge value padding for out-of-boundary values, making this operation independent of input size when `scale_factor` is kept the same. This only has an effect when `mode` is `'bilinear'`. Default: `False`\nConstraints:\ndtype:bool\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.gt(input,other,out=None)\nParameter description: out(Tensor, optional) - the output tensor that must be a BoolTensor\nConstraints:\ndtype:booltensor\nstructure:tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.empty_like(input,dtype=None,layout=None,device=None,requires_grad=False,memory_format=torch.preserve_format)\nParameter description: dtype(`torch.dtype`, optional) - the desired data type of returned Tensor. Default: if `None`, defaults to the dtype of `input`.\nConstraints:\ndtype:torch.dtype\nstructure:Null\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.nn.RNN(*args,**kwargs,input_size=None,hidden_size=None,num_layers=None,nonlinearity=None,bias=None,batch_first=None,dropout=None,bidirectional=None)\nParameter description: hidden_size - The number of features in the hidden state h\nConstraints:\ndtype:int\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:[0,inf)\n\n###\n\nSignature: torch.nn.functional.avg_pool1d(input,kernel_size,stride=None,padding=0,ceil_mode=False,count_include_pad=True)\nParameter description: padding - implicit zero paddings on both sides of the input. Can be a single number or a tuple (padW,). Default: 0\nConstraints:\ndtype:bool;int\nstructure:tuple\nshape:Null\nndim:0;1\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.autograd.grad(outputs,inputs,grad_outputs=None,retain_graph=None,create_graph=False,only_inputs=True,allow_unused=False)\nParameter description: outputs(sequence of Tensor) - outputs of the differentiated function.\nConstraints:\ndtype:Null\nstructure:sequence;tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.repeat_interleave(input,repeats,dim=None)\nParameter description: dim(int, optional) - The dimension along which to repeat values.By default, use the flattened input array, and return a flat outputarray.\nConstraints:\ndtype:int\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.ormqr(input,input2,input3,left=True,transpose=False)\nParameter description: input2(Tensor) - the tau from `torch.geqrf()`.\nConstraints:",
            "constr": {
                "dtype": [],
                "structure": [
                    "tensor"
                ],
                "shape": [],
                "ndim": [],
                "enum": [],
                "range": []
            },
            "signature": "torch.ormqr(input,input2,input3,left=True,transpose=False)",
            "descp": "input2(Tensor) - the tau from `torch.geqrf()`."
        },
        {
            "fname": "torch.nn.maxpool2d.yaml",
            "arg": "return_indices",
            "prompt": "Signature: torch.distributed.recv(tensor,src=None,group=<objectobject>,tag=0)\nParameter description: tensor(Tensor) - Tensor to fill with received data.\nConstraints:\ndtype:Null\nstructure:tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.div(input,other,out=None)\nParameter description: out(Tensor, optional) - the output tensor.\nConstraints:\ndtype:Null\nstructure:tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.nn.TransformerDecoder(decoder_layer,num_layers,norm=None)\nParameter description: num_layers - the number of sub-decoder-layers in the decoder (required).\nConstraints:\ndtype:int\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:[0,inf)\n\n###\n\nSignature: torch.nn.quantized.functional.interpolate(input,size=None,scale_factor=None,mode=nearest,align_corners=None)\nParameter description: align_corners(bool, optional) - Geometrically, we consider the pixels of the input and output as squares rather than points. If set to `True`, the input and output tensors are aligned by the center points of their corner pixels, preserving the values at the corner pixels. If set to `False`, the input and output tensors are aligned by the corner points of their corner pixels, and the interpolation uses edge value padding for out-of-boundary values, making this operation independent of input size when `scale_factor` is kept the same. This only has an effect when `mode` is `'bilinear'`. Default: `False`\nConstraints:\ndtype:bool\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.gt(input,other,out=None)\nParameter description: out(Tensor, optional) - the output tensor that must be a BoolTensor\nConstraints:\ndtype:booltensor\nstructure:tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.empty_like(input,dtype=None,layout=None,device=None,requires_grad=False,memory_format=torch.preserve_format)\nParameter description: dtype(`torch.dtype`, optional) - the desired data type of returned Tensor. Default: if `None`, defaults to the dtype of `input`.\nConstraints:\ndtype:torch.dtype\nstructure:Null\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.nn.RNN(*args,**kwargs,input_size=None,hidden_size=None,num_layers=None,nonlinearity=None,bias=None,batch_first=None,dropout=None,bidirectional=None)\nParameter description: hidden_size - The number of features in the hidden state h\nConstraints:\ndtype:int\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:[0,inf)\n\n###\n\nSignature: torch.nn.functional.avg_pool1d(input,kernel_size,stride=None,padding=0,ceil_mode=False,count_include_pad=True)\nParameter description: padding - implicit zero paddings on both sides of the input. Can be a single number or a tuple (padW,). Default: 0\nConstraints:\ndtype:bool;int\nstructure:tuple\nshape:Null\nndim:0;1\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.autograd.grad(outputs,inputs,grad_outputs=None,retain_graph=None,create_graph=False,only_inputs=True,allow_unused=False)\nParameter description: outputs(sequence of Tensor) - outputs of the differentiated function.\nConstraints:\ndtype:Null\nstructure:sequence;tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.repeat_interleave(input,repeats,dim=None)\nParameter description: dim(int, optional) - The dimension along which to repeat values.By default, use the flattened input array, and return a flat outputarray.\nConstraints:\ndtype:int\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.nn.MaxPool2d(kernel_size,stride=None,padding=0,dilation=1,return_indices=False,ceil_mode=False)\nParameter description: return_indices - if `True`, will return the max indices along with the outputs. Useful for `torch.nn.MaxUnpool2d` later\nConstraints:",
            "constr": {
                "dtype": [
                    "bool"
                ],
                "structure": [],
                "shape": [],
                "ndim": [
                    "0"
                ],
                "enum": [],
                "range": []
            },
            "signature": "torch.nn.MaxPool2d(kernel_size,stride=None,padding=0,dilation=1,return_indices=False,ceil_mode=False)",
            "descp": "return_indices - if `True`, will return the max indices along with the outputs. Useful for `torch.nn.MaxUnpool2d` later"
        },
        {
            "fname": "torch.square.yaml",
            "arg": "input",
            "prompt": "Signature: torch.distributed.recv(tensor,src=None,group=<objectobject>,tag=0)\nParameter description: tensor(Tensor) - Tensor to fill with received data.\nConstraints:\ndtype:Null\nstructure:tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.div(input,other,out=None)\nParameter description: out(Tensor, optional) - the output tensor.\nConstraints:\ndtype:Null\nstructure:tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.nn.TransformerDecoder(decoder_layer,num_layers,norm=None)\nParameter description: num_layers - the number of sub-decoder-layers in the decoder (required).\nConstraints:\ndtype:int\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:[0,inf)\n\n###\n\nSignature: torch.nn.quantized.functional.interpolate(input,size=None,scale_factor=None,mode=nearest,align_corners=None)\nParameter description: align_corners(bool, optional) - Geometrically, we consider the pixels of the input and output as squares rather than points. If set to `True`, the input and output tensors are aligned by the center points of their corner pixels, preserving the values at the corner pixels. If set to `False`, the input and output tensors are aligned by the corner points of their corner pixels, and the interpolation uses edge value padding for out-of-boundary values, making this operation independent of input size when `scale_factor` is kept the same. This only has an effect when `mode` is `'bilinear'`. Default: `False`\nConstraints:\ndtype:bool\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.gt(input,other,out=None)\nParameter description: out(Tensor, optional) - the output tensor that must be a BoolTensor\nConstraints:\ndtype:booltensor\nstructure:tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.empty_like(input,dtype=None,layout=None,device=None,requires_grad=False,memory_format=torch.preserve_format)\nParameter description: dtype(`torch.dtype`, optional) - the desired data type of returned Tensor. Default: if `None`, defaults to the dtype of `input`.\nConstraints:\ndtype:torch.dtype\nstructure:Null\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.nn.RNN(*args,**kwargs,input_size=None,hidden_size=None,num_layers=None,nonlinearity=None,bias=None,batch_first=None,dropout=None,bidirectional=None)\nParameter description: hidden_size - The number of features in the hidden state h\nConstraints:\ndtype:int\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:[0,inf)\n\n###\n\nSignature: torch.nn.functional.avg_pool1d(input,kernel_size,stride=None,padding=0,ceil_mode=False,count_include_pad=True)\nParameter description: padding - implicit zero paddings on both sides of the input. Can be a single number or a tuple (padW,). Default: 0\nConstraints:\ndtype:bool;int\nstructure:tuple\nshape:Null\nndim:0;1\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.autograd.grad(outputs,inputs,grad_outputs=None,retain_graph=None,create_graph=False,only_inputs=True,allow_unused=False)\nParameter description: outputs(sequence of Tensor) - outputs of the differentiated function.\nConstraints:\ndtype:Null\nstructure:sequence;tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.repeat_interleave(input,repeats,dim=None)\nParameter description: dim(int, optional) - The dimension along which to repeat values.By default, use the flattened input array, and return a flat outputarray.\nConstraints:\ndtype:int\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.square(input,out=None)\nParameter description: input(Tensor) - the input tensor.\nConstraints:",
            "constr": {
                "dtype": [],
                "structure": [
                    "tensor"
                ],
                "shape": [],
                "ndim": [],
                "enum": [],
                "range": []
            },
            "signature": "torch.square(input,out=None)",
            "descp": "input(Tensor) - the input tensor."
        },
        {
            "fname": "torch.can_cast.yaml",
            "arg": "from",
            "prompt": "Signature: torch.distributed.recv(tensor,src=None,group=<objectobject>,tag=0)\nParameter description: tensor(Tensor) - Tensor to fill with received data.\nConstraints:\ndtype:Null\nstructure:tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.div(input,other,out=None)\nParameter description: out(Tensor, optional) - the output tensor.\nConstraints:\ndtype:Null\nstructure:tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.nn.TransformerDecoder(decoder_layer,num_layers,norm=None)\nParameter description: num_layers - the number of sub-decoder-layers in the decoder (required).\nConstraints:\ndtype:int\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:[0,inf)\n\n###\n\nSignature: torch.nn.quantized.functional.interpolate(input,size=None,scale_factor=None,mode=nearest,align_corners=None)\nParameter description: align_corners(bool, optional) - Geometrically, we consider the pixels of the input and output as squares rather than points. If set to `True`, the input and output tensors are aligned by the center points of their corner pixels, preserving the values at the corner pixels. If set to `False`, the input and output tensors are aligned by the corner points of their corner pixels, and the interpolation uses edge value padding for out-of-boundary values, making this operation independent of input size when `scale_factor` is kept the same. This only has an effect when `mode` is `'bilinear'`. Default: `False`\nConstraints:\ndtype:bool\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.gt(input,other,out=None)\nParameter description: out(Tensor, optional) - the output tensor that must be a BoolTensor\nConstraints:\ndtype:booltensor\nstructure:tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.empty_like(input,dtype=None,layout=None,device=None,requires_grad=False,memory_format=torch.preserve_format)\nParameter description: dtype(`torch.dtype`, optional) - the desired data type of returned Tensor. Default: if `None`, defaults to the dtype of `input`.\nConstraints:\ndtype:torch.dtype\nstructure:Null\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.nn.RNN(*args,**kwargs,input_size=None,hidden_size=None,num_layers=None,nonlinearity=None,bias=None,batch_first=None,dropout=None,bidirectional=None)\nParameter description: hidden_size - The number of features in the hidden state h\nConstraints:\ndtype:int\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:[0,inf)\n\n###\n\nSignature: torch.nn.functional.avg_pool1d(input,kernel_size,stride=None,padding=0,ceil_mode=False,count_include_pad=True)\nParameter description: padding - implicit zero paddings on both sides of the input. Can be a single number or a tuple (padW,). Default: 0\nConstraints:\ndtype:bool;int\nstructure:tuple\nshape:Null\nndim:0;1\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.autograd.grad(outputs,inputs,grad_outputs=None,retain_graph=None,create_graph=False,only_inputs=True,allow_unused=False)\nParameter description: outputs(sequence of Tensor) - outputs of the differentiated function.\nConstraints:\ndtype:Null\nstructure:sequence;tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.repeat_interleave(input,repeats,dim=None)\nParameter description: dim(int, optional) - The dimension along which to repeat values.By default, use the flattened input array, and return a flat outputarray.\nConstraints:\ndtype:int\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.can_cast(from,to)\nParameter description: from(dpython:type) - The original `torch.dtype`.\nConstraints:",
            "constr": {
                "dtype": [
                    "torch.dtype"
                ],
                "structure": [],
                "shape": [],
                "ndim": [],
                "enum": [],
                "range": []
            },
            "signature": "torch.can_cast(from,to)",
            "descp": "from(dpython:type) - The original `torch.dtype`."
        },
        {
            "fname": "torch.mode.yaml",
            "arg": "out",
            "prompt": "Signature: torch.distributed.recv(tensor,src=None,group=<objectobject>,tag=0)\nParameter description: tensor(Tensor) - Tensor to fill with received data.\nConstraints:\ndtype:Null\nstructure:tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.div(input,other,out=None)\nParameter description: out(Tensor, optional) - the output tensor.\nConstraints:\ndtype:Null\nstructure:tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.nn.TransformerDecoder(decoder_layer,num_layers,norm=None)\nParameter description: num_layers - the number of sub-decoder-layers in the decoder (required).\nConstraints:\ndtype:int\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:[0,inf)\n\n###\n\nSignature: torch.nn.quantized.functional.interpolate(input,size=None,scale_factor=None,mode=nearest,align_corners=None)\nParameter description: align_corners(bool, optional) - Geometrically, we consider the pixels of the input and output as squares rather than points. If set to `True`, the input and output tensors are aligned by the center points of their corner pixels, preserving the values at the corner pixels. If set to `False`, the input and output tensors are aligned by the corner points of their corner pixels, and the interpolation uses edge value padding for out-of-boundary values, making this operation independent of input size when `scale_factor` is kept the same. This only has an effect when `mode` is `'bilinear'`. Default: `False`\nConstraints:\ndtype:bool\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.gt(input,other,out=None)\nParameter description: out(Tensor, optional) - the output tensor that must be a BoolTensor\nConstraints:\ndtype:booltensor\nstructure:tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.empty_like(input,dtype=None,layout=None,device=None,requires_grad=False,memory_format=torch.preserve_format)\nParameter description: dtype(`torch.dtype`, optional) - the desired data type of returned Tensor. Default: if `None`, defaults to the dtype of `input`.\nConstraints:\ndtype:torch.dtype\nstructure:Null\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.nn.RNN(*args,**kwargs,input_size=None,hidden_size=None,num_layers=None,nonlinearity=None,bias=None,batch_first=None,dropout=None,bidirectional=None)\nParameter description: hidden_size - The number of features in the hidden state h\nConstraints:\ndtype:int\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:[0,inf)\n\n###\n\nSignature: torch.nn.functional.avg_pool1d(input,kernel_size,stride=None,padding=0,ceil_mode=False,count_include_pad=True)\nParameter description: padding - implicit zero paddings on both sides of the input. Can be a single number or a tuple (padW,). Default: 0\nConstraints:\ndtype:bool;int\nstructure:tuple\nshape:Null\nndim:0;1\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.autograd.grad(outputs,inputs,grad_outputs=None,retain_graph=None,create_graph=False,only_inputs=True,allow_unused=False)\nParameter description: outputs(sequence of Tensor) - outputs of the differentiated function.\nConstraints:\ndtype:Null\nstructure:sequence;tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.repeat_interleave(input,repeats,dim=None)\nParameter description: dim(int, optional) - The dimension along which to repeat values.By default, use the flattened input array, and return a flat outputarray.\nConstraints:\ndtype:int\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.mode(input,dim=-1,keepdim=False,out=None)\nParameter description: out(tuple, optional) - the result tuple of two output tensors (values, indices)\nConstraints:",
            "constr": {
                "dtype": [],
                "structure": [
                    "tuple",
                    "tensor"
                ],
                "shape": [],
                "ndim": [],
                "enum": [],
                "range": []
            },
            "signature": "torch.mode(input,dim=-1,keepdim=False,out=None)",
            "descp": "out(tuple, optional) - the result tuple of two output tensors (values, indices)"
        },
        {
            "fname": "torch.baddbmm.yaml",
            "arg": "beta",
            "prompt": "Signature: torch.distributed.recv(tensor,src=None,group=<objectobject>,tag=0)\nParameter description: tensor(Tensor) - Tensor to fill with received data.\nConstraints:\ndtype:Null\nstructure:tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.div(input,other,out=None)\nParameter description: out(Tensor, optional) - the output tensor.\nConstraints:\ndtype:Null\nstructure:tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.nn.TransformerDecoder(decoder_layer,num_layers,norm=None)\nParameter description: num_layers - the number of sub-decoder-layers in the decoder (required).\nConstraints:\ndtype:int\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:[0,inf)\n\n###\n\nSignature: torch.nn.quantized.functional.interpolate(input,size=None,scale_factor=None,mode=nearest,align_corners=None)\nParameter description: align_corners(bool, optional) - Geometrically, we consider the pixels of the input and output as squares rather than points. If set to `True`, the input and output tensors are aligned by the center points of their corner pixels, preserving the values at the corner pixels. If set to `False`, the input and output tensors are aligned by the corner points of their corner pixels, and the interpolation uses edge value padding for out-of-boundary values, making this operation independent of input size when `scale_factor` is kept the same. This only has an effect when `mode` is `'bilinear'`. Default: `False`\nConstraints:\ndtype:bool\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.gt(input,other,out=None)\nParameter description: out(Tensor, optional) - the output tensor that must be a BoolTensor\nConstraints:\ndtype:booltensor\nstructure:tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.empty_like(input,dtype=None,layout=None,device=None,requires_grad=False,memory_format=torch.preserve_format)\nParameter description: dtype(`torch.dtype`, optional) - the desired data type of returned Tensor. Default: if `None`, defaults to the dtype of `input`.\nConstraints:\ndtype:torch.dtype\nstructure:Null\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.nn.RNN(*args,**kwargs,input_size=None,hidden_size=None,num_layers=None,nonlinearity=None,bias=None,batch_first=None,dropout=None,bidirectional=None)\nParameter description: hidden_size - The number of features in the hidden state h\nConstraints:\ndtype:int\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:[0,inf)\n\n###\n\nSignature: torch.nn.functional.avg_pool1d(input,kernel_size,stride=None,padding=0,ceil_mode=False,count_include_pad=True)\nParameter description: padding - implicit zero paddings on both sides of the input. Can be a single number or a tuple (padW,). Default: 0\nConstraints:\ndtype:bool;int\nstructure:tuple\nshape:Null\nndim:0;1\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.autograd.grad(outputs,inputs,grad_outputs=None,retain_graph=None,create_graph=False,only_inputs=True,allow_unused=False)\nParameter description: outputs(sequence of Tensor) - outputs of the differentiated function.\nConstraints:\ndtype:Null\nstructure:sequence;tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.repeat_interleave(input,repeats,dim=None)\nParameter description: dim(int, optional) - The dimension along which to repeat values.By default, use the flattened input array, and return a flat outputarray.\nConstraints:\ndtype:int\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.baddbmm(input,batch1,batch2,beta=1,alpha=1,out=None)\nParameter description: beta(Number, optional) - multiplier for `input` ( beta )\nConstraints:",
            "constr": {
                "dtype": [
                    "numeric",
                    "int"
                ],
                "structure": [],
                "shape": [],
                "ndim": [
                    "0"
                ],
                "enum": [],
                "range": []
            },
            "signature": "torch.baddbmm(input,batch1,batch2,beta=1,alpha=1,out=None)",
            "descp": "beta(Number, optional) - multiplier for `input` ( beta )"
        },
        {
            "fname": "torch.var2.yaml",
            "arg": "unbiased",
            "prompt": "Signature: torch.distributed.recv(tensor,src=None,group=<objectobject>,tag=0)\nParameter description: tensor(Tensor) - Tensor to fill with received data.\nConstraints:\ndtype:Null\nstructure:tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.div(input,other,out=None)\nParameter description: out(Tensor, optional) - the output tensor.\nConstraints:\ndtype:Null\nstructure:tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.nn.TransformerDecoder(decoder_layer,num_layers,norm=None)\nParameter description: num_layers - the number of sub-decoder-layers in the decoder (required).\nConstraints:\ndtype:int\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:[0,inf)\n\n###\n\nSignature: torch.nn.quantized.functional.interpolate(input,size=None,scale_factor=None,mode=nearest,align_corners=None)\nParameter description: align_corners(bool, optional) - Geometrically, we consider the pixels of the input and output as squares rather than points. If set to `True`, the input and output tensors are aligned by the center points of their corner pixels, preserving the values at the corner pixels. If set to `False`, the input and output tensors are aligned by the corner points of their corner pixels, and the interpolation uses edge value padding for out-of-boundary values, making this operation independent of input size when `scale_factor` is kept the same. This only has an effect when `mode` is `'bilinear'`. Default: `False`\nConstraints:\ndtype:bool\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.gt(input,other,out=None)\nParameter description: out(Tensor, optional) - the output tensor that must be a BoolTensor\nConstraints:\ndtype:booltensor\nstructure:tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.empty_like(input,dtype=None,layout=None,device=None,requires_grad=False,memory_format=torch.preserve_format)\nParameter description: dtype(`torch.dtype`, optional) - the desired data type of returned Tensor. Default: if `None`, defaults to the dtype of `input`.\nConstraints:\ndtype:torch.dtype\nstructure:Null\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.nn.RNN(*args,**kwargs,input_size=None,hidden_size=None,num_layers=None,nonlinearity=None,bias=None,batch_first=None,dropout=None,bidirectional=None)\nParameter description: hidden_size - The number of features in the hidden state h\nConstraints:\ndtype:int\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:[0,inf)\n\n###\n\nSignature: torch.nn.functional.avg_pool1d(input,kernel_size,stride=None,padding=0,ceil_mode=False,count_include_pad=True)\nParameter description: padding - implicit zero paddings on both sides of the input. Can be a single number or a tuple (padW,). Default: 0\nConstraints:\ndtype:bool;int\nstructure:tuple\nshape:Null\nndim:0;1\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.autograd.grad(outputs,inputs,grad_outputs=None,retain_graph=None,create_graph=False,only_inputs=True,allow_unused=False)\nParameter description: outputs(sequence of Tensor) - outputs of the differentiated function.\nConstraints:\ndtype:Null\nstructure:sequence;tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.repeat_interleave(input,repeats,dim=None)\nParameter description: dim(int, optional) - The dimension along which to repeat values.By default, use the flattened input array, and return a flat outputarray.\nConstraints:\ndtype:int\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.var(input,dim,keepdim=False,unbiased=True,out=None)\nParameter description: unbiased(bool) - whether to use the unbiased estimation or not\nConstraints:",
            "constr": {
                "dtype": [
                    "bool"
                ],
                "structure": [],
                "shape": [],
                "ndim": [
                    "0"
                ],
                "enum": [],
                "range": []
            },
            "signature": "torch.var(input,dim,keepdim=False,unbiased=True,out=None)",
            "descp": "unbiased(bool) - whether to use the unbiased estimation or not"
        },
        {
            "fname": "torch.nn.utils.prune.random_unstructured.yaml",
            "arg": "amount",
            "prompt": "Signature: torch.distributed.recv(tensor,src=None,group=<objectobject>,tag=0)\nParameter description: tensor(Tensor) - Tensor to fill with received data.\nConstraints:\ndtype:Null\nstructure:tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.div(input,other,out=None)\nParameter description: out(Tensor, optional) - the output tensor.\nConstraints:\ndtype:Null\nstructure:tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.nn.TransformerDecoder(decoder_layer,num_layers,norm=None)\nParameter description: num_layers - the number of sub-decoder-layers in the decoder (required).\nConstraints:\ndtype:int\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:[0,inf)\n\n###\n\nSignature: torch.nn.quantized.functional.interpolate(input,size=None,scale_factor=None,mode=nearest,align_corners=None)\nParameter description: align_corners(bool, optional) - Geometrically, we consider the pixels of the input and output as squares rather than points. If set to `True`, the input and output tensors are aligned by the center points of their corner pixels, preserving the values at the corner pixels. If set to `False`, the input and output tensors are aligned by the corner points of their corner pixels, and the interpolation uses edge value padding for out-of-boundary values, making this operation independent of input size when `scale_factor` is kept the same. This only has an effect when `mode` is `'bilinear'`. Default: `False`\nConstraints:\ndtype:bool\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.gt(input,other,out=None)\nParameter description: out(Tensor, optional) - the output tensor that must be a BoolTensor\nConstraints:\ndtype:booltensor\nstructure:tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.empty_like(input,dtype=None,layout=None,device=None,requires_grad=False,memory_format=torch.preserve_format)\nParameter description: dtype(`torch.dtype`, optional) - the desired data type of returned Tensor. Default: if `None`, defaults to the dtype of `input`.\nConstraints:\ndtype:torch.dtype\nstructure:Null\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.nn.RNN(*args,**kwargs,input_size=None,hidden_size=None,num_layers=None,nonlinearity=None,bias=None,batch_first=None,dropout=None,bidirectional=None)\nParameter description: hidden_size - The number of features in the hidden state h\nConstraints:\ndtype:int\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:[0,inf)\n\n###\n\nSignature: torch.nn.functional.avg_pool1d(input,kernel_size,stride=None,padding=0,ceil_mode=False,count_include_pad=True)\nParameter description: padding - implicit zero paddings on both sides of the input. Can be a single number or a tuple (padW,). Default: 0\nConstraints:\ndtype:bool;int\nstructure:tuple\nshape:Null\nndim:0;1\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.autograd.grad(outputs,inputs,grad_outputs=None,retain_graph=None,create_graph=False,only_inputs=True,allow_unused=False)\nParameter description: outputs(sequence of Tensor) - outputs of the differentiated function.\nConstraints:\ndtype:Null\nstructure:sequence;tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.repeat_interleave(input,repeats,dim=None)\nParameter description: dim(int, optional) - The dimension along which to repeat values.By default, use the flattened input array, and return a flat outputarray.\nConstraints:\ndtype:int\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.nn.utils.prune.random_unstructured(module,name,amount)\nParameter description: amount(int or float) - quantity of parameters to prune. If `float`, should be between 0.0 and 1.0 and represent the fraction of parameters to prune. If `int`, it represents the absolute number of parameters to prune.\nConstraints:",
            "constr": {
                "dtype": [
                    "float",
                    "int"
                ],
                "structure": [],
                "shape": [],
                "ndim": [
                    "0"
                ],
                "enum": [],
                "range": [
                    "[0,inf)"
                ]
            },
            "signature": "torch.nn.utils.prune.random_unstructured(module,name,amount)",
            "descp": "amount(int or float) - quantity of parameters to prune. If `float`, should be between 0.0 and 1.0 and represent the fraction of parameters to prune. If `int`, it represents the absolute number of parameters to prune."
        },
        {
            "fname": "torch.log10.yaml",
            "arg": "input",
            "prompt": "Signature: torch.distributed.recv(tensor,src=None,group=<objectobject>,tag=0)\nParameter description: tensor(Tensor) - Tensor to fill with received data.\nConstraints:\ndtype:Null\nstructure:tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.div(input,other,out=None)\nParameter description: out(Tensor, optional) - the output tensor.\nConstraints:\ndtype:Null\nstructure:tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.nn.TransformerDecoder(decoder_layer,num_layers,norm=None)\nParameter description: num_layers - the number of sub-decoder-layers in the decoder (required).\nConstraints:\ndtype:int\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:[0,inf)\n\n###\n\nSignature: torch.nn.quantized.functional.interpolate(input,size=None,scale_factor=None,mode=nearest,align_corners=None)\nParameter description: align_corners(bool, optional) - Geometrically, we consider the pixels of the input and output as squares rather than points. If set to `True`, the input and output tensors are aligned by the center points of their corner pixels, preserving the values at the corner pixels. If set to `False`, the input and output tensors are aligned by the corner points of their corner pixels, and the interpolation uses edge value padding for out-of-boundary values, making this operation independent of input size when `scale_factor` is kept the same. This only has an effect when `mode` is `'bilinear'`. Default: `False`\nConstraints:\ndtype:bool\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.gt(input,other,out=None)\nParameter description: out(Tensor, optional) - the output tensor that must be a BoolTensor\nConstraints:\ndtype:booltensor\nstructure:tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.empty_like(input,dtype=None,layout=None,device=None,requires_grad=False,memory_format=torch.preserve_format)\nParameter description: dtype(`torch.dtype`, optional) - the desired data type of returned Tensor. Default: if `None`, defaults to the dtype of `input`.\nConstraints:\ndtype:torch.dtype\nstructure:Null\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.nn.RNN(*args,**kwargs,input_size=None,hidden_size=None,num_layers=None,nonlinearity=None,bias=None,batch_first=None,dropout=None,bidirectional=None)\nParameter description: hidden_size - The number of features in the hidden state h\nConstraints:\ndtype:int\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:[0,inf)\n\n###\n\nSignature: torch.nn.functional.avg_pool1d(input,kernel_size,stride=None,padding=0,ceil_mode=False,count_include_pad=True)\nParameter description: padding - implicit zero paddings on both sides of the input. Can be a single number or a tuple (padW,). Default: 0\nConstraints:\ndtype:bool;int\nstructure:tuple\nshape:Null\nndim:0;1\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.autograd.grad(outputs,inputs,grad_outputs=None,retain_graph=None,create_graph=False,only_inputs=True,allow_unused=False)\nParameter description: outputs(sequence of Tensor) - outputs of the differentiated function.\nConstraints:\ndtype:Null\nstructure:sequence;tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.repeat_interleave(input,repeats,dim=None)\nParameter description: dim(int, optional) - The dimension along which to repeat values.By default, use the flattened input array, and return a flat outputarray.\nConstraints:\ndtype:int\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.log10(input,out=None)\nParameter description: input(Tensor) - the input tensor.\nConstraints:",
            "constr": {
                "dtype": [],
                "structure": [
                    "tensor"
                ],
                "shape": [],
                "ndim": [],
                "enum": [],
                "range": []
            },
            "signature": "torch.log10(input,out=None)",
            "descp": "input(Tensor) - the input tensor."
        },
        {
            "fname": "torch.reshape.yaml",
            "arg": "input",
            "prompt": "Signature: torch.distributed.recv(tensor,src=None,group=<objectobject>,tag=0)\nParameter description: tensor(Tensor) - Tensor to fill with received data.\nConstraints:\ndtype:Null\nstructure:tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.div(input,other,out=None)\nParameter description: out(Tensor, optional) - the output tensor.\nConstraints:\ndtype:Null\nstructure:tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.nn.TransformerDecoder(decoder_layer,num_layers,norm=None)\nParameter description: num_layers - the number of sub-decoder-layers in the decoder (required).\nConstraints:\ndtype:int\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:[0,inf)\n\n###\n\nSignature: torch.nn.quantized.functional.interpolate(input,size=None,scale_factor=None,mode=nearest,align_corners=None)\nParameter description: align_corners(bool, optional) - Geometrically, we consider the pixels of the input and output as squares rather than points. If set to `True`, the input and output tensors are aligned by the center points of their corner pixels, preserving the values at the corner pixels. If set to `False`, the input and output tensors are aligned by the corner points of their corner pixels, and the interpolation uses edge value padding for out-of-boundary values, making this operation independent of input size when `scale_factor` is kept the same. This only has an effect when `mode` is `'bilinear'`. Default: `False`\nConstraints:\ndtype:bool\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.gt(input,other,out=None)\nParameter description: out(Tensor, optional) - the output tensor that must be a BoolTensor\nConstraints:\ndtype:booltensor\nstructure:tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.empty_like(input,dtype=None,layout=None,device=None,requires_grad=False,memory_format=torch.preserve_format)\nParameter description: dtype(`torch.dtype`, optional) - the desired data type of returned Tensor. Default: if `None`, defaults to the dtype of `input`.\nConstraints:\ndtype:torch.dtype\nstructure:Null\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.nn.RNN(*args,**kwargs,input_size=None,hidden_size=None,num_layers=None,nonlinearity=None,bias=None,batch_first=None,dropout=None,bidirectional=None)\nParameter description: hidden_size - The number of features in the hidden state h\nConstraints:\ndtype:int\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:[0,inf)\n\n###\n\nSignature: torch.nn.functional.avg_pool1d(input,kernel_size,stride=None,padding=0,ceil_mode=False,count_include_pad=True)\nParameter description: padding - implicit zero paddings on both sides of the input. Can be a single number or a tuple (padW,). Default: 0\nConstraints:\ndtype:bool;int\nstructure:tuple\nshape:Null\nndim:0;1\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.autograd.grad(outputs,inputs,grad_outputs=None,retain_graph=None,create_graph=False,only_inputs=True,allow_unused=False)\nParameter description: outputs(sequence of Tensor) - outputs of the differentiated function.\nConstraints:\ndtype:Null\nstructure:sequence;tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.repeat_interleave(input,repeats,dim=None)\nParameter description: dim(int, optional) - The dimension along which to repeat values.By default, use the flattened input array, and return a flat outputarray.\nConstraints:\ndtype:int\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.reshape(input,shape)\nParameter description: input(Tensor) - the tensor to be reshaped\nConstraints:",
            "constr": {
                "dtype": [],
                "structure": [
                    "tensor"
                ],
                "shape": [],
                "ndim": [],
                "enum": [],
                "range": []
            },
            "signature": "torch.reshape(input,shape)",
            "descp": "input(Tensor) - the tensor to be reshaped"
        },
        {
            "fname": "torch.transpose.yaml",
            "arg": "dim1",
            "prompt": "Signature: torch.distributed.recv(tensor,src=None,group=<objectobject>,tag=0)\nParameter description: tensor(Tensor) - Tensor to fill with received data.\nConstraints:\ndtype:Null\nstructure:tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.div(input,other,out=None)\nParameter description: out(Tensor, optional) - the output tensor.\nConstraints:\ndtype:Null\nstructure:tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.nn.TransformerDecoder(decoder_layer,num_layers,norm=None)\nParameter description: num_layers - the number of sub-decoder-layers in the decoder (required).\nConstraints:\ndtype:int\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:[0,inf)\n\n###\n\nSignature: torch.nn.quantized.functional.interpolate(input,size=None,scale_factor=None,mode=nearest,align_corners=None)\nParameter description: align_corners(bool, optional) - Geometrically, we consider the pixels of the input and output as squares rather than points. If set to `True`, the input and output tensors are aligned by the center points of their corner pixels, preserving the values at the corner pixels. If set to `False`, the input and output tensors are aligned by the corner points of their corner pixels, and the interpolation uses edge value padding for out-of-boundary values, making this operation independent of input size when `scale_factor` is kept the same. This only has an effect when `mode` is `'bilinear'`. Default: `False`\nConstraints:\ndtype:bool\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.gt(input,other,out=None)\nParameter description: out(Tensor, optional) - the output tensor that must be a BoolTensor\nConstraints:\ndtype:booltensor\nstructure:tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.empty_like(input,dtype=None,layout=None,device=None,requires_grad=False,memory_format=torch.preserve_format)\nParameter description: dtype(`torch.dtype`, optional) - the desired data type of returned Tensor. Default: if `None`, defaults to the dtype of `input`.\nConstraints:\ndtype:torch.dtype\nstructure:Null\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.nn.RNN(*args,**kwargs,input_size=None,hidden_size=None,num_layers=None,nonlinearity=None,bias=None,batch_first=None,dropout=None,bidirectional=None)\nParameter description: hidden_size - The number of features in the hidden state h\nConstraints:\ndtype:int\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:[0,inf)\n\n###\n\nSignature: torch.nn.functional.avg_pool1d(input,kernel_size,stride=None,padding=0,ceil_mode=False,count_include_pad=True)\nParameter description: padding - implicit zero paddings on both sides of the input. Can be a single number or a tuple (padW,). Default: 0\nConstraints:\ndtype:bool;int\nstructure:tuple\nshape:Null\nndim:0;1\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.autograd.grad(outputs,inputs,grad_outputs=None,retain_graph=None,create_graph=False,only_inputs=True,allow_unused=False)\nParameter description: outputs(sequence of Tensor) - outputs of the differentiated function.\nConstraints:\ndtype:Null\nstructure:sequence;tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.repeat_interleave(input,repeats,dim=None)\nParameter description: dim(int, optional) - The dimension along which to repeat values.By default, use the flattened input array, and return a flat outputarray.\nConstraints:\ndtype:int\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.transpose(input,dim0,dim1)\nParameter description: dim1(int) - the second dimension to be transposed\nConstraints:",
            "constr": {
                "dtype": [
                    "int"
                ],
                "structure": [],
                "shape": [],
                "ndim": [
                    "0"
                ],
                "enum": [],
                "range": []
            },
            "signature": "torch.transpose(input,dim0,dim1)",
            "descp": "dim1(int) - the second dimension to be transposed"
        },
        {
            "fname": "torch.nn.crossentropyloss.yaml",
            "arg": "reduction",
            "prompt": "Signature: torch.distributed.recv(tensor,src=None,group=<objectobject>,tag=0)\nParameter description: tensor(Tensor) - Tensor to fill with received data.\nConstraints:\ndtype:Null\nstructure:tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.div(input,other,out=None)\nParameter description: out(Tensor, optional) - the output tensor.\nConstraints:\ndtype:Null\nstructure:tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.nn.TransformerDecoder(decoder_layer,num_layers,norm=None)\nParameter description: num_layers - the number of sub-decoder-layers in the decoder (required).\nConstraints:\ndtype:int\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:[0,inf)\n\n###\n\nSignature: torch.nn.quantized.functional.interpolate(input,size=None,scale_factor=None,mode=nearest,align_corners=None)\nParameter description: align_corners(bool, optional) - Geometrically, we consider the pixels of the input and output as squares rather than points. If set to `True`, the input and output tensors are aligned by the center points of their corner pixels, preserving the values at the corner pixels. If set to `False`, the input and output tensors are aligned by the corner points of their corner pixels, and the interpolation uses edge value padding for out-of-boundary values, making this operation independent of input size when `scale_factor` is kept the same. This only has an effect when `mode` is `'bilinear'`. Default: `False`\nConstraints:\ndtype:bool\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.gt(input,other,out=None)\nParameter description: out(Tensor, optional) - the output tensor that must be a BoolTensor\nConstraints:\ndtype:booltensor\nstructure:tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.empty_like(input,dtype=None,layout=None,device=None,requires_grad=False,memory_format=torch.preserve_format)\nParameter description: dtype(`torch.dtype`, optional) - the desired data type of returned Tensor. Default: if `None`, defaults to the dtype of `input`.\nConstraints:\ndtype:torch.dtype\nstructure:Null\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.nn.RNN(*args,**kwargs,input_size=None,hidden_size=None,num_layers=None,nonlinearity=None,bias=None,batch_first=None,dropout=None,bidirectional=None)\nParameter description: hidden_size - The number of features in the hidden state h\nConstraints:\ndtype:int\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:[0,inf)\n\n###\n\nSignature: torch.nn.functional.avg_pool1d(input,kernel_size,stride=None,padding=0,ceil_mode=False,count_include_pad=True)\nParameter description: padding - implicit zero paddings on both sides of the input. Can be a single number or a tuple (padW,). Default: 0\nConstraints:\ndtype:bool;int\nstructure:tuple\nshape:Null\nndim:0;1\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.autograd.grad(outputs,inputs,grad_outputs=None,retain_graph=None,create_graph=False,only_inputs=True,allow_unused=False)\nParameter description: outputs(sequence of Tensor) - outputs of the differentiated function.\nConstraints:\ndtype:Null\nstructure:sequence;tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.repeat_interleave(input,repeats,dim=None)\nParameter description: dim(int, optional) - The dimension along which to repeat values.By default, use the flattened input array, and return a flat outputarray.\nConstraints:\ndtype:int\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.nn.CrossEntropyLoss(weight=None,size_average=None,ignore_index=-100,reduce=None,reduction=mean)\nParameter description: reduction(string, optional) - Specifies the reduction to apply to the output: `'none'` | `'mean'` | `'sum'`. `'none'`: no reduction will be applied, `'mean'`: the sum of the output will be divided by the number of elements in the output, `'sum'`: the output will be summed. Note: `size_average` and `reduce` are in the process of being deprecated, and in the meantime, specifying either of those two args will override `reduction`. Default: `'mean'`\nConstraints:",
            "constr": {
                "dtype": [
                    "string"
                ],
                "structure": [],
                "shape": [],
                "ndim": [],
                "enum": [
                    "none",
                    "mean",
                    "sum"
                ],
                "range": []
            },
            "signature": "torch.nn.CrossEntropyLoss(weight=None,size_average=None,ignore_index=-100,reduce=None,reduction=mean)",
            "descp": "reduction(string, optional) - Specifies the reduction to apply to the output: `'none'` | `'mean'` | `'sum'`. `'none'`: no reduction will be applied, `'mean'`: the sum of the output will be divided by the number of elements in the output, `'sum'`: the output will be summed. Note: `size_average` and `reduce` are in the process of being deprecated, and in the meantime, specifying either of those two args will override `reduction`. Default: `'mean'`"
        },
        {
            "fname": "torch.nn.rrelu.yaml",
            "arg": "inplace",
            "prompt": "Signature: torch.distributed.recv(tensor,src=None,group=<objectobject>,tag=0)\nParameter description: tensor(Tensor) - Tensor to fill with received data.\nConstraints:\ndtype:Null\nstructure:tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.div(input,other,out=None)\nParameter description: out(Tensor, optional) - the output tensor.\nConstraints:\ndtype:Null\nstructure:tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.nn.TransformerDecoder(decoder_layer,num_layers,norm=None)\nParameter description: num_layers - the number of sub-decoder-layers in the decoder (required).\nConstraints:\ndtype:int\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:[0,inf)\n\n###\n\nSignature: torch.nn.quantized.functional.interpolate(input,size=None,scale_factor=None,mode=nearest,align_corners=None)\nParameter description: align_corners(bool, optional) - Geometrically, we consider the pixels of the input and output as squares rather than points. If set to `True`, the input and output tensors are aligned by the center points of their corner pixels, preserving the values at the corner pixels. If set to `False`, the input and output tensors are aligned by the corner points of their corner pixels, and the interpolation uses edge value padding for out-of-boundary values, making this operation independent of input size when `scale_factor` is kept the same. This only has an effect when `mode` is `'bilinear'`. Default: `False`\nConstraints:\ndtype:bool\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.gt(input,other,out=None)\nParameter description: out(Tensor, optional) - the output tensor that must be a BoolTensor\nConstraints:\ndtype:booltensor\nstructure:tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.empty_like(input,dtype=None,layout=None,device=None,requires_grad=False,memory_format=torch.preserve_format)\nParameter description: dtype(`torch.dtype`, optional) - the desired data type of returned Tensor. Default: if `None`, defaults to the dtype of `input`.\nConstraints:\ndtype:torch.dtype\nstructure:Null\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.nn.RNN(*args,**kwargs,input_size=None,hidden_size=None,num_layers=None,nonlinearity=None,bias=None,batch_first=None,dropout=None,bidirectional=None)\nParameter description: hidden_size - The number of features in the hidden state h\nConstraints:\ndtype:int\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:[0,inf)\n\n###\n\nSignature: torch.nn.functional.avg_pool1d(input,kernel_size,stride=None,padding=0,ceil_mode=False,count_include_pad=True)\nParameter description: padding - implicit zero paddings on both sides of the input. Can be a single number or a tuple (padW,). Default: 0\nConstraints:\ndtype:bool;int\nstructure:tuple\nshape:Null\nndim:0;1\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.autograd.grad(outputs,inputs,grad_outputs=None,retain_graph=None,create_graph=False,only_inputs=True,allow_unused=False)\nParameter description: outputs(sequence of Tensor) - outputs of the differentiated function.\nConstraints:\ndtype:Null\nstructure:sequence;tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.repeat_interleave(input,repeats,dim=None)\nParameter description: dim(int, optional) - The dimension along which to repeat values.By default, use the flattened input array, and return a flat outputarray.\nConstraints:\ndtype:int\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.nn.RReLU(lower=0.125,upper=0.3333333333333333,inplace=False)\nParameter description: inplace - can optionally do the operation in-place. Default: `False`\nConstraints:",
            "constr": {
                "dtype": [
                    "bool"
                ],
                "structure": [],
                "shape": [],
                "ndim": [
                    "0"
                ],
                "enum": [],
                "range": []
            },
            "signature": "torch.nn.RReLU(lower=0.125,upper=0.3333333333333333,inplace=False)",
            "descp": "inplace - can optionally do the operation in-place. Default: `False`"
        },
        {
            "fname": "torch.solve.yaml",
            "arg": "A",
            "prompt": "Signature: torch.distributed.recv(tensor,src=None,group=<objectobject>,tag=0)\nParameter description: tensor(Tensor) - Tensor to fill with received data.\nConstraints:\ndtype:Null\nstructure:tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.div(input,other,out=None)\nParameter description: out(Tensor, optional) - the output tensor.\nConstraints:\ndtype:Null\nstructure:tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.nn.TransformerDecoder(decoder_layer,num_layers,norm=None)\nParameter description: num_layers - the number of sub-decoder-layers in the decoder (required).\nConstraints:\ndtype:int\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:[0,inf)\n\n###\n\nSignature: torch.nn.quantized.functional.interpolate(input,size=None,scale_factor=None,mode=nearest,align_corners=None)\nParameter description: align_corners(bool, optional) - Geometrically, we consider the pixels of the input and output as squares rather than points. If set to `True`, the input and output tensors are aligned by the center points of their corner pixels, preserving the values at the corner pixels. If set to `False`, the input and output tensors are aligned by the corner points of their corner pixels, and the interpolation uses edge value padding for out-of-boundary values, making this operation independent of input size when `scale_factor` is kept the same. This only has an effect when `mode` is `'bilinear'`. Default: `False`\nConstraints:\ndtype:bool\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.gt(input,other,out=None)\nParameter description: out(Tensor, optional) - the output tensor that must be a BoolTensor\nConstraints:\ndtype:booltensor\nstructure:tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.empty_like(input,dtype=None,layout=None,device=None,requires_grad=False,memory_format=torch.preserve_format)\nParameter description: dtype(`torch.dtype`, optional) - the desired data type of returned Tensor. Default: if `None`, defaults to the dtype of `input`.\nConstraints:\ndtype:torch.dtype\nstructure:Null\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.nn.RNN(*args,**kwargs,input_size=None,hidden_size=None,num_layers=None,nonlinearity=None,bias=None,batch_first=None,dropout=None,bidirectional=None)\nParameter description: hidden_size - The number of features in the hidden state h\nConstraints:\ndtype:int\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:[0,inf)\n\n###\n\nSignature: torch.nn.functional.avg_pool1d(input,kernel_size,stride=None,padding=0,ceil_mode=False,count_include_pad=True)\nParameter description: padding - implicit zero paddings on both sides of the input. Can be a single number or a tuple (padW,). Default: 0\nConstraints:\ndtype:bool;int\nstructure:tuple\nshape:Null\nndim:0;1\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.autograd.grad(outputs,inputs,grad_outputs=None,retain_graph=None,create_graph=False,only_inputs=True,allow_unused=False)\nParameter description: outputs(sequence of Tensor) - outputs of the differentiated function.\nConstraints:\ndtype:Null\nstructure:sequence;tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.repeat_interleave(input,repeats,dim=None)\nParameter description: dim(int, optional) - The dimension along which to repeat values.By default, use the flattened input array, and return a flat outputarray.\nConstraints:\ndtype:int\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.solve(input,A,out=None)\nParameter description: A(Tensor) - input square matrix of size (*, m, m) , where *  is zero or more batch dimensions.\nConstraints:",
            "constr": {
                "dtype": [
                    "numeric"
                ],
                "structure": [
                    "tensor"
                ],
                "shape": [
                    "(*, m, m)"
                ],
                "ndim": [],
                "enum": [],
                "range": []
            },
            "signature": "torch.solve(input,A,out=None)",
            "descp": "A(Tensor) - input square matrix of size (*, m, m) , where *  is zero or more batch dimensions."
        },
        {
            "fname": "torch.std_mean.yaml",
            "arg": "unbiased",
            "prompt": "Signature: torch.distributed.recv(tensor,src=None,group=<objectobject>,tag=0)\nParameter description: tensor(Tensor) - Tensor to fill with received data.\nConstraints:\ndtype:Null\nstructure:tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.div(input,other,out=None)\nParameter description: out(Tensor, optional) - the output tensor.\nConstraints:\ndtype:Null\nstructure:tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.nn.TransformerDecoder(decoder_layer,num_layers,norm=None)\nParameter description: num_layers - the number of sub-decoder-layers in the decoder (required).\nConstraints:\ndtype:int\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:[0,inf)\n\n###\n\nSignature: torch.nn.quantized.functional.interpolate(input,size=None,scale_factor=None,mode=nearest,align_corners=None)\nParameter description: align_corners(bool, optional) - Geometrically, we consider the pixels of the input and output as squares rather than points. If set to `True`, the input and output tensors are aligned by the center points of their corner pixels, preserving the values at the corner pixels. If set to `False`, the input and output tensors are aligned by the corner points of their corner pixels, and the interpolation uses edge value padding for out-of-boundary values, making this operation independent of input size when `scale_factor` is kept the same. This only has an effect when `mode` is `'bilinear'`. Default: `False`\nConstraints:\ndtype:bool\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.gt(input,other,out=None)\nParameter description: out(Tensor, optional) - the output tensor that must be a BoolTensor\nConstraints:\ndtype:booltensor\nstructure:tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.empty_like(input,dtype=None,layout=None,device=None,requires_grad=False,memory_format=torch.preserve_format)\nParameter description: dtype(`torch.dtype`, optional) - the desired data type of returned Tensor. Default: if `None`, defaults to the dtype of `input`.\nConstraints:\ndtype:torch.dtype\nstructure:Null\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.nn.RNN(*args,**kwargs,input_size=None,hidden_size=None,num_layers=None,nonlinearity=None,bias=None,batch_first=None,dropout=None,bidirectional=None)\nParameter description: hidden_size - The number of features in the hidden state h\nConstraints:\ndtype:int\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:[0,inf)\n\n###\n\nSignature: torch.nn.functional.avg_pool1d(input,kernel_size,stride=None,padding=0,ceil_mode=False,count_include_pad=True)\nParameter description: padding - implicit zero paddings on both sides of the input. Can be a single number or a tuple (padW,). Default: 0\nConstraints:\ndtype:bool;int\nstructure:tuple\nshape:Null\nndim:0;1\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.autograd.grad(outputs,inputs,grad_outputs=None,retain_graph=None,create_graph=False,only_inputs=True,allow_unused=False)\nParameter description: outputs(sequence of Tensor) - outputs of the differentiated function.\nConstraints:\ndtype:Null\nstructure:sequence;tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.repeat_interleave(input,repeats,dim=None)\nParameter description: dim(int, optional) - The dimension along which to repeat values.By default, use the flattened input array, and return a flat outputarray.\nConstraints:\ndtype:int\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.std_mean(input,unbiased=True)\nParameter description: unbiased(bool) - whether to use the unbiased estimation or not\nConstraints:",
            "constr": {
                "dtype": [
                    "bool"
                ],
                "structure": [],
                "shape": [],
                "ndim": [
                    "0"
                ],
                "enum": [],
                "range": []
            },
            "signature": "torch.std_mean(input,unbiased=True)",
            "descp": "unbiased(bool) - whether to use the unbiased estimation or not"
        },
        {
            "fname": "torch.hub.load_state_dict_from_url.yaml",
            "arg": "url",
            "prompt": "Signature: torch.distributed.recv(tensor,src=None,group=<objectobject>,tag=0)\nParameter description: tensor(Tensor) - Tensor to fill with received data.\nConstraints:\ndtype:Null\nstructure:tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.div(input,other,out=None)\nParameter description: out(Tensor, optional) - the output tensor.\nConstraints:\ndtype:Null\nstructure:tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.nn.TransformerDecoder(decoder_layer,num_layers,norm=None)\nParameter description: num_layers - the number of sub-decoder-layers in the decoder (required).\nConstraints:\ndtype:int\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:[0,inf)\n\n###\n\nSignature: torch.nn.quantized.functional.interpolate(input,size=None,scale_factor=None,mode=nearest,align_corners=None)\nParameter description: align_corners(bool, optional) - Geometrically, we consider the pixels of the input and output as squares rather than points. If set to `True`, the input and output tensors are aligned by the center points of their corner pixels, preserving the values at the corner pixels. If set to `False`, the input and output tensors are aligned by the corner points of their corner pixels, and the interpolation uses edge value padding for out-of-boundary values, making this operation independent of input size when `scale_factor` is kept the same. This only has an effect when `mode` is `'bilinear'`. Default: `False`\nConstraints:\ndtype:bool\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.gt(input,other,out=None)\nParameter description: out(Tensor, optional) - the output tensor that must be a BoolTensor\nConstraints:\ndtype:booltensor\nstructure:tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.empty_like(input,dtype=None,layout=None,device=None,requires_grad=False,memory_format=torch.preserve_format)\nParameter description: dtype(`torch.dtype`, optional) - the desired data type of returned Tensor. Default: if `None`, defaults to the dtype of `input`.\nConstraints:\ndtype:torch.dtype\nstructure:Null\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.nn.RNN(*args,**kwargs,input_size=None,hidden_size=None,num_layers=None,nonlinearity=None,bias=None,batch_first=None,dropout=None,bidirectional=None)\nParameter description: hidden_size - The number of features in the hidden state h\nConstraints:\ndtype:int\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:[0,inf)\n\n###\n\nSignature: torch.nn.functional.avg_pool1d(input,kernel_size,stride=None,padding=0,ceil_mode=False,count_include_pad=True)\nParameter description: padding - implicit zero paddings on both sides of the input. Can be a single number or a tuple (padW,). Default: 0\nConstraints:\ndtype:bool;int\nstructure:tuple\nshape:Null\nndim:0;1\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.autograd.grad(outputs,inputs,grad_outputs=None,retain_graph=None,create_graph=False,only_inputs=True,allow_unused=False)\nParameter description: outputs(sequence of Tensor) - outputs of the differentiated function.\nConstraints:\ndtype:Null\nstructure:sequence;tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.repeat_interleave(input,repeats,dim=None)\nParameter description: dim(int, optional) - The dimension along which to repeat values.By default, use the flattened input array, and return a flat outputarray.\nConstraints:\ndtype:int\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.hub.load_state_dict_from_url(url,model_dir=None,map_location=None,progress=True,check_hash=False)\nParameter description: url(string) - URL of the object to download\nConstraints:",
            "constr": {
                "dtype": [
                    "string"
                ],
                "structure": [],
                "shape": [],
                "ndim": [],
                "enum": [],
                "range": []
            },
            "signature": "torch.hub.load_state_dict_from_url(url,model_dir=None,map_location=None,progress=True,check_hash=False)",
            "descp": "url(string) - URL of the object to download"
        },
        {
            "fname": "torch.lt.yaml",
            "arg": "other",
            "prompt": "Signature: torch.distributed.recv(tensor,src=None,group=<objectobject>,tag=0)\nParameter description: tensor(Tensor) - Tensor to fill with received data.\nConstraints:\ndtype:Null\nstructure:tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.div(input,other,out=None)\nParameter description: out(Tensor, optional) - the output tensor.\nConstraints:\ndtype:Null\nstructure:tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.nn.TransformerDecoder(decoder_layer,num_layers,norm=None)\nParameter description: num_layers - the number of sub-decoder-layers in the decoder (required).\nConstraints:\ndtype:int\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:[0,inf)\n\n###\n\nSignature: torch.nn.quantized.functional.interpolate(input,size=None,scale_factor=None,mode=nearest,align_corners=None)\nParameter description: align_corners(bool, optional) - Geometrically, we consider the pixels of the input and output as squares rather than points. If set to `True`, the input and output tensors are aligned by the center points of their corner pixels, preserving the values at the corner pixels. If set to `False`, the input and output tensors are aligned by the corner points of their corner pixels, and the interpolation uses edge value padding for out-of-boundary values, making this operation independent of input size when `scale_factor` is kept the same. This only has an effect when `mode` is `'bilinear'`. Default: `False`\nConstraints:\ndtype:bool\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.gt(input,other,out=None)\nParameter description: out(Tensor, optional) - the output tensor that must be a BoolTensor\nConstraints:\ndtype:booltensor\nstructure:tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.empty_like(input,dtype=None,layout=None,device=None,requires_grad=False,memory_format=torch.preserve_format)\nParameter description: dtype(`torch.dtype`, optional) - the desired data type of returned Tensor. Default: if `None`, defaults to the dtype of `input`.\nConstraints:\ndtype:torch.dtype\nstructure:Null\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.nn.RNN(*args,**kwargs,input_size=None,hidden_size=None,num_layers=None,nonlinearity=None,bias=None,batch_first=None,dropout=None,bidirectional=None)\nParameter description: hidden_size - The number of features in the hidden state h\nConstraints:\ndtype:int\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:[0,inf)\n\n###\n\nSignature: torch.nn.functional.avg_pool1d(input,kernel_size,stride=None,padding=0,ceil_mode=False,count_include_pad=True)\nParameter description: padding - implicit zero paddings on both sides of the input. Can be a single number or a tuple (padW,). Default: 0\nConstraints:\ndtype:bool;int\nstructure:tuple\nshape:Null\nndim:0;1\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.autograd.grad(outputs,inputs,grad_outputs=None,retain_graph=None,create_graph=False,only_inputs=True,allow_unused=False)\nParameter description: outputs(sequence of Tensor) - outputs of the differentiated function.\nConstraints:\ndtype:Null\nstructure:sequence;tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.repeat_interleave(input,repeats,dim=None)\nParameter description: dim(int, optional) - The dimension along which to repeat values.By default, use the flattened input array, and return a flat outputarray.\nConstraints:\ndtype:int\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.lt(input,other,out=None)\nParameter description: other(Tensor or float) - the tensor or value to compare\nConstraints:",
            "constr": {
                "dtype": [
                    "float"
                ],
                "structure": [
                    "tensor"
                ],
                "shape": [],
                "ndim": [],
                "enum": [],
                "range": []
            },
            "signature": "torch.lt(input,other,out=None)",
            "descp": "other(Tensor or float) - the tensor or value to compare"
        },
        {
            "fname": "torch.neg.yaml",
            "arg": "input",
            "prompt": "Signature: torch.distributed.recv(tensor,src=None,group=<objectobject>,tag=0)\nParameter description: tensor(Tensor) - Tensor to fill with received data.\nConstraints:\ndtype:Null\nstructure:tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.div(input,other,out=None)\nParameter description: out(Tensor, optional) - the output tensor.\nConstraints:\ndtype:Null\nstructure:tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.nn.TransformerDecoder(decoder_layer,num_layers,norm=None)\nParameter description: num_layers - the number of sub-decoder-layers in the decoder (required).\nConstraints:\ndtype:int\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:[0,inf)\n\n###\n\nSignature: torch.nn.quantized.functional.interpolate(input,size=None,scale_factor=None,mode=nearest,align_corners=None)\nParameter description: align_corners(bool, optional) - Geometrically, we consider the pixels of the input and output as squares rather than points. If set to `True`, the input and output tensors are aligned by the center points of their corner pixels, preserving the values at the corner pixels. If set to `False`, the input and output tensors are aligned by the corner points of their corner pixels, and the interpolation uses edge value padding for out-of-boundary values, making this operation independent of input size when `scale_factor` is kept the same. This only has an effect when `mode` is `'bilinear'`. Default: `False`\nConstraints:\ndtype:bool\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.gt(input,other,out=None)\nParameter description: out(Tensor, optional) - the output tensor that must be a BoolTensor\nConstraints:\ndtype:booltensor\nstructure:tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.empty_like(input,dtype=None,layout=None,device=None,requires_grad=False,memory_format=torch.preserve_format)\nParameter description: dtype(`torch.dtype`, optional) - the desired data type of returned Tensor. Default: if `None`, defaults to the dtype of `input`.\nConstraints:\ndtype:torch.dtype\nstructure:Null\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.nn.RNN(*args,**kwargs,input_size=None,hidden_size=None,num_layers=None,nonlinearity=None,bias=None,batch_first=None,dropout=None,bidirectional=None)\nParameter description: hidden_size - The number of features in the hidden state h\nConstraints:\ndtype:int\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:[0,inf)\n\n###\n\nSignature: torch.nn.functional.avg_pool1d(input,kernel_size,stride=None,padding=0,ceil_mode=False,count_include_pad=True)\nParameter description: padding - implicit zero paddings on both sides of the input. Can be a single number or a tuple (padW,). Default: 0\nConstraints:\ndtype:bool;int\nstructure:tuple\nshape:Null\nndim:0;1\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.autograd.grad(outputs,inputs,grad_outputs=None,retain_graph=None,create_graph=False,only_inputs=True,allow_unused=False)\nParameter description: outputs(sequence of Tensor) - outputs of the differentiated function.\nConstraints:\ndtype:Null\nstructure:sequence;tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.repeat_interleave(input,repeats,dim=None)\nParameter description: dim(int, optional) - The dimension along which to repeat values.By default, use the flattened input array, and return a flat outputarray.\nConstraints:\ndtype:int\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.neg(input,out=None)\nParameter description: input(Tensor) - the input tensor.\nConstraints:",
            "constr": {
                "dtype": [],
                "structure": [
                    "tensor"
                ],
                "shape": [],
                "ndim": [],
                "enum": [],
                "range": []
            },
            "signature": "torch.neg(input,out=None)",
            "descp": "input(Tensor) - the input tensor."
        },
        {
            "fname": "torch.combinations.yaml",
            "arg": "with_replacement",
            "prompt": "Signature: torch.distributed.recv(tensor,src=None,group=<objectobject>,tag=0)\nParameter description: tensor(Tensor) - Tensor to fill with received data.\nConstraints:\ndtype:Null\nstructure:tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.div(input,other,out=None)\nParameter description: out(Tensor, optional) - the output tensor.\nConstraints:\ndtype:Null\nstructure:tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.nn.TransformerDecoder(decoder_layer,num_layers,norm=None)\nParameter description: num_layers - the number of sub-decoder-layers in the decoder (required).\nConstraints:\ndtype:int\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:[0,inf)\n\n###\n\nSignature: torch.nn.quantized.functional.interpolate(input,size=None,scale_factor=None,mode=nearest,align_corners=None)\nParameter description: align_corners(bool, optional) - Geometrically, we consider the pixels of the input and output as squares rather than points. If set to `True`, the input and output tensors are aligned by the center points of their corner pixels, preserving the values at the corner pixels. If set to `False`, the input and output tensors are aligned by the corner points of their corner pixels, and the interpolation uses edge value padding for out-of-boundary values, making this operation independent of input size when `scale_factor` is kept the same. This only has an effect when `mode` is `'bilinear'`. Default: `False`\nConstraints:\ndtype:bool\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.gt(input,other,out=None)\nParameter description: out(Tensor, optional) - the output tensor that must be a BoolTensor\nConstraints:\ndtype:booltensor\nstructure:tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.empty_like(input,dtype=None,layout=None,device=None,requires_grad=False,memory_format=torch.preserve_format)\nParameter description: dtype(`torch.dtype`, optional) - the desired data type of returned Tensor. Default: if `None`, defaults to the dtype of `input`.\nConstraints:\ndtype:torch.dtype\nstructure:Null\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.nn.RNN(*args,**kwargs,input_size=None,hidden_size=None,num_layers=None,nonlinearity=None,bias=None,batch_first=None,dropout=None,bidirectional=None)\nParameter description: hidden_size - The number of features in the hidden state h\nConstraints:\ndtype:int\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:[0,inf)\n\n###\n\nSignature: torch.nn.functional.avg_pool1d(input,kernel_size,stride=None,padding=0,ceil_mode=False,count_include_pad=True)\nParameter description: padding - implicit zero paddings on both sides of the input. Can be a single number or a tuple (padW,). Default: 0\nConstraints:\ndtype:bool;int\nstructure:tuple\nshape:Null\nndim:0;1\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.autograd.grad(outputs,inputs,grad_outputs=None,retain_graph=None,create_graph=False,only_inputs=True,allow_unused=False)\nParameter description: outputs(sequence of Tensor) - outputs of the differentiated function.\nConstraints:\ndtype:Null\nstructure:sequence;tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.repeat_interleave(input,repeats,dim=None)\nParameter description: dim(int, optional) - The dimension along which to repeat values.By default, use the flattened input array, and return a flat outputarray.\nConstraints:\ndtype:int\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.combinations(input,r=2,with_replacement=False)\nParameter description: with_replacement(boolean, optional) - whether to allow duplication in combination\nConstraints:",
            "constr": {
                "dtype": [
                    "boolean",
                    "bool"
                ],
                "structure": [],
                "shape": [],
                "ndim": [
                    "0"
                ],
                "enum": [],
                "range": []
            },
            "signature": "torch.combinations(input,r=2,with_replacement=False)",
            "descp": "with_replacement(boolean, optional) - whether to allow duplication in combination"
        },
        {
            "fname": "torch.bincount.yaml",
            "arg": "weights",
            "prompt": "Signature: torch.distributed.recv(tensor,src=None,group=<objectobject>,tag=0)\nParameter description: tensor(Tensor) - Tensor to fill with received data.\nConstraints:\ndtype:Null\nstructure:tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.div(input,other,out=None)\nParameter description: out(Tensor, optional) - the output tensor.\nConstraints:\ndtype:Null\nstructure:tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.nn.TransformerDecoder(decoder_layer,num_layers,norm=None)\nParameter description: num_layers - the number of sub-decoder-layers in the decoder (required).\nConstraints:\ndtype:int\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:[0,inf)\n\n###\n\nSignature: torch.nn.quantized.functional.interpolate(input,size=None,scale_factor=None,mode=nearest,align_corners=None)\nParameter description: align_corners(bool, optional) - Geometrically, we consider the pixels of the input and output as squares rather than points. If set to `True`, the input and output tensors are aligned by the center points of their corner pixels, preserving the values at the corner pixels. If set to `False`, the input and output tensors are aligned by the corner points of their corner pixels, and the interpolation uses edge value padding for out-of-boundary values, making this operation independent of input size when `scale_factor` is kept the same. This only has an effect when `mode` is `'bilinear'`. Default: `False`\nConstraints:\ndtype:bool\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.gt(input,other,out=None)\nParameter description: out(Tensor, optional) - the output tensor that must be a BoolTensor\nConstraints:\ndtype:booltensor\nstructure:tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.empty_like(input,dtype=None,layout=None,device=None,requires_grad=False,memory_format=torch.preserve_format)\nParameter description: dtype(`torch.dtype`, optional) - the desired data type of returned Tensor. Default: if `None`, defaults to the dtype of `input`.\nConstraints:\ndtype:torch.dtype\nstructure:Null\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.nn.RNN(*args,**kwargs,input_size=None,hidden_size=None,num_layers=None,nonlinearity=None,bias=None,batch_first=None,dropout=None,bidirectional=None)\nParameter description: hidden_size - The number of features in the hidden state h\nConstraints:\ndtype:int\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:[0,inf)\n\n###\n\nSignature: torch.nn.functional.avg_pool1d(input,kernel_size,stride=None,padding=0,ceil_mode=False,count_include_pad=True)\nParameter description: padding - implicit zero paddings on both sides of the input. Can be a single number or a tuple (padW,). Default: 0\nConstraints:\ndtype:bool;int\nstructure:tuple\nshape:Null\nndim:0;1\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.autograd.grad(outputs,inputs,grad_outputs=None,retain_graph=None,create_graph=False,only_inputs=True,allow_unused=False)\nParameter description: outputs(sequence of Tensor) - outputs of the differentiated function.\nConstraints:\ndtype:Null\nstructure:sequence;tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.repeat_interleave(input,repeats,dim=None)\nParameter description: dim(int, optional) - The dimension along which to repeat values.By default, use the flattened input array, and return a flat outputarray.\nConstraints:\ndtype:int\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.bincount(input,weights=None,minlength=0)\nParameter description: weights(Tensor) - optional, weight for each value in the input tensor. Should be of same size as input tensor.\nConstraints:",
            "constr": {
                "dtype": [
                    "numeric"
                ],
                "structure": [
                    "tensor"
                ],
                "shape": [],
                "ndim": [],
                "enum": [],
                "range": []
            },
            "signature": "torch.bincount(input,weights=None,minlength=0)",
            "descp": "weights(Tensor) - optional, weight for each value in the input tensor. Should be of same size as input tensor."
        },
        {
            "fname": "torch.remainder.yaml",
            "arg": "out",
            "prompt": "Signature: torch.distributed.recv(tensor,src=None,group=<objectobject>,tag=0)\nParameter description: tensor(Tensor) - Tensor to fill with received data.\nConstraints:\ndtype:Null\nstructure:tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.div(input,other,out=None)\nParameter description: out(Tensor, optional) - the output tensor.\nConstraints:\ndtype:Null\nstructure:tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.nn.TransformerDecoder(decoder_layer,num_layers,norm=None)\nParameter description: num_layers - the number of sub-decoder-layers in the decoder (required).\nConstraints:\ndtype:int\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:[0,inf)\n\n###\n\nSignature: torch.nn.quantized.functional.interpolate(input,size=None,scale_factor=None,mode=nearest,align_corners=None)\nParameter description: align_corners(bool, optional) - Geometrically, we consider the pixels of the input and output as squares rather than points. If set to `True`, the input and output tensors are aligned by the center points of their corner pixels, preserving the values at the corner pixels. If set to `False`, the input and output tensors are aligned by the corner points of their corner pixels, and the interpolation uses edge value padding for out-of-boundary values, making this operation independent of input size when `scale_factor` is kept the same. This only has an effect when `mode` is `'bilinear'`. Default: `False`\nConstraints:\ndtype:bool\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.gt(input,other,out=None)\nParameter description: out(Tensor, optional) - the output tensor that must be a BoolTensor\nConstraints:\ndtype:booltensor\nstructure:tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.empty_like(input,dtype=None,layout=None,device=None,requires_grad=False,memory_format=torch.preserve_format)\nParameter description: dtype(`torch.dtype`, optional) - the desired data type of returned Tensor. Default: if `None`, defaults to the dtype of `input`.\nConstraints:\ndtype:torch.dtype\nstructure:Null\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.nn.RNN(*args,**kwargs,input_size=None,hidden_size=None,num_layers=None,nonlinearity=None,bias=None,batch_first=None,dropout=None,bidirectional=None)\nParameter description: hidden_size - The number of features in the hidden state h\nConstraints:\ndtype:int\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:[0,inf)\n\n###\n\nSignature: torch.nn.functional.avg_pool1d(input,kernel_size,stride=None,padding=0,ceil_mode=False,count_include_pad=True)\nParameter description: padding - implicit zero paddings on both sides of the input. Can be a single number or a tuple (padW,). Default: 0\nConstraints:\ndtype:bool;int\nstructure:tuple\nshape:Null\nndim:0;1\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.autograd.grad(outputs,inputs,grad_outputs=None,retain_graph=None,create_graph=False,only_inputs=True,allow_unused=False)\nParameter description: outputs(sequence of Tensor) - outputs of the differentiated function.\nConstraints:\ndtype:Null\nstructure:sequence;tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.repeat_interleave(input,repeats,dim=None)\nParameter description: dim(int, optional) - The dimension along which to repeat values.By default, use the flattened input array, and return a flat outputarray.\nConstraints:\ndtype:int\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.remainder(input,other,out=None)\nParameter description: out(Tensor, optional) - the output tensor.\nConstraints:",
            "constr": {
                "dtype": [],
                "structure": [
                    "tensor"
                ],
                "shape": [],
                "ndim": [],
                "enum": [],
                "range": []
            },
            "signature": "torch.remainder(input,other,out=None)",
            "descp": "out(Tensor, optional) - the output tensor."
        },
        {
            "fname": "torch.logical_or.yaml",
            "arg": "other",
            "prompt": "Signature: torch.distributed.recv(tensor,src=None,group=<objectobject>,tag=0)\nParameter description: tensor(Tensor) - Tensor to fill with received data.\nConstraints:\ndtype:Null\nstructure:tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.div(input,other,out=None)\nParameter description: out(Tensor, optional) - the output tensor.\nConstraints:\ndtype:Null\nstructure:tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.nn.TransformerDecoder(decoder_layer,num_layers,norm=None)\nParameter description: num_layers - the number of sub-decoder-layers in the decoder (required).\nConstraints:\ndtype:int\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:[0,inf)\n\n###\n\nSignature: torch.nn.quantized.functional.interpolate(input,size=None,scale_factor=None,mode=nearest,align_corners=None)\nParameter description: align_corners(bool, optional) - Geometrically, we consider the pixels of the input and output as squares rather than points. If set to `True`, the input and output tensors are aligned by the center points of their corner pixels, preserving the values at the corner pixels. If set to `False`, the input and output tensors are aligned by the corner points of their corner pixels, and the interpolation uses edge value padding for out-of-boundary values, making this operation independent of input size when `scale_factor` is kept the same. This only has an effect when `mode` is `'bilinear'`. Default: `False`\nConstraints:\ndtype:bool\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.gt(input,other,out=None)\nParameter description: out(Tensor, optional) - the output tensor that must be a BoolTensor\nConstraints:\ndtype:booltensor\nstructure:tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.empty_like(input,dtype=None,layout=None,device=None,requires_grad=False,memory_format=torch.preserve_format)\nParameter description: dtype(`torch.dtype`, optional) - the desired data type of returned Tensor. Default: if `None`, defaults to the dtype of `input`.\nConstraints:\ndtype:torch.dtype\nstructure:Null\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.nn.RNN(*args,**kwargs,input_size=None,hidden_size=None,num_layers=None,nonlinearity=None,bias=None,batch_first=None,dropout=None,bidirectional=None)\nParameter description: hidden_size - The number of features in the hidden state h\nConstraints:\ndtype:int\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:[0,inf)\n\n###\n\nSignature: torch.nn.functional.avg_pool1d(input,kernel_size,stride=None,padding=0,ceil_mode=False,count_include_pad=True)\nParameter description: padding - implicit zero paddings on both sides of the input. Can be a single number or a tuple (padW,). Default: 0\nConstraints:\ndtype:bool;int\nstructure:tuple\nshape:Null\nndim:0;1\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.autograd.grad(outputs,inputs,grad_outputs=None,retain_graph=None,create_graph=False,only_inputs=True,allow_unused=False)\nParameter description: outputs(sequence of Tensor) - outputs of the differentiated function.\nConstraints:\ndtype:Null\nstructure:sequence;tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.repeat_interleave(input,repeats,dim=None)\nParameter description: dim(int, optional) - The dimension along which to repeat values.By default, use the flattened input array, and return a flat outputarray.\nConstraints:\ndtype:int\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.logical_or(input,other,out=None)\nParameter description: other(Tensor) - the tensor to compute OR with\nConstraints:",
            "constr": {
                "dtype": [],
                "structure": [
                    "tensor"
                ],
                "shape": [],
                "ndim": [],
                "enum": [],
                "range": []
            },
            "signature": "torch.logical_or(input,other,out=None)",
            "descp": "other(Tensor) - the tensor to compute OR with"
        },
        {
            "fname": "torch.gather.yaml",
            "arg": "input",
            "prompt": "Signature: torch.distributed.recv(tensor,src=None,group=<objectobject>,tag=0)\nParameter description: tensor(Tensor) - Tensor to fill with received data.\nConstraints:\ndtype:Null\nstructure:tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.div(input,other,out=None)\nParameter description: out(Tensor, optional) - the output tensor.\nConstraints:\ndtype:Null\nstructure:tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.nn.TransformerDecoder(decoder_layer,num_layers,norm=None)\nParameter description: num_layers - the number of sub-decoder-layers in the decoder (required).\nConstraints:\ndtype:int\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:[0,inf)\n\n###\n\nSignature: torch.nn.quantized.functional.interpolate(input,size=None,scale_factor=None,mode=nearest,align_corners=None)\nParameter description: align_corners(bool, optional) - Geometrically, we consider the pixels of the input and output as squares rather than points. If set to `True`, the input and output tensors are aligned by the center points of their corner pixels, preserving the values at the corner pixels. If set to `False`, the input and output tensors are aligned by the corner points of their corner pixels, and the interpolation uses edge value padding for out-of-boundary values, making this operation independent of input size when `scale_factor` is kept the same. This only has an effect when `mode` is `'bilinear'`. Default: `False`\nConstraints:\ndtype:bool\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.gt(input,other,out=None)\nParameter description: out(Tensor, optional) - the output tensor that must be a BoolTensor\nConstraints:\ndtype:booltensor\nstructure:tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.empty_like(input,dtype=None,layout=None,device=None,requires_grad=False,memory_format=torch.preserve_format)\nParameter description: dtype(`torch.dtype`, optional) - the desired data type of returned Tensor. Default: if `None`, defaults to the dtype of `input`.\nConstraints:\ndtype:torch.dtype\nstructure:Null\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.nn.RNN(*args,**kwargs,input_size=None,hidden_size=None,num_layers=None,nonlinearity=None,bias=None,batch_first=None,dropout=None,bidirectional=None)\nParameter description: hidden_size - The number of features in the hidden state h\nConstraints:\ndtype:int\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:[0,inf)\n\n###\n\nSignature: torch.nn.functional.avg_pool1d(input,kernel_size,stride=None,padding=0,ceil_mode=False,count_include_pad=True)\nParameter description: padding - implicit zero paddings on both sides of the input. Can be a single number or a tuple (padW,). Default: 0\nConstraints:\ndtype:bool;int\nstructure:tuple\nshape:Null\nndim:0;1\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.autograd.grad(outputs,inputs,grad_outputs=None,retain_graph=None,create_graph=False,only_inputs=True,allow_unused=False)\nParameter description: outputs(sequence of Tensor) - outputs of the differentiated function.\nConstraints:\ndtype:Null\nstructure:sequence;tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.repeat_interleave(input,repeats,dim=None)\nParameter description: dim(int, optional) - The dimension along which to repeat values.By default, use the flattened input array, and return a flat outputarray.\nConstraints:\ndtype:int\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.gather(input,dim,index,out=None,sparse_grad=False)\nParameter description: input(Tensor) - the source tensor\nConstraints:",
            "constr": {
                "dtype": [],
                "structure": [
                    "tensor"
                ],
                "shape": [],
                "ndim": [],
                "enum": [],
                "range": []
            },
            "signature": "torch.gather(input,dim,index,out=None,sparse_grad=False)",
            "descp": "input(Tensor) - the source tensor"
        },
        {
            "fname": "torch.nn.syncbatchnorm.yaml",
            "arg": "track_running_stats",
            "prompt": "Signature: torch.distributed.recv(tensor,src=None,group=<objectobject>,tag=0)\nParameter description: tensor(Tensor) - Tensor to fill with received data.\nConstraints:\ndtype:Null\nstructure:tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.div(input,other,out=None)\nParameter description: out(Tensor, optional) - the output tensor.\nConstraints:\ndtype:Null\nstructure:tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.nn.TransformerDecoder(decoder_layer,num_layers,norm=None)\nParameter description: num_layers - the number of sub-decoder-layers in the decoder (required).\nConstraints:\ndtype:int\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:[0,inf)\n\n###\n\nSignature: torch.nn.quantized.functional.interpolate(input,size=None,scale_factor=None,mode=nearest,align_corners=None)\nParameter description: align_corners(bool, optional) - Geometrically, we consider the pixels of the input and output as squares rather than points. If set to `True`, the input and output tensors are aligned by the center points of their corner pixels, preserving the values at the corner pixels. If set to `False`, the input and output tensors are aligned by the corner points of their corner pixels, and the interpolation uses edge value padding for out-of-boundary values, making this operation independent of input size when `scale_factor` is kept the same. This only has an effect when `mode` is `'bilinear'`. Default: `False`\nConstraints:\ndtype:bool\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.gt(input,other,out=None)\nParameter description: out(Tensor, optional) - the output tensor that must be a BoolTensor\nConstraints:\ndtype:booltensor\nstructure:tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.empty_like(input,dtype=None,layout=None,device=None,requires_grad=False,memory_format=torch.preserve_format)\nParameter description: dtype(`torch.dtype`, optional) - the desired data type of returned Tensor. Default: if `None`, defaults to the dtype of `input`.\nConstraints:\ndtype:torch.dtype\nstructure:Null\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.nn.RNN(*args,**kwargs,input_size=None,hidden_size=None,num_layers=None,nonlinearity=None,bias=None,batch_first=None,dropout=None,bidirectional=None)\nParameter description: hidden_size - The number of features in the hidden state h\nConstraints:\ndtype:int\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:[0,inf)\n\n###\n\nSignature: torch.nn.functional.avg_pool1d(input,kernel_size,stride=None,padding=0,ceil_mode=False,count_include_pad=True)\nParameter description: padding - implicit zero paddings on both sides of the input. Can be a single number or a tuple (padW,). Default: 0\nConstraints:\ndtype:bool;int\nstructure:tuple\nshape:Null\nndim:0;1\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.autograd.grad(outputs,inputs,grad_outputs=None,retain_graph=None,create_graph=False,only_inputs=True,allow_unused=False)\nParameter description: outputs(sequence of Tensor) - outputs of the differentiated function.\nConstraints:\ndtype:Null\nstructure:sequence;tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.repeat_interleave(input,repeats,dim=None)\nParameter description: dim(int, optional) - The dimension along which to repeat values.By default, use the flattened input array, and return a flat outputarray.\nConstraints:\ndtype:int\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.nn.SyncBatchNorm(num_features,eps=1e-05,momentum=0.1,affine=True,track_running_stats=True,process_group=None)\nParameter description: track_running_stats - a boolean value that when set to `True`, this module tracks the running mean and variance, and when set to `False`, this module does not track such statistics and always uses batch statistics in both training and eval modes. Default: `True`\nConstraints:",
            "constr": {
                "dtype": [
                    "boolean",
                    "bool"
                ],
                "structure": [],
                "shape": [],
                "ndim": [
                    "0"
                ],
                "enum": [],
                "range": []
            },
            "signature": "torch.nn.SyncBatchNorm(num_features,eps=1e-05,momentum=0.1,affine=True,track_running_stats=True,process_group=None)",
            "descp": "track_running_stats - a boolean value that when set to `True`, this module tracks the running mean and variance, and when set to `False`, this module does not track such statistics and always uses batch statistics in both training and eval modes. Default: `True`"
        },
        {
            "fname": "torch.ones_like.yaml",
            "arg": "requires_grad",
            "prompt": "Signature: torch.distributed.recv(tensor,src=None,group=<objectobject>,tag=0)\nParameter description: tensor(Tensor) - Tensor to fill with received data.\nConstraints:\ndtype:Null\nstructure:tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.div(input,other,out=None)\nParameter description: out(Tensor, optional) - the output tensor.\nConstraints:\ndtype:Null\nstructure:tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.nn.TransformerDecoder(decoder_layer,num_layers,norm=None)\nParameter description: num_layers - the number of sub-decoder-layers in the decoder (required).\nConstraints:\ndtype:int\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:[0,inf)\n\n###\n\nSignature: torch.nn.quantized.functional.interpolate(input,size=None,scale_factor=None,mode=nearest,align_corners=None)\nParameter description: align_corners(bool, optional) - Geometrically, we consider the pixels of the input and output as squares rather than points. If set to `True`, the input and output tensors are aligned by the center points of their corner pixels, preserving the values at the corner pixels. If set to `False`, the input and output tensors are aligned by the corner points of their corner pixels, and the interpolation uses edge value padding for out-of-boundary values, making this operation independent of input size when `scale_factor` is kept the same. This only has an effect when `mode` is `'bilinear'`. Default: `False`\nConstraints:\ndtype:bool\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.gt(input,other,out=None)\nParameter description: out(Tensor, optional) - the output tensor that must be a BoolTensor\nConstraints:\ndtype:booltensor\nstructure:tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.empty_like(input,dtype=None,layout=None,device=None,requires_grad=False,memory_format=torch.preserve_format)\nParameter description: dtype(`torch.dtype`, optional) - the desired data type of returned Tensor. Default: if `None`, defaults to the dtype of `input`.\nConstraints:\ndtype:torch.dtype\nstructure:Null\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.nn.RNN(*args,**kwargs,input_size=None,hidden_size=None,num_layers=None,nonlinearity=None,bias=None,batch_first=None,dropout=None,bidirectional=None)\nParameter description: hidden_size - The number of features in the hidden state h\nConstraints:\ndtype:int\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:[0,inf)\n\n###\n\nSignature: torch.nn.functional.avg_pool1d(input,kernel_size,stride=None,padding=0,ceil_mode=False,count_include_pad=True)\nParameter description: padding - implicit zero paddings on both sides of the input. Can be a single number or a tuple (padW,). Default: 0\nConstraints:\ndtype:bool;int\nstructure:tuple\nshape:Null\nndim:0;1\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.autograd.grad(outputs,inputs,grad_outputs=None,retain_graph=None,create_graph=False,only_inputs=True,allow_unused=False)\nParameter description: outputs(sequence of Tensor) - outputs of the differentiated function.\nConstraints:\ndtype:Null\nstructure:sequence;tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.repeat_interleave(input,repeats,dim=None)\nParameter description: dim(int, optional) - The dimension along which to repeat values.By default, use the flattened input array, and return a flat outputarray.\nConstraints:\ndtype:int\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.ones_like(input,dtype=None,layout=None,device=None,requires_grad=False,memory_format=torch.preserve_format)\nParameter description: requires_grad(bool, optional) - If autograd should record operations on the returned tensor. Default: `False`.\nConstraints:",
            "constr": {
                "dtype": [
                    "bool"
                ],
                "structure": [],
                "shape": [],
                "ndim": [
                    "0"
                ],
                "enum": [],
                "range": []
            },
            "signature": "torch.ones_like(input,dtype=None,layout=None,device=None,requires_grad=False,memory_format=torch.preserve_format)",
            "descp": "requires_grad(bool, optional) - If autograd should record operations on the returned tensor. Default: `False`."
        },
        {
            "fname": "torch.logical_and.yaml",
            "arg": "other",
            "prompt": "Signature: torch.distributed.recv(tensor,src=None,group=<objectobject>,tag=0)\nParameter description: tensor(Tensor) - Tensor to fill with received data.\nConstraints:\ndtype:Null\nstructure:tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.div(input,other,out=None)\nParameter description: out(Tensor, optional) - the output tensor.\nConstraints:\ndtype:Null\nstructure:tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.nn.TransformerDecoder(decoder_layer,num_layers,norm=None)\nParameter description: num_layers - the number of sub-decoder-layers in the decoder (required).\nConstraints:\ndtype:int\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:[0,inf)\n\n###\n\nSignature: torch.nn.quantized.functional.interpolate(input,size=None,scale_factor=None,mode=nearest,align_corners=None)\nParameter description: align_corners(bool, optional) - Geometrically, we consider the pixels of the input and output as squares rather than points. If set to `True`, the input and output tensors are aligned by the center points of their corner pixels, preserving the values at the corner pixels. If set to `False`, the input and output tensors are aligned by the corner points of their corner pixels, and the interpolation uses edge value padding for out-of-boundary values, making this operation independent of input size when `scale_factor` is kept the same. This only has an effect when `mode` is `'bilinear'`. Default: `False`\nConstraints:\ndtype:bool\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.gt(input,other,out=None)\nParameter description: out(Tensor, optional) - the output tensor that must be a BoolTensor\nConstraints:\ndtype:booltensor\nstructure:tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.empty_like(input,dtype=None,layout=None,device=None,requires_grad=False,memory_format=torch.preserve_format)\nParameter description: dtype(`torch.dtype`, optional) - the desired data type of returned Tensor. Default: if `None`, defaults to the dtype of `input`.\nConstraints:\ndtype:torch.dtype\nstructure:Null\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.nn.RNN(*args,**kwargs,input_size=None,hidden_size=None,num_layers=None,nonlinearity=None,bias=None,batch_first=None,dropout=None,bidirectional=None)\nParameter description: hidden_size - The number of features in the hidden state h\nConstraints:\ndtype:int\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:[0,inf)\n\n###\n\nSignature: torch.nn.functional.avg_pool1d(input,kernel_size,stride=None,padding=0,ceil_mode=False,count_include_pad=True)\nParameter description: padding - implicit zero paddings on both sides of the input. Can be a single number or a tuple (padW,). Default: 0\nConstraints:\ndtype:bool;int\nstructure:tuple\nshape:Null\nndim:0;1\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.autograd.grad(outputs,inputs,grad_outputs=None,retain_graph=None,create_graph=False,only_inputs=True,allow_unused=False)\nParameter description: outputs(sequence of Tensor) - outputs of the differentiated function.\nConstraints:\ndtype:Null\nstructure:sequence;tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.repeat_interleave(input,repeats,dim=None)\nParameter description: dim(int, optional) - The dimension along which to repeat values.By default, use the flattened input array, and return a flat outputarray.\nConstraints:\ndtype:int\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.logical_and(input,other,out=None)\nParameter description: other(Tensor) - the tensor to compute AND with\nConstraints:",
            "constr": {
                "dtype": [],
                "structure": [
                    "tensor"
                ],
                "shape": [],
                "ndim": [],
                "enum": [],
                "range": []
            },
            "signature": "torch.logical_and(input,other,out=None)",
            "descp": "other(Tensor) - the tensor to compute AND with"
        },
        {
            "fname": "torch.nn.convtranspose2d.yaml",
            "arg": "groups",
            "prompt": "Signature: torch.distributed.recv(tensor,src=None,group=<objectobject>,tag=0)\nParameter description: tensor(Tensor) - Tensor to fill with received data.\nConstraints:\ndtype:Null\nstructure:tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.div(input,other,out=None)\nParameter description: out(Tensor, optional) - the output tensor.\nConstraints:\ndtype:Null\nstructure:tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.nn.TransformerDecoder(decoder_layer,num_layers,norm=None)\nParameter description: num_layers - the number of sub-decoder-layers in the decoder (required).\nConstraints:\ndtype:int\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:[0,inf)\n\n###\n\nSignature: torch.nn.quantized.functional.interpolate(input,size=None,scale_factor=None,mode=nearest,align_corners=None)\nParameter description: align_corners(bool, optional) - Geometrically, we consider the pixels of the input and output as squares rather than points. If set to `True`, the input and output tensors are aligned by the center points of their corner pixels, preserving the values at the corner pixels. If set to `False`, the input and output tensors are aligned by the corner points of their corner pixels, and the interpolation uses edge value padding for out-of-boundary values, making this operation independent of input size when `scale_factor` is kept the same. This only has an effect when `mode` is `'bilinear'`. Default: `False`\nConstraints:\ndtype:bool\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.gt(input,other,out=None)\nParameter description: out(Tensor, optional) - the output tensor that must be a BoolTensor\nConstraints:\ndtype:booltensor\nstructure:tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.empty_like(input,dtype=None,layout=None,device=None,requires_grad=False,memory_format=torch.preserve_format)\nParameter description: dtype(`torch.dtype`, optional) - the desired data type of returned Tensor. Default: if `None`, defaults to the dtype of `input`.\nConstraints:\ndtype:torch.dtype\nstructure:Null\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.nn.RNN(*args,**kwargs,input_size=None,hidden_size=None,num_layers=None,nonlinearity=None,bias=None,batch_first=None,dropout=None,bidirectional=None)\nParameter description: hidden_size - The number of features in the hidden state h\nConstraints:\ndtype:int\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:[0,inf)\n\n###\n\nSignature: torch.nn.functional.avg_pool1d(input,kernel_size,stride=None,padding=0,ceil_mode=False,count_include_pad=True)\nParameter description: padding - implicit zero paddings on both sides of the input. Can be a single number or a tuple (padW,). Default: 0\nConstraints:\ndtype:bool;int\nstructure:tuple\nshape:Null\nndim:0;1\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.autograd.grad(outputs,inputs,grad_outputs=None,retain_graph=None,create_graph=False,only_inputs=True,allow_unused=False)\nParameter description: outputs(sequence of Tensor) - outputs of the differentiated function.\nConstraints:\ndtype:Null\nstructure:sequence;tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.repeat_interleave(input,repeats,dim=None)\nParameter description: dim(int, optional) - The dimension along which to repeat values.By default, use the flattened input array, and return a flat outputarray.\nConstraints:\ndtype:int\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.nn.ConvTranspose2d(in_channels,out_channels,kernel_size,stride=1,padding=0,output_padding=0,groups=1,bias=True,dilation=1,padding_mode=zeros)\nParameter description: groups(int, optional) - Number of blocked connections from input channels to output channels. Default: 1\nConstraints:",
            "constr": {
                "dtype": [
                    "bool",
                    "int"
                ],
                "structure": [],
                "shape": [],
                "ndim": [
                    "0"
                ],
                "enum": [],
                "range": [
                    "[0,inf)"
                ]
            },
            "signature": "torch.nn.ConvTranspose2d(in_channels,out_channels,kernel_size,stride=1,padding=0,output_padding=0,groups=1,bias=True,dilation=1,padding_mode=zeros)",
            "descp": "groups(int, optional) - Number of blocked connections from input channels to output channels. Default: 1"
        },
        {
            "fname": "torch.utils.cpp_extension.load.yaml",
            "arg": "sources",
            "prompt": "Signature: torch.distributed.recv(tensor,src=None,group=<objectobject>,tag=0)\nParameter description: tensor(Tensor) - Tensor to fill with received data.\nConstraints:\ndtype:Null\nstructure:tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.div(input,other,out=None)\nParameter description: out(Tensor, optional) - the output tensor.\nConstraints:\ndtype:Null\nstructure:tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.nn.TransformerDecoder(decoder_layer,num_layers,norm=None)\nParameter description: num_layers - the number of sub-decoder-layers in the decoder (required).\nConstraints:\ndtype:int\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:[0,inf)\n\n###\n\nSignature: torch.nn.quantized.functional.interpolate(input,size=None,scale_factor=None,mode=nearest,align_corners=None)\nParameter description: align_corners(bool, optional) - Geometrically, we consider the pixels of the input and output as squares rather than points. If set to `True`, the input and output tensors are aligned by the center points of their corner pixels, preserving the values at the corner pixels. If set to `False`, the input and output tensors are aligned by the corner points of their corner pixels, and the interpolation uses edge value padding for out-of-boundary values, making this operation independent of input size when `scale_factor` is kept the same. This only has an effect when `mode` is `'bilinear'`. Default: `False`\nConstraints:\ndtype:bool\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.gt(input,other,out=None)\nParameter description: out(Tensor, optional) - the output tensor that must be a BoolTensor\nConstraints:\ndtype:booltensor\nstructure:tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.empty_like(input,dtype=None,layout=None,device=None,requires_grad=False,memory_format=torch.preserve_format)\nParameter description: dtype(`torch.dtype`, optional) - the desired data type of returned Tensor. Default: if `None`, defaults to the dtype of `input`.\nConstraints:\ndtype:torch.dtype\nstructure:Null\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.nn.RNN(*args,**kwargs,input_size=None,hidden_size=None,num_layers=None,nonlinearity=None,bias=None,batch_first=None,dropout=None,bidirectional=None)\nParameter description: hidden_size - The number of features in the hidden state h\nConstraints:\ndtype:int\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:[0,inf)\n\n###\n\nSignature: torch.nn.functional.avg_pool1d(input,kernel_size,stride=None,padding=0,ceil_mode=False,count_include_pad=True)\nParameter description: padding - implicit zero paddings on both sides of the input. Can be a single number or a tuple (padW,). Default: 0\nConstraints:\ndtype:bool;int\nstructure:tuple\nshape:Null\nndim:0;1\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.autograd.grad(outputs,inputs,grad_outputs=None,retain_graph=None,create_graph=False,only_inputs=True,allow_unused=False)\nParameter description: outputs(sequence of Tensor) - outputs of the differentiated function.\nConstraints:\ndtype:Null\nstructure:sequence;tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.repeat_interleave(input,repeats,dim=None)\nParameter description: dim(int, optional) - The dimension along which to repeat values.By default, use the flattened input array, and return a flat outputarray.\nConstraints:\ndtype:int\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.utils.cpp_extension.load(name,sources,extra_cflags=None,extra_cuda_cflags=None,extra_ldflags=None,extra_include_paths=None,build_directory=None,verbose=False,with_cuda=None,is_python_module=True)\nParameter description: sources - A list of relative or absolute paths to C++ source files.\nConstraints:",
            "constr": {
                "dtype": [],
                "structure": [
                    "list"
                ],
                "shape": [],
                "ndim": [],
                "enum": [],
                "range": []
            },
            "signature": "torch.utils.cpp_extension.load(name,sources,extra_cflags=None,extra_cuda_cflags=None,extra_ldflags=None,extra_include_paths=None,build_directory=None,verbose=False,with_cuda=None,is_python_module=True)",
            "descp": "sources - A list of relative or absolute paths to C++ source files."
        },
        {
            "fname": "torch.clamp.yaml",
            "arg": "max",
            "prompt": "Signature: torch.distributed.recv(tensor,src=None,group=<objectobject>,tag=0)\nParameter description: tensor(Tensor) - Tensor to fill with received data.\nConstraints:\ndtype:Null\nstructure:tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.div(input,other,out=None)\nParameter description: out(Tensor, optional) - the output tensor.\nConstraints:\ndtype:Null\nstructure:tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.nn.TransformerDecoder(decoder_layer,num_layers,norm=None)\nParameter description: num_layers - the number of sub-decoder-layers in the decoder (required).\nConstraints:\ndtype:int\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:[0,inf)\n\n###\n\nSignature: torch.nn.quantized.functional.interpolate(input,size=None,scale_factor=None,mode=nearest,align_corners=None)\nParameter description: align_corners(bool, optional) - Geometrically, we consider the pixels of the input and output as squares rather than points. If set to `True`, the input and output tensors are aligned by the center points of their corner pixels, preserving the values at the corner pixels. If set to `False`, the input and output tensors are aligned by the corner points of their corner pixels, and the interpolation uses edge value padding for out-of-boundary values, making this operation independent of input size when `scale_factor` is kept the same. This only has an effect when `mode` is `'bilinear'`. Default: `False`\nConstraints:\ndtype:bool\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.gt(input,other,out=None)\nParameter description: out(Tensor, optional) - the output tensor that must be a BoolTensor\nConstraints:\ndtype:booltensor\nstructure:tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.empty_like(input,dtype=None,layout=None,device=None,requires_grad=False,memory_format=torch.preserve_format)\nParameter description: dtype(`torch.dtype`, optional) - the desired data type of returned Tensor. Default: if `None`, defaults to the dtype of `input`.\nConstraints:\ndtype:torch.dtype\nstructure:Null\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.nn.RNN(*args,**kwargs,input_size=None,hidden_size=None,num_layers=None,nonlinearity=None,bias=None,batch_first=None,dropout=None,bidirectional=None)\nParameter description: hidden_size - The number of features in the hidden state h\nConstraints:\ndtype:int\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:[0,inf)\n\n###\n\nSignature: torch.nn.functional.avg_pool1d(input,kernel_size,stride=None,padding=0,ceil_mode=False,count_include_pad=True)\nParameter description: padding - implicit zero paddings on both sides of the input. Can be a single number or a tuple (padW,). Default: 0\nConstraints:\ndtype:bool;int\nstructure:tuple\nshape:Null\nndim:0;1\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.autograd.grad(outputs,inputs,grad_outputs=None,retain_graph=None,create_graph=False,only_inputs=True,allow_unused=False)\nParameter description: outputs(sequence of Tensor) - outputs of the differentiated function.\nConstraints:\ndtype:Null\nstructure:sequence;tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.repeat_interleave(input,repeats,dim=None)\nParameter description: dim(int, optional) - The dimension along which to repeat values.By default, use the flattened input array, and return a flat outputarray.\nConstraints:\ndtype:int\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.clamp(input,min,max,out=None)\nParameter description: max(Number) - upper-bound of the range to be clamped to\nConstraints:",
            "constr": {
                "dtype": [
                    "numeric"
                ],
                "structure": [],
                "shape": [],
                "ndim": [],
                "enum": [],
                "range": []
            },
            "signature": "torch.clamp(input,min,max,out=None)",
            "descp": "max(Number) - upper-bound of the range to be clamped to"
        },
        {
            "fname": "torch.utils.cpp_extension.load.yaml",
            "arg": "is_python_module",
            "prompt": "Signature: torch.distributed.recv(tensor,src=None,group=<objectobject>,tag=0)\nParameter description: tensor(Tensor) - Tensor to fill with received data.\nConstraints:\ndtype:Null\nstructure:tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.div(input,other,out=None)\nParameter description: out(Tensor, optional) - the output tensor.\nConstraints:\ndtype:Null\nstructure:tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.nn.TransformerDecoder(decoder_layer,num_layers,norm=None)\nParameter description: num_layers - the number of sub-decoder-layers in the decoder (required).\nConstraints:\ndtype:int\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:[0,inf)\n\n###\n\nSignature: torch.nn.quantized.functional.interpolate(input,size=None,scale_factor=None,mode=nearest,align_corners=None)\nParameter description: align_corners(bool, optional) - Geometrically, we consider the pixels of the input and output as squares rather than points. If set to `True`, the input and output tensors are aligned by the center points of their corner pixels, preserving the values at the corner pixels. If set to `False`, the input and output tensors are aligned by the corner points of their corner pixels, and the interpolation uses edge value padding for out-of-boundary values, making this operation independent of input size when `scale_factor` is kept the same. This only has an effect when `mode` is `'bilinear'`. Default: `False`\nConstraints:\ndtype:bool\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.gt(input,other,out=None)\nParameter description: out(Tensor, optional) - the output tensor that must be a BoolTensor\nConstraints:\ndtype:booltensor\nstructure:tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.empty_like(input,dtype=None,layout=None,device=None,requires_grad=False,memory_format=torch.preserve_format)\nParameter description: dtype(`torch.dtype`, optional) - the desired data type of returned Tensor. Default: if `None`, defaults to the dtype of `input`.\nConstraints:\ndtype:torch.dtype\nstructure:Null\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.nn.RNN(*args,**kwargs,input_size=None,hidden_size=None,num_layers=None,nonlinearity=None,bias=None,batch_first=None,dropout=None,bidirectional=None)\nParameter description: hidden_size - The number of features in the hidden state h\nConstraints:\ndtype:int\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:[0,inf)\n\n###\n\nSignature: torch.nn.functional.avg_pool1d(input,kernel_size,stride=None,padding=0,ceil_mode=False,count_include_pad=True)\nParameter description: padding - implicit zero paddings on both sides of the input. Can be a single number or a tuple (padW,). Default: 0\nConstraints:\ndtype:bool;int\nstructure:tuple\nshape:Null\nndim:0;1\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.autograd.grad(outputs,inputs,grad_outputs=None,retain_graph=None,create_graph=False,only_inputs=True,allow_unused=False)\nParameter description: outputs(sequence of Tensor) - outputs of the differentiated function.\nConstraints:\ndtype:Null\nstructure:sequence;tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.repeat_interleave(input,repeats,dim=None)\nParameter description: dim(int, optional) - The dimension along which to repeat values.By default, use the flattened input array, and return a flat outputarray.\nConstraints:\ndtype:int\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.utils.cpp_extension.load(name,sources,extra_cflags=None,extra_cuda_cflags=None,extra_ldflags=None,extra_include_paths=None,build_directory=None,verbose=False,with_cuda=None,is_python_module=True)\nParameter description: is_python_module - If `True` (default), imports the produced shared library as a Python module. If `False`, loads it into the process as a plain dynamic library.\nConstraints:",
            "constr": {
                "dtype": [
                    "bool"
                ],
                "structure": [],
                "shape": [],
                "ndim": [
                    "0"
                ],
                "enum": [],
                "range": []
            },
            "signature": "torch.utils.cpp_extension.load(name,sources,extra_cflags=None,extra_cuda_cflags=None,extra_ldflags=None,extra_include_paths=None,build_directory=None,verbose=False,with_cuda=None,is_python_module=True)",
            "descp": "is_python_module - If `True` (default), imports the produced shared library as a Python module. If `False`, loads it into the process as a plain dynamic library."
        },
        {
            "fname": "torch.nn.functional.cross_entropy.yaml",
            "arg": "target",
            "prompt": "Signature: torch.distributed.recv(tensor,src=None,group=<objectobject>,tag=0)\nParameter description: tensor(Tensor) - Tensor to fill with received data.\nConstraints:\ndtype:Null\nstructure:tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.div(input,other,out=None)\nParameter description: out(Tensor, optional) - the output tensor.\nConstraints:\ndtype:Null\nstructure:tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.nn.TransformerDecoder(decoder_layer,num_layers,norm=None)\nParameter description: num_layers - the number of sub-decoder-layers in the decoder (required).\nConstraints:\ndtype:int\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:[0,inf)\n\n###\n\nSignature: torch.nn.quantized.functional.interpolate(input,size=None,scale_factor=None,mode=nearest,align_corners=None)\nParameter description: align_corners(bool, optional) - Geometrically, we consider the pixels of the input and output as squares rather than points. If set to `True`, the input and output tensors are aligned by the center points of their corner pixels, preserving the values at the corner pixels. If set to `False`, the input and output tensors are aligned by the corner points of their corner pixels, and the interpolation uses edge value padding for out-of-boundary values, making this operation independent of input size when `scale_factor` is kept the same. This only has an effect when `mode` is `'bilinear'`. Default: `False`\nConstraints:\ndtype:bool\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.gt(input,other,out=None)\nParameter description: out(Tensor, optional) - the output tensor that must be a BoolTensor\nConstraints:\ndtype:booltensor\nstructure:tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.empty_like(input,dtype=None,layout=None,device=None,requires_grad=False,memory_format=torch.preserve_format)\nParameter description: dtype(`torch.dtype`, optional) - the desired data type of returned Tensor. Default: if `None`, defaults to the dtype of `input`.\nConstraints:\ndtype:torch.dtype\nstructure:Null\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.nn.RNN(*args,**kwargs,input_size=None,hidden_size=None,num_layers=None,nonlinearity=None,bias=None,batch_first=None,dropout=None,bidirectional=None)\nParameter description: hidden_size - The number of features in the hidden state h\nConstraints:\ndtype:int\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:[0,inf)\n\n###\n\nSignature: torch.nn.functional.avg_pool1d(input,kernel_size,stride=None,padding=0,ceil_mode=False,count_include_pad=True)\nParameter description: padding - implicit zero paddings on both sides of the input. Can be a single number or a tuple (padW,). Default: 0\nConstraints:\ndtype:bool;int\nstructure:tuple\nshape:Null\nndim:0;1\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.autograd.grad(outputs,inputs,grad_outputs=None,retain_graph=None,create_graph=False,only_inputs=True,allow_unused=False)\nParameter description: outputs(sequence of Tensor) - outputs of the differentiated function.\nConstraints:\ndtype:Null\nstructure:sequence;tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.repeat_interleave(input,repeats,dim=None)\nParameter description: dim(int, optional) - The dimension along which to repeat values.By default, use the flattened input array, and return a flat outputarray.\nConstraints:\ndtype:int\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.nn.functional.cross_entropy(input,target,weight=None,size_average=None,ignore_index=-100,reduce=None,reduction=mean)\nParameter description: target(Tensor) - (N)  where each value is 0  <= targets[i]  <= C-1 , or (N, d_1, d_2, ..., d_K)  where K  >= 1  for K-dimensional loss.\nConstraints:",
            "constr": {
                "dtype": [],
                "structure": [
                    "tensor"
                ],
                "shape": [
                    "(N)",
                    "(N, d_1, d_2, ..., d_K)"
                ],
                "ndim": [],
                "enum": [],
                "range": []
            },
            "signature": "torch.nn.functional.cross_entropy(input,target,weight=None,size_average=None,ignore_index=-100,reduce=None,reduction=mean)",
            "descp": "target(Tensor) - (N)  where each value is 0  <= targets[i]  <= C-1 , or (N, d_1, d_2, ..., d_K)  where K  >= 1  for K-dimensional loss."
        },
        {
            "fname": "torch.nn.tripletmarginloss.yaml",
            "arg": "reduce",
            "prompt": "Signature: torch.distributed.recv(tensor,src=None,group=<objectobject>,tag=0)\nParameter description: tensor(Tensor) - Tensor to fill with received data.\nConstraints:\ndtype:Null\nstructure:tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.div(input,other,out=None)\nParameter description: out(Tensor, optional) - the output tensor.\nConstraints:\ndtype:Null\nstructure:tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.nn.TransformerDecoder(decoder_layer,num_layers,norm=None)\nParameter description: num_layers - the number of sub-decoder-layers in the decoder (required).\nConstraints:\ndtype:int\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:[0,inf)\n\n###\n\nSignature: torch.nn.quantized.functional.interpolate(input,size=None,scale_factor=None,mode=nearest,align_corners=None)\nParameter description: align_corners(bool, optional) - Geometrically, we consider the pixels of the input and output as squares rather than points. If set to `True`, the input and output tensors are aligned by the center points of their corner pixels, preserving the values at the corner pixels. If set to `False`, the input and output tensors are aligned by the corner points of their corner pixels, and the interpolation uses edge value padding for out-of-boundary values, making this operation independent of input size when `scale_factor` is kept the same. This only has an effect when `mode` is `'bilinear'`. Default: `False`\nConstraints:\ndtype:bool\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.gt(input,other,out=None)\nParameter description: out(Tensor, optional) - the output tensor that must be a BoolTensor\nConstraints:\ndtype:booltensor\nstructure:tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.empty_like(input,dtype=None,layout=None,device=None,requires_grad=False,memory_format=torch.preserve_format)\nParameter description: dtype(`torch.dtype`, optional) - the desired data type of returned Tensor. Default: if `None`, defaults to the dtype of `input`.\nConstraints:\ndtype:torch.dtype\nstructure:Null\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.nn.RNN(*args,**kwargs,input_size=None,hidden_size=None,num_layers=None,nonlinearity=None,bias=None,batch_first=None,dropout=None,bidirectional=None)\nParameter description: hidden_size - The number of features in the hidden state h\nConstraints:\ndtype:int\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:[0,inf)\n\n###\n\nSignature: torch.nn.functional.avg_pool1d(input,kernel_size,stride=None,padding=0,ceil_mode=False,count_include_pad=True)\nParameter description: padding - implicit zero paddings on both sides of the input. Can be a single number or a tuple (padW,). Default: 0\nConstraints:\ndtype:bool;int\nstructure:tuple\nshape:Null\nndim:0;1\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.autograd.grad(outputs,inputs,grad_outputs=None,retain_graph=None,create_graph=False,only_inputs=True,allow_unused=False)\nParameter description: outputs(sequence of Tensor) - outputs of the differentiated function.\nConstraints:\ndtype:Null\nstructure:sequence;tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.repeat_interleave(input,repeats,dim=None)\nParameter description: dim(int, optional) - The dimension along which to repeat values.By default, use the flattened input array, and return a flat outputarray.\nConstraints:\ndtype:int\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.nn.TripletMarginLoss(margin=1.0,p=2.0,eps=1e-06,swap=False,size_average=None,reduce=None,reduction=mean)\nParameter description: reduce(bool, optional) - Deprecated (see `reduction`). By default, the losses are averaged or summed over observations for each minibatch depending on `size_average`. When `reduce` is `False`, returns a loss per batch element instead and ignores `size_average`. Default: `True`\nConstraints:",
            "constr": {
                "dtype": [
                    "bool"
                ],
                "structure": [],
                "shape": [],
                "ndim": [
                    "0"
                ],
                "enum": [],
                "range": []
            },
            "signature": "torch.nn.TripletMarginLoss(margin=1.0,p=2.0,eps=1e-06,swap=False,size_average=None,reduce=None,reduction=mean)",
            "descp": "reduce(bool, optional) - Deprecated (see `reduction`). By default, the losses are averaged or summed over observations for each minibatch depending on `size_average`. When `reduce` is `False`, returns a loss per batch element instead and ignores `size_average`. Default: `True`"
        },
        {
            "fname": "torch.linspace.yaml",
            "arg": "steps",
            "prompt": "Signature: torch.distributed.recv(tensor,src=None,group=<objectobject>,tag=0)\nParameter description: tensor(Tensor) - Tensor to fill with received data.\nConstraints:\ndtype:Null\nstructure:tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.div(input,other,out=None)\nParameter description: out(Tensor, optional) - the output tensor.\nConstraints:\ndtype:Null\nstructure:tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.nn.TransformerDecoder(decoder_layer,num_layers,norm=None)\nParameter description: num_layers - the number of sub-decoder-layers in the decoder (required).\nConstraints:\ndtype:int\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:[0,inf)\n\n###\n\nSignature: torch.nn.quantized.functional.interpolate(input,size=None,scale_factor=None,mode=nearest,align_corners=None)\nParameter description: align_corners(bool, optional) - Geometrically, we consider the pixels of the input and output as squares rather than points. If set to `True`, the input and output tensors are aligned by the center points of their corner pixels, preserving the values at the corner pixels. If set to `False`, the input and output tensors are aligned by the corner points of their corner pixels, and the interpolation uses edge value padding for out-of-boundary values, making this operation independent of input size when `scale_factor` is kept the same. This only has an effect when `mode` is `'bilinear'`. Default: `False`\nConstraints:\ndtype:bool\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.gt(input,other,out=None)\nParameter description: out(Tensor, optional) - the output tensor that must be a BoolTensor\nConstraints:\ndtype:booltensor\nstructure:tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.empty_like(input,dtype=None,layout=None,device=None,requires_grad=False,memory_format=torch.preserve_format)\nParameter description: dtype(`torch.dtype`, optional) - the desired data type of returned Tensor. Default: if `None`, defaults to the dtype of `input`.\nConstraints:\ndtype:torch.dtype\nstructure:Null\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.nn.RNN(*args,**kwargs,input_size=None,hidden_size=None,num_layers=None,nonlinearity=None,bias=None,batch_first=None,dropout=None,bidirectional=None)\nParameter description: hidden_size - The number of features in the hidden state h\nConstraints:\ndtype:int\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:[0,inf)\n\n###\n\nSignature: torch.nn.functional.avg_pool1d(input,kernel_size,stride=None,padding=0,ceil_mode=False,count_include_pad=True)\nParameter description: padding - implicit zero paddings on both sides of the input. Can be a single number or a tuple (padW,). Default: 0\nConstraints:\ndtype:bool;int\nstructure:tuple\nshape:Null\nndim:0;1\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.autograd.grad(outputs,inputs,grad_outputs=None,retain_graph=None,create_graph=False,only_inputs=True,allow_unused=False)\nParameter description: outputs(sequence of Tensor) - outputs of the differentiated function.\nConstraints:\ndtype:Null\nstructure:sequence;tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.repeat_interleave(input,repeats,dim=None)\nParameter description: dim(int, optional) - The dimension along which to repeat values.By default, use the flattened input array, and return a flat outputarray.\nConstraints:\ndtype:int\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.linspace(start,end,steps=100,out=None,dtype=None,layout=torch.strided,device=None,requires_grad=False)\nParameter description: steps(int) - number of points to sample between `start` and `end`. Default: `100`.\nConstraints:",
            "constr": {
                "dtype": [
                    "bool",
                    "int"
                ],
                "structure": [],
                "shape": [],
                "ndim": [
                    "0"
                ],
                "enum": [],
                "range": [
                    "[0,inf)"
                ]
            },
            "signature": "torch.linspace(start,end,steps=100,out=None,dtype=None,layout=torch.strided,device=None,requires_grad=False)",
            "descp": "steps(int) - number of points to sample between `start` and `end`. Default: `100`."
        },
        {
            "fname": "torch.addr.yaml",
            "arg": "beta",
            "prompt": "Signature: torch.distributed.recv(tensor,src=None,group=<objectobject>,tag=0)\nParameter description: tensor(Tensor) - Tensor to fill with received data.\nConstraints:\ndtype:Null\nstructure:tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.div(input,other,out=None)\nParameter description: out(Tensor, optional) - the output tensor.\nConstraints:\ndtype:Null\nstructure:tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.nn.TransformerDecoder(decoder_layer,num_layers,norm=None)\nParameter description: num_layers - the number of sub-decoder-layers in the decoder (required).\nConstraints:\ndtype:int\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:[0,inf)\n\n###\n\nSignature: torch.nn.quantized.functional.interpolate(input,size=None,scale_factor=None,mode=nearest,align_corners=None)\nParameter description: align_corners(bool, optional) - Geometrically, we consider the pixels of the input and output as squares rather than points. If set to `True`, the input and output tensors are aligned by the center points of their corner pixels, preserving the values at the corner pixels. If set to `False`, the input and output tensors are aligned by the corner points of their corner pixels, and the interpolation uses edge value padding for out-of-boundary values, making this operation independent of input size when `scale_factor` is kept the same. This only has an effect when `mode` is `'bilinear'`. Default: `False`\nConstraints:\ndtype:bool\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.gt(input,other,out=None)\nParameter description: out(Tensor, optional) - the output tensor that must be a BoolTensor\nConstraints:\ndtype:booltensor\nstructure:tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.empty_like(input,dtype=None,layout=None,device=None,requires_grad=False,memory_format=torch.preserve_format)\nParameter description: dtype(`torch.dtype`, optional) - the desired data type of returned Tensor. Default: if `None`, defaults to the dtype of `input`.\nConstraints:\ndtype:torch.dtype\nstructure:Null\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.nn.RNN(*args,**kwargs,input_size=None,hidden_size=None,num_layers=None,nonlinearity=None,bias=None,batch_first=None,dropout=None,bidirectional=None)\nParameter description: hidden_size - The number of features in the hidden state h\nConstraints:\ndtype:int\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:[0,inf)\n\n###\n\nSignature: torch.nn.functional.avg_pool1d(input,kernel_size,stride=None,padding=0,ceil_mode=False,count_include_pad=True)\nParameter description: padding - implicit zero paddings on both sides of the input. Can be a single number or a tuple (padW,). Default: 0\nConstraints:\ndtype:bool;int\nstructure:tuple\nshape:Null\nndim:0;1\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.autograd.grad(outputs,inputs,grad_outputs=None,retain_graph=None,create_graph=False,only_inputs=True,allow_unused=False)\nParameter description: outputs(sequence of Tensor) - outputs of the differentiated function.\nConstraints:\ndtype:Null\nstructure:sequence;tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.repeat_interleave(input,repeats,dim=None)\nParameter description: dim(int, optional) - The dimension along which to repeat values.By default, use the flattened input array, and return a flat outputarray.\nConstraints:\ndtype:int\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.addr(input,vec1,vec2,beta=1,alpha=1,out=None)\nParameter description: beta(Number, optional) - multiplier for `input` ( beta )\nConstraints:",
            "constr": {
                "dtype": [
                    "numeric",
                    "int"
                ],
                "structure": [],
                "shape": [],
                "ndim": [
                    "0"
                ],
                "enum": [],
                "range": []
            },
            "signature": "torch.addr(input,vec1,vec2,beta=1,alpha=1,out=None)",
            "descp": "beta(Number, optional) - multiplier for `input` ( beta )"
        },
        {
            "fname": "torch.nn.embedding.yaml",
            "arg": "norm_type",
            "prompt": "Signature: torch.distributed.recv(tensor,src=None,group=<objectobject>,tag=0)\nParameter description: tensor(Tensor) - Tensor to fill with received data.\nConstraints:\ndtype:Null\nstructure:tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.div(input,other,out=None)\nParameter description: out(Tensor, optional) - the output tensor.\nConstraints:\ndtype:Null\nstructure:tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.nn.TransformerDecoder(decoder_layer,num_layers,norm=None)\nParameter description: num_layers - the number of sub-decoder-layers in the decoder (required).\nConstraints:\ndtype:int\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:[0,inf)\n\n###\n\nSignature: torch.nn.quantized.functional.interpolate(input,size=None,scale_factor=None,mode=nearest,align_corners=None)\nParameter description: align_corners(bool, optional) - Geometrically, we consider the pixels of the input and output as squares rather than points. If set to `True`, the input and output tensors are aligned by the center points of their corner pixels, preserving the values at the corner pixels. If set to `False`, the input and output tensors are aligned by the corner points of their corner pixels, and the interpolation uses edge value padding for out-of-boundary values, making this operation independent of input size when `scale_factor` is kept the same. This only has an effect when `mode` is `'bilinear'`. Default: `False`\nConstraints:\ndtype:bool\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.gt(input,other,out=None)\nParameter description: out(Tensor, optional) - the output tensor that must be a BoolTensor\nConstraints:\ndtype:booltensor\nstructure:tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.empty_like(input,dtype=None,layout=None,device=None,requires_grad=False,memory_format=torch.preserve_format)\nParameter description: dtype(`torch.dtype`, optional) - the desired data type of returned Tensor. Default: if `None`, defaults to the dtype of `input`.\nConstraints:\ndtype:torch.dtype\nstructure:Null\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.nn.RNN(*args,**kwargs,input_size=None,hidden_size=None,num_layers=None,nonlinearity=None,bias=None,batch_first=None,dropout=None,bidirectional=None)\nParameter description: hidden_size - The number of features in the hidden state h\nConstraints:\ndtype:int\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:[0,inf)\n\n###\n\nSignature: torch.nn.functional.avg_pool1d(input,kernel_size,stride=None,padding=0,ceil_mode=False,count_include_pad=True)\nParameter description: padding - implicit zero paddings on both sides of the input. Can be a single number or a tuple (padW,). Default: 0\nConstraints:\ndtype:bool;int\nstructure:tuple\nshape:Null\nndim:0;1\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.autograd.grad(outputs,inputs,grad_outputs=None,retain_graph=None,create_graph=False,only_inputs=True,allow_unused=False)\nParameter description: outputs(sequence of Tensor) - outputs of the differentiated function.\nConstraints:\ndtype:Null\nstructure:sequence;tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.repeat_interleave(input,repeats,dim=None)\nParameter description: dim(int, optional) - The dimension along which to repeat values.By default, use the flattened input array, and return a flat outputarray.\nConstraints:\ndtype:int\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.nn.Embedding(num_embeddings,embedding_dim,padding_idx=None,max_norm=None,norm_type=2.0,scale_grad_by_freq=False,sparse=False,_weight=None)\nParameter description: norm_type(float, optional) - The p of the p-norm to compute for the `max_norm` option. Default `2`.\nConstraints:",
            "constr": {
                "dtype": [
                    "float",
                    "int"
                ],
                "structure": [],
                "shape": [],
                "ndim": [
                    "0"
                ],
                "enum": [],
                "range": []
            },
            "signature": "torch.nn.Embedding(num_embeddings,embedding_dim,padding_idx=None,max_norm=None,norm_type=2.0,scale_grad_by_freq=False,sparse=False,_weight=None)",
            "descp": "norm_type(float, optional) - The p of the p-norm to compute for the `max_norm` option. Default `2`."
        },
        {
            "fname": "torch.sigmoid.yaml",
            "arg": "input",
            "prompt": "Signature: torch.distributed.recv(tensor,src=None,group=<objectobject>,tag=0)\nParameter description: tensor(Tensor) - Tensor to fill with received data.\nConstraints:\ndtype:Null\nstructure:tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.div(input,other,out=None)\nParameter description: out(Tensor, optional) - the output tensor.\nConstraints:\ndtype:Null\nstructure:tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.nn.TransformerDecoder(decoder_layer,num_layers,norm=None)\nParameter description: num_layers - the number of sub-decoder-layers in the decoder (required).\nConstraints:\ndtype:int\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:[0,inf)\n\n###\n\nSignature: torch.nn.quantized.functional.interpolate(input,size=None,scale_factor=None,mode=nearest,align_corners=None)\nParameter description: align_corners(bool, optional) - Geometrically, we consider the pixels of the input and output as squares rather than points. If set to `True`, the input and output tensors are aligned by the center points of their corner pixels, preserving the values at the corner pixels. If set to `False`, the input and output tensors are aligned by the corner points of their corner pixels, and the interpolation uses edge value padding for out-of-boundary values, making this operation independent of input size when `scale_factor` is kept the same. This only has an effect when `mode` is `'bilinear'`. Default: `False`\nConstraints:\ndtype:bool\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.gt(input,other,out=None)\nParameter description: out(Tensor, optional) - the output tensor that must be a BoolTensor\nConstraints:\ndtype:booltensor\nstructure:tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.empty_like(input,dtype=None,layout=None,device=None,requires_grad=False,memory_format=torch.preserve_format)\nParameter description: dtype(`torch.dtype`, optional) - the desired data type of returned Tensor. Default: if `None`, defaults to the dtype of `input`.\nConstraints:\ndtype:torch.dtype\nstructure:Null\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.nn.RNN(*args,**kwargs,input_size=None,hidden_size=None,num_layers=None,nonlinearity=None,bias=None,batch_first=None,dropout=None,bidirectional=None)\nParameter description: hidden_size - The number of features in the hidden state h\nConstraints:\ndtype:int\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:[0,inf)\n\n###\n\nSignature: torch.nn.functional.avg_pool1d(input,kernel_size,stride=None,padding=0,ceil_mode=False,count_include_pad=True)\nParameter description: padding - implicit zero paddings on both sides of the input. Can be a single number or a tuple (padW,). Default: 0\nConstraints:\ndtype:bool;int\nstructure:tuple\nshape:Null\nndim:0;1\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.autograd.grad(outputs,inputs,grad_outputs=None,retain_graph=None,create_graph=False,only_inputs=True,allow_unused=False)\nParameter description: outputs(sequence of Tensor) - outputs of the differentiated function.\nConstraints:\ndtype:Null\nstructure:sequence;tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.repeat_interleave(input,repeats,dim=None)\nParameter description: dim(int, optional) - The dimension along which to repeat values.By default, use the flattened input array, and return a flat outputarray.\nConstraints:\ndtype:int\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.sigmoid(input,out=None)\nParameter description: input(Tensor) - the input tensor.\nConstraints:",
            "constr": {
                "dtype": [],
                "structure": [
                    "tensor"
                ],
                "shape": [],
                "ndim": [],
                "enum": [],
                "range": []
            },
            "signature": "torch.sigmoid(input,out=None)",
            "descp": "input(Tensor) - the input tensor."
        },
        {
            "fname": "torch.min2.yaml",
            "arg": "keepdim",
            "prompt": "Signature: torch.distributed.recv(tensor,src=None,group=<objectobject>,tag=0)\nParameter description: tensor(Tensor) - Tensor to fill with received data.\nConstraints:\ndtype:Null\nstructure:tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.div(input,other,out=None)\nParameter description: out(Tensor, optional) - the output tensor.\nConstraints:\ndtype:Null\nstructure:tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.nn.TransformerDecoder(decoder_layer,num_layers,norm=None)\nParameter description: num_layers - the number of sub-decoder-layers in the decoder (required).\nConstraints:\ndtype:int\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:[0,inf)\n\n###\n\nSignature: torch.nn.quantized.functional.interpolate(input,size=None,scale_factor=None,mode=nearest,align_corners=None)\nParameter description: align_corners(bool, optional) - Geometrically, we consider the pixels of the input and output as squares rather than points. If set to `True`, the input and output tensors are aligned by the center points of their corner pixels, preserving the values at the corner pixels. If set to `False`, the input and output tensors are aligned by the corner points of their corner pixels, and the interpolation uses edge value padding for out-of-boundary values, making this operation independent of input size when `scale_factor` is kept the same. This only has an effect when `mode` is `'bilinear'`. Default: `False`\nConstraints:\ndtype:bool\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.gt(input,other,out=None)\nParameter description: out(Tensor, optional) - the output tensor that must be a BoolTensor\nConstraints:\ndtype:booltensor\nstructure:tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.empty_like(input,dtype=None,layout=None,device=None,requires_grad=False,memory_format=torch.preserve_format)\nParameter description: dtype(`torch.dtype`, optional) - the desired data type of returned Tensor. Default: if `None`, defaults to the dtype of `input`.\nConstraints:\ndtype:torch.dtype\nstructure:Null\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.nn.RNN(*args,**kwargs,input_size=None,hidden_size=None,num_layers=None,nonlinearity=None,bias=None,batch_first=None,dropout=None,bidirectional=None)\nParameter description: hidden_size - The number of features in the hidden state h\nConstraints:\ndtype:int\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:[0,inf)\n\n###\n\nSignature: torch.nn.functional.avg_pool1d(input,kernel_size,stride=None,padding=0,ceil_mode=False,count_include_pad=True)\nParameter description: padding - implicit zero paddings on both sides of the input. Can be a single number or a tuple (padW,). Default: 0\nConstraints:\ndtype:bool;int\nstructure:tuple\nshape:Null\nndim:0;1\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.autograd.grad(outputs,inputs,grad_outputs=None,retain_graph=None,create_graph=False,only_inputs=True,allow_unused=False)\nParameter description: outputs(sequence of Tensor) - outputs of the differentiated function.\nConstraints:\ndtype:Null\nstructure:sequence;tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.repeat_interleave(input,repeats,dim=None)\nParameter description: dim(int, optional) - The dimension along which to repeat values.By default, use the flattened input array, and return a flat outputarray.\nConstraints:\ndtype:int\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.min(input,dim,keepdim=False,out=None)\nParameter description: keepdim(bool) - whether the output tensor has `dim` retained or not.\nConstraints:",
            "constr": {
                "dtype": [
                    "bool"
                ],
                "structure": [],
                "shape": [],
                "ndim": [
                    "0"
                ],
                "enum": [],
                "range": []
            },
            "signature": "torch.min(input,dim,keepdim=False,out=None)",
            "descp": "keepdim(bool) - whether the output tensor has `dim` retained or not."
        },
        {
            "fname": "torch.nn.transformerencoderlayer.yaml",
            "arg": "d_model",
            "prompt": "Signature: torch.distributed.recv(tensor,src=None,group=<objectobject>,tag=0)\nParameter description: tensor(Tensor) - Tensor to fill with received data.\nConstraints:\ndtype:Null\nstructure:tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.div(input,other,out=None)\nParameter description: out(Tensor, optional) - the output tensor.\nConstraints:\ndtype:Null\nstructure:tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.nn.TransformerDecoder(decoder_layer,num_layers,norm=None)\nParameter description: num_layers - the number of sub-decoder-layers in the decoder (required).\nConstraints:\ndtype:int\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:[0,inf)\n\n###\n\nSignature: torch.nn.quantized.functional.interpolate(input,size=None,scale_factor=None,mode=nearest,align_corners=None)\nParameter description: align_corners(bool, optional) - Geometrically, we consider the pixels of the input and output as squares rather than points. If set to `True`, the input and output tensors are aligned by the center points of their corner pixels, preserving the values at the corner pixels. If set to `False`, the input and output tensors are aligned by the corner points of their corner pixels, and the interpolation uses edge value padding for out-of-boundary values, making this operation independent of input size when `scale_factor` is kept the same. This only has an effect when `mode` is `'bilinear'`. Default: `False`\nConstraints:\ndtype:bool\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.gt(input,other,out=None)\nParameter description: out(Tensor, optional) - the output tensor that must be a BoolTensor\nConstraints:\ndtype:booltensor\nstructure:tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.empty_like(input,dtype=None,layout=None,device=None,requires_grad=False,memory_format=torch.preserve_format)\nParameter description: dtype(`torch.dtype`, optional) - the desired data type of returned Tensor. Default: if `None`, defaults to the dtype of `input`.\nConstraints:\ndtype:torch.dtype\nstructure:Null\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.nn.RNN(*args,**kwargs,input_size=None,hidden_size=None,num_layers=None,nonlinearity=None,bias=None,batch_first=None,dropout=None,bidirectional=None)\nParameter description: hidden_size - The number of features in the hidden state h\nConstraints:\ndtype:int\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:[0,inf)\n\n###\n\nSignature: torch.nn.functional.avg_pool1d(input,kernel_size,stride=None,padding=0,ceil_mode=False,count_include_pad=True)\nParameter description: padding - implicit zero paddings on both sides of the input. Can be a single number or a tuple (padW,). Default: 0\nConstraints:\ndtype:bool;int\nstructure:tuple\nshape:Null\nndim:0;1\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.autograd.grad(outputs,inputs,grad_outputs=None,retain_graph=None,create_graph=False,only_inputs=True,allow_unused=False)\nParameter description: outputs(sequence of Tensor) - outputs of the differentiated function.\nConstraints:\ndtype:Null\nstructure:sequence;tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.repeat_interleave(input,repeats,dim=None)\nParameter description: dim(int, optional) - The dimension along which to repeat values.By default, use the flattened input array, and return a flat outputarray.\nConstraints:\ndtype:int\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.nn.TransformerEncoderLayer(d_model,nhead,dim_feedforward=2048,dropout=0.1,activation=relu)\nParameter description: d_model - the number of expected features in the input (required).\nConstraints:",
            "constr": {
                "dtype": [
                    "int"
                ],
                "structure": [],
                "shape": [],
                "ndim": [
                    "0"
                ],
                "enum": [],
                "range": [
                    "[0,inf)"
                ]
            },
            "signature": "torch.nn.TransformerEncoderLayer(d_model,nhead,dim_feedforward=2048,dropout=0.1,activation=relu)",
            "descp": "d_model - the number of expected features in the input (required)."
        },
        {
            "fname": "torch.cumprod.yaml",
            "arg": "dtype",
            "prompt": "Signature: torch.distributed.recv(tensor,src=None,group=<objectobject>,tag=0)\nParameter description: tensor(Tensor) - Tensor to fill with received data.\nConstraints:\ndtype:Null\nstructure:tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.div(input,other,out=None)\nParameter description: out(Tensor, optional) - the output tensor.\nConstraints:\ndtype:Null\nstructure:tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.nn.TransformerDecoder(decoder_layer,num_layers,norm=None)\nParameter description: num_layers - the number of sub-decoder-layers in the decoder (required).\nConstraints:\ndtype:int\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:[0,inf)\n\n###\n\nSignature: torch.nn.quantized.functional.interpolate(input,size=None,scale_factor=None,mode=nearest,align_corners=None)\nParameter description: align_corners(bool, optional) - Geometrically, we consider the pixels of the input and output as squares rather than points. If set to `True`, the input and output tensors are aligned by the center points of their corner pixels, preserving the values at the corner pixels. If set to `False`, the input and output tensors are aligned by the corner points of their corner pixels, and the interpolation uses edge value padding for out-of-boundary values, making this operation independent of input size when `scale_factor` is kept the same. This only has an effect when `mode` is `'bilinear'`. Default: `False`\nConstraints:\ndtype:bool\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.gt(input,other,out=None)\nParameter description: out(Tensor, optional) - the output tensor that must be a BoolTensor\nConstraints:\ndtype:booltensor\nstructure:tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.empty_like(input,dtype=None,layout=None,device=None,requires_grad=False,memory_format=torch.preserve_format)\nParameter description: dtype(`torch.dtype`, optional) - the desired data type of returned Tensor. Default: if `None`, defaults to the dtype of `input`.\nConstraints:\ndtype:torch.dtype\nstructure:Null\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.nn.RNN(*args,**kwargs,input_size=None,hidden_size=None,num_layers=None,nonlinearity=None,bias=None,batch_first=None,dropout=None,bidirectional=None)\nParameter description: hidden_size - The number of features in the hidden state h\nConstraints:\ndtype:int\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:[0,inf)\n\n###\n\nSignature: torch.nn.functional.avg_pool1d(input,kernel_size,stride=None,padding=0,ceil_mode=False,count_include_pad=True)\nParameter description: padding - implicit zero paddings on both sides of the input. Can be a single number or a tuple (padW,). Default: 0\nConstraints:\ndtype:bool;int\nstructure:tuple\nshape:Null\nndim:0;1\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.autograd.grad(outputs,inputs,grad_outputs=None,retain_graph=None,create_graph=False,only_inputs=True,allow_unused=False)\nParameter description: outputs(sequence of Tensor) - outputs of the differentiated function.\nConstraints:\ndtype:Null\nstructure:sequence;tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.repeat_interleave(input,repeats,dim=None)\nParameter description: dim(int, optional) - The dimension along which to repeat values.By default, use the flattened input array, and return a flat outputarray.\nConstraints:\ndtype:int\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.cumprod(input,dim,out=None,dtype=None)\nParameter description: dtype(`torch.dtype`, optional) - the desired data type of returned tensor. If specified, the input tensor is casted to `dtype` before the operation is performed. This is useful for preventing data type overflows. Default: None.\nConstraints:",
            "constr": {
                "dtype": [
                    "torch.dtype"
                ],
                "structure": [],
                "shape": [],
                "ndim": [],
                "enum": [],
                "range": []
            },
            "signature": "torch.cumprod(input,dim,out=None,dtype=None)",
            "descp": "dtype(`torch.dtype`, optional) - the desired data type of returned tensor. If specified, the input tensor is casted to `dtype` before the operation is performed. This is useful for preventing data type overflows. Default: None."
        },
        {
            "fname": "torch.distributed.broadcast_multigpu.yaml",
            "arg": "src",
            "prompt": "Signature: torch.distributed.recv(tensor,src=None,group=<objectobject>,tag=0)\nParameter description: tensor(Tensor) - Tensor to fill with received data.\nConstraints:\ndtype:Null\nstructure:tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.div(input,other,out=None)\nParameter description: out(Tensor, optional) - the output tensor.\nConstraints:\ndtype:Null\nstructure:tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.nn.TransformerDecoder(decoder_layer,num_layers,norm=None)\nParameter description: num_layers - the number of sub-decoder-layers in the decoder (required).\nConstraints:\ndtype:int\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:[0,inf)\n\n###\n\nSignature: torch.nn.quantized.functional.interpolate(input,size=None,scale_factor=None,mode=nearest,align_corners=None)\nParameter description: align_corners(bool, optional) - Geometrically, we consider the pixels of the input and output as squares rather than points. If set to `True`, the input and output tensors are aligned by the center points of their corner pixels, preserving the values at the corner pixels. If set to `False`, the input and output tensors are aligned by the corner points of their corner pixels, and the interpolation uses edge value padding for out-of-boundary values, making this operation independent of input size when `scale_factor` is kept the same. This only has an effect when `mode` is `'bilinear'`. Default: `False`\nConstraints:\ndtype:bool\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.gt(input,other,out=None)\nParameter description: out(Tensor, optional) - the output tensor that must be a BoolTensor\nConstraints:\ndtype:booltensor\nstructure:tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.empty_like(input,dtype=None,layout=None,device=None,requires_grad=False,memory_format=torch.preserve_format)\nParameter description: dtype(`torch.dtype`, optional) - the desired data type of returned Tensor. Default: if `None`, defaults to the dtype of `input`.\nConstraints:\ndtype:torch.dtype\nstructure:Null\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.nn.RNN(*args,**kwargs,input_size=None,hidden_size=None,num_layers=None,nonlinearity=None,bias=None,batch_first=None,dropout=None,bidirectional=None)\nParameter description: hidden_size - The number of features in the hidden state h\nConstraints:\ndtype:int\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:[0,inf)\n\n###\n\nSignature: torch.nn.functional.avg_pool1d(input,kernel_size,stride=None,padding=0,ceil_mode=False,count_include_pad=True)\nParameter description: padding - implicit zero paddings on both sides of the input. Can be a single number or a tuple (padW,). Default: 0\nConstraints:\ndtype:bool;int\nstructure:tuple\nshape:Null\nndim:0;1\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.autograd.grad(outputs,inputs,grad_outputs=None,retain_graph=None,create_graph=False,only_inputs=True,allow_unused=False)\nParameter description: outputs(sequence of Tensor) - outputs of the differentiated function.\nConstraints:\ndtype:Null\nstructure:sequence;tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.repeat_interleave(input,repeats,dim=None)\nParameter description: dim(int, optional) - The dimension along which to repeat values.By default, use the flattened input array, and return a flat outputarray.\nConstraints:\ndtype:int\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.distributed.broadcast_multigpu(tensor_list,src,group=<objectobject>,async_op=False,src_tensor=0)\nParameter description: src(int) - Source rank.\nConstraints:",
            "constr": {
                "dtype": [
                    "int"
                ],
                "structure": [],
                "shape": [],
                "ndim": [],
                "enum": [],
                "range": []
            },
            "signature": "torch.distributed.broadcast_multigpu(tensor_list,src,group=<objectobject>,async_op=False,src_tensor=0)",
            "descp": "src(int) - Source rank."
        },
        {
            "fname": "torch.normal.yaml",
            "arg": "std",
            "prompt": "Signature: torch.distributed.recv(tensor,src=None,group=<objectobject>,tag=0)\nParameter description: tensor(Tensor) - Tensor to fill with received data.\nConstraints:\ndtype:Null\nstructure:tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.div(input,other,out=None)\nParameter description: out(Tensor, optional) - the output tensor.\nConstraints:\ndtype:Null\nstructure:tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.nn.TransformerDecoder(decoder_layer,num_layers,norm=None)\nParameter description: num_layers - the number of sub-decoder-layers in the decoder (required).\nConstraints:\ndtype:int\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:[0,inf)\n\n###\n\nSignature: torch.nn.quantized.functional.interpolate(input,size=None,scale_factor=None,mode=nearest,align_corners=None)\nParameter description: align_corners(bool, optional) - Geometrically, we consider the pixels of the input and output as squares rather than points. If set to `True`, the input and output tensors are aligned by the center points of their corner pixels, preserving the values at the corner pixels. If set to `False`, the input and output tensors are aligned by the corner points of their corner pixels, and the interpolation uses edge value padding for out-of-boundary values, making this operation independent of input size when `scale_factor` is kept the same. This only has an effect when `mode` is `'bilinear'`. Default: `False`\nConstraints:\ndtype:bool\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.gt(input,other,out=None)\nParameter description: out(Tensor, optional) - the output tensor that must be a BoolTensor\nConstraints:\ndtype:booltensor\nstructure:tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.empty_like(input,dtype=None,layout=None,device=None,requires_grad=False,memory_format=torch.preserve_format)\nParameter description: dtype(`torch.dtype`, optional) - the desired data type of returned Tensor. Default: if `None`, defaults to the dtype of `input`.\nConstraints:\ndtype:torch.dtype\nstructure:Null\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.nn.RNN(*args,**kwargs,input_size=None,hidden_size=None,num_layers=None,nonlinearity=None,bias=None,batch_first=None,dropout=None,bidirectional=None)\nParameter description: hidden_size - The number of features in the hidden state h\nConstraints:\ndtype:int\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:[0,inf)\n\n###\n\nSignature: torch.nn.functional.avg_pool1d(input,kernel_size,stride=None,padding=0,ceil_mode=False,count_include_pad=True)\nParameter description: padding - implicit zero paddings on both sides of the input. Can be a single number or a tuple (padW,). Default: 0\nConstraints:\ndtype:bool;int\nstructure:tuple\nshape:Null\nndim:0;1\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.autograd.grad(outputs,inputs,grad_outputs=None,retain_graph=None,create_graph=False,only_inputs=True,allow_unused=False)\nParameter description: outputs(sequence of Tensor) - outputs of the differentiated function.\nConstraints:\ndtype:Null\nstructure:sequence;tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.repeat_interleave(input,repeats,dim=None)\nParameter description: dim(int, optional) - The dimension along which to repeat values.By default, use the flattened input array, and return a flat outputarray.\nConstraints:\ndtype:int\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.normal(mean,std,generator=None,out=None)\nParameter description: std(Tensor) - the tensor of per-element standard deviations\nConstraints:",
            "constr": {
                "dtype": [
                    "numeric"
                ],
                "structure": [
                    "tensor"
                ],
                "shape": [],
                "ndim": [],
                "enum": [],
                "range": []
            },
            "signature": "torch.normal(mean,std,generator=None,out=None)",
            "descp": "std(Tensor) - the tensor of per-element standard deviations"
        },
        {
            "fname": "torch.nn.functional.softmax.yaml",
            "arg": "dtype",
            "prompt": "Signature: torch.distributed.recv(tensor,src=None,group=<objectobject>,tag=0)\nParameter description: tensor(Tensor) - Tensor to fill with received data.\nConstraints:\ndtype:Null\nstructure:tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.div(input,other,out=None)\nParameter description: out(Tensor, optional) - the output tensor.\nConstraints:\ndtype:Null\nstructure:tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.nn.TransformerDecoder(decoder_layer,num_layers,norm=None)\nParameter description: num_layers - the number of sub-decoder-layers in the decoder (required).\nConstraints:\ndtype:int\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:[0,inf)\n\n###\n\nSignature: torch.nn.quantized.functional.interpolate(input,size=None,scale_factor=None,mode=nearest,align_corners=None)\nParameter description: align_corners(bool, optional) - Geometrically, we consider the pixels of the input and output as squares rather than points. If set to `True`, the input and output tensors are aligned by the center points of their corner pixels, preserving the values at the corner pixels. If set to `False`, the input and output tensors are aligned by the corner points of their corner pixels, and the interpolation uses edge value padding for out-of-boundary values, making this operation independent of input size when `scale_factor` is kept the same. This only has an effect when `mode` is `'bilinear'`. Default: `False`\nConstraints:\ndtype:bool\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.gt(input,other,out=None)\nParameter description: out(Tensor, optional) - the output tensor that must be a BoolTensor\nConstraints:\ndtype:booltensor\nstructure:tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.empty_like(input,dtype=None,layout=None,device=None,requires_grad=False,memory_format=torch.preserve_format)\nParameter description: dtype(`torch.dtype`, optional) - the desired data type of returned Tensor. Default: if `None`, defaults to the dtype of `input`.\nConstraints:\ndtype:torch.dtype\nstructure:Null\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.nn.RNN(*args,**kwargs,input_size=None,hidden_size=None,num_layers=None,nonlinearity=None,bias=None,batch_first=None,dropout=None,bidirectional=None)\nParameter description: hidden_size - The number of features in the hidden state h\nConstraints:\ndtype:int\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:[0,inf)\n\n###\n\nSignature: torch.nn.functional.avg_pool1d(input,kernel_size,stride=None,padding=0,ceil_mode=False,count_include_pad=True)\nParameter description: padding - implicit zero paddings on both sides of the input. Can be a single number or a tuple (padW,). Default: 0\nConstraints:\ndtype:bool;int\nstructure:tuple\nshape:Null\nndim:0;1\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.autograd.grad(outputs,inputs,grad_outputs=None,retain_graph=None,create_graph=False,only_inputs=True,allow_unused=False)\nParameter description: outputs(sequence of Tensor) - outputs of the differentiated function.\nConstraints:\ndtype:Null\nstructure:sequence;tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.repeat_interleave(input,repeats,dim=None)\nParameter description: dim(int, optional) - The dimension along which to repeat values.By default, use the flattened input array, and return a flat outputarray.\nConstraints:\ndtype:int\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.nn.functional.softmax(input,dim=None,_stacklevel=3,dtype=None)\nParameter description: dtype(`torch.dtype`, optional) - the desired data type of returned tensor. If specified, the input tensor is casted to `dtype` before the operation is performed. This is useful for preventing data type overflows. Default: None.\nConstraints:",
            "constr": {
                "dtype": [
                    "torch.dtype"
                ],
                "structure": [],
                "shape": [],
                "ndim": [],
                "enum": [],
                "range": []
            },
            "signature": "torch.nn.functional.softmax(input,dim=None,_stacklevel=3,dtype=None)",
            "descp": "dtype(`torch.dtype`, optional) - the desired data type of returned tensor. If specified, the input tensor is casted to `dtype` before the operation is performed. This is useful for preventing data type overflows. Default: None."
        },
        {
            "fname": "torch.nn.convtranspose3d.yaml",
            "arg": "bias",
            "prompt": "Signature: torch.distributed.recv(tensor,src=None,group=<objectobject>,tag=0)\nParameter description: tensor(Tensor) - Tensor to fill with received data.\nConstraints:\ndtype:Null\nstructure:tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.div(input,other,out=None)\nParameter description: out(Tensor, optional) - the output tensor.\nConstraints:\ndtype:Null\nstructure:tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.nn.TransformerDecoder(decoder_layer,num_layers,norm=None)\nParameter description: num_layers - the number of sub-decoder-layers in the decoder (required).\nConstraints:\ndtype:int\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:[0,inf)\n\n###\n\nSignature: torch.nn.quantized.functional.interpolate(input,size=None,scale_factor=None,mode=nearest,align_corners=None)\nParameter description: align_corners(bool, optional) - Geometrically, we consider the pixels of the input and output as squares rather than points. If set to `True`, the input and output tensors are aligned by the center points of their corner pixels, preserving the values at the corner pixels. If set to `False`, the input and output tensors are aligned by the corner points of their corner pixels, and the interpolation uses edge value padding for out-of-boundary values, making this operation independent of input size when `scale_factor` is kept the same. This only has an effect when `mode` is `'bilinear'`. Default: `False`\nConstraints:\ndtype:bool\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.gt(input,other,out=None)\nParameter description: out(Tensor, optional) - the output tensor that must be a BoolTensor\nConstraints:\ndtype:booltensor\nstructure:tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.empty_like(input,dtype=None,layout=None,device=None,requires_grad=False,memory_format=torch.preserve_format)\nParameter description: dtype(`torch.dtype`, optional) - the desired data type of returned Tensor. Default: if `None`, defaults to the dtype of `input`.\nConstraints:\ndtype:torch.dtype\nstructure:Null\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.nn.RNN(*args,**kwargs,input_size=None,hidden_size=None,num_layers=None,nonlinearity=None,bias=None,batch_first=None,dropout=None,bidirectional=None)\nParameter description: hidden_size - The number of features in the hidden state h\nConstraints:\ndtype:int\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:[0,inf)\n\n###\n\nSignature: torch.nn.functional.avg_pool1d(input,kernel_size,stride=None,padding=0,ceil_mode=False,count_include_pad=True)\nParameter description: padding - implicit zero paddings on both sides of the input. Can be a single number or a tuple (padW,). Default: 0\nConstraints:\ndtype:bool;int\nstructure:tuple\nshape:Null\nndim:0;1\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.autograd.grad(outputs,inputs,grad_outputs=None,retain_graph=None,create_graph=False,only_inputs=True,allow_unused=False)\nParameter description: outputs(sequence of Tensor) - outputs of the differentiated function.\nConstraints:\ndtype:Null\nstructure:sequence;tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.repeat_interleave(input,repeats,dim=None)\nParameter description: dim(int, optional) - The dimension along which to repeat values.By default, use the flattened input array, and return a flat outputarray.\nConstraints:\ndtype:int\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.nn.ConvTranspose3d(in_channels,out_channels,kernel_size,stride=1,padding=0,output_padding=0,groups=1,bias=True,dilation=1,padding_mode=zeros)\nParameter description: bias(bool, optional) - If `True`, adds a learnable bias to the output. Default: `True`\nConstraints:",
            "constr": {
                "dtype": [
                    "bool"
                ],
                "structure": [],
                "shape": [],
                "ndim": [
                    "0"
                ],
                "enum": [],
                "range": []
            },
            "signature": "torch.nn.ConvTranspose3d(in_channels,out_channels,kernel_size,stride=1,padding=0,output_padding=0,groups=1,bias=True,dilation=1,padding_mode=zeros)",
            "descp": "bias(bool, optional) - If `True`, adds a learnable bias to the output. Default: `True`"
        },
        {
            "fname": "torch.addcdiv.yaml",
            "arg": "value",
            "prompt": "Signature: torch.distributed.recv(tensor,src=None,group=<objectobject>,tag=0)\nParameter description: tensor(Tensor) - Tensor to fill with received data.\nConstraints:\ndtype:Null\nstructure:tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.div(input,other,out=None)\nParameter description: out(Tensor, optional) - the output tensor.\nConstraints:\ndtype:Null\nstructure:tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.nn.TransformerDecoder(decoder_layer,num_layers,norm=None)\nParameter description: num_layers - the number of sub-decoder-layers in the decoder (required).\nConstraints:\ndtype:int\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:[0,inf)\n\n###\n\nSignature: torch.nn.quantized.functional.interpolate(input,size=None,scale_factor=None,mode=nearest,align_corners=None)\nParameter description: align_corners(bool, optional) - Geometrically, we consider the pixels of the input and output as squares rather than points. If set to `True`, the input and output tensors are aligned by the center points of their corner pixels, preserving the values at the corner pixels. If set to `False`, the input and output tensors are aligned by the corner points of their corner pixels, and the interpolation uses edge value padding for out-of-boundary values, making this operation independent of input size when `scale_factor` is kept the same. This only has an effect when `mode` is `'bilinear'`. Default: `False`\nConstraints:\ndtype:bool\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.gt(input,other,out=None)\nParameter description: out(Tensor, optional) - the output tensor that must be a BoolTensor\nConstraints:\ndtype:booltensor\nstructure:tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.empty_like(input,dtype=None,layout=None,device=None,requires_grad=False,memory_format=torch.preserve_format)\nParameter description: dtype(`torch.dtype`, optional) - the desired data type of returned Tensor. Default: if `None`, defaults to the dtype of `input`.\nConstraints:\ndtype:torch.dtype\nstructure:Null\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.nn.RNN(*args,**kwargs,input_size=None,hidden_size=None,num_layers=None,nonlinearity=None,bias=None,batch_first=None,dropout=None,bidirectional=None)\nParameter description: hidden_size - The number of features in the hidden state h\nConstraints:\ndtype:int\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:[0,inf)\n\n###\n\nSignature: torch.nn.functional.avg_pool1d(input,kernel_size,stride=None,padding=0,ceil_mode=False,count_include_pad=True)\nParameter description: padding - implicit zero paddings on both sides of the input. Can be a single number or a tuple (padW,). Default: 0\nConstraints:\ndtype:bool;int\nstructure:tuple\nshape:Null\nndim:0;1\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.autograd.grad(outputs,inputs,grad_outputs=None,retain_graph=None,create_graph=False,only_inputs=True,allow_unused=False)\nParameter description: outputs(sequence of Tensor) - outputs of the differentiated function.\nConstraints:\ndtype:Null\nstructure:sequence;tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.repeat_interleave(input,repeats,dim=None)\nParameter description: dim(int, optional) - The dimension along which to repeat values.By default, use the flattened input array, and return a flat outputarray.\nConstraints:\ndtype:int\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.addcdiv(input,tensor1,tensor2,value=1,out=None)\nParameter description: value(Number, optional) - multiplier for tensor1 / tensor2 \nConstraints:",
            "constr": {
                "dtype": [
                    "numeric",
                    "int"
                ],
                "structure": [],
                "shape": [],
                "ndim": [
                    "0"
                ],
                "enum": [],
                "range": []
            },
            "signature": "torch.addcdiv(input,tensor1,tensor2,value=1,out=None)",
            "descp": "value(Number, optional) - multiplier for tensor1 / tensor2 "
        },
        {
            "fname": "torch.nn.utils.spectral_norm.yaml",
            "arg": "name",
            "prompt": "Signature: torch.distributed.recv(tensor,src=None,group=<objectobject>,tag=0)\nParameter description: tensor(Tensor) - Tensor to fill with received data.\nConstraints:\ndtype:Null\nstructure:tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.div(input,other,out=None)\nParameter description: out(Tensor, optional) - the output tensor.\nConstraints:\ndtype:Null\nstructure:tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.nn.TransformerDecoder(decoder_layer,num_layers,norm=None)\nParameter description: num_layers - the number of sub-decoder-layers in the decoder (required).\nConstraints:\ndtype:int\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:[0,inf)\n\n###\n\nSignature: torch.nn.quantized.functional.interpolate(input,size=None,scale_factor=None,mode=nearest,align_corners=None)\nParameter description: align_corners(bool, optional) - Geometrically, we consider the pixels of the input and output as squares rather than points. If set to `True`, the input and output tensors are aligned by the center points of their corner pixels, preserving the values at the corner pixels. If set to `False`, the input and output tensors are aligned by the corner points of their corner pixels, and the interpolation uses edge value padding for out-of-boundary values, making this operation independent of input size when `scale_factor` is kept the same. This only has an effect when `mode` is `'bilinear'`. Default: `False`\nConstraints:\ndtype:bool\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.gt(input,other,out=None)\nParameter description: out(Tensor, optional) - the output tensor that must be a BoolTensor\nConstraints:\ndtype:booltensor\nstructure:tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.empty_like(input,dtype=None,layout=None,device=None,requires_grad=False,memory_format=torch.preserve_format)\nParameter description: dtype(`torch.dtype`, optional) - the desired data type of returned Tensor. Default: if `None`, defaults to the dtype of `input`.\nConstraints:\ndtype:torch.dtype\nstructure:Null\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.nn.RNN(*args,**kwargs,input_size=None,hidden_size=None,num_layers=None,nonlinearity=None,bias=None,batch_first=None,dropout=None,bidirectional=None)\nParameter description: hidden_size - The number of features in the hidden state h\nConstraints:\ndtype:int\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:[0,inf)\n\n###\n\nSignature: torch.nn.functional.avg_pool1d(input,kernel_size,stride=None,padding=0,ceil_mode=False,count_include_pad=True)\nParameter description: padding - implicit zero paddings on both sides of the input. Can be a single number or a tuple (padW,). Default: 0\nConstraints:\ndtype:bool;int\nstructure:tuple\nshape:Null\nndim:0;1\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.autograd.grad(outputs,inputs,grad_outputs=None,retain_graph=None,create_graph=False,only_inputs=True,allow_unused=False)\nParameter description: outputs(sequence of Tensor) - outputs of the differentiated function.\nConstraints:\ndtype:Null\nstructure:sequence;tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.repeat_interleave(input,repeats,dim=None)\nParameter description: dim(int, optional) - The dimension along which to repeat values.By default, use the flattened input array, and return a flat outputarray.\nConstraints:\ndtype:int\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.nn.utils.spectral_norm(module,name=weight,n_power_iterations=1,eps=1e-12,dim=None)\nParameter description: name(str, optional) - name of weight parameter\nConstraints:",
            "constr": {
                "dtype": [
                    "string",
                    "str"
                ],
                "structure": [],
                "shape": [],
                "ndim": [
                    "0"
                ],
                "enum": [],
                "range": []
            },
            "signature": "torch.nn.utils.spectral_norm(module,name=weight,n_power_iterations=1,eps=1e-12,dim=None)",
            "descp": "name(str, optional) - name of weight parameter"
        },
        {
            "fname": "torch.ge.yaml",
            "arg": "input",
            "prompt": "Signature: torch.distributed.recv(tensor,src=None,group=<objectobject>,tag=0)\nParameter description: tensor(Tensor) - Tensor to fill with received data.\nConstraints:\ndtype:Null\nstructure:tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.div(input,other,out=None)\nParameter description: out(Tensor, optional) - the output tensor.\nConstraints:\ndtype:Null\nstructure:tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.nn.TransformerDecoder(decoder_layer,num_layers,norm=None)\nParameter description: num_layers - the number of sub-decoder-layers in the decoder (required).\nConstraints:\ndtype:int\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:[0,inf)\n\n###\n\nSignature: torch.nn.quantized.functional.interpolate(input,size=None,scale_factor=None,mode=nearest,align_corners=None)\nParameter description: align_corners(bool, optional) - Geometrically, we consider the pixels of the input and output as squares rather than points. If set to `True`, the input and output tensors are aligned by the center points of their corner pixels, preserving the values at the corner pixels. If set to `False`, the input and output tensors are aligned by the corner points of their corner pixels, and the interpolation uses edge value padding for out-of-boundary values, making this operation independent of input size when `scale_factor` is kept the same. This only has an effect when `mode` is `'bilinear'`. Default: `False`\nConstraints:\ndtype:bool\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.gt(input,other,out=None)\nParameter description: out(Tensor, optional) - the output tensor that must be a BoolTensor\nConstraints:\ndtype:booltensor\nstructure:tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.empty_like(input,dtype=None,layout=None,device=None,requires_grad=False,memory_format=torch.preserve_format)\nParameter description: dtype(`torch.dtype`, optional) - the desired data type of returned Tensor. Default: if `None`, defaults to the dtype of `input`.\nConstraints:\ndtype:torch.dtype\nstructure:Null\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.nn.RNN(*args,**kwargs,input_size=None,hidden_size=None,num_layers=None,nonlinearity=None,bias=None,batch_first=None,dropout=None,bidirectional=None)\nParameter description: hidden_size - The number of features in the hidden state h\nConstraints:\ndtype:int\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:[0,inf)\n\n###\n\nSignature: torch.nn.functional.avg_pool1d(input,kernel_size,stride=None,padding=0,ceil_mode=False,count_include_pad=True)\nParameter description: padding - implicit zero paddings on both sides of the input. Can be a single number or a tuple (padW,). Default: 0\nConstraints:\ndtype:bool;int\nstructure:tuple\nshape:Null\nndim:0;1\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.autograd.grad(outputs,inputs,grad_outputs=None,retain_graph=None,create_graph=False,only_inputs=True,allow_unused=False)\nParameter description: outputs(sequence of Tensor) - outputs of the differentiated function.\nConstraints:\ndtype:Null\nstructure:sequence;tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.repeat_interleave(input,repeats,dim=None)\nParameter description: dim(int, optional) - The dimension along which to repeat values.By default, use the flattened input array, and return a flat outputarray.\nConstraints:\ndtype:int\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.ge(input,other,out=None)\nParameter description: input(Tensor) - the tensor to compare\nConstraints:",
            "constr": {
                "dtype": [],
                "structure": [
                    "tensor"
                ],
                "shape": [],
                "ndim": [],
                "enum": [],
                "range": []
            },
            "signature": "torch.ge(input,other,out=None)",
            "descp": "input(Tensor) - the tensor to compare"
        },
        {
            "fname": "torch.nn.functional.conv_transpose1d.yaml",
            "arg": "padding",
            "prompt": "Signature: torch.distributed.recv(tensor,src=None,group=<objectobject>,tag=0)\nParameter description: tensor(Tensor) - Tensor to fill with received data.\nConstraints:\ndtype:Null\nstructure:tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.div(input,other,out=None)\nParameter description: out(Tensor, optional) - the output tensor.\nConstraints:\ndtype:Null\nstructure:tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.nn.TransformerDecoder(decoder_layer,num_layers,norm=None)\nParameter description: num_layers - the number of sub-decoder-layers in the decoder (required).\nConstraints:\ndtype:int\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:[0,inf)\n\n###\n\nSignature: torch.nn.quantized.functional.interpolate(input,size=None,scale_factor=None,mode=nearest,align_corners=None)\nParameter description: align_corners(bool, optional) - Geometrically, we consider the pixels of the input and output as squares rather than points. If set to `True`, the input and output tensors are aligned by the center points of their corner pixels, preserving the values at the corner pixels. If set to `False`, the input and output tensors are aligned by the corner points of their corner pixels, and the interpolation uses edge value padding for out-of-boundary values, making this operation independent of input size when `scale_factor` is kept the same. This only has an effect when `mode` is `'bilinear'`. Default: `False`\nConstraints:\ndtype:bool\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.gt(input,other,out=None)\nParameter description: out(Tensor, optional) - the output tensor that must be a BoolTensor\nConstraints:\ndtype:booltensor\nstructure:tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.empty_like(input,dtype=None,layout=None,device=None,requires_grad=False,memory_format=torch.preserve_format)\nParameter description: dtype(`torch.dtype`, optional) - the desired data type of returned Tensor. Default: if `None`, defaults to the dtype of `input`.\nConstraints:\ndtype:torch.dtype\nstructure:Null\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.nn.RNN(*args,**kwargs,input_size=None,hidden_size=None,num_layers=None,nonlinearity=None,bias=None,batch_first=None,dropout=None,bidirectional=None)\nParameter description: hidden_size - The number of features in the hidden state h\nConstraints:\ndtype:int\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:[0,inf)\n\n###\n\nSignature: torch.nn.functional.avg_pool1d(input,kernel_size,stride=None,padding=0,ceil_mode=False,count_include_pad=True)\nParameter description: padding - implicit zero paddings on both sides of the input. Can be a single number or a tuple (padW,). Default: 0\nConstraints:\ndtype:bool;int\nstructure:tuple\nshape:Null\nndim:0;1\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.autograd.grad(outputs,inputs,grad_outputs=None,retain_graph=None,create_graph=False,only_inputs=True,allow_unused=False)\nParameter description: outputs(sequence of Tensor) - outputs of the differentiated function.\nConstraints:\ndtype:Null\nstructure:sequence;tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.repeat_interleave(input,repeats,dim=None)\nParameter description: dim(int, optional) - The dimension along which to repeat values.By default, use the flattened input array, and return a flat outputarray.\nConstraints:\ndtype:int\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.nn.functional.conv_transpose1d(input,weight,bias=None,stride=1,padding=0,output_padding=0,groups=1,dilation=1)\nParameter description: padding - `dilation * (kernel_size - 1) - padding` zero-padding will be added to both sides of each dimension in the input. Can be a single number or a tuple `(padW,)`. Default: 0\nConstraints:",
            "constr": {
                "dtype": [
                    "numeric",
                    "int"
                ],
                "structure": [
                    "tuple"
                ],
                "shape": [],
                "ndim": [
                    "0",
                    "1"
                ],
                "enum": [],
                "range": []
            },
            "signature": "torch.nn.functional.conv_transpose1d(input,weight,bias=None,stride=1,padding=0,output_padding=0,groups=1,dilation=1)",
            "descp": "padding - `dilation * (kernel_size - 1) - padding` zero-padding will be added to both sides of each dimension in the input. Can be a single number or a tuple `(padW,)`. Default: 0"
        },
        {
            "fname": "torch.nn.functional.poisson_nll_loss.yaml",
            "arg": "full",
            "prompt": "Signature: torch.distributed.recv(tensor,src=None,group=<objectobject>,tag=0)\nParameter description: tensor(Tensor) - Tensor to fill with received data.\nConstraints:\ndtype:Null\nstructure:tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.div(input,other,out=None)\nParameter description: out(Tensor, optional) - the output tensor.\nConstraints:\ndtype:Null\nstructure:tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.nn.TransformerDecoder(decoder_layer,num_layers,norm=None)\nParameter description: num_layers - the number of sub-decoder-layers in the decoder (required).\nConstraints:\ndtype:int\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:[0,inf)\n\n###\n\nSignature: torch.nn.quantized.functional.interpolate(input,size=None,scale_factor=None,mode=nearest,align_corners=None)\nParameter description: align_corners(bool, optional) - Geometrically, we consider the pixels of the input and output as squares rather than points. If set to `True`, the input and output tensors are aligned by the center points of their corner pixels, preserving the values at the corner pixels. If set to `False`, the input and output tensors are aligned by the corner points of their corner pixels, and the interpolation uses edge value padding for out-of-boundary values, making this operation independent of input size when `scale_factor` is kept the same. This only has an effect when `mode` is `'bilinear'`. Default: `False`\nConstraints:\ndtype:bool\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.gt(input,other,out=None)\nParameter description: out(Tensor, optional) - the output tensor that must be a BoolTensor\nConstraints:\ndtype:booltensor\nstructure:tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.empty_like(input,dtype=None,layout=None,device=None,requires_grad=False,memory_format=torch.preserve_format)\nParameter description: dtype(`torch.dtype`, optional) - the desired data type of returned Tensor. Default: if `None`, defaults to the dtype of `input`.\nConstraints:\ndtype:torch.dtype\nstructure:Null\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.nn.RNN(*args,**kwargs,input_size=None,hidden_size=None,num_layers=None,nonlinearity=None,bias=None,batch_first=None,dropout=None,bidirectional=None)\nParameter description: hidden_size - The number of features in the hidden state h\nConstraints:\ndtype:int\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:[0,inf)\n\n###\n\nSignature: torch.nn.functional.avg_pool1d(input,kernel_size,stride=None,padding=0,ceil_mode=False,count_include_pad=True)\nParameter description: padding - implicit zero paddings on both sides of the input. Can be a single number or a tuple (padW,). Default: 0\nConstraints:\ndtype:bool;int\nstructure:tuple\nshape:Null\nndim:0;1\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.autograd.grad(outputs,inputs,grad_outputs=None,retain_graph=None,create_graph=False,only_inputs=True,allow_unused=False)\nParameter description: outputs(sequence of Tensor) - outputs of the differentiated function.\nConstraints:\ndtype:Null\nstructure:sequence;tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.repeat_interleave(input,repeats,dim=None)\nParameter description: dim(int, optional) - The dimension along which to repeat values.By default, use the flattened input array, and return a flat outputarray.\nConstraints:\ndtype:int\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.nn.functional.poisson_nll_loss(input,target,log_input=True,full=False,size_average=None,eps=1e-08,reduce=None,reduction=mean)\nParameter description: full - whether to compute full loss, i. e. to add the Stirling approximation term. Default: `False` target *  log(target) - target + 0.5 *  log(2 *  pi * target) .\nConstraints:",
            "constr": {
                "dtype": [
                    "bool"
                ],
                "structure": [],
                "shape": [],
                "ndim": [
                    "0"
                ],
                "enum": [],
                "range": []
            },
            "signature": "torch.nn.functional.poisson_nll_loss(input,target,log_input=True,full=False,size_average=None,eps=1e-08,reduce=None,reduction=mean)",
            "descp": "full - whether to compute full loss, i. e. to add the Stirling approximation term. Default: `False` target *  log(target) - target + 0.5 *  log(2 *  pi * target) ."
        },
        {
            "fname": "torch.onnx.export.yaml",
            "arg": "verbose",
            "prompt": "Signature: torch.distributed.recv(tensor,src=None,group=<objectobject>,tag=0)\nParameter description: tensor(Tensor) - Tensor to fill with received data.\nConstraints:\ndtype:Null\nstructure:tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.div(input,other,out=None)\nParameter description: out(Tensor, optional) - the output tensor.\nConstraints:\ndtype:Null\nstructure:tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.nn.TransformerDecoder(decoder_layer,num_layers,norm=None)\nParameter description: num_layers - the number of sub-decoder-layers in the decoder (required).\nConstraints:\ndtype:int\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:[0,inf)\n\n###\n\nSignature: torch.nn.quantized.functional.interpolate(input,size=None,scale_factor=None,mode=nearest,align_corners=None)\nParameter description: align_corners(bool, optional) - Geometrically, we consider the pixels of the input and output as squares rather than points. If set to `True`, the input and output tensors are aligned by the center points of their corner pixels, preserving the values at the corner pixels. If set to `False`, the input and output tensors are aligned by the corner points of their corner pixels, and the interpolation uses edge value padding for out-of-boundary values, making this operation independent of input size when `scale_factor` is kept the same. This only has an effect when `mode` is `'bilinear'`. Default: `False`\nConstraints:\ndtype:bool\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.gt(input,other,out=None)\nParameter description: out(Tensor, optional) - the output tensor that must be a BoolTensor\nConstraints:\ndtype:booltensor\nstructure:tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.empty_like(input,dtype=None,layout=None,device=None,requires_grad=False,memory_format=torch.preserve_format)\nParameter description: dtype(`torch.dtype`, optional) - the desired data type of returned Tensor. Default: if `None`, defaults to the dtype of `input`.\nConstraints:\ndtype:torch.dtype\nstructure:Null\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.nn.RNN(*args,**kwargs,input_size=None,hidden_size=None,num_layers=None,nonlinearity=None,bias=None,batch_first=None,dropout=None,bidirectional=None)\nParameter description: hidden_size - The number of features in the hidden state h\nConstraints:\ndtype:int\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:[0,inf)\n\n###\n\nSignature: torch.nn.functional.avg_pool1d(input,kernel_size,stride=None,padding=0,ceil_mode=False,count_include_pad=True)\nParameter description: padding - implicit zero paddings on both sides of the input. Can be a single number or a tuple (padW,). Default: 0\nConstraints:\ndtype:bool;int\nstructure:tuple\nshape:Null\nndim:0;1\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.autograd.grad(outputs,inputs,grad_outputs=None,retain_graph=None,create_graph=False,only_inputs=True,allow_unused=False)\nParameter description: outputs(sequence of Tensor) - outputs of the differentiated function.\nConstraints:\ndtype:Null\nstructure:sequence;tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.repeat_interleave(input,repeats,dim=None)\nParameter description: dim(int, optional) - The dimension along which to repeat values.By default, use the flattened input array, and return a flat outputarray.\nConstraints:\ndtype:int\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.onnx.export(model,args,f,export_params=True,verbose=False,training=False,input_names=None,output_names=None,aten=False,export_raw_ir=False,operator_export_type=None,opset_version=None,_retain_param_name=True,do_constant_folding=True,example_outputs=None,strip_doc_string=True,dynamic_axes=None,keep_initializers_as_inputs=None,custom_opsets=None,enable_onnx_checker=True,use_external_data_format=False)\nParameter description: verbose(bool, default False) - if specified, we will print out a debug description of the trace being exported.\nConstraints:",
            "constr": {
                "dtype": [
                    "bool"
                ],
                "structure": [],
                "shape": [],
                "ndim": [
                    "0"
                ],
                "enum": [],
                "range": []
            },
            "signature": "torch.onnx.export(model,args,f,export_params=True,verbose=False,training=False,input_names=None,output_names=None,aten=False,export_raw_ir=False,operator_export_type=None,opset_version=None,_retain_param_name=True,do_constant_folding=True,example_outputs=None,strip_doc_string=True,dynamic_axes=None,keep_initializers_as_inputs=None,custom_opsets=None,enable_onnx_checker=True,use_external_data_format=False)",
            "descp": "verbose(bool, default False) - if specified, we will print out a debug description of the trace being exported."
        },
        {
            "fname": "torch.nn.quantized.functional.conv3d.yaml",
            "arg": "stride",
            "prompt": "Signature: torch.distributed.recv(tensor,src=None,group=<objectobject>,tag=0)\nParameter description: tensor(Tensor) - Tensor to fill with received data.\nConstraints:\ndtype:Null\nstructure:tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.div(input,other,out=None)\nParameter description: out(Tensor, optional) - the output tensor.\nConstraints:\ndtype:Null\nstructure:tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.nn.TransformerDecoder(decoder_layer,num_layers,norm=None)\nParameter description: num_layers - the number of sub-decoder-layers in the decoder (required).\nConstraints:\ndtype:int\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:[0,inf)\n\n###\n\nSignature: torch.nn.quantized.functional.interpolate(input,size=None,scale_factor=None,mode=nearest,align_corners=None)\nParameter description: align_corners(bool, optional) - Geometrically, we consider the pixels of the input and output as squares rather than points. If set to `True`, the input and output tensors are aligned by the center points of their corner pixels, preserving the values at the corner pixels. If set to `False`, the input and output tensors are aligned by the corner points of their corner pixels, and the interpolation uses edge value padding for out-of-boundary values, making this operation independent of input size when `scale_factor` is kept the same. This only has an effect when `mode` is `'bilinear'`. Default: `False`\nConstraints:\ndtype:bool\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.gt(input,other,out=None)\nParameter description: out(Tensor, optional) - the output tensor that must be a BoolTensor\nConstraints:\ndtype:booltensor\nstructure:tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.empty_like(input,dtype=None,layout=None,device=None,requires_grad=False,memory_format=torch.preserve_format)\nParameter description: dtype(`torch.dtype`, optional) - the desired data type of returned Tensor. Default: if `None`, defaults to the dtype of `input`.\nConstraints:\ndtype:torch.dtype\nstructure:Null\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.nn.RNN(*args,**kwargs,input_size=None,hidden_size=None,num_layers=None,nonlinearity=None,bias=None,batch_first=None,dropout=None,bidirectional=None)\nParameter description: hidden_size - The number of features in the hidden state h\nConstraints:\ndtype:int\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:[0,inf)\n\n###\n\nSignature: torch.nn.functional.avg_pool1d(input,kernel_size,stride=None,padding=0,ceil_mode=False,count_include_pad=True)\nParameter description: padding - implicit zero paddings on both sides of the input. Can be a single number or a tuple (padW,). Default: 0\nConstraints:\ndtype:bool;int\nstructure:tuple\nshape:Null\nndim:0;1\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.autograd.grad(outputs,inputs,grad_outputs=None,retain_graph=None,create_graph=False,only_inputs=True,allow_unused=False)\nParameter description: outputs(sequence of Tensor) - outputs of the differentiated function.\nConstraints:\ndtype:Null\nstructure:sequence;tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.repeat_interleave(input,repeats,dim=None)\nParameter description: dim(int, optional) - The dimension along which to repeat values.By default, use the flattened input array, and return a flat outputarray.\nConstraints:\ndtype:int\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.nn.quantized.functional.conv3d(input,weight,bias,stride=1,padding=0,dilation=1,groups=1,padding_mode=zeros,scale=1.0,zero_point=0,dtype=torch.quint8)\nParameter description: stride - the stride of the convolving kernel. Can be a single number or a tuple (sD, sH, sW). Default: 1\nConstraints:",
            "constr": {
                "dtype": [
                    "numeric",
                    "int"
                ],
                "structure": [
                    "tuple"
                ],
                "shape": [],
                "ndim": [
                    "0",
                    "1"
                ],
                "enum": [],
                "range": [
                    "[0,inf)"
                ]
            },
            "signature": "torch.nn.quantized.functional.conv3d(input,weight,bias,stride=1,padding=0,dilation=1,groups=1,padding_mode=zeros,scale=1.0,zero_point=0,dtype=torch.quint8)",
            "descp": "stride - the stride of the convolving kernel. Can be a single number or a tuple (sD, sH, sW). Default: 1"
        },
        {
            "fname": "torch.rand_like.yaml",
            "arg": "dtype",
            "prompt": "Signature: torch.distributed.recv(tensor,src=None,group=<objectobject>,tag=0)\nParameter description: tensor(Tensor) - Tensor to fill with received data.\nConstraints:\ndtype:Null\nstructure:tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.div(input,other,out=None)\nParameter description: out(Tensor, optional) - the output tensor.\nConstraints:\ndtype:Null\nstructure:tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.nn.TransformerDecoder(decoder_layer,num_layers,norm=None)\nParameter description: num_layers - the number of sub-decoder-layers in the decoder (required).\nConstraints:\ndtype:int\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:[0,inf)\n\n###\n\nSignature: torch.nn.quantized.functional.interpolate(input,size=None,scale_factor=None,mode=nearest,align_corners=None)\nParameter description: align_corners(bool, optional) - Geometrically, we consider the pixels of the input and output as squares rather than points. If set to `True`, the input and output tensors are aligned by the center points of their corner pixels, preserving the values at the corner pixels. If set to `False`, the input and output tensors are aligned by the corner points of their corner pixels, and the interpolation uses edge value padding for out-of-boundary values, making this operation independent of input size when `scale_factor` is kept the same. This only has an effect when `mode` is `'bilinear'`. Default: `False`\nConstraints:\ndtype:bool\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.gt(input,other,out=None)\nParameter description: out(Tensor, optional) - the output tensor that must be a BoolTensor\nConstraints:\ndtype:booltensor\nstructure:tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.empty_like(input,dtype=None,layout=None,device=None,requires_grad=False,memory_format=torch.preserve_format)\nParameter description: dtype(`torch.dtype`, optional) - the desired data type of returned Tensor. Default: if `None`, defaults to the dtype of `input`.\nConstraints:\ndtype:torch.dtype\nstructure:Null\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.nn.RNN(*args,**kwargs,input_size=None,hidden_size=None,num_layers=None,nonlinearity=None,bias=None,batch_first=None,dropout=None,bidirectional=None)\nParameter description: hidden_size - The number of features in the hidden state h\nConstraints:\ndtype:int\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:[0,inf)\n\n###\n\nSignature: torch.nn.functional.avg_pool1d(input,kernel_size,stride=None,padding=0,ceil_mode=False,count_include_pad=True)\nParameter description: padding - implicit zero paddings on both sides of the input. Can be a single number or a tuple (padW,). Default: 0\nConstraints:\ndtype:bool;int\nstructure:tuple\nshape:Null\nndim:0;1\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.autograd.grad(outputs,inputs,grad_outputs=None,retain_graph=None,create_graph=False,only_inputs=True,allow_unused=False)\nParameter description: outputs(sequence of Tensor) - outputs of the differentiated function.\nConstraints:\ndtype:Null\nstructure:sequence;tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.repeat_interleave(input,repeats,dim=None)\nParameter description: dim(int, optional) - The dimension along which to repeat values.By default, use the flattened input array, and return a flat outputarray.\nConstraints:\ndtype:int\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.rand_like(input,dtype=None,layout=None,device=None,requires_grad=False,memory_format=torch.preserve_format)\nParameter description: dtype(`torch.dtype`, optional) - the desired data type of returned Tensor. Default: if `None`, defaults to the dtype of `input`.\nConstraints:",
            "constr": {
                "dtype": [
                    "torch.dtype"
                ],
                "structure": [],
                "shape": [],
                "ndim": [],
                "enum": [],
                "range": []
            },
            "signature": "torch.rand_like(input,dtype=None,layout=None,device=None,requires_grad=False,memory_format=torch.preserve_format)",
            "descp": "dtype(`torch.dtype`, optional) - the desired data type of returned Tensor. Default: if `None`, defaults to the dtype of `input`."
        },
        {
            "fname": "torch.nn.functional.conv_transpose1d.yaml",
            "arg": "weight",
            "prompt": "Signature: torch.distributed.recv(tensor,src=None,group=<objectobject>,tag=0)\nParameter description: tensor(Tensor) - Tensor to fill with received data.\nConstraints:\ndtype:Null\nstructure:tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.div(input,other,out=None)\nParameter description: out(Tensor, optional) - the output tensor.\nConstraints:\ndtype:Null\nstructure:tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.nn.TransformerDecoder(decoder_layer,num_layers,norm=None)\nParameter description: num_layers - the number of sub-decoder-layers in the decoder (required).\nConstraints:\ndtype:int\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:[0,inf)\n\n###\n\nSignature: torch.nn.quantized.functional.interpolate(input,size=None,scale_factor=None,mode=nearest,align_corners=None)\nParameter description: align_corners(bool, optional) - Geometrically, we consider the pixels of the input and output as squares rather than points. If set to `True`, the input and output tensors are aligned by the center points of their corner pixels, preserving the values at the corner pixels. If set to `False`, the input and output tensors are aligned by the corner points of their corner pixels, and the interpolation uses edge value padding for out-of-boundary values, making this operation independent of input size when `scale_factor` is kept the same. This only has an effect when `mode` is `'bilinear'`. Default: `False`\nConstraints:\ndtype:bool\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.gt(input,other,out=None)\nParameter description: out(Tensor, optional) - the output tensor that must be a BoolTensor\nConstraints:\ndtype:booltensor\nstructure:tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.empty_like(input,dtype=None,layout=None,device=None,requires_grad=False,memory_format=torch.preserve_format)\nParameter description: dtype(`torch.dtype`, optional) - the desired data type of returned Tensor. Default: if `None`, defaults to the dtype of `input`.\nConstraints:\ndtype:torch.dtype\nstructure:Null\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.nn.RNN(*args,**kwargs,input_size=None,hidden_size=None,num_layers=None,nonlinearity=None,bias=None,batch_first=None,dropout=None,bidirectional=None)\nParameter description: hidden_size - The number of features in the hidden state h\nConstraints:\ndtype:int\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:[0,inf)\n\n###\n\nSignature: torch.nn.functional.avg_pool1d(input,kernel_size,stride=None,padding=0,ceil_mode=False,count_include_pad=True)\nParameter description: padding - implicit zero paddings on both sides of the input. Can be a single number or a tuple (padW,). Default: 0\nConstraints:\ndtype:bool;int\nstructure:tuple\nshape:Null\nndim:0;1\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.autograd.grad(outputs,inputs,grad_outputs=None,retain_graph=None,create_graph=False,only_inputs=True,allow_unused=False)\nParameter description: outputs(sequence of Tensor) - outputs of the differentiated function.\nConstraints:\ndtype:Null\nstructure:sequence;tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.repeat_interleave(input,repeats,dim=None)\nParameter description: dim(int, optional) - The dimension along which to repeat values.By default, use the flattened input array, and return a flat outputarray.\nConstraints:\ndtype:int\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.nn.functional.conv_transpose1d(input,weight,bias=None,stride=1,padding=0,output_padding=0,groups=1,dilation=1)\nParameter description: weight - filters of shape (in _channels ,  out _channels/groups , kW) \nConstraints:",
            "constr": {
                "dtype": [],
                "structure": [],
                "shape": [
                    "(in _channels , out _channels/groups , kW)"
                ],
                "ndim": [],
                "enum": [],
                "range": []
            },
            "signature": "torch.nn.functional.conv_transpose1d(input,weight,bias=None,stride=1,padding=0,output_padding=0,groups=1,dilation=1)",
            "descp": "weight - filters of shape (in _channels ,  out _channels/groups , kW) "
        },
        {
            "fname": "torch.nn.fractionalmaxpool2d.yaml",
            "arg": "kernel_size",
            "prompt": "Signature: torch.distributed.recv(tensor,src=None,group=<objectobject>,tag=0)\nParameter description: tensor(Tensor) - Tensor to fill with received data.\nConstraints:\ndtype:Null\nstructure:tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.div(input,other,out=None)\nParameter description: out(Tensor, optional) - the output tensor.\nConstraints:\ndtype:Null\nstructure:tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.nn.TransformerDecoder(decoder_layer,num_layers,norm=None)\nParameter description: num_layers - the number of sub-decoder-layers in the decoder (required).\nConstraints:\ndtype:int\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:[0,inf)\n\n###\n\nSignature: torch.nn.quantized.functional.interpolate(input,size=None,scale_factor=None,mode=nearest,align_corners=None)\nParameter description: align_corners(bool, optional) - Geometrically, we consider the pixels of the input and output as squares rather than points. If set to `True`, the input and output tensors are aligned by the center points of their corner pixels, preserving the values at the corner pixels. If set to `False`, the input and output tensors are aligned by the corner points of their corner pixels, and the interpolation uses edge value padding for out-of-boundary values, making this operation independent of input size when `scale_factor` is kept the same. This only has an effect when `mode` is `'bilinear'`. Default: `False`\nConstraints:\ndtype:bool\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.gt(input,other,out=None)\nParameter description: out(Tensor, optional) - the output tensor that must be a BoolTensor\nConstraints:\ndtype:booltensor\nstructure:tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.empty_like(input,dtype=None,layout=None,device=None,requires_grad=False,memory_format=torch.preserve_format)\nParameter description: dtype(`torch.dtype`, optional) - the desired data type of returned Tensor. Default: if `None`, defaults to the dtype of `input`.\nConstraints:\ndtype:torch.dtype\nstructure:Null\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.nn.RNN(*args,**kwargs,input_size=None,hidden_size=None,num_layers=None,nonlinearity=None,bias=None,batch_first=None,dropout=None,bidirectional=None)\nParameter description: hidden_size - The number of features in the hidden state h\nConstraints:\ndtype:int\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:[0,inf)\n\n###\n\nSignature: torch.nn.functional.avg_pool1d(input,kernel_size,stride=None,padding=0,ceil_mode=False,count_include_pad=True)\nParameter description: padding - implicit zero paddings on both sides of the input. Can be a single number or a tuple (padW,). Default: 0\nConstraints:\ndtype:bool;int\nstructure:tuple\nshape:Null\nndim:0;1\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.autograd.grad(outputs,inputs,grad_outputs=None,retain_graph=None,create_graph=False,only_inputs=True,allow_unused=False)\nParameter description: outputs(sequence of Tensor) - outputs of the differentiated function.\nConstraints:\ndtype:Null\nstructure:sequence;tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.repeat_interleave(input,repeats,dim=None)\nParameter description: dim(int, optional) - The dimension along which to repeat values.By default, use the flattened input array, and return a flat outputarray.\nConstraints:\ndtype:int\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.nn.FractionalMaxPool2d(kernel_size,output_size=None,output_ratio=None,return_indices=False,_random_samples=None)\nParameter description: kernel_size - the size of the window to take a max over. Can be a single number k (for a square kernel of k x k) or a tuple (kh, kw)\nConstraints:",
            "constr": {
                "dtype": [
                    "int"
                ],
                "structure": [],
                "shape": [],
                "ndim": [
                    "0"
                ],
                "enum": [],
                "range": [
                    "[0,inf)"
                ]
            },
            "signature": "torch.nn.FractionalMaxPool2d(kernel_size,output_size=None,output_ratio=None,return_indices=False,_random_samples=None)",
            "descp": "kernel_size - the size of the window to take a max over. Can be a single number k (for a square kernel of k x k) or a tuple (kh, kw)"
        },
        {
            "fname": "torch.rot90.yaml",
            "arg": "input",
            "prompt": "Signature: torch.distributed.recv(tensor,src=None,group=<objectobject>,tag=0)\nParameter description: tensor(Tensor) - Tensor to fill with received data.\nConstraints:\ndtype:Null\nstructure:tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.div(input,other,out=None)\nParameter description: out(Tensor, optional) - the output tensor.\nConstraints:\ndtype:Null\nstructure:tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.nn.TransformerDecoder(decoder_layer,num_layers,norm=None)\nParameter description: num_layers - the number of sub-decoder-layers in the decoder (required).\nConstraints:\ndtype:int\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:[0,inf)\n\n###\n\nSignature: torch.nn.quantized.functional.interpolate(input,size=None,scale_factor=None,mode=nearest,align_corners=None)\nParameter description: align_corners(bool, optional) - Geometrically, we consider the pixels of the input and output as squares rather than points. If set to `True`, the input and output tensors are aligned by the center points of their corner pixels, preserving the values at the corner pixels. If set to `False`, the input and output tensors are aligned by the corner points of their corner pixels, and the interpolation uses edge value padding for out-of-boundary values, making this operation independent of input size when `scale_factor` is kept the same. This only has an effect when `mode` is `'bilinear'`. Default: `False`\nConstraints:\ndtype:bool\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.gt(input,other,out=None)\nParameter description: out(Tensor, optional) - the output tensor that must be a BoolTensor\nConstraints:\ndtype:booltensor\nstructure:tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.empty_like(input,dtype=None,layout=None,device=None,requires_grad=False,memory_format=torch.preserve_format)\nParameter description: dtype(`torch.dtype`, optional) - the desired data type of returned Tensor. Default: if `None`, defaults to the dtype of `input`.\nConstraints:\ndtype:torch.dtype\nstructure:Null\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.nn.RNN(*args,**kwargs,input_size=None,hidden_size=None,num_layers=None,nonlinearity=None,bias=None,batch_first=None,dropout=None,bidirectional=None)\nParameter description: hidden_size - The number of features in the hidden state h\nConstraints:\ndtype:int\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:[0,inf)\n\n###\n\nSignature: torch.nn.functional.avg_pool1d(input,kernel_size,stride=None,padding=0,ceil_mode=False,count_include_pad=True)\nParameter description: padding - implicit zero paddings on both sides of the input. Can be a single number or a tuple (padW,). Default: 0\nConstraints:\ndtype:bool;int\nstructure:tuple\nshape:Null\nndim:0;1\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.autograd.grad(outputs,inputs,grad_outputs=None,retain_graph=None,create_graph=False,only_inputs=True,allow_unused=False)\nParameter description: outputs(sequence of Tensor) - outputs of the differentiated function.\nConstraints:\ndtype:Null\nstructure:sequence;tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.repeat_interleave(input,repeats,dim=None)\nParameter description: dim(int, optional) - The dimension along which to repeat values.By default, use the flattened input array, and return a flat outputarray.\nConstraints:\ndtype:int\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.rot90(input,k,dims)\nParameter description: input(Tensor) - the input tensor.\nConstraints:",
            "constr": {
                "dtype": [],
                "structure": [
                    "tensor"
                ],
                "shape": [],
                "ndim": [],
                "enum": [],
                "range": []
            },
            "signature": "torch.rot90(input,k,dims)",
            "descp": "input(Tensor) - the input tensor."
        },
        {
            "fname": "torch.triu_indices.yaml",
            "arg": "dtype",
            "prompt": "Signature: torch.distributed.recv(tensor,src=None,group=<objectobject>,tag=0)\nParameter description: tensor(Tensor) - Tensor to fill with received data.\nConstraints:\ndtype:Null\nstructure:tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.div(input,other,out=None)\nParameter description: out(Tensor, optional) - the output tensor.\nConstraints:\ndtype:Null\nstructure:tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.nn.TransformerDecoder(decoder_layer,num_layers,norm=None)\nParameter description: num_layers - the number of sub-decoder-layers in the decoder (required).\nConstraints:\ndtype:int\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:[0,inf)\n\n###\n\nSignature: torch.nn.quantized.functional.interpolate(input,size=None,scale_factor=None,mode=nearest,align_corners=None)\nParameter description: align_corners(bool, optional) - Geometrically, we consider the pixels of the input and output as squares rather than points. If set to `True`, the input and output tensors are aligned by the center points of their corner pixels, preserving the values at the corner pixels. If set to `False`, the input and output tensors are aligned by the corner points of their corner pixels, and the interpolation uses edge value padding for out-of-boundary values, making this operation independent of input size when `scale_factor` is kept the same. This only has an effect when `mode` is `'bilinear'`. Default: `False`\nConstraints:\ndtype:bool\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.gt(input,other,out=None)\nParameter description: out(Tensor, optional) - the output tensor that must be a BoolTensor\nConstraints:\ndtype:booltensor\nstructure:tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.empty_like(input,dtype=None,layout=None,device=None,requires_grad=False,memory_format=torch.preserve_format)\nParameter description: dtype(`torch.dtype`, optional) - the desired data type of returned Tensor. Default: if `None`, defaults to the dtype of `input`.\nConstraints:\ndtype:torch.dtype\nstructure:Null\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.nn.RNN(*args,**kwargs,input_size=None,hidden_size=None,num_layers=None,nonlinearity=None,bias=None,batch_first=None,dropout=None,bidirectional=None)\nParameter description: hidden_size - The number of features in the hidden state h\nConstraints:\ndtype:int\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:[0,inf)\n\n###\n\nSignature: torch.nn.functional.avg_pool1d(input,kernel_size,stride=None,padding=0,ceil_mode=False,count_include_pad=True)\nParameter description: padding - implicit zero paddings on both sides of the input. Can be a single number or a tuple (padW,). Default: 0\nConstraints:\ndtype:bool;int\nstructure:tuple\nshape:Null\nndim:0;1\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.autograd.grad(outputs,inputs,grad_outputs=None,retain_graph=None,create_graph=False,only_inputs=True,allow_unused=False)\nParameter description: outputs(sequence of Tensor) - outputs of the differentiated function.\nConstraints:\ndtype:Null\nstructure:sequence;tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.repeat_interleave(input,repeats,dim=None)\nParameter description: dim(int, optional) - The dimension along which to repeat values.By default, use the flattened input array, and return a flat outputarray.\nConstraints:\ndtype:int\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.triu_indices(row,col,offset=0,dtype=torch.long,device=cpu,layout=torch.strided)\nParameter description: dtype(`torch.dtype`, optional) - the desired data type of returned tensor. Default: if `None`, `torch.long`.\nConstraints:",
            "constr": {
                "dtype": [
                    "torch.dtype"
                ],
                "structure": [],
                "shape": [],
                "ndim": [],
                "enum": [],
                "range": []
            },
            "signature": "torch.triu_indices(row,col,offset=0,dtype=torch.long,device=cpu,layout=torch.strided)",
            "descp": "dtype(`torch.dtype`, optional) - the desired data type of returned tensor. Default: if `None`, `torch.long`."
        },
        {
            "fname": "torch.nn.functional.binary_cross_entropy_with_logits.yaml",
            "arg": "pos_weight",
            "prompt": "Signature: torch.distributed.recv(tensor,src=None,group=<objectobject>,tag=0)\nParameter description: tensor(Tensor) - Tensor to fill with received data.\nConstraints:\ndtype:Null\nstructure:tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.div(input,other,out=None)\nParameter description: out(Tensor, optional) - the output tensor.\nConstraints:\ndtype:Null\nstructure:tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.nn.TransformerDecoder(decoder_layer,num_layers,norm=None)\nParameter description: num_layers - the number of sub-decoder-layers in the decoder (required).\nConstraints:\ndtype:int\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:[0,inf)\n\n###\n\nSignature: torch.nn.quantized.functional.interpolate(input,size=None,scale_factor=None,mode=nearest,align_corners=None)\nParameter description: align_corners(bool, optional) - Geometrically, we consider the pixels of the input and output as squares rather than points. If set to `True`, the input and output tensors are aligned by the center points of their corner pixels, preserving the values at the corner pixels. If set to `False`, the input and output tensors are aligned by the corner points of their corner pixels, and the interpolation uses edge value padding for out-of-boundary values, making this operation independent of input size when `scale_factor` is kept the same. This only has an effect when `mode` is `'bilinear'`. Default: `False`\nConstraints:\ndtype:bool\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.gt(input,other,out=None)\nParameter description: out(Tensor, optional) - the output tensor that must be a BoolTensor\nConstraints:\ndtype:booltensor\nstructure:tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.empty_like(input,dtype=None,layout=None,device=None,requires_grad=False,memory_format=torch.preserve_format)\nParameter description: dtype(`torch.dtype`, optional) - the desired data type of returned Tensor. Default: if `None`, defaults to the dtype of `input`.\nConstraints:\ndtype:torch.dtype\nstructure:Null\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.nn.RNN(*args,**kwargs,input_size=None,hidden_size=None,num_layers=None,nonlinearity=None,bias=None,batch_first=None,dropout=None,bidirectional=None)\nParameter description: hidden_size - The number of features in the hidden state h\nConstraints:\ndtype:int\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:[0,inf)\n\n###\n\nSignature: torch.nn.functional.avg_pool1d(input,kernel_size,stride=None,padding=0,ceil_mode=False,count_include_pad=True)\nParameter description: padding - implicit zero paddings on both sides of the input. Can be a single number or a tuple (padW,). Default: 0\nConstraints:\ndtype:bool;int\nstructure:tuple\nshape:Null\nndim:0;1\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.autograd.grad(outputs,inputs,grad_outputs=None,retain_graph=None,create_graph=False,only_inputs=True,allow_unused=False)\nParameter description: outputs(sequence of Tensor) - outputs of the differentiated function.\nConstraints:\ndtype:Null\nstructure:sequence;tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.repeat_interleave(input,repeats,dim=None)\nParameter description: dim(int, optional) - The dimension along which to repeat values.By default, use the flattened input array, and return a flat outputarray.\nConstraints:\ndtype:int\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.nn.functional.binary_cross_entropy_with_logits(input,target,weight=None,size_average=None,reduce=None,reduction=mean,pos_weight=None)\nParameter description: pos_weight(Tensor, optional) - a weight of positive examples. Must be a vector with length equal to the number of classes.\nConstraints:",
            "constr": {
                "dtype": [
                    "numeric"
                ],
                "structure": [
                    "vector",
                    "tensor"
                ],
                "shape": [],
                "ndim": [
                    "1"
                ],
                "enum": [],
                "range": []
            },
            "signature": "torch.nn.functional.binary_cross_entropy_with_logits(input,target,weight=None,size_average=None,reduce=None,reduction=mean,pos_weight=None)",
            "descp": "pos_weight(Tensor, optional) - a weight of positive examples. Must be a vector with length equal to the number of classes."
        },
        {
            "fname": "torch.autograd.grad.yaml",
            "arg": "inputs",
            "prompt": "Signature: torch.distributed.recv(tensor,src=None,group=<objectobject>,tag=0)\nParameter description: tensor(Tensor) - Tensor to fill with received data.\nConstraints:\ndtype:Null\nstructure:tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.div(input,other,out=None)\nParameter description: out(Tensor, optional) - the output tensor.\nConstraints:\ndtype:Null\nstructure:tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.nn.TransformerDecoder(decoder_layer,num_layers,norm=None)\nParameter description: num_layers - the number of sub-decoder-layers in the decoder (required).\nConstraints:\ndtype:int\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:[0,inf)\n\n###\n\nSignature: torch.nn.quantized.functional.interpolate(input,size=None,scale_factor=None,mode=nearest,align_corners=None)\nParameter description: align_corners(bool, optional) - Geometrically, we consider the pixels of the input and output as squares rather than points. If set to `True`, the input and output tensors are aligned by the center points of their corner pixels, preserving the values at the corner pixels. If set to `False`, the input and output tensors are aligned by the corner points of their corner pixels, and the interpolation uses edge value padding for out-of-boundary values, making this operation independent of input size when `scale_factor` is kept the same. This only has an effect when `mode` is `'bilinear'`. Default: `False`\nConstraints:\ndtype:bool\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.gt(input,other,out=None)\nParameter description: out(Tensor, optional) - the output tensor that must be a BoolTensor\nConstraints:\ndtype:booltensor\nstructure:tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.empty_like(input,dtype=None,layout=None,device=None,requires_grad=False,memory_format=torch.preserve_format)\nParameter description: dtype(`torch.dtype`, optional) - the desired data type of returned Tensor. Default: if `None`, defaults to the dtype of `input`.\nConstraints:\ndtype:torch.dtype\nstructure:Null\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.nn.RNN(*args,**kwargs,input_size=None,hidden_size=None,num_layers=None,nonlinearity=None,bias=None,batch_first=None,dropout=None,bidirectional=None)\nParameter description: hidden_size - The number of features in the hidden state h\nConstraints:\ndtype:int\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:[0,inf)\n\n###\n\nSignature: torch.nn.functional.avg_pool1d(input,kernel_size,stride=None,padding=0,ceil_mode=False,count_include_pad=True)\nParameter description: padding - implicit zero paddings on both sides of the input. Can be a single number or a tuple (padW,). Default: 0\nConstraints:\ndtype:bool;int\nstructure:tuple\nshape:Null\nndim:0;1\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.autograd.grad(outputs,inputs,grad_outputs=None,retain_graph=None,create_graph=False,only_inputs=True,allow_unused=False)\nParameter description: outputs(sequence of Tensor) - outputs of the differentiated function.\nConstraints:\ndtype:Null\nstructure:sequence;tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.repeat_interleave(input,repeats,dim=None)\nParameter description: dim(int, optional) - The dimension along which to repeat values.By default, use the flattened input array, and return a flat outputarray.\nConstraints:\ndtype:int\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.autograd.grad(outputs,inputs,grad_outputs=None,retain_graph=None,create_graph=False,only_inputs=True,allow_unused=False)\nParameter description: inputs(sequence of Tensor) - Inputs w.r.t. which the gradient will be returned (and not accumulated into `.grad`).\nConstraints:",
            "constr": {
                "dtype": [
                    "bool"
                ],
                "structure": [
                    "sequence",
                    "tensor"
                ],
                "shape": [],
                "ndim": [
                    "0"
                ],
                "enum": [],
                "range": []
            },
            "signature": "torch.autograd.grad(outputs,inputs,grad_outputs=None,retain_graph=None,create_graph=False,only_inputs=True,allow_unused=False)",
            "descp": "inputs(sequence of Tensor) - Inputs w.r.t. which the gradient will be returned (and not accumulated into `.grad`)."
        },
        {
            "fname": "torch.addmm.yaml",
            "arg": "out",
            "prompt": "Signature: torch.distributed.recv(tensor,src=None,group=<objectobject>,tag=0)\nParameter description: tensor(Tensor) - Tensor to fill with received data.\nConstraints:\ndtype:Null\nstructure:tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.div(input,other,out=None)\nParameter description: out(Tensor, optional) - the output tensor.\nConstraints:\ndtype:Null\nstructure:tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.nn.TransformerDecoder(decoder_layer,num_layers,norm=None)\nParameter description: num_layers - the number of sub-decoder-layers in the decoder (required).\nConstraints:\ndtype:int\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:[0,inf)\n\n###\n\nSignature: torch.nn.quantized.functional.interpolate(input,size=None,scale_factor=None,mode=nearest,align_corners=None)\nParameter description: align_corners(bool, optional) - Geometrically, we consider the pixels of the input and output as squares rather than points. If set to `True`, the input and output tensors are aligned by the center points of their corner pixels, preserving the values at the corner pixels. If set to `False`, the input and output tensors are aligned by the corner points of their corner pixels, and the interpolation uses edge value padding for out-of-boundary values, making this operation independent of input size when `scale_factor` is kept the same. This only has an effect when `mode` is `'bilinear'`. Default: `False`\nConstraints:\ndtype:bool\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.gt(input,other,out=None)\nParameter description: out(Tensor, optional) - the output tensor that must be a BoolTensor\nConstraints:\ndtype:booltensor\nstructure:tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.empty_like(input,dtype=None,layout=None,device=None,requires_grad=False,memory_format=torch.preserve_format)\nParameter description: dtype(`torch.dtype`, optional) - the desired data type of returned Tensor. Default: if `None`, defaults to the dtype of `input`.\nConstraints:\ndtype:torch.dtype\nstructure:Null\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.nn.RNN(*args,**kwargs,input_size=None,hidden_size=None,num_layers=None,nonlinearity=None,bias=None,batch_first=None,dropout=None,bidirectional=None)\nParameter description: hidden_size - The number of features in the hidden state h\nConstraints:\ndtype:int\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:[0,inf)\n\n###\n\nSignature: torch.nn.functional.avg_pool1d(input,kernel_size,stride=None,padding=0,ceil_mode=False,count_include_pad=True)\nParameter description: padding - implicit zero paddings on both sides of the input. Can be a single number or a tuple (padW,). Default: 0\nConstraints:\ndtype:bool;int\nstructure:tuple\nshape:Null\nndim:0;1\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.autograd.grad(outputs,inputs,grad_outputs=None,retain_graph=None,create_graph=False,only_inputs=True,allow_unused=False)\nParameter description: outputs(sequence of Tensor) - outputs of the differentiated function.\nConstraints:\ndtype:Null\nstructure:sequence;tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.repeat_interleave(input,repeats,dim=None)\nParameter description: dim(int, optional) - The dimension along which to repeat values.By default, use the flattened input array, and return a flat outputarray.\nConstraints:\ndtype:int\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.addmm(input,mat1,mat2,beta=1,alpha=1,out=None)\nParameter description: out(Tensor, optional) - the output tensor.\nConstraints:",
            "constr": {
                "dtype": [],
                "structure": [
                    "tensor"
                ],
                "shape": [],
                "ndim": [],
                "enum": [],
                "range": []
            },
            "signature": "torch.addmm(input,mat1,mat2,beta=1,alpha=1,out=None)",
            "descp": "out(Tensor, optional) - the output tensor."
        },
        {
            "fname": "torch.bmm.yaml",
            "arg": "input",
            "prompt": "Signature: torch.distributed.recv(tensor,src=None,group=<objectobject>,tag=0)\nParameter description: tensor(Tensor) - Tensor to fill with received data.\nConstraints:\ndtype:Null\nstructure:tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.div(input,other,out=None)\nParameter description: out(Tensor, optional) - the output tensor.\nConstraints:\ndtype:Null\nstructure:tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.nn.TransformerDecoder(decoder_layer,num_layers,norm=None)\nParameter description: num_layers - the number of sub-decoder-layers in the decoder (required).\nConstraints:\ndtype:int\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:[0,inf)\n\n###\n\nSignature: torch.nn.quantized.functional.interpolate(input,size=None,scale_factor=None,mode=nearest,align_corners=None)\nParameter description: align_corners(bool, optional) - Geometrically, we consider the pixels of the input and output as squares rather than points. If set to `True`, the input and output tensors are aligned by the center points of their corner pixels, preserving the values at the corner pixels. If set to `False`, the input and output tensors are aligned by the corner points of their corner pixels, and the interpolation uses edge value padding for out-of-boundary values, making this operation independent of input size when `scale_factor` is kept the same. This only has an effect when `mode` is `'bilinear'`. Default: `False`\nConstraints:\ndtype:bool\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.gt(input,other,out=None)\nParameter description: out(Tensor, optional) - the output tensor that must be a BoolTensor\nConstraints:\ndtype:booltensor\nstructure:tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.empty_like(input,dtype=None,layout=None,device=None,requires_grad=False,memory_format=torch.preserve_format)\nParameter description: dtype(`torch.dtype`, optional) - the desired data type of returned Tensor. Default: if `None`, defaults to the dtype of `input`.\nConstraints:\ndtype:torch.dtype\nstructure:Null\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.nn.RNN(*args,**kwargs,input_size=None,hidden_size=None,num_layers=None,nonlinearity=None,bias=None,batch_first=None,dropout=None,bidirectional=None)\nParameter description: hidden_size - The number of features in the hidden state h\nConstraints:\ndtype:int\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:[0,inf)\n\n###\n\nSignature: torch.nn.functional.avg_pool1d(input,kernel_size,stride=None,padding=0,ceil_mode=False,count_include_pad=True)\nParameter description: padding - implicit zero paddings on both sides of the input. Can be a single number or a tuple (padW,). Default: 0\nConstraints:\ndtype:bool;int\nstructure:tuple\nshape:Null\nndim:0;1\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.autograd.grad(outputs,inputs,grad_outputs=None,retain_graph=None,create_graph=False,only_inputs=True,allow_unused=False)\nParameter description: outputs(sequence of Tensor) - outputs of the differentiated function.\nConstraints:\ndtype:Null\nstructure:sequence;tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.repeat_interleave(input,repeats,dim=None)\nParameter description: dim(int, optional) - The dimension along which to repeat values.By default, use the flattened input array, and return a flat outputarray.\nConstraints:\ndtype:int\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.bmm(input,mat2,out=None)\nParameter description: input(Tensor) - the first batch of matrices to be multiplied\nConstraints:",
            "constr": {
                "dtype": [
                    "numeric"
                ],
                "structure": [
                    "tensor"
                ],
                "shape": [],
                "ndim": [],
                "enum": [],
                "range": []
            },
            "signature": "torch.bmm(input,mat2,out=None)",
            "descp": "input(Tensor) - the first batch of matrices to be multiplied"
        },
        {
            "fname": "torch.sum2.yaml",
            "arg": "input",
            "prompt": "Signature: torch.distributed.recv(tensor,src=None,group=<objectobject>,tag=0)\nParameter description: tensor(Tensor) - Tensor to fill with received data.\nConstraints:\ndtype:Null\nstructure:tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.div(input,other,out=None)\nParameter description: out(Tensor, optional) - the output tensor.\nConstraints:\ndtype:Null\nstructure:tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.nn.TransformerDecoder(decoder_layer,num_layers,norm=None)\nParameter description: num_layers - the number of sub-decoder-layers in the decoder (required).\nConstraints:\ndtype:int\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:[0,inf)\n\n###\n\nSignature: torch.nn.quantized.functional.interpolate(input,size=None,scale_factor=None,mode=nearest,align_corners=None)\nParameter description: align_corners(bool, optional) - Geometrically, we consider the pixels of the input and output as squares rather than points. If set to `True`, the input and output tensors are aligned by the center points of their corner pixels, preserving the values at the corner pixels. If set to `False`, the input and output tensors are aligned by the corner points of their corner pixels, and the interpolation uses edge value padding for out-of-boundary values, making this operation independent of input size when `scale_factor` is kept the same. This only has an effect when `mode` is `'bilinear'`. Default: `False`\nConstraints:\ndtype:bool\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.gt(input,other,out=None)\nParameter description: out(Tensor, optional) - the output tensor that must be a BoolTensor\nConstraints:\ndtype:booltensor\nstructure:tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.empty_like(input,dtype=None,layout=None,device=None,requires_grad=False,memory_format=torch.preserve_format)\nParameter description: dtype(`torch.dtype`, optional) - the desired data type of returned Tensor. Default: if `None`, defaults to the dtype of `input`.\nConstraints:\ndtype:torch.dtype\nstructure:Null\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.nn.RNN(*args,**kwargs,input_size=None,hidden_size=None,num_layers=None,nonlinearity=None,bias=None,batch_first=None,dropout=None,bidirectional=None)\nParameter description: hidden_size - The number of features in the hidden state h\nConstraints:\ndtype:int\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:[0,inf)\n\n###\n\nSignature: torch.nn.functional.avg_pool1d(input,kernel_size,stride=None,padding=0,ceil_mode=False,count_include_pad=True)\nParameter description: padding - implicit zero paddings on both sides of the input. Can be a single number or a tuple (padW,). Default: 0\nConstraints:\ndtype:bool;int\nstructure:tuple\nshape:Null\nndim:0;1\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.autograd.grad(outputs,inputs,grad_outputs=None,retain_graph=None,create_graph=False,only_inputs=True,allow_unused=False)\nParameter description: outputs(sequence of Tensor) - outputs of the differentiated function.\nConstraints:\ndtype:Null\nstructure:sequence;tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.repeat_interleave(input,repeats,dim=None)\nParameter description: dim(int, optional) - The dimension along which to repeat values.By default, use the flattened input array, and return a flat outputarray.\nConstraints:\ndtype:int\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.sum(input,dim,keepdim=False,dtype=None)\nParameter description: input(Tensor) - the input tensor.\nConstraints:",
            "constr": {
                "dtype": [],
                "structure": [
                    "tensor"
                ],
                "shape": [],
                "ndim": [],
                "enum": [],
                "range": []
            },
            "signature": "torch.sum(input,dim,keepdim=False,dtype=None)",
            "descp": "input(Tensor) - the input tensor."
        },
        {
            "fname": "torch.addr.yaml",
            "arg": "vec1",
            "prompt": "Signature: torch.distributed.recv(tensor,src=None,group=<objectobject>,tag=0)\nParameter description: tensor(Tensor) - Tensor to fill with received data.\nConstraints:\ndtype:Null\nstructure:tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.div(input,other,out=None)\nParameter description: out(Tensor, optional) - the output tensor.\nConstraints:\ndtype:Null\nstructure:tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.nn.TransformerDecoder(decoder_layer,num_layers,norm=None)\nParameter description: num_layers - the number of sub-decoder-layers in the decoder (required).\nConstraints:\ndtype:int\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:[0,inf)\n\n###\n\nSignature: torch.nn.quantized.functional.interpolate(input,size=None,scale_factor=None,mode=nearest,align_corners=None)\nParameter description: align_corners(bool, optional) - Geometrically, we consider the pixels of the input and output as squares rather than points. If set to `True`, the input and output tensors are aligned by the center points of their corner pixels, preserving the values at the corner pixels. If set to `False`, the input and output tensors are aligned by the corner points of their corner pixels, and the interpolation uses edge value padding for out-of-boundary values, making this operation independent of input size when `scale_factor` is kept the same. This only has an effect when `mode` is `'bilinear'`. Default: `False`\nConstraints:\ndtype:bool\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.gt(input,other,out=None)\nParameter description: out(Tensor, optional) - the output tensor that must be a BoolTensor\nConstraints:\ndtype:booltensor\nstructure:tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.empty_like(input,dtype=None,layout=None,device=None,requires_grad=False,memory_format=torch.preserve_format)\nParameter description: dtype(`torch.dtype`, optional) - the desired data type of returned Tensor. Default: if `None`, defaults to the dtype of `input`.\nConstraints:\ndtype:torch.dtype\nstructure:Null\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.nn.RNN(*args,**kwargs,input_size=None,hidden_size=None,num_layers=None,nonlinearity=None,bias=None,batch_first=None,dropout=None,bidirectional=None)\nParameter description: hidden_size - The number of features in the hidden state h\nConstraints:\ndtype:int\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:[0,inf)\n\n###\n\nSignature: torch.nn.functional.avg_pool1d(input,kernel_size,stride=None,padding=0,ceil_mode=False,count_include_pad=True)\nParameter description: padding - implicit zero paddings on both sides of the input. Can be a single number or a tuple (padW,). Default: 0\nConstraints:\ndtype:bool;int\nstructure:tuple\nshape:Null\nndim:0;1\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.autograd.grad(outputs,inputs,grad_outputs=None,retain_graph=None,create_graph=False,only_inputs=True,allow_unused=False)\nParameter description: outputs(sequence of Tensor) - outputs of the differentiated function.\nConstraints:\ndtype:Null\nstructure:sequence;tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.repeat_interleave(input,repeats,dim=None)\nParameter description: dim(int, optional) - The dimension along which to repeat values.By default, use the flattened input array, and return a flat outputarray.\nConstraints:\ndtype:int\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.addr(input,vec1,vec2,beta=1,alpha=1,out=None)\nParameter description: vec1(Tensor) - the first vector of the outer product\nConstraints:",
            "constr": {
                "dtype": [],
                "structure": [
                    "vector",
                    "tensor"
                ],
                "shape": [],
                "ndim": [],
                "enum": [],
                "range": []
            },
            "signature": "torch.addr(input,vec1,vec2,beta=1,alpha=1,out=None)",
            "descp": "vec1(Tensor) - the first vector of the outer product"
        },
        {
            "fname": "torch.nn.replicationpad3d.yaml",
            "arg": "padding",
            "prompt": "Signature: torch.distributed.recv(tensor,src=None,group=<objectobject>,tag=0)\nParameter description: tensor(Tensor) - Tensor to fill with received data.\nConstraints:\ndtype:Null\nstructure:tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.div(input,other,out=None)\nParameter description: out(Tensor, optional) - the output tensor.\nConstraints:\ndtype:Null\nstructure:tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.nn.TransformerDecoder(decoder_layer,num_layers,norm=None)\nParameter description: num_layers - the number of sub-decoder-layers in the decoder (required).\nConstraints:\ndtype:int\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:[0,inf)\n\n###\n\nSignature: torch.nn.quantized.functional.interpolate(input,size=None,scale_factor=None,mode=nearest,align_corners=None)\nParameter description: align_corners(bool, optional) - Geometrically, we consider the pixels of the input and output as squares rather than points. If set to `True`, the input and output tensors are aligned by the center points of their corner pixels, preserving the values at the corner pixels. If set to `False`, the input and output tensors are aligned by the corner points of their corner pixels, and the interpolation uses edge value padding for out-of-boundary values, making this operation independent of input size when `scale_factor` is kept the same. This only has an effect when `mode` is `'bilinear'`. Default: `False`\nConstraints:\ndtype:bool\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.gt(input,other,out=None)\nParameter description: out(Tensor, optional) - the output tensor that must be a BoolTensor\nConstraints:\ndtype:booltensor\nstructure:tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.empty_like(input,dtype=None,layout=None,device=None,requires_grad=False,memory_format=torch.preserve_format)\nParameter description: dtype(`torch.dtype`, optional) - the desired data type of returned Tensor. Default: if `None`, defaults to the dtype of `input`.\nConstraints:\ndtype:torch.dtype\nstructure:Null\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.nn.RNN(*args,**kwargs,input_size=None,hidden_size=None,num_layers=None,nonlinearity=None,bias=None,batch_first=None,dropout=None,bidirectional=None)\nParameter description: hidden_size - The number of features in the hidden state h\nConstraints:\ndtype:int\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:[0,inf)\n\n###\n\nSignature: torch.nn.functional.avg_pool1d(input,kernel_size,stride=None,padding=0,ceil_mode=False,count_include_pad=True)\nParameter description: padding - implicit zero paddings on both sides of the input. Can be a single number or a tuple (padW,). Default: 0\nConstraints:\ndtype:bool;int\nstructure:tuple\nshape:Null\nndim:0;1\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.autograd.grad(outputs,inputs,grad_outputs=None,retain_graph=None,create_graph=False,only_inputs=True,allow_unused=False)\nParameter description: outputs(sequence of Tensor) - outputs of the differentiated function.\nConstraints:\ndtype:Null\nstructure:sequence;tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.repeat_interleave(input,repeats,dim=None)\nParameter description: dim(int, optional) - The dimension along which to repeat values.By default, use the flattened input array, and return a flat outputarray.\nConstraints:\ndtype:int\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.nn.ReplicationPad3d(padding)\nParameter description: padding(int, tuple) - the size of the padding. If is int, uses the same padding in all boundaries. If a 6-tuple, uses (padding _left , padding _right , padding _top , padding _bottom , padding _front , padding _back )\nConstraints:",
            "constr": {
                "dtype": [
                    "int"
                ],
                "structure": [
                    "tuple"
                ],
                "shape": [
                    "[6]"
                ],
                "ndim": [],
                "enum": [],
                "range": [
                    "[0,inf)"
                ]
            },
            "signature": "torch.nn.ReplicationPad3d(padding)",
            "descp": "padding(int, tuple) - the size of the padding. If is int, uses the same padding in all boundaries. If a 6-tuple, uses (padding _left , padding _right , padding _top , padding _bottom , padding _front , padding _back )"
        },
        {
            "fname": "torch.nn.maxpool1d.yaml",
            "arg": "kernel_size",
            "prompt": "Signature: torch.distributed.recv(tensor,src=None,group=<objectobject>,tag=0)\nParameter description: tensor(Tensor) - Tensor to fill with received data.\nConstraints:\ndtype:Null\nstructure:tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.div(input,other,out=None)\nParameter description: out(Tensor, optional) - the output tensor.\nConstraints:\ndtype:Null\nstructure:tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.nn.TransformerDecoder(decoder_layer,num_layers,norm=None)\nParameter description: num_layers - the number of sub-decoder-layers in the decoder (required).\nConstraints:\ndtype:int\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:[0,inf)\n\n###\n\nSignature: torch.nn.quantized.functional.interpolate(input,size=None,scale_factor=None,mode=nearest,align_corners=None)\nParameter description: align_corners(bool, optional) - Geometrically, we consider the pixels of the input and output as squares rather than points. If set to `True`, the input and output tensors are aligned by the center points of their corner pixels, preserving the values at the corner pixels. If set to `False`, the input and output tensors are aligned by the corner points of their corner pixels, and the interpolation uses edge value padding for out-of-boundary values, making this operation independent of input size when `scale_factor` is kept the same. This only has an effect when `mode` is `'bilinear'`. Default: `False`\nConstraints:\ndtype:bool\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.gt(input,other,out=None)\nParameter description: out(Tensor, optional) - the output tensor that must be a BoolTensor\nConstraints:\ndtype:booltensor\nstructure:tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.empty_like(input,dtype=None,layout=None,device=None,requires_grad=False,memory_format=torch.preserve_format)\nParameter description: dtype(`torch.dtype`, optional) - the desired data type of returned Tensor. Default: if `None`, defaults to the dtype of `input`.\nConstraints:\ndtype:torch.dtype\nstructure:Null\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.nn.RNN(*args,**kwargs,input_size=None,hidden_size=None,num_layers=None,nonlinearity=None,bias=None,batch_first=None,dropout=None,bidirectional=None)\nParameter description: hidden_size - The number of features in the hidden state h\nConstraints:\ndtype:int\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:[0,inf)\n\n###\n\nSignature: torch.nn.functional.avg_pool1d(input,kernel_size,stride=None,padding=0,ceil_mode=False,count_include_pad=True)\nParameter description: padding - implicit zero paddings on both sides of the input. Can be a single number or a tuple (padW,). Default: 0\nConstraints:\ndtype:bool;int\nstructure:tuple\nshape:Null\nndim:0;1\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.autograd.grad(outputs,inputs,grad_outputs=None,retain_graph=None,create_graph=False,only_inputs=True,allow_unused=False)\nParameter description: outputs(sequence of Tensor) - outputs of the differentiated function.\nConstraints:\ndtype:Null\nstructure:sequence;tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.repeat_interleave(input,repeats,dim=None)\nParameter description: dim(int, optional) - The dimension along which to repeat values.By default, use the flattened input array, and return a flat outputarray.\nConstraints:\ndtype:int\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.nn.MaxPool1d(kernel_size,stride=None,padding=0,dilation=1,return_indices=False,ceil_mode=False)\nParameter description: kernel_size - the size of the window to take a max over\nConstraints:",
            "constr": {
                "dtype": [
                    "int"
                ],
                "structure": [],
                "shape": [],
                "ndim": [],
                "enum": [],
                "range": [
                    "[0,inf)"
                ]
            },
            "signature": "torch.nn.MaxPool1d(kernel_size,stride=None,padding=0,dilation=1,return_indices=False,ceil_mode=False)",
            "descp": "kernel_size - the size of the window to take a max over"
        },
        {
            "fname": "torch.nn.functional.grid_sample.yaml",
            "arg": "input",
            "prompt": "Signature: torch.distributed.recv(tensor,src=None,group=<objectobject>,tag=0)\nParameter description: tensor(Tensor) - Tensor to fill with received data.\nConstraints:\ndtype:Null\nstructure:tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.div(input,other,out=None)\nParameter description: out(Tensor, optional) - the output tensor.\nConstraints:\ndtype:Null\nstructure:tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.nn.TransformerDecoder(decoder_layer,num_layers,norm=None)\nParameter description: num_layers - the number of sub-decoder-layers in the decoder (required).\nConstraints:\ndtype:int\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:[0,inf)\n\n###\n\nSignature: torch.nn.quantized.functional.interpolate(input,size=None,scale_factor=None,mode=nearest,align_corners=None)\nParameter description: align_corners(bool, optional) - Geometrically, we consider the pixels of the input and output as squares rather than points. If set to `True`, the input and output tensors are aligned by the center points of their corner pixels, preserving the values at the corner pixels. If set to `False`, the input and output tensors are aligned by the corner points of their corner pixels, and the interpolation uses edge value padding for out-of-boundary values, making this operation independent of input size when `scale_factor` is kept the same. This only has an effect when `mode` is `'bilinear'`. Default: `False`\nConstraints:\ndtype:bool\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.gt(input,other,out=None)\nParameter description: out(Tensor, optional) - the output tensor that must be a BoolTensor\nConstraints:\ndtype:booltensor\nstructure:tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.empty_like(input,dtype=None,layout=None,device=None,requires_grad=False,memory_format=torch.preserve_format)\nParameter description: dtype(`torch.dtype`, optional) - the desired data type of returned Tensor. Default: if `None`, defaults to the dtype of `input`.\nConstraints:\ndtype:torch.dtype\nstructure:Null\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.nn.RNN(*args,**kwargs,input_size=None,hidden_size=None,num_layers=None,nonlinearity=None,bias=None,batch_first=None,dropout=None,bidirectional=None)\nParameter description: hidden_size - The number of features in the hidden state h\nConstraints:\ndtype:int\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:[0,inf)\n\n###\n\nSignature: torch.nn.functional.avg_pool1d(input,kernel_size,stride=None,padding=0,ceil_mode=False,count_include_pad=True)\nParameter description: padding - implicit zero paddings on both sides of the input. Can be a single number or a tuple (padW,). Default: 0\nConstraints:\ndtype:bool;int\nstructure:tuple\nshape:Null\nndim:0;1\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.autograd.grad(outputs,inputs,grad_outputs=None,retain_graph=None,create_graph=False,only_inputs=True,allow_unused=False)\nParameter description: outputs(sequence of Tensor) - outputs of the differentiated function.\nConstraints:\ndtype:Null\nstructure:sequence;tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.repeat_interleave(input,repeats,dim=None)\nParameter description: dim(int, optional) - The dimension along which to repeat values.By default, use the flattened input array, and return a flat outputarray.\nConstraints:\ndtype:int\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.nn.functional.grid_sample(input,grid,mode=bilinear,padding_mode=zeros,align_corners=None)\nParameter description: input(Tensor) - input of shape (N, C, H_in, W_in)  (4-D case) or (N, C, D_in, H_in, W_in)  (5-D case)\nConstraints:",
            "constr": {
                "dtype": [],
                "structure": [
                    "tensor"
                ],
                "shape": [
                    "(N, C, H_in, W_in)",
                    "(N, C, D_in, H_in, W_in)"
                ],
                "ndim": [
                    "4",
                    "5"
                ],
                "enum": [],
                "range": []
            },
            "signature": "torch.nn.functional.grid_sample(input,grid,mode=bilinear,padding_mode=zeros,align_corners=None)",
            "descp": "input(Tensor) - input of shape (N, C, H_in, W_in)  (4-D case) or (N, C, D_in, H_in, W_in)  (5-D case)"
        },
        {
            "fname": "torch.nn.cosinesimilarity.yaml",
            "arg": "dim",
            "prompt": "Signature: torch.distributed.recv(tensor,src=None,group=<objectobject>,tag=0)\nParameter description: tensor(Tensor) - Tensor to fill with received data.\nConstraints:\ndtype:Null\nstructure:tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.div(input,other,out=None)\nParameter description: out(Tensor, optional) - the output tensor.\nConstraints:\ndtype:Null\nstructure:tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.nn.TransformerDecoder(decoder_layer,num_layers,norm=None)\nParameter description: num_layers - the number of sub-decoder-layers in the decoder (required).\nConstraints:\ndtype:int\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:[0,inf)\n\n###\n\nSignature: torch.nn.quantized.functional.interpolate(input,size=None,scale_factor=None,mode=nearest,align_corners=None)\nParameter description: align_corners(bool, optional) - Geometrically, we consider the pixels of the input and output as squares rather than points. If set to `True`, the input and output tensors are aligned by the center points of their corner pixels, preserving the values at the corner pixels. If set to `False`, the input and output tensors are aligned by the corner points of their corner pixels, and the interpolation uses edge value padding for out-of-boundary values, making this operation independent of input size when `scale_factor` is kept the same. This only has an effect when `mode` is `'bilinear'`. Default: `False`\nConstraints:\ndtype:bool\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.gt(input,other,out=None)\nParameter description: out(Tensor, optional) - the output tensor that must be a BoolTensor\nConstraints:\ndtype:booltensor\nstructure:tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.empty_like(input,dtype=None,layout=None,device=None,requires_grad=False,memory_format=torch.preserve_format)\nParameter description: dtype(`torch.dtype`, optional) - the desired data type of returned Tensor. Default: if `None`, defaults to the dtype of `input`.\nConstraints:\ndtype:torch.dtype\nstructure:Null\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.nn.RNN(*args,**kwargs,input_size=None,hidden_size=None,num_layers=None,nonlinearity=None,bias=None,batch_first=None,dropout=None,bidirectional=None)\nParameter description: hidden_size - The number of features in the hidden state h\nConstraints:\ndtype:int\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:[0,inf)\n\n###\n\nSignature: torch.nn.functional.avg_pool1d(input,kernel_size,stride=None,padding=0,ceil_mode=False,count_include_pad=True)\nParameter description: padding - implicit zero paddings on both sides of the input. Can be a single number or a tuple (padW,). Default: 0\nConstraints:\ndtype:bool;int\nstructure:tuple\nshape:Null\nndim:0;1\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.autograd.grad(outputs,inputs,grad_outputs=None,retain_graph=None,create_graph=False,only_inputs=True,allow_unused=False)\nParameter description: outputs(sequence of Tensor) - outputs of the differentiated function.\nConstraints:\ndtype:Null\nstructure:sequence;tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.repeat_interleave(input,repeats,dim=None)\nParameter description: dim(int, optional) - The dimension along which to repeat values.By default, use the flattened input array, and return a flat outputarray.\nConstraints:\ndtype:int\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.nn.CosineSimilarity(dim=1,eps=1e-08)\nParameter description: dim(int, optional) - Dimension where cosine similarity is computed. Default: 1\nConstraints:",
            "constr": {
                "dtype": [
                    "bool",
                    "int"
                ],
                "structure": [],
                "shape": [],
                "ndim": [
                    "0"
                ],
                "enum": [],
                "range": []
            },
            "signature": "torch.nn.CosineSimilarity(dim=1,eps=1e-08)",
            "descp": "dim(int, optional) - Dimension where cosine similarity is computed. Default: 1"
        },
        {
            "fname": "torch.nn.functional.dropout3d.yaml",
            "arg": "inplace",
            "prompt": "Signature: torch.distributed.recv(tensor,src=None,group=<objectobject>,tag=0)\nParameter description: tensor(Tensor) - Tensor to fill with received data.\nConstraints:\ndtype:Null\nstructure:tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.div(input,other,out=None)\nParameter description: out(Tensor, optional) - the output tensor.\nConstraints:\ndtype:Null\nstructure:tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.nn.TransformerDecoder(decoder_layer,num_layers,norm=None)\nParameter description: num_layers - the number of sub-decoder-layers in the decoder (required).\nConstraints:\ndtype:int\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:[0,inf)\n\n###\n\nSignature: torch.nn.quantized.functional.interpolate(input,size=None,scale_factor=None,mode=nearest,align_corners=None)\nParameter description: align_corners(bool, optional) - Geometrically, we consider the pixels of the input and output as squares rather than points. If set to `True`, the input and output tensors are aligned by the center points of their corner pixels, preserving the values at the corner pixels. If set to `False`, the input and output tensors are aligned by the corner points of their corner pixels, and the interpolation uses edge value padding for out-of-boundary values, making this operation independent of input size when `scale_factor` is kept the same. This only has an effect when `mode` is `'bilinear'`. Default: `False`\nConstraints:\ndtype:bool\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.gt(input,other,out=None)\nParameter description: out(Tensor, optional) - the output tensor that must be a BoolTensor\nConstraints:\ndtype:booltensor\nstructure:tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.empty_like(input,dtype=None,layout=None,device=None,requires_grad=False,memory_format=torch.preserve_format)\nParameter description: dtype(`torch.dtype`, optional) - the desired data type of returned Tensor. Default: if `None`, defaults to the dtype of `input`.\nConstraints:\ndtype:torch.dtype\nstructure:Null\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.nn.RNN(*args,**kwargs,input_size=None,hidden_size=None,num_layers=None,nonlinearity=None,bias=None,batch_first=None,dropout=None,bidirectional=None)\nParameter description: hidden_size - The number of features in the hidden state h\nConstraints:\ndtype:int\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:[0,inf)\n\n###\n\nSignature: torch.nn.functional.avg_pool1d(input,kernel_size,stride=None,padding=0,ceil_mode=False,count_include_pad=True)\nParameter description: padding - implicit zero paddings on both sides of the input. Can be a single number or a tuple (padW,). Default: 0\nConstraints:\ndtype:bool;int\nstructure:tuple\nshape:Null\nndim:0;1\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.autograd.grad(outputs,inputs,grad_outputs=None,retain_graph=None,create_graph=False,only_inputs=True,allow_unused=False)\nParameter description: outputs(sequence of Tensor) - outputs of the differentiated function.\nConstraints:\ndtype:Null\nstructure:sequence;tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.repeat_interleave(input,repeats,dim=None)\nParameter description: dim(int, optional) - The dimension along which to repeat values.By default, use the flattened input array, and return a flat outputarray.\nConstraints:\ndtype:int\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.nn.functional.dropout3d(input,p=0.5,training=True,inplace=False)\nParameter description: inplace - If set to `True`, will do this operation in-place. Default: `False`\nConstraints:",
            "constr": {
                "dtype": [
                    "bool"
                ],
                "structure": [],
                "shape": [],
                "ndim": [
                    "0"
                ],
                "enum": [],
                "range": []
            },
            "signature": "torch.nn.functional.dropout3d(input,p=0.5,training=True,inplace=False)",
            "descp": "inplace - If set to `True`, will do this operation in-place. Default: `False`"
        },
        {
            "fname": "torch.nn.functional.nll_loss.yaml",
            "arg": "input",
            "prompt": "Signature: torch.distributed.recv(tensor,src=None,group=<objectobject>,tag=0)\nParameter description: tensor(Tensor) - Tensor to fill with received data.\nConstraints:\ndtype:Null\nstructure:tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.div(input,other,out=None)\nParameter description: out(Tensor, optional) - the output tensor.\nConstraints:\ndtype:Null\nstructure:tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.nn.TransformerDecoder(decoder_layer,num_layers,norm=None)\nParameter description: num_layers - the number of sub-decoder-layers in the decoder (required).\nConstraints:\ndtype:int\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:[0,inf)\n\n###\n\nSignature: torch.nn.quantized.functional.interpolate(input,size=None,scale_factor=None,mode=nearest,align_corners=None)\nParameter description: align_corners(bool, optional) - Geometrically, we consider the pixels of the input and output as squares rather than points. If set to `True`, the input and output tensors are aligned by the center points of their corner pixels, preserving the values at the corner pixels. If set to `False`, the input and output tensors are aligned by the corner points of their corner pixels, and the interpolation uses edge value padding for out-of-boundary values, making this operation independent of input size when `scale_factor` is kept the same. This only has an effect when `mode` is `'bilinear'`. Default: `False`\nConstraints:\ndtype:bool\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.gt(input,other,out=None)\nParameter description: out(Tensor, optional) - the output tensor that must be a BoolTensor\nConstraints:\ndtype:booltensor\nstructure:tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.empty_like(input,dtype=None,layout=None,device=None,requires_grad=False,memory_format=torch.preserve_format)\nParameter description: dtype(`torch.dtype`, optional) - the desired data type of returned Tensor. Default: if `None`, defaults to the dtype of `input`.\nConstraints:\ndtype:torch.dtype\nstructure:Null\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.nn.RNN(*args,**kwargs,input_size=None,hidden_size=None,num_layers=None,nonlinearity=None,bias=None,batch_first=None,dropout=None,bidirectional=None)\nParameter description: hidden_size - The number of features in the hidden state h\nConstraints:\ndtype:int\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:[0,inf)\n\n###\n\nSignature: torch.nn.functional.avg_pool1d(input,kernel_size,stride=None,padding=0,ceil_mode=False,count_include_pad=True)\nParameter description: padding - implicit zero paddings on both sides of the input. Can be a single number or a tuple (padW,). Default: 0\nConstraints:\ndtype:bool;int\nstructure:tuple\nshape:Null\nndim:0;1\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.autograd.grad(outputs,inputs,grad_outputs=None,retain_graph=None,create_graph=False,only_inputs=True,allow_unused=False)\nParameter description: outputs(sequence of Tensor) - outputs of the differentiated function.\nConstraints:\ndtype:Null\nstructure:sequence;tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.repeat_interleave(input,repeats,dim=None)\nParameter description: dim(int, optional) - The dimension along which to repeat values.By default, use the flattened input array, and return a flat outputarray.\nConstraints:\ndtype:int\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.nn.functional.nll_loss(input,target,weight=None,size_average=None,ignore_index=-100,reduce=None,reduction=mean)\nParameter description: input - (N, C)  where C = number of classes or (N, C, H, W)  in case of 2D Loss, or (N, C, d_1, d_2, ..., d_K)  where K  >= 1  in the case of K-dimensional loss.\nConstraints:",
            "constr": {
                "dtype": [],
                "structure": [],
                "shape": [
                    "(N, C, H, W)",
                    "(N, C, d_1, d_2, ..., d_K)",
                    "(N, C)"
                ],
                "ndim": [],
                "enum": [],
                "range": []
            },
            "signature": "torch.nn.functional.nll_loss(input,target,weight=None,size_average=None,ignore_index=-100,reduce=None,reduction=mean)",
            "descp": "input - (N, C)  where C = number of classes or (N, C, H, W)  in case of 2D Loss, or (N, C, d_1, d_2, ..., d_K)  where K  >= 1  in the case of K-dimensional loss."
        },
        {
            "fname": "torch.normal222.yaml",
            "arg": "size",
            "prompt": "Signature: torch.distributed.recv(tensor,src=None,group=<objectobject>,tag=0)\nParameter description: tensor(Tensor) - Tensor to fill with received data.\nConstraints:\ndtype:Null\nstructure:tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.div(input,other,out=None)\nParameter description: out(Tensor, optional) - the output tensor.\nConstraints:\ndtype:Null\nstructure:tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.nn.TransformerDecoder(decoder_layer,num_layers,norm=None)\nParameter description: num_layers - the number of sub-decoder-layers in the decoder (required).\nConstraints:\ndtype:int\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:[0,inf)\n\n###\n\nSignature: torch.nn.quantized.functional.interpolate(input,size=None,scale_factor=None,mode=nearest,align_corners=None)\nParameter description: align_corners(bool, optional) - Geometrically, we consider the pixels of the input and output as squares rather than points. If set to `True`, the input and output tensors are aligned by the center points of their corner pixels, preserving the values at the corner pixels. If set to `False`, the input and output tensors are aligned by the corner points of their corner pixels, and the interpolation uses edge value padding for out-of-boundary values, making this operation independent of input size when `scale_factor` is kept the same. This only has an effect when `mode` is `'bilinear'`. Default: `False`\nConstraints:\ndtype:bool\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.gt(input,other,out=None)\nParameter description: out(Tensor, optional) - the output tensor that must be a BoolTensor\nConstraints:\ndtype:booltensor\nstructure:tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.empty_like(input,dtype=None,layout=None,device=None,requires_grad=False,memory_format=torch.preserve_format)\nParameter description: dtype(`torch.dtype`, optional) - the desired data type of returned Tensor. Default: if `None`, defaults to the dtype of `input`.\nConstraints:\ndtype:torch.dtype\nstructure:Null\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.nn.RNN(*args,**kwargs,input_size=None,hidden_size=None,num_layers=None,nonlinearity=None,bias=None,batch_first=None,dropout=None,bidirectional=None)\nParameter description: hidden_size - The number of features in the hidden state h\nConstraints:\ndtype:int\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:[0,inf)\n\n###\n\nSignature: torch.nn.functional.avg_pool1d(input,kernel_size,stride=None,padding=0,ceil_mode=False,count_include_pad=True)\nParameter description: padding - implicit zero paddings on both sides of the input. Can be a single number or a tuple (padW,). Default: 0\nConstraints:\ndtype:bool;int\nstructure:tuple\nshape:Null\nndim:0;1\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.autograd.grad(outputs,inputs,grad_outputs=None,retain_graph=None,create_graph=False,only_inputs=True,allow_unused=False)\nParameter description: outputs(sequence of Tensor) - outputs of the differentiated function.\nConstraints:\ndtype:Null\nstructure:sequence;tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.repeat_interleave(input,repeats,dim=None)\nParameter description: dim(int, optional) - The dimension along which to repeat values.By default, use the flattened input array, and return a flat outputarray.\nConstraints:\ndtype:int\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.normal(mean,std,size,out=None)\nParameter description: size(int...) - a sequence of integers defining the shape of the output tensor.\nConstraints:",
            "constr": {
                "dtype": [
                    "integer",
                    "int"
                ],
                "structure": [
                    "sequence",
                    "tensor"
                ],
                "shape": [],
                "ndim": [
                    "1"
                ],
                "enum": [],
                "range": []
            },
            "signature": "torch.normal(mean,std,size,out=None)",
            "descp": "size(int...) - a sequence of integers defining the shape of the output tensor."
        },
        {
            "fname": "torch.nn.relu6.yaml",
            "arg": "inplace",
            "prompt": "Signature: torch.distributed.recv(tensor,src=None,group=<objectobject>,tag=0)\nParameter description: tensor(Tensor) - Tensor to fill with received data.\nConstraints:\ndtype:Null\nstructure:tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.div(input,other,out=None)\nParameter description: out(Tensor, optional) - the output tensor.\nConstraints:\ndtype:Null\nstructure:tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.nn.TransformerDecoder(decoder_layer,num_layers,norm=None)\nParameter description: num_layers - the number of sub-decoder-layers in the decoder (required).\nConstraints:\ndtype:int\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:[0,inf)\n\n###\n\nSignature: torch.nn.quantized.functional.interpolate(input,size=None,scale_factor=None,mode=nearest,align_corners=None)\nParameter description: align_corners(bool, optional) - Geometrically, we consider the pixels of the input and output as squares rather than points. If set to `True`, the input and output tensors are aligned by the center points of their corner pixels, preserving the values at the corner pixels. If set to `False`, the input and output tensors are aligned by the corner points of their corner pixels, and the interpolation uses edge value padding for out-of-boundary values, making this operation independent of input size when `scale_factor` is kept the same. This only has an effect when `mode` is `'bilinear'`. Default: `False`\nConstraints:\ndtype:bool\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.gt(input,other,out=None)\nParameter description: out(Tensor, optional) - the output tensor that must be a BoolTensor\nConstraints:\ndtype:booltensor\nstructure:tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.empty_like(input,dtype=None,layout=None,device=None,requires_grad=False,memory_format=torch.preserve_format)\nParameter description: dtype(`torch.dtype`, optional) - the desired data type of returned Tensor. Default: if `None`, defaults to the dtype of `input`.\nConstraints:\ndtype:torch.dtype\nstructure:Null\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.nn.RNN(*args,**kwargs,input_size=None,hidden_size=None,num_layers=None,nonlinearity=None,bias=None,batch_first=None,dropout=None,bidirectional=None)\nParameter description: hidden_size - The number of features in the hidden state h\nConstraints:\ndtype:int\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:[0,inf)\n\n###\n\nSignature: torch.nn.functional.avg_pool1d(input,kernel_size,stride=None,padding=0,ceil_mode=False,count_include_pad=True)\nParameter description: padding - implicit zero paddings on both sides of the input. Can be a single number or a tuple (padW,). Default: 0\nConstraints:\ndtype:bool;int\nstructure:tuple\nshape:Null\nndim:0;1\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.autograd.grad(outputs,inputs,grad_outputs=None,retain_graph=None,create_graph=False,only_inputs=True,allow_unused=False)\nParameter description: outputs(sequence of Tensor) - outputs of the differentiated function.\nConstraints:\ndtype:Null\nstructure:sequence;tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.repeat_interleave(input,repeats,dim=None)\nParameter description: dim(int, optional) - The dimension along which to repeat values.By default, use the flattened input array, and return a flat outputarray.\nConstraints:\ndtype:int\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.nn.ReLU6(inplace=False)\nParameter description: inplace - can optionally do the operation in-place. Default: `False`\nConstraints:",
            "constr": {
                "dtype": [
                    "bool"
                ],
                "structure": [],
                "shape": [],
                "ndim": [
                    "0"
                ],
                "enum": [],
                "range": []
            },
            "signature": "torch.nn.ReLU6(inplace=False)",
            "descp": "inplace - can optionally do the operation in-place. Default: `False`"
        },
        {
            "fname": "torch.rand_like.yaml",
            "arg": "input",
            "prompt": "Signature: torch.distributed.recv(tensor,src=None,group=<objectobject>,tag=0)\nParameter description: tensor(Tensor) - Tensor to fill with received data.\nConstraints:\ndtype:Null\nstructure:tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.div(input,other,out=None)\nParameter description: out(Tensor, optional) - the output tensor.\nConstraints:\ndtype:Null\nstructure:tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.nn.TransformerDecoder(decoder_layer,num_layers,norm=None)\nParameter description: num_layers - the number of sub-decoder-layers in the decoder (required).\nConstraints:\ndtype:int\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:[0,inf)\n\n###\n\nSignature: torch.nn.quantized.functional.interpolate(input,size=None,scale_factor=None,mode=nearest,align_corners=None)\nParameter description: align_corners(bool, optional) - Geometrically, we consider the pixels of the input and output as squares rather than points. If set to `True`, the input and output tensors are aligned by the center points of their corner pixels, preserving the values at the corner pixels. If set to `False`, the input and output tensors are aligned by the corner points of their corner pixels, and the interpolation uses edge value padding for out-of-boundary values, making this operation independent of input size when `scale_factor` is kept the same. This only has an effect when `mode` is `'bilinear'`. Default: `False`\nConstraints:\ndtype:bool\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.gt(input,other,out=None)\nParameter description: out(Tensor, optional) - the output tensor that must be a BoolTensor\nConstraints:\ndtype:booltensor\nstructure:tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.empty_like(input,dtype=None,layout=None,device=None,requires_grad=False,memory_format=torch.preserve_format)\nParameter description: dtype(`torch.dtype`, optional) - the desired data type of returned Tensor. Default: if `None`, defaults to the dtype of `input`.\nConstraints:\ndtype:torch.dtype\nstructure:Null\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.nn.RNN(*args,**kwargs,input_size=None,hidden_size=None,num_layers=None,nonlinearity=None,bias=None,batch_first=None,dropout=None,bidirectional=None)\nParameter description: hidden_size - The number of features in the hidden state h\nConstraints:\ndtype:int\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:[0,inf)\n\n###\n\nSignature: torch.nn.functional.avg_pool1d(input,kernel_size,stride=None,padding=0,ceil_mode=False,count_include_pad=True)\nParameter description: padding - implicit zero paddings on both sides of the input. Can be a single number or a tuple (padW,). Default: 0\nConstraints:\ndtype:bool;int\nstructure:tuple\nshape:Null\nndim:0;1\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.autograd.grad(outputs,inputs,grad_outputs=None,retain_graph=None,create_graph=False,only_inputs=True,allow_unused=False)\nParameter description: outputs(sequence of Tensor) - outputs of the differentiated function.\nConstraints:\ndtype:Null\nstructure:sequence;tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.repeat_interleave(input,repeats,dim=None)\nParameter description: dim(int, optional) - The dimension along which to repeat values.By default, use the flattened input array, and return a flat outputarray.\nConstraints:\ndtype:int\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.rand_like(input,dtype=None,layout=None,device=None,requires_grad=False,memory_format=torch.preserve_format)\nParameter description: input(Tensor) - the size of `input` will determine size of the output tensor.\nConstraints:",
            "constr": {
                "dtype": [
                    "int"
                ],
                "structure": [
                    "tensor"
                ],
                "shape": [],
                "ndim": [],
                "enum": [],
                "range": [
                    "[0,inf)"
                ]
            },
            "signature": "torch.rand_like(input,dtype=None,layout=None,device=None,requires_grad=False,memory_format=torch.preserve_format)",
            "descp": "input(Tensor) - the size of `input` will determine size of the output tensor."
        },
        {
            "fname": "torch.hub.download_url_to_file.yaml",
            "arg": "url",
            "prompt": "Signature: torch.distributed.recv(tensor,src=None,group=<objectobject>,tag=0)\nParameter description: tensor(Tensor) - Tensor to fill with received data.\nConstraints:\ndtype:Null\nstructure:tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.div(input,other,out=None)\nParameter description: out(Tensor, optional) - the output tensor.\nConstraints:\ndtype:Null\nstructure:tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.nn.TransformerDecoder(decoder_layer,num_layers,norm=None)\nParameter description: num_layers - the number of sub-decoder-layers in the decoder (required).\nConstraints:\ndtype:int\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:[0,inf)\n\n###\n\nSignature: torch.nn.quantized.functional.interpolate(input,size=None,scale_factor=None,mode=nearest,align_corners=None)\nParameter description: align_corners(bool, optional) - Geometrically, we consider the pixels of the input and output as squares rather than points. If set to `True`, the input and output tensors are aligned by the center points of their corner pixels, preserving the values at the corner pixels. If set to `False`, the input and output tensors are aligned by the corner points of their corner pixels, and the interpolation uses edge value padding for out-of-boundary values, making this operation independent of input size when `scale_factor` is kept the same. This only has an effect when `mode` is `'bilinear'`. Default: `False`\nConstraints:\ndtype:bool\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.gt(input,other,out=None)\nParameter description: out(Tensor, optional) - the output tensor that must be a BoolTensor\nConstraints:\ndtype:booltensor\nstructure:tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.empty_like(input,dtype=None,layout=None,device=None,requires_grad=False,memory_format=torch.preserve_format)\nParameter description: dtype(`torch.dtype`, optional) - the desired data type of returned Tensor. Default: if `None`, defaults to the dtype of `input`.\nConstraints:\ndtype:torch.dtype\nstructure:Null\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.nn.RNN(*args,**kwargs,input_size=None,hidden_size=None,num_layers=None,nonlinearity=None,bias=None,batch_first=None,dropout=None,bidirectional=None)\nParameter description: hidden_size - The number of features in the hidden state h\nConstraints:\ndtype:int\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:[0,inf)\n\n###\n\nSignature: torch.nn.functional.avg_pool1d(input,kernel_size,stride=None,padding=0,ceil_mode=False,count_include_pad=True)\nParameter description: padding - implicit zero paddings on both sides of the input. Can be a single number or a tuple (padW,). Default: 0\nConstraints:\ndtype:bool;int\nstructure:tuple\nshape:Null\nndim:0;1\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.autograd.grad(outputs,inputs,grad_outputs=None,retain_graph=None,create_graph=False,only_inputs=True,allow_unused=False)\nParameter description: outputs(sequence of Tensor) - outputs of the differentiated function.\nConstraints:\ndtype:Null\nstructure:sequence;tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.repeat_interleave(input,repeats,dim=None)\nParameter description: dim(int, optional) - The dimension along which to repeat values.By default, use the flattened input array, and return a flat outputarray.\nConstraints:\ndtype:int\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.hub.download_url_to_file(url,dst,hash_prefix=None,progress=True)\nParameter description: url(string) - URL of the object to download\nConstraints:",
            "constr": {
                "dtype": [
                    "string"
                ],
                "structure": [],
                "shape": [],
                "ndim": [],
                "enum": [],
                "range": []
            },
            "signature": "torch.hub.download_url_to_file(url,dst,hash_prefix=None,progress=True)",
            "descp": "url(string) - URL of the object to download"
        },
        {
            "fname": "torch.nn.quantized.functional.conv3d.yaml",
            "arg": "scale",
            "prompt": "Signature: torch.distributed.recv(tensor,src=None,group=<objectobject>,tag=0)\nParameter description: tensor(Tensor) - Tensor to fill with received data.\nConstraints:\ndtype:Null\nstructure:tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.div(input,other,out=None)\nParameter description: out(Tensor, optional) - the output tensor.\nConstraints:\ndtype:Null\nstructure:tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.nn.TransformerDecoder(decoder_layer,num_layers,norm=None)\nParameter description: num_layers - the number of sub-decoder-layers in the decoder (required).\nConstraints:\ndtype:int\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:[0,inf)\n\n###\n\nSignature: torch.nn.quantized.functional.interpolate(input,size=None,scale_factor=None,mode=nearest,align_corners=None)\nParameter description: align_corners(bool, optional) - Geometrically, we consider the pixels of the input and output as squares rather than points. If set to `True`, the input and output tensors are aligned by the center points of their corner pixels, preserving the values at the corner pixels. If set to `False`, the input and output tensors are aligned by the corner points of their corner pixels, and the interpolation uses edge value padding for out-of-boundary values, making this operation independent of input size when `scale_factor` is kept the same. This only has an effect when `mode` is `'bilinear'`. Default: `False`\nConstraints:\ndtype:bool\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.gt(input,other,out=None)\nParameter description: out(Tensor, optional) - the output tensor that must be a BoolTensor\nConstraints:\ndtype:booltensor\nstructure:tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.empty_like(input,dtype=None,layout=None,device=None,requires_grad=False,memory_format=torch.preserve_format)\nParameter description: dtype(`torch.dtype`, optional) - the desired data type of returned Tensor. Default: if `None`, defaults to the dtype of `input`.\nConstraints:\ndtype:torch.dtype\nstructure:Null\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.nn.RNN(*args,**kwargs,input_size=None,hidden_size=None,num_layers=None,nonlinearity=None,bias=None,batch_first=None,dropout=None,bidirectional=None)\nParameter description: hidden_size - The number of features in the hidden state h\nConstraints:\ndtype:int\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:[0,inf)\n\n###\n\nSignature: torch.nn.functional.avg_pool1d(input,kernel_size,stride=None,padding=0,ceil_mode=False,count_include_pad=True)\nParameter description: padding - implicit zero paddings on both sides of the input. Can be a single number or a tuple (padW,). Default: 0\nConstraints:\ndtype:bool;int\nstructure:tuple\nshape:Null\nndim:0;1\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.autograd.grad(outputs,inputs,grad_outputs=None,retain_graph=None,create_graph=False,only_inputs=True,allow_unused=False)\nParameter description: outputs(sequence of Tensor) - outputs of the differentiated function.\nConstraints:\ndtype:Null\nstructure:sequence;tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.repeat_interleave(input,repeats,dim=None)\nParameter description: dim(int, optional) - The dimension along which to repeat values.By default, use the flattened input array, and return a flat outputarray.\nConstraints:\ndtype:int\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.nn.quantized.functional.conv3d(input,weight,bias,stride=1,padding=0,dilation=1,groups=1,padding_mode=zeros,scale=1.0,zero_point=0,dtype=torch.quint8)\nParameter description: scale - quantization scale for the output. Default: 1.0\nConstraints:",
            "constr": {
                "dtype": [
                    "bool",
                    "float"
                ],
                "structure": [],
                "shape": [],
                "ndim": [
                    "0"
                ],
                "enum": [],
                "range": []
            },
            "signature": "torch.nn.quantized.functional.conv3d(input,weight,bias,stride=1,padding=0,dilation=1,groups=1,padding_mode=zeros,scale=1.0,zero_point=0,dtype=torch.quint8)",
            "descp": "scale - quantization scale for the output. Default: 1.0"
        },
        {
            "fname": "torch.nn.fold.yaml",
            "arg": "kernel_size",
            "prompt": "Signature: torch.distributed.recv(tensor,src=None,group=<objectobject>,tag=0)\nParameter description: tensor(Tensor) - Tensor to fill with received data.\nConstraints:\ndtype:Null\nstructure:tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.div(input,other,out=None)\nParameter description: out(Tensor, optional) - the output tensor.\nConstraints:\ndtype:Null\nstructure:tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.nn.TransformerDecoder(decoder_layer,num_layers,norm=None)\nParameter description: num_layers - the number of sub-decoder-layers in the decoder (required).\nConstraints:\ndtype:int\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:[0,inf)\n\n###\n\nSignature: torch.nn.quantized.functional.interpolate(input,size=None,scale_factor=None,mode=nearest,align_corners=None)\nParameter description: align_corners(bool, optional) - Geometrically, we consider the pixels of the input and output as squares rather than points. If set to `True`, the input and output tensors are aligned by the center points of their corner pixels, preserving the values at the corner pixels. If set to `False`, the input and output tensors are aligned by the corner points of their corner pixels, and the interpolation uses edge value padding for out-of-boundary values, making this operation independent of input size when `scale_factor` is kept the same. This only has an effect when `mode` is `'bilinear'`. Default: `False`\nConstraints:\ndtype:bool\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.gt(input,other,out=None)\nParameter description: out(Tensor, optional) - the output tensor that must be a BoolTensor\nConstraints:\ndtype:booltensor\nstructure:tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.empty_like(input,dtype=None,layout=None,device=None,requires_grad=False,memory_format=torch.preserve_format)\nParameter description: dtype(`torch.dtype`, optional) - the desired data type of returned Tensor. Default: if `None`, defaults to the dtype of `input`.\nConstraints:\ndtype:torch.dtype\nstructure:Null\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.nn.RNN(*args,**kwargs,input_size=None,hidden_size=None,num_layers=None,nonlinearity=None,bias=None,batch_first=None,dropout=None,bidirectional=None)\nParameter description: hidden_size - The number of features in the hidden state h\nConstraints:\ndtype:int\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:[0,inf)\n\n###\n\nSignature: torch.nn.functional.avg_pool1d(input,kernel_size,stride=None,padding=0,ceil_mode=False,count_include_pad=True)\nParameter description: padding - implicit zero paddings on both sides of the input. Can be a single number or a tuple (padW,). Default: 0\nConstraints:\ndtype:bool;int\nstructure:tuple\nshape:Null\nndim:0;1\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.autograd.grad(outputs,inputs,grad_outputs=None,retain_graph=None,create_graph=False,only_inputs=True,allow_unused=False)\nParameter description: outputs(sequence of Tensor) - outputs of the differentiated function.\nConstraints:\ndtype:Null\nstructure:sequence;tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.repeat_interleave(input,repeats,dim=None)\nParameter description: dim(int, optional) - The dimension along which to repeat values.By default, use the flattened input array, and return a flat outputarray.\nConstraints:\ndtype:int\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.nn.Fold(output_size,kernel_size,dilation=1,padding=0,stride=1)\nParameter description: kernel_size(int or tuple) - the size of the sliding blocks\nConstraints:",
            "constr": {
                "dtype": [
                    "int"
                ],
                "structure": [
                    "tuple"
                ],
                "shape": [],
                "ndim": [],
                "enum": [],
                "range": [
                    "[0,inf)"
                ]
            },
            "signature": "torch.nn.Fold(output_size,kernel_size,dilation=1,padding=0,stride=1)",
            "descp": "kernel_size(int or tuple) - the size of the sliding blocks"
        },
        {
            "fname": "torch.nn.relu.yaml",
            "arg": "inplace",
            "prompt": "Signature: torch.distributed.recv(tensor,src=None,group=<objectobject>,tag=0)\nParameter description: tensor(Tensor) - Tensor to fill with received data.\nConstraints:\ndtype:Null\nstructure:tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.div(input,other,out=None)\nParameter description: out(Tensor, optional) - the output tensor.\nConstraints:\ndtype:Null\nstructure:tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.nn.TransformerDecoder(decoder_layer,num_layers,norm=None)\nParameter description: num_layers - the number of sub-decoder-layers in the decoder (required).\nConstraints:\ndtype:int\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:[0,inf)\n\n###\n\nSignature: torch.nn.quantized.functional.interpolate(input,size=None,scale_factor=None,mode=nearest,align_corners=None)\nParameter description: align_corners(bool, optional) - Geometrically, we consider the pixels of the input and output as squares rather than points. If set to `True`, the input and output tensors are aligned by the center points of their corner pixels, preserving the values at the corner pixels. If set to `False`, the input and output tensors are aligned by the corner points of their corner pixels, and the interpolation uses edge value padding for out-of-boundary values, making this operation independent of input size when `scale_factor` is kept the same. This only has an effect when `mode` is `'bilinear'`. Default: `False`\nConstraints:\ndtype:bool\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.gt(input,other,out=None)\nParameter description: out(Tensor, optional) - the output tensor that must be a BoolTensor\nConstraints:\ndtype:booltensor\nstructure:tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.empty_like(input,dtype=None,layout=None,device=None,requires_grad=False,memory_format=torch.preserve_format)\nParameter description: dtype(`torch.dtype`, optional) - the desired data type of returned Tensor. Default: if `None`, defaults to the dtype of `input`.\nConstraints:\ndtype:torch.dtype\nstructure:Null\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.nn.RNN(*args,**kwargs,input_size=None,hidden_size=None,num_layers=None,nonlinearity=None,bias=None,batch_first=None,dropout=None,bidirectional=None)\nParameter description: hidden_size - The number of features in the hidden state h\nConstraints:\ndtype:int\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:[0,inf)\n\n###\n\nSignature: torch.nn.functional.avg_pool1d(input,kernel_size,stride=None,padding=0,ceil_mode=False,count_include_pad=True)\nParameter description: padding - implicit zero paddings on both sides of the input. Can be a single number or a tuple (padW,). Default: 0\nConstraints:\ndtype:bool;int\nstructure:tuple\nshape:Null\nndim:0;1\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.autograd.grad(outputs,inputs,grad_outputs=None,retain_graph=None,create_graph=False,only_inputs=True,allow_unused=False)\nParameter description: outputs(sequence of Tensor) - outputs of the differentiated function.\nConstraints:\ndtype:Null\nstructure:sequence;tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.repeat_interleave(input,repeats,dim=None)\nParameter description: dim(int, optional) - The dimension along which to repeat values.By default, use the flattened input array, and return a flat outputarray.\nConstraints:\ndtype:int\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.nn.ReLU(inplace=False)\nParameter description: inplace - can optionally do the operation in-place. Default: `False`\nConstraints:",
            "constr": {
                "dtype": [
                    "bool"
                ],
                "structure": [],
                "shape": [],
                "ndim": [
                    "0"
                ],
                "enum": [],
                "range": []
            },
            "signature": "torch.nn.ReLU(inplace=False)",
            "descp": "inplace - can optionally do the operation in-place. Default: `False`"
        },
        {
            "fname": "torch.nn.maxpool2d.yaml",
            "arg": "kernel_size",
            "prompt": "Signature: torch.distributed.recv(tensor,src=None,group=<objectobject>,tag=0)\nParameter description: tensor(Tensor) - Tensor to fill with received data.\nConstraints:\ndtype:Null\nstructure:tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.div(input,other,out=None)\nParameter description: out(Tensor, optional) - the output tensor.\nConstraints:\ndtype:Null\nstructure:tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.nn.TransformerDecoder(decoder_layer,num_layers,norm=None)\nParameter description: num_layers - the number of sub-decoder-layers in the decoder (required).\nConstraints:\ndtype:int\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:[0,inf)\n\n###\n\nSignature: torch.nn.quantized.functional.interpolate(input,size=None,scale_factor=None,mode=nearest,align_corners=None)\nParameter description: align_corners(bool, optional) - Geometrically, we consider the pixels of the input and output as squares rather than points. If set to `True`, the input and output tensors are aligned by the center points of their corner pixels, preserving the values at the corner pixels. If set to `False`, the input and output tensors are aligned by the corner points of their corner pixels, and the interpolation uses edge value padding for out-of-boundary values, making this operation independent of input size when `scale_factor` is kept the same. This only has an effect when `mode` is `'bilinear'`. Default: `False`\nConstraints:\ndtype:bool\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.gt(input,other,out=None)\nParameter description: out(Tensor, optional) - the output tensor that must be a BoolTensor\nConstraints:\ndtype:booltensor\nstructure:tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.empty_like(input,dtype=None,layout=None,device=None,requires_grad=False,memory_format=torch.preserve_format)\nParameter description: dtype(`torch.dtype`, optional) - the desired data type of returned Tensor. Default: if `None`, defaults to the dtype of `input`.\nConstraints:\ndtype:torch.dtype\nstructure:Null\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.nn.RNN(*args,**kwargs,input_size=None,hidden_size=None,num_layers=None,nonlinearity=None,bias=None,batch_first=None,dropout=None,bidirectional=None)\nParameter description: hidden_size - The number of features in the hidden state h\nConstraints:\ndtype:int\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:[0,inf)\n\n###\n\nSignature: torch.nn.functional.avg_pool1d(input,kernel_size,stride=None,padding=0,ceil_mode=False,count_include_pad=True)\nParameter description: padding - implicit zero paddings on both sides of the input. Can be a single number or a tuple (padW,). Default: 0\nConstraints:\ndtype:bool;int\nstructure:tuple\nshape:Null\nndim:0;1\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.autograd.grad(outputs,inputs,grad_outputs=None,retain_graph=None,create_graph=False,only_inputs=True,allow_unused=False)\nParameter description: outputs(sequence of Tensor) - outputs of the differentiated function.\nConstraints:\ndtype:Null\nstructure:sequence;tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.repeat_interleave(input,repeats,dim=None)\nParameter description: dim(int, optional) - The dimension along which to repeat values.By default, use the flattened input array, and return a flat outputarray.\nConstraints:\ndtype:int\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.nn.MaxPool2d(kernel_size,stride=None,padding=0,dilation=1,return_indices=False,ceil_mode=False)\nParameter description: kernel_size - the size of the window to take a max over\nConstraints:",
            "constr": {
                "dtype": [
                    "int"
                ],
                "structure": [],
                "shape": [],
                "ndim": [],
                "enum": [],
                "range": [
                    "[0,inf)"
                ]
            },
            "signature": "torch.nn.MaxPool2d(kernel_size,stride=None,padding=0,dilation=1,return_indices=False,ceil_mode=False)",
            "descp": "kernel_size - the size of the window to take a max over"
        },
        {
            "fname": "torch.nn.avgpool2d.yaml",
            "arg": "ceil_mode",
            "prompt": "Signature: torch.distributed.recv(tensor,src=None,group=<objectobject>,tag=0)\nParameter description: tensor(Tensor) - Tensor to fill with received data.\nConstraints:\ndtype:Null\nstructure:tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.div(input,other,out=None)\nParameter description: out(Tensor, optional) - the output tensor.\nConstraints:\ndtype:Null\nstructure:tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.nn.TransformerDecoder(decoder_layer,num_layers,norm=None)\nParameter description: num_layers - the number of sub-decoder-layers in the decoder (required).\nConstraints:\ndtype:int\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:[0,inf)\n\n###\n\nSignature: torch.nn.quantized.functional.interpolate(input,size=None,scale_factor=None,mode=nearest,align_corners=None)\nParameter description: align_corners(bool, optional) - Geometrically, we consider the pixels of the input and output as squares rather than points. If set to `True`, the input and output tensors are aligned by the center points of their corner pixels, preserving the values at the corner pixels. If set to `False`, the input and output tensors are aligned by the corner points of their corner pixels, and the interpolation uses edge value padding for out-of-boundary values, making this operation independent of input size when `scale_factor` is kept the same. This only has an effect when `mode` is `'bilinear'`. Default: `False`\nConstraints:\ndtype:bool\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.gt(input,other,out=None)\nParameter description: out(Tensor, optional) - the output tensor that must be a BoolTensor\nConstraints:\ndtype:booltensor\nstructure:tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.empty_like(input,dtype=None,layout=None,device=None,requires_grad=False,memory_format=torch.preserve_format)\nParameter description: dtype(`torch.dtype`, optional) - the desired data type of returned Tensor. Default: if `None`, defaults to the dtype of `input`.\nConstraints:\ndtype:torch.dtype\nstructure:Null\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.nn.RNN(*args,**kwargs,input_size=None,hidden_size=None,num_layers=None,nonlinearity=None,bias=None,batch_first=None,dropout=None,bidirectional=None)\nParameter description: hidden_size - The number of features in the hidden state h\nConstraints:\ndtype:int\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:[0,inf)\n\n###\n\nSignature: torch.nn.functional.avg_pool1d(input,kernel_size,stride=None,padding=0,ceil_mode=False,count_include_pad=True)\nParameter description: padding - implicit zero paddings on both sides of the input. Can be a single number or a tuple (padW,). Default: 0\nConstraints:\ndtype:bool;int\nstructure:tuple\nshape:Null\nndim:0;1\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.autograd.grad(outputs,inputs,grad_outputs=None,retain_graph=None,create_graph=False,only_inputs=True,allow_unused=False)\nParameter description: outputs(sequence of Tensor) - outputs of the differentiated function.\nConstraints:\ndtype:Null\nstructure:sequence;tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.repeat_interleave(input,repeats,dim=None)\nParameter description: dim(int, optional) - The dimension along which to repeat values.By default, use the flattened input array, and return a flat outputarray.\nConstraints:\ndtype:int\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.nn.AvgPool2d(kernel_size,stride=None,padding=0,ceil_mode=False,count_include_pad=True,divisor_override=None)\nParameter description: ceil_mode - when True, will use ceil instead of floor to compute the output shape\nConstraints:",
            "constr": {
                "dtype": [
                    "bool"
                ],
                "structure": [],
                "shape": [],
                "ndim": [
                    "0"
                ],
                "enum": [],
                "range": []
            },
            "signature": "torch.nn.AvgPool2d(kernel_size,stride=None,padding=0,ceil_mode=False,count_include_pad=True,divisor_override=None)",
            "descp": "ceil_mode - when True, will use ceil instead of floor to compute the output shape"
        },
        {
            "fname": "torch.nn.utils.weight_norm.yaml",
            "arg": "name",
            "prompt": "Signature: torch.distributed.recv(tensor,src=None,group=<objectobject>,tag=0)\nParameter description: tensor(Tensor) - Tensor to fill with received data.\nConstraints:\ndtype:Null\nstructure:tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.div(input,other,out=None)\nParameter description: out(Tensor, optional) - the output tensor.\nConstraints:\ndtype:Null\nstructure:tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.nn.TransformerDecoder(decoder_layer,num_layers,norm=None)\nParameter description: num_layers - the number of sub-decoder-layers in the decoder (required).\nConstraints:\ndtype:int\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:[0,inf)\n\n###\n\nSignature: torch.nn.quantized.functional.interpolate(input,size=None,scale_factor=None,mode=nearest,align_corners=None)\nParameter description: align_corners(bool, optional) - Geometrically, we consider the pixels of the input and output as squares rather than points. If set to `True`, the input and output tensors are aligned by the center points of their corner pixels, preserving the values at the corner pixels. If set to `False`, the input and output tensors are aligned by the corner points of their corner pixels, and the interpolation uses edge value padding for out-of-boundary values, making this operation independent of input size when `scale_factor` is kept the same. This only has an effect when `mode` is `'bilinear'`. Default: `False`\nConstraints:\ndtype:bool\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.gt(input,other,out=None)\nParameter description: out(Tensor, optional) - the output tensor that must be a BoolTensor\nConstraints:\ndtype:booltensor\nstructure:tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.empty_like(input,dtype=None,layout=None,device=None,requires_grad=False,memory_format=torch.preserve_format)\nParameter description: dtype(`torch.dtype`, optional) - the desired data type of returned Tensor. Default: if `None`, defaults to the dtype of `input`.\nConstraints:\ndtype:torch.dtype\nstructure:Null\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.nn.RNN(*args,**kwargs,input_size=None,hidden_size=None,num_layers=None,nonlinearity=None,bias=None,batch_first=None,dropout=None,bidirectional=None)\nParameter description: hidden_size - The number of features in the hidden state h\nConstraints:\ndtype:int\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:[0,inf)\n\n###\n\nSignature: torch.nn.functional.avg_pool1d(input,kernel_size,stride=None,padding=0,ceil_mode=False,count_include_pad=True)\nParameter description: padding - implicit zero paddings on both sides of the input. Can be a single number or a tuple (padW,). Default: 0\nConstraints:\ndtype:bool;int\nstructure:tuple\nshape:Null\nndim:0;1\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.autograd.grad(outputs,inputs,grad_outputs=None,retain_graph=None,create_graph=False,only_inputs=True,allow_unused=False)\nParameter description: outputs(sequence of Tensor) - outputs of the differentiated function.\nConstraints:\ndtype:Null\nstructure:sequence;tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.repeat_interleave(input,repeats,dim=None)\nParameter description: dim(int, optional) - The dimension along which to repeat values.By default, use the flattened input array, and return a flat outputarray.\nConstraints:\ndtype:int\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.nn.utils.weight_norm(module,name=weight,dim=0)\nParameter description: name(str, optional) - name of weight parameter\nConstraints:",
            "constr": {
                "dtype": [
                    "string",
                    "str"
                ],
                "structure": [],
                "shape": [],
                "ndim": [
                    "0"
                ],
                "enum": [],
                "range": []
            },
            "signature": "torch.nn.utils.weight_norm(module,name=weight,dim=0)",
            "descp": "name(str, optional) - name of weight parameter"
        },
        {
            "fname": "torch.bartlett_window.yaml",
            "arg": "dtype",
            "prompt": "Signature: torch.distributed.recv(tensor,src=None,group=<objectobject>,tag=0)\nParameter description: tensor(Tensor) - Tensor to fill with received data.\nConstraints:\ndtype:Null\nstructure:tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.div(input,other,out=None)\nParameter description: out(Tensor, optional) - the output tensor.\nConstraints:\ndtype:Null\nstructure:tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.nn.TransformerDecoder(decoder_layer,num_layers,norm=None)\nParameter description: num_layers - the number of sub-decoder-layers in the decoder (required).\nConstraints:\ndtype:int\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:[0,inf)\n\n###\n\nSignature: torch.nn.quantized.functional.interpolate(input,size=None,scale_factor=None,mode=nearest,align_corners=None)\nParameter description: align_corners(bool, optional) - Geometrically, we consider the pixels of the input and output as squares rather than points. If set to `True`, the input and output tensors are aligned by the center points of their corner pixels, preserving the values at the corner pixels. If set to `False`, the input and output tensors are aligned by the corner points of their corner pixels, and the interpolation uses edge value padding for out-of-boundary values, making this operation independent of input size when `scale_factor` is kept the same. This only has an effect when `mode` is `'bilinear'`. Default: `False`\nConstraints:\ndtype:bool\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.gt(input,other,out=None)\nParameter description: out(Tensor, optional) - the output tensor that must be a BoolTensor\nConstraints:\ndtype:booltensor\nstructure:tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.empty_like(input,dtype=None,layout=None,device=None,requires_grad=False,memory_format=torch.preserve_format)\nParameter description: dtype(`torch.dtype`, optional) - the desired data type of returned Tensor. Default: if `None`, defaults to the dtype of `input`.\nConstraints:\ndtype:torch.dtype\nstructure:Null\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.nn.RNN(*args,**kwargs,input_size=None,hidden_size=None,num_layers=None,nonlinearity=None,bias=None,batch_first=None,dropout=None,bidirectional=None)\nParameter description: hidden_size - The number of features in the hidden state h\nConstraints:\ndtype:int\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:[0,inf)\n\n###\n\nSignature: torch.nn.functional.avg_pool1d(input,kernel_size,stride=None,padding=0,ceil_mode=False,count_include_pad=True)\nParameter description: padding - implicit zero paddings on both sides of the input. Can be a single number or a tuple (padW,). Default: 0\nConstraints:\ndtype:bool;int\nstructure:tuple\nshape:Null\nndim:0;1\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.autograd.grad(outputs,inputs,grad_outputs=None,retain_graph=None,create_graph=False,only_inputs=True,allow_unused=False)\nParameter description: outputs(sequence of Tensor) - outputs of the differentiated function.\nConstraints:\ndtype:Null\nstructure:sequence;tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.repeat_interleave(input,repeats,dim=None)\nParameter description: dim(int, optional) - The dimension along which to repeat values.By default, use the flattened input array, and return a flat outputarray.\nConstraints:\ndtype:int\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.bartlett_window(window_length,periodic=True,dtype=None,layout=torch.strided,device=None,requires_grad=False)\nParameter description: dtype(`torch.dtype`, optional) - the desired data type of returned tensor. Default: if `None`, uses a global default (see `torch.set_default_tensor_type()`). Only floating point types are supported.\nConstraints:",
            "constr": {
                "dtype": [
                    "torch.dtype"
                ],
                "structure": [],
                "shape": [],
                "ndim": [],
                "enum": [],
                "range": []
            },
            "signature": "torch.bartlett_window(window_length,periodic=True,dtype=None,layout=torch.strided,device=None,requires_grad=False)",
            "descp": "dtype(`torch.dtype`, optional) - the desired data type of returned tensor. Default: if `None`, uses a global default (see `torch.set_default_tensor_type()`). Only floating point types are supported."
        },
        {
            "fname": "torch.nn.functional.conv_transpose2d.yaml",
            "arg": "output_padding",
            "prompt": "Signature: torch.distributed.recv(tensor,src=None,group=<objectobject>,tag=0)\nParameter description: tensor(Tensor) - Tensor to fill with received data.\nConstraints:\ndtype:Null\nstructure:tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.div(input,other,out=None)\nParameter description: out(Tensor, optional) - the output tensor.\nConstraints:\ndtype:Null\nstructure:tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.nn.TransformerDecoder(decoder_layer,num_layers,norm=None)\nParameter description: num_layers - the number of sub-decoder-layers in the decoder (required).\nConstraints:\ndtype:int\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:[0,inf)\n\n###\n\nSignature: torch.nn.quantized.functional.interpolate(input,size=None,scale_factor=None,mode=nearest,align_corners=None)\nParameter description: align_corners(bool, optional) - Geometrically, we consider the pixels of the input and output as squares rather than points. If set to `True`, the input and output tensors are aligned by the center points of their corner pixels, preserving the values at the corner pixels. If set to `False`, the input and output tensors are aligned by the corner points of their corner pixels, and the interpolation uses edge value padding for out-of-boundary values, making this operation independent of input size when `scale_factor` is kept the same. This only has an effect when `mode` is `'bilinear'`. Default: `False`\nConstraints:\ndtype:bool\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.gt(input,other,out=None)\nParameter description: out(Tensor, optional) - the output tensor that must be a BoolTensor\nConstraints:\ndtype:booltensor\nstructure:tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.empty_like(input,dtype=None,layout=None,device=None,requires_grad=False,memory_format=torch.preserve_format)\nParameter description: dtype(`torch.dtype`, optional) - the desired data type of returned Tensor. Default: if `None`, defaults to the dtype of `input`.\nConstraints:\ndtype:torch.dtype\nstructure:Null\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.nn.RNN(*args,**kwargs,input_size=None,hidden_size=None,num_layers=None,nonlinearity=None,bias=None,batch_first=None,dropout=None,bidirectional=None)\nParameter description: hidden_size - The number of features in the hidden state h\nConstraints:\ndtype:int\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:[0,inf)\n\n###\n\nSignature: torch.nn.functional.avg_pool1d(input,kernel_size,stride=None,padding=0,ceil_mode=False,count_include_pad=True)\nParameter description: padding - implicit zero paddings on both sides of the input. Can be a single number or a tuple (padW,). Default: 0\nConstraints:\ndtype:bool;int\nstructure:tuple\nshape:Null\nndim:0;1\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.autograd.grad(outputs,inputs,grad_outputs=None,retain_graph=None,create_graph=False,only_inputs=True,allow_unused=False)\nParameter description: outputs(sequence of Tensor) - outputs of the differentiated function.\nConstraints:\ndtype:Null\nstructure:sequence;tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.repeat_interleave(input,repeats,dim=None)\nParameter description: dim(int, optional) - The dimension along which to repeat values.By default, use the flattened input array, and return a flat outputarray.\nConstraints:\ndtype:int\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.nn.functional.conv_transpose2d(input,weight,bias=None,stride=1,padding=0,output_padding=0,groups=1,dilation=1)\nParameter description: output_padding - additional size added to one side of each dimension in the output shape. Can be a single number or a tuple `(out_padH, out_padW)`. Default: 0\nConstraints:",
            "constr": {
                "dtype": [
                    "numeric",
                    "int"
                ],
                "structure": [
                    "tuple"
                ],
                "shape": [],
                "ndim": [
                    "0",
                    "1"
                ],
                "enum": [],
                "range": [
                    "[0,inf)"
                ]
            },
            "signature": "torch.nn.functional.conv_transpose2d(input,weight,bias=None,stride=1,padding=0,output_padding=0,groups=1,dilation=1)",
            "descp": "output_padding - additional size added to one side of each dimension in the output shape. Can be a single number or a tuple `(out_padH, out_padW)`. Default: 0"
        },
        {
            "fname": "torch.sparse.sum.yaml",
            "arg": "dtype",
            "prompt": "Signature: torch.distributed.recv(tensor,src=None,group=<objectobject>,tag=0)\nParameter description: tensor(Tensor) - Tensor to fill with received data.\nConstraints:\ndtype:Null\nstructure:tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.div(input,other,out=None)\nParameter description: out(Tensor, optional) - the output tensor.\nConstraints:\ndtype:Null\nstructure:tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.nn.TransformerDecoder(decoder_layer,num_layers,norm=None)\nParameter description: num_layers - the number of sub-decoder-layers in the decoder (required).\nConstraints:\ndtype:int\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:[0,inf)\n\n###\n\nSignature: torch.nn.quantized.functional.interpolate(input,size=None,scale_factor=None,mode=nearest,align_corners=None)\nParameter description: align_corners(bool, optional) - Geometrically, we consider the pixels of the input and output as squares rather than points. If set to `True`, the input and output tensors are aligned by the center points of their corner pixels, preserving the values at the corner pixels. If set to `False`, the input and output tensors are aligned by the corner points of their corner pixels, and the interpolation uses edge value padding for out-of-boundary values, making this operation independent of input size when `scale_factor` is kept the same. This only has an effect when `mode` is `'bilinear'`. Default: `False`\nConstraints:\ndtype:bool\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.gt(input,other,out=None)\nParameter description: out(Tensor, optional) - the output tensor that must be a BoolTensor\nConstraints:\ndtype:booltensor\nstructure:tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.empty_like(input,dtype=None,layout=None,device=None,requires_grad=False,memory_format=torch.preserve_format)\nParameter description: dtype(`torch.dtype`, optional) - the desired data type of returned Tensor. Default: if `None`, defaults to the dtype of `input`.\nConstraints:\ndtype:torch.dtype\nstructure:Null\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.nn.RNN(*args,**kwargs,input_size=None,hidden_size=None,num_layers=None,nonlinearity=None,bias=None,batch_first=None,dropout=None,bidirectional=None)\nParameter description: hidden_size - The number of features in the hidden state h\nConstraints:\ndtype:int\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:[0,inf)\n\n###\n\nSignature: torch.nn.functional.avg_pool1d(input,kernel_size,stride=None,padding=0,ceil_mode=False,count_include_pad=True)\nParameter description: padding - implicit zero paddings on both sides of the input. Can be a single number or a tuple (padW,). Default: 0\nConstraints:\ndtype:bool;int\nstructure:tuple\nshape:Null\nndim:0;1\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.autograd.grad(outputs,inputs,grad_outputs=None,retain_graph=None,create_graph=False,only_inputs=True,allow_unused=False)\nParameter description: outputs(sequence of Tensor) - outputs of the differentiated function.\nConstraints:\ndtype:Null\nstructure:sequence;tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.repeat_interleave(input,repeats,dim=None)\nParameter description: dim(int, optional) - The dimension along which to repeat values.By default, use the flattened input array, and return a flat outputarray.\nConstraints:\ndtype:int\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.sparse.sum(input,dim=None,dtype=None)\nParameter description: dtype(`torch.dtype`, optional) - the desired data type of returned Tensor. Default: dtype of `input`.\nConstraints:",
            "constr": {
                "dtype": [
                    "torch.dtype"
                ],
                "structure": [],
                "shape": [],
                "ndim": [],
                "enum": [],
                "range": []
            },
            "signature": "torch.sparse.sum(input,dim=None,dtype=None)",
            "descp": "dtype(`torch.dtype`, optional) - the desired data type of returned Tensor. Default: dtype of `input`."
        },
        {
            "fname": "torch.nn.quantized.functional.conv3d.yaml",
            "arg": "bias",
            "prompt": "Signature: torch.distributed.recv(tensor,src=None,group=<objectobject>,tag=0)\nParameter description: tensor(Tensor) - Tensor to fill with received data.\nConstraints:\ndtype:Null\nstructure:tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.div(input,other,out=None)\nParameter description: out(Tensor, optional) - the output tensor.\nConstraints:\ndtype:Null\nstructure:tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.nn.TransformerDecoder(decoder_layer,num_layers,norm=None)\nParameter description: num_layers - the number of sub-decoder-layers in the decoder (required).\nConstraints:\ndtype:int\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:[0,inf)\n\n###\n\nSignature: torch.nn.quantized.functional.interpolate(input,size=None,scale_factor=None,mode=nearest,align_corners=None)\nParameter description: align_corners(bool, optional) - Geometrically, we consider the pixels of the input and output as squares rather than points. If set to `True`, the input and output tensors are aligned by the center points of their corner pixels, preserving the values at the corner pixels. If set to `False`, the input and output tensors are aligned by the corner points of their corner pixels, and the interpolation uses edge value padding for out-of-boundary values, making this operation independent of input size when `scale_factor` is kept the same. This only has an effect when `mode` is `'bilinear'`. Default: `False`\nConstraints:\ndtype:bool\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.gt(input,other,out=None)\nParameter description: out(Tensor, optional) - the output tensor that must be a BoolTensor\nConstraints:\ndtype:booltensor\nstructure:tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.empty_like(input,dtype=None,layout=None,device=None,requires_grad=False,memory_format=torch.preserve_format)\nParameter description: dtype(`torch.dtype`, optional) - the desired data type of returned Tensor. Default: if `None`, defaults to the dtype of `input`.\nConstraints:\ndtype:torch.dtype\nstructure:Null\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.nn.RNN(*args,**kwargs,input_size=None,hidden_size=None,num_layers=None,nonlinearity=None,bias=None,batch_first=None,dropout=None,bidirectional=None)\nParameter description: hidden_size - The number of features in the hidden state h\nConstraints:\ndtype:int\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:[0,inf)\n\n###\n\nSignature: torch.nn.functional.avg_pool1d(input,kernel_size,stride=None,padding=0,ceil_mode=False,count_include_pad=True)\nParameter description: padding - implicit zero paddings on both sides of the input. Can be a single number or a tuple (padW,). Default: 0\nConstraints:\ndtype:bool;int\nstructure:tuple\nshape:Null\nndim:0;1\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.autograd.grad(outputs,inputs,grad_outputs=None,retain_graph=None,create_graph=False,only_inputs=True,allow_unused=False)\nParameter description: outputs(sequence of Tensor) - outputs of the differentiated function.\nConstraints:\ndtype:Null\nstructure:sequence;tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.repeat_interleave(input,repeats,dim=None)\nParameter description: dim(int, optional) - The dimension along which to repeat values.By default, use the flattened input array, and return a flat outputarray.\nConstraints:\ndtype:int\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.nn.quantized.functional.conv3d(input,weight,bias,stride=1,padding=0,dilation=1,groups=1,padding_mode=zeros,scale=1.0,zero_point=0,dtype=torch.quint8)\nParameter description: bias - non-quantized bias tensor of shape (out _channels) . The tensor type must be torch.float.\nConstraints:",
            "constr": {
                "dtype": [
                    "torch.float"
                ],
                "structure": [
                    "tensor"
                ],
                "shape": [
                    "(out _channels)"
                ],
                "ndim": [],
                "enum": [],
                "range": []
            },
            "signature": "torch.nn.quantized.functional.conv3d(input,weight,bias,stride=1,padding=0,dilation=1,groups=1,padding_mode=zeros,scale=1.0,zero_point=0,dtype=torch.quint8)",
            "descp": "bias - non-quantized bias tensor of shape (out _channels) . The tensor type must be torch.float."
        },
        {
            "fname": "torch.cuda.memory_stats.yaml",
            "arg": "device",
            "prompt": "Signature: torch.distributed.recv(tensor,src=None,group=<objectobject>,tag=0)\nParameter description: tensor(Tensor) - Tensor to fill with received data.\nConstraints:\ndtype:Null\nstructure:tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.div(input,other,out=None)\nParameter description: out(Tensor, optional) - the output tensor.\nConstraints:\ndtype:Null\nstructure:tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.nn.TransformerDecoder(decoder_layer,num_layers,norm=None)\nParameter description: num_layers - the number of sub-decoder-layers in the decoder (required).\nConstraints:\ndtype:int\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:[0,inf)\n\n###\n\nSignature: torch.nn.quantized.functional.interpolate(input,size=None,scale_factor=None,mode=nearest,align_corners=None)\nParameter description: align_corners(bool, optional) - Geometrically, we consider the pixels of the input and output as squares rather than points. If set to `True`, the input and output tensors are aligned by the center points of their corner pixels, preserving the values at the corner pixels. If set to `False`, the input and output tensors are aligned by the corner points of their corner pixels, and the interpolation uses edge value padding for out-of-boundary values, making this operation independent of input size when `scale_factor` is kept the same. This only has an effect when `mode` is `'bilinear'`. Default: `False`\nConstraints:\ndtype:bool\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.gt(input,other,out=None)\nParameter description: out(Tensor, optional) - the output tensor that must be a BoolTensor\nConstraints:\ndtype:booltensor\nstructure:tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.empty_like(input,dtype=None,layout=None,device=None,requires_grad=False,memory_format=torch.preserve_format)\nParameter description: dtype(`torch.dtype`, optional) - the desired data type of returned Tensor. Default: if `None`, defaults to the dtype of `input`.\nConstraints:\ndtype:torch.dtype\nstructure:Null\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.nn.RNN(*args,**kwargs,input_size=None,hidden_size=None,num_layers=None,nonlinearity=None,bias=None,batch_first=None,dropout=None,bidirectional=None)\nParameter description: hidden_size - The number of features in the hidden state h\nConstraints:\ndtype:int\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:[0,inf)\n\n###\n\nSignature: torch.nn.functional.avg_pool1d(input,kernel_size,stride=None,padding=0,ceil_mode=False,count_include_pad=True)\nParameter description: padding - implicit zero paddings on both sides of the input. Can be a single number or a tuple (padW,). Default: 0\nConstraints:\ndtype:bool;int\nstructure:tuple\nshape:Null\nndim:0;1\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.autograd.grad(outputs,inputs,grad_outputs=None,retain_graph=None,create_graph=False,only_inputs=True,allow_unused=False)\nParameter description: outputs(sequence of Tensor) - outputs of the differentiated function.\nConstraints:\ndtype:Null\nstructure:sequence;tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.repeat_interleave(input,repeats,dim=None)\nParameter description: dim(int, optional) - The dimension along which to repeat values.By default, use the flattened input array, and return a flat outputarray.\nConstraints:\ndtype:int\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.cuda.memory_stats(device=None)\nParameter description: device(torch.device or int, optional) - selected device. Returns statistics for the current device, given by `current_device()`, if `device` is `None` (default).\nConstraints:",
            "constr": {
                "dtype": [
                    "int"
                ],
                "structure": [],
                "shape": [],
                "ndim": [],
                "enum": [],
                "range": []
            },
            "signature": "torch.cuda.memory_stats(device=None)",
            "descp": "device(torch.device or int, optional) - selected device. Returns statistics for the current device, given by `current_device()`, if `device` is `None` (default)."
        },
        {
            "fname": "torch.addcmul.yaml",
            "arg": "input",
            "prompt": "Signature: torch.distributed.recv(tensor,src=None,group=<objectobject>,tag=0)\nParameter description: tensor(Tensor) - Tensor to fill with received data.\nConstraints:\ndtype:Null\nstructure:tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.div(input,other,out=None)\nParameter description: out(Tensor, optional) - the output tensor.\nConstraints:\ndtype:Null\nstructure:tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.nn.TransformerDecoder(decoder_layer,num_layers,norm=None)\nParameter description: num_layers - the number of sub-decoder-layers in the decoder (required).\nConstraints:\ndtype:int\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:[0,inf)\n\n###\n\nSignature: torch.nn.quantized.functional.interpolate(input,size=None,scale_factor=None,mode=nearest,align_corners=None)\nParameter description: align_corners(bool, optional) - Geometrically, we consider the pixels of the input and output as squares rather than points. If set to `True`, the input and output tensors are aligned by the center points of their corner pixels, preserving the values at the corner pixels. If set to `False`, the input and output tensors are aligned by the corner points of their corner pixels, and the interpolation uses edge value padding for out-of-boundary values, making this operation independent of input size when `scale_factor` is kept the same. This only has an effect when `mode` is `'bilinear'`. Default: `False`\nConstraints:\ndtype:bool\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.gt(input,other,out=None)\nParameter description: out(Tensor, optional) - the output tensor that must be a BoolTensor\nConstraints:\ndtype:booltensor\nstructure:tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.empty_like(input,dtype=None,layout=None,device=None,requires_grad=False,memory_format=torch.preserve_format)\nParameter description: dtype(`torch.dtype`, optional) - the desired data type of returned Tensor. Default: if `None`, defaults to the dtype of `input`.\nConstraints:\ndtype:torch.dtype\nstructure:Null\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.nn.RNN(*args,**kwargs,input_size=None,hidden_size=None,num_layers=None,nonlinearity=None,bias=None,batch_first=None,dropout=None,bidirectional=None)\nParameter description: hidden_size - The number of features in the hidden state h\nConstraints:\ndtype:int\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:[0,inf)\n\n###\n\nSignature: torch.nn.functional.avg_pool1d(input,kernel_size,stride=None,padding=0,ceil_mode=False,count_include_pad=True)\nParameter description: padding - implicit zero paddings on both sides of the input. Can be a single number or a tuple (padW,). Default: 0\nConstraints:\ndtype:bool;int\nstructure:tuple\nshape:Null\nndim:0;1\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.autograd.grad(outputs,inputs,grad_outputs=None,retain_graph=None,create_graph=False,only_inputs=True,allow_unused=False)\nParameter description: outputs(sequence of Tensor) - outputs of the differentiated function.\nConstraints:\ndtype:Null\nstructure:sequence;tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.repeat_interleave(input,repeats,dim=None)\nParameter description: dim(int, optional) - The dimension along which to repeat values.By default, use the flattened input array, and return a flat outputarray.\nConstraints:\ndtype:int\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.addcmul(input,tensor1,tensor2,value=1,out=None)\nParameter description: input(Tensor) - the tensor to be added\nConstraints:",
            "constr": {
                "dtype": [],
                "structure": [
                    "tensor"
                ],
                "shape": [],
                "ndim": [],
                "enum": [],
                "range": []
            },
            "signature": "torch.addcmul(input,tensor1,tensor2,value=1,out=None)",
            "descp": "input(Tensor) - the tensor to be added"
        },
        {
            "fname": "torch.nn.adaptivelogsoftmaxwithloss.yaml",
            "arg": "div_value",
            "prompt": "Signature: torch.distributed.recv(tensor,src=None,group=<objectobject>,tag=0)\nParameter description: tensor(Tensor) - Tensor to fill with received data.\nConstraints:\ndtype:Null\nstructure:tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.div(input,other,out=None)\nParameter description: out(Tensor, optional) - the output tensor.\nConstraints:\ndtype:Null\nstructure:tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.nn.TransformerDecoder(decoder_layer,num_layers,norm=None)\nParameter description: num_layers - the number of sub-decoder-layers in the decoder (required).\nConstraints:\ndtype:int\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:[0,inf)\n\n###\n\nSignature: torch.nn.quantized.functional.interpolate(input,size=None,scale_factor=None,mode=nearest,align_corners=None)\nParameter description: align_corners(bool, optional) - Geometrically, we consider the pixels of the input and output as squares rather than points. If set to `True`, the input and output tensors are aligned by the center points of their corner pixels, preserving the values at the corner pixels. If set to `False`, the input and output tensors are aligned by the corner points of their corner pixels, and the interpolation uses edge value padding for out-of-boundary values, making this operation independent of input size when `scale_factor` is kept the same. This only has an effect when `mode` is `'bilinear'`. Default: `False`\nConstraints:\ndtype:bool\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.gt(input,other,out=None)\nParameter description: out(Tensor, optional) - the output tensor that must be a BoolTensor\nConstraints:\ndtype:booltensor\nstructure:tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.empty_like(input,dtype=None,layout=None,device=None,requires_grad=False,memory_format=torch.preserve_format)\nParameter description: dtype(`torch.dtype`, optional) - the desired data type of returned Tensor. Default: if `None`, defaults to the dtype of `input`.\nConstraints:\ndtype:torch.dtype\nstructure:Null\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.nn.RNN(*args,**kwargs,input_size=None,hidden_size=None,num_layers=None,nonlinearity=None,bias=None,batch_first=None,dropout=None,bidirectional=None)\nParameter description: hidden_size - The number of features in the hidden state h\nConstraints:\ndtype:int\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:[0,inf)\n\n###\n\nSignature: torch.nn.functional.avg_pool1d(input,kernel_size,stride=None,padding=0,ceil_mode=False,count_include_pad=True)\nParameter description: padding - implicit zero paddings on both sides of the input. Can be a single number or a tuple (padW,). Default: 0\nConstraints:\ndtype:bool;int\nstructure:tuple\nshape:Null\nndim:0;1\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.autograd.grad(outputs,inputs,grad_outputs=None,retain_graph=None,create_graph=False,only_inputs=True,allow_unused=False)\nParameter description: outputs(sequence of Tensor) - outputs of the differentiated function.\nConstraints:\ndtype:Null\nstructure:sequence;tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.repeat_interleave(input,repeats,dim=None)\nParameter description: dim(int, optional) - The dimension along which to repeat values.By default, use the flattened input array, and return a flat outputarray.\nConstraints:\ndtype:int\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.nn.AdaptiveLogSoftmaxWithLoss(in_features,n_classes,cutoffs,div_value=4.0,head_bias=False)\nParameter description: div_value(float, optional) - value used as an exponent to compute sizes of the clusters. Default: 4.0\nConstraints:",
            "constr": {
                "dtype": [
                    "float",
                    "bool"
                ],
                "structure": [],
                "shape": [],
                "ndim": [
                    "0"
                ],
                "enum": [],
                "range": []
            },
            "signature": "torch.nn.AdaptiveLogSoftmaxWithLoss(in_features,n_classes,cutoffs,div_value=4.0,head_bias=False)",
            "descp": "div_value(float, optional) - value used as an exponent to compute sizes of the clusters. Default: 4.0"
        },
        {
            "fname": "torch.autograd.functional.hessian.yaml",
            "arg": "strict",
            "prompt": "Signature: torch.distributed.recv(tensor,src=None,group=<objectobject>,tag=0)\nParameter description: tensor(Tensor) - Tensor to fill with received data.\nConstraints:\ndtype:Null\nstructure:tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.div(input,other,out=None)\nParameter description: out(Tensor, optional) - the output tensor.\nConstraints:\ndtype:Null\nstructure:tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.nn.TransformerDecoder(decoder_layer,num_layers,norm=None)\nParameter description: num_layers - the number of sub-decoder-layers in the decoder (required).\nConstraints:\ndtype:int\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:[0,inf)\n\n###\n\nSignature: torch.nn.quantized.functional.interpolate(input,size=None,scale_factor=None,mode=nearest,align_corners=None)\nParameter description: align_corners(bool, optional) - Geometrically, we consider the pixels of the input and output as squares rather than points. If set to `True`, the input and output tensors are aligned by the center points of their corner pixels, preserving the values at the corner pixels. If set to `False`, the input and output tensors are aligned by the corner points of their corner pixels, and the interpolation uses edge value padding for out-of-boundary values, making this operation independent of input size when `scale_factor` is kept the same. This only has an effect when `mode` is `'bilinear'`. Default: `False`\nConstraints:\ndtype:bool\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.gt(input,other,out=None)\nParameter description: out(Tensor, optional) - the output tensor that must be a BoolTensor\nConstraints:\ndtype:booltensor\nstructure:tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.empty_like(input,dtype=None,layout=None,device=None,requires_grad=False,memory_format=torch.preserve_format)\nParameter description: dtype(`torch.dtype`, optional) - the desired data type of returned Tensor. Default: if `None`, defaults to the dtype of `input`.\nConstraints:\ndtype:torch.dtype\nstructure:Null\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.nn.RNN(*args,**kwargs,input_size=None,hidden_size=None,num_layers=None,nonlinearity=None,bias=None,batch_first=None,dropout=None,bidirectional=None)\nParameter description: hidden_size - The number of features in the hidden state h\nConstraints:\ndtype:int\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:[0,inf)\n\n###\n\nSignature: torch.nn.functional.avg_pool1d(input,kernel_size,stride=None,padding=0,ceil_mode=False,count_include_pad=True)\nParameter description: padding - implicit zero paddings on both sides of the input. Can be a single number or a tuple (padW,). Default: 0\nConstraints:\ndtype:bool;int\nstructure:tuple\nshape:Null\nndim:0;1\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.autograd.grad(outputs,inputs,grad_outputs=None,retain_graph=None,create_graph=False,only_inputs=True,allow_unused=False)\nParameter description: outputs(sequence of Tensor) - outputs of the differentiated function.\nConstraints:\ndtype:Null\nstructure:sequence;tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.repeat_interleave(input,repeats,dim=None)\nParameter description: dim(int, optional) - The dimension along which to repeat values.By default, use the flattened input array, and return a flat outputarray.\nConstraints:\ndtype:int\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.autograd.functional.hessian(func,inputs,create_graph=False,strict=False)\nParameter description: strict(bool, optional) - If `True`, an error will be raised when we detect that there exists an input such that all the outputs are independent of it. If `False`, we return a Tensor of zeros as the hessian for said inputs, which is the expected mathematical value. Defaults to `False`.\nConstraints:",
            "constr": {
                "dtype": [
                    "bool"
                ],
                "structure": [],
                "shape": [],
                "ndim": [
                    "0"
                ],
                "enum": [],
                "range": []
            },
            "signature": "torch.autograd.functional.hessian(func,inputs,create_graph=False,strict=False)",
            "descp": "strict(bool, optional) - If `True`, an error will be raised when we detect that there exists an input such that all the outputs are independent of it. If `False`, we return a Tensor of zeros as the hessian for said inputs, which is the expected mathematical value. Defaults to `False`."
        },
        {
            "fname": "torch.nn.pairwisedistance.yaml",
            "arg": "keepdim",
            "prompt": "Signature: torch.distributed.recv(tensor,src=None,group=<objectobject>,tag=0)\nParameter description: tensor(Tensor) - Tensor to fill with received data.\nConstraints:\ndtype:Null\nstructure:tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.div(input,other,out=None)\nParameter description: out(Tensor, optional) - the output tensor.\nConstraints:\ndtype:Null\nstructure:tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.nn.TransformerDecoder(decoder_layer,num_layers,norm=None)\nParameter description: num_layers - the number of sub-decoder-layers in the decoder (required).\nConstraints:\ndtype:int\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:[0,inf)\n\n###\n\nSignature: torch.nn.quantized.functional.interpolate(input,size=None,scale_factor=None,mode=nearest,align_corners=None)\nParameter description: align_corners(bool, optional) - Geometrically, we consider the pixels of the input and output as squares rather than points. If set to `True`, the input and output tensors are aligned by the center points of their corner pixels, preserving the values at the corner pixels. If set to `False`, the input and output tensors are aligned by the corner points of their corner pixels, and the interpolation uses edge value padding for out-of-boundary values, making this operation independent of input size when `scale_factor` is kept the same. This only has an effect when `mode` is `'bilinear'`. Default: `False`\nConstraints:\ndtype:bool\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.gt(input,other,out=None)\nParameter description: out(Tensor, optional) - the output tensor that must be a BoolTensor\nConstraints:\ndtype:booltensor\nstructure:tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.empty_like(input,dtype=None,layout=None,device=None,requires_grad=False,memory_format=torch.preserve_format)\nParameter description: dtype(`torch.dtype`, optional) - the desired data type of returned Tensor. Default: if `None`, defaults to the dtype of `input`.\nConstraints:\ndtype:torch.dtype\nstructure:Null\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.nn.RNN(*args,**kwargs,input_size=None,hidden_size=None,num_layers=None,nonlinearity=None,bias=None,batch_first=None,dropout=None,bidirectional=None)\nParameter description: hidden_size - The number of features in the hidden state h\nConstraints:\ndtype:int\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:[0,inf)\n\n###\n\nSignature: torch.nn.functional.avg_pool1d(input,kernel_size,stride=None,padding=0,ceil_mode=False,count_include_pad=True)\nParameter description: padding - implicit zero paddings on both sides of the input. Can be a single number or a tuple (padW,). Default: 0\nConstraints:\ndtype:bool;int\nstructure:tuple\nshape:Null\nndim:0;1\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.autograd.grad(outputs,inputs,grad_outputs=None,retain_graph=None,create_graph=False,only_inputs=True,allow_unused=False)\nParameter description: outputs(sequence of Tensor) - outputs of the differentiated function.\nConstraints:\ndtype:Null\nstructure:sequence;tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.repeat_interleave(input,repeats,dim=None)\nParameter description: dim(int, optional) - The dimension along which to repeat values.By default, use the flattened input array, and return a flat outputarray.\nConstraints:\ndtype:int\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.nn.PairwiseDistance(p=2.0,eps=1e-06,keepdim=False)\nParameter description: keepdim(bool, optional) - Determines whether or not to keep the vector dimension. Default: False\nConstraints:",
            "constr": {
                "dtype": [
                    "bool"
                ],
                "structure": [],
                "shape": [],
                "ndim": [
                    "0"
                ],
                "enum": [],
                "range": []
            },
            "signature": "torch.nn.PairwiseDistance(p=2.0,eps=1e-06,keepdim=False)",
            "descp": "keepdim(bool, optional) - Determines whether or not to keep the vector dimension. Default: False"
        },
        {
            "fname": "torch.mv.yaml",
            "arg": "input",
            "prompt": "Signature: torch.distributed.recv(tensor,src=None,group=<objectobject>,tag=0)\nParameter description: tensor(Tensor) - Tensor to fill with received data.\nConstraints:\ndtype:Null\nstructure:tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.div(input,other,out=None)\nParameter description: out(Tensor, optional) - the output tensor.\nConstraints:\ndtype:Null\nstructure:tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.nn.TransformerDecoder(decoder_layer,num_layers,norm=None)\nParameter description: num_layers - the number of sub-decoder-layers in the decoder (required).\nConstraints:\ndtype:int\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:[0,inf)\n\n###\n\nSignature: torch.nn.quantized.functional.interpolate(input,size=None,scale_factor=None,mode=nearest,align_corners=None)\nParameter description: align_corners(bool, optional) - Geometrically, we consider the pixels of the input and output as squares rather than points. If set to `True`, the input and output tensors are aligned by the center points of their corner pixels, preserving the values at the corner pixels. If set to `False`, the input and output tensors are aligned by the corner points of their corner pixels, and the interpolation uses edge value padding for out-of-boundary values, making this operation independent of input size when `scale_factor` is kept the same. This only has an effect when `mode` is `'bilinear'`. Default: `False`\nConstraints:\ndtype:bool\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.gt(input,other,out=None)\nParameter description: out(Tensor, optional) - the output tensor that must be a BoolTensor\nConstraints:\ndtype:booltensor\nstructure:tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.empty_like(input,dtype=None,layout=None,device=None,requires_grad=False,memory_format=torch.preserve_format)\nParameter description: dtype(`torch.dtype`, optional) - the desired data type of returned Tensor. Default: if `None`, defaults to the dtype of `input`.\nConstraints:\ndtype:torch.dtype\nstructure:Null\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.nn.RNN(*args,**kwargs,input_size=None,hidden_size=None,num_layers=None,nonlinearity=None,bias=None,batch_first=None,dropout=None,bidirectional=None)\nParameter description: hidden_size - The number of features in the hidden state h\nConstraints:\ndtype:int\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:[0,inf)\n\n###\n\nSignature: torch.nn.functional.avg_pool1d(input,kernel_size,stride=None,padding=0,ceil_mode=False,count_include_pad=True)\nParameter description: padding - implicit zero paddings on both sides of the input. Can be a single number or a tuple (padW,). Default: 0\nConstraints:\ndtype:bool;int\nstructure:tuple\nshape:Null\nndim:0;1\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.autograd.grad(outputs,inputs,grad_outputs=None,retain_graph=None,create_graph=False,only_inputs=True,allow_unused=False)\nParameter description: outputs(sequence of Tensor) - outputs of the differentiated function.\nConstraints:\ndtype:Null\nstructure:sequence;tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.repeat_interleave(input,repeats,dim=None)\nParameter description: dim(int, optional) - The dimension along which to repeat values.By default, use the flattened input array, and return a flat outputarray.\nConstraints:\ndtype:int\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.mv(input,vec,out=None)\nParameter description: input(Tensor) - matrix to be multiplied\nConstraints:",
            "constr": {
                "dtype": [
                    "numeric"
                ],
                "structure": [
                    "tensor"
                ],
                "shape": [],
                "ndim": [],
                "enum": [],
                "range": []
            },
            "signature": "torch.mv(input,vec,out=None)",
            "descp": "input(Tensor) - matrix to be multiplied"
        },
        {
            "fname": "torch.nn.reflectionpad2d.yaml",
            "arg": "padding",
            "prompt": "Signature: torch.distributed.recv(tensor,src=None,group=<objectobject>,tag=0)\nParameter description: tensor(Tensor) - Tensor to fill with received data.\nConstraints:\ndtype:Null\nstructure:tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.div(input,other,out=None)\nParameter description: out(Tensor, optional) - the output tensor.\nConstraints:\ndtype:Null\nstructure:tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.nn.TransformerDecoder(decoder_layer,num_layers,norm=None)\nParameter description: num_layers - the number of sub-decoder-layers in the decoder (required).\nConstraints:\ndtype:int\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:[0,inf)\n\n###\n\nSignature: torch.nn.quantized.functional.interpolate(input,size=None,scale_factor=None,mode=nearest,align_corners=None)\nParameter description: align_corners(bool, optional) - Geometrically, we consider the pixels of the input and output as squares rather than points. If set to `True`, the input and output tensors are aligned by the center points of their corner pixels, preserving the values at the corner pixels. If set to `False`, the input and output tensors are aligned by the corner points of their corner pixels, and the interpolation uses edge value padding for out-of-boundary values, making this operation independent of input size when `scale_factor` is kept the same. This only has an effect when `mode` is `'bilinear'`. Default: `False`\nConstraints:\ndtype:bool\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.gt(input,other,out=None)\nParameter description: out(Tensor, optional) - the output tensor that must be a BoolTensor\nConstraints:\ndtype:booltensor\nstructure:tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.empty_like(input,dtype=None,layout=None,device=None,requires_grad=False,memory_format=torch.preserve_format)\nParameter description: dtype(`torch.dtype`, optional) - the desired data type of returned Tensor. Default: if `None`, defaults to the dtype of `input`.\nConstraints:\ndtype:torch.dtype\nstructure:Null\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.nn.RNN(*args,**kwargs,input_size=None,hidden_size=None,num_layers=None,nonlinearity=None,bias=None,batch_first=None,dropout=None,bidirectional=None)\nParameter description: hidden_size - The number of features in the hidden state h\nConstraints:\ndtype:int\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:[0,inf)\n\n###\n\nSignature: torch.nn.functional.avg_pool1d(input,kernel_size,stride=None,padding=0,ceil_mode=False,count_include_pad=True)\nParameter description: padding - implicit zero paddings on both sides of the input. Can be a single number or a tuple (padW,). Default: 0\nConstraints:\ndtype:bool;int\nstructure:tuple\nshape:Null\nndim:0;1\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.autograd.grad(outputs,inputs,grad_outputs=None,retain_graph=None,create_graph=False,only_inputs=True,allow_unused=False)\nParameter description: outputs(sequence of Tensor) - outputs of the differentiated function.\nConstraints:\ndtype:Null\nstructure:sequence;tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.repeat_interleave(input,repeats,dim=None)\nParameter description: dim(int, optional) - The dimension along which to repeat values.By default, use the flattened input array, and return a flat outputarray.\nConstraints:\ndtype:int\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.nn.ReflectionPad2d(padding)\nParameter description: padding(int, tuple) - the size of the padding. If is int, uses the same padding in all boundaries. If a 4-tuple, uses (padding _left , padding _right , padding _top , padding _bottom )\nConstraints:",
            "constr": {
                "dtype": [
                    "int"
                ],
                "structure": [
                    "tuple"
                ],
                "shape": [
                    "[4]"
                ],
                "ndim": [],
                "enum": [],
                "range": [
                    "[0,inf)"
                ]
            },
            "signature": "torch.nn.ReflectionPad2d(padding)",
            "descp": "padding(int, tuple) - the size of the padding. If is int, uses the same padding in all boundaries. If a 4-tuple, uses (padding _left , padding _right , padding _top , padding _bottom )"
        },
        {
            "fname": "torch.distributed.send.yaml",
            "arg": "tag",
            "prompt": "Signature: torch.distributed.recv(tensor,src=None,group=<objectobject>,tag=0)\nParameter description: tensor(Tensor) - Tensor to fill with received data.\nConstraints:\ndtype:Null\nstructure:tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.div(input,other,out=None)\nParameter description: out(Tensor, optional) - the output tensor.\nConstraints:\ndtype:Null\nstructure:tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.nn.TransformerDecoder(decoder_layer,num_layers,norm=None)\nParameter description: num_layers - the number of sub-decoder-layers in the decoder (required).\nConstraints:\ndtype:int\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:[0,inf)\n\n###\n\nSignature: torch.nn.quantized.functional.interpolate(input,size=None,scale_factor=None,mode=nearest,align_corners=None)\nParameter description: align_corners(bool, optional) - Geometrically, we consider the pixels of the input and output as squares rather than points. If set to `True`, the input and output tensors are aligned by the center points of their corner pixels, preserving the values at the corner pixels. If set to `False`, the input and output tensors are aligned by the corner points of their corner pixels, and the interpolation uses edge value padding for out-of-boundary values, making this operation independent of input size when `scale_factor` is kept the same. This only has an effect when `mode` is `'bilinear'`. Default: `False`\nConstraints:\ndtype:bool\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.gt(input,other,out=None)\nParameter description: out(Tensor, optional) - the output tensor that must be a BoolTensor\nConstraints:\ndtype:booltensor\nstructure:tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.empty_like(input,dtype=None,layout=None,device=None,requires_grad=False,memory_format=torch.preserve_format)\nParameter description: dtype(`torch.dtype`, optional) - the desired data type of returned Tensor. Default: if `None`, defaults to the dtype of `input`.\nConstraints:\ndtype:torch.dtype\nstructure:Null\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.nn.RNN(*args,**kwargs,input_size=None,hidden_size=None,num_layers=None,nonlinearity=None,bias=None,batch_first=None,dropout=None,bidirectional=None)\nParameter description: hidden_size - The number of features in the hidden state h\nConstraints:\ndtype:int\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:[0,inf)\n\n###\n\nSignature: torch.nn.functional.avg_pool1d(input,kernel_size,stride=None,padding=0,ceil_mode=False,count_include_pad=True)\nParameter description: padding - implicit zero paddings on both sides of the input. Can be a single number or a tuple (padW,). Default: 0\nConstraints:\ndtype:bool;int\nstructure:tuple\nshape:Null\nndim:0;1\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.autograd.grad(outputs,inputs,grad_outputs=None,retain_graph=None,create_graph=False,only_inputs=True,allow_unused=False)\nParameter description: outputs(sequence of Tensor) - outputs of the differentiated function.\nConstraints:\ndtype:Null\nstructure:sequence;tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.repeat_interleave(input,repeats,dim=None)\nParameter description: dim(int, optional) - The dimension along which to repeat values.By default, use the flattened input array, and return a flat outputarray.\nConstraints:\ndtype:int\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.distributed.send(tensor,dst,group=<objectobject>,tag=0)\nParameter description: tag(int, optional) - Tag to match send with remote recv\nConstraints:",
            "constr": {
                "dtype": [
                    "int"
                ],
                "structure": [],
                "shape": [],
                "ndim": [
                    "0"
                ],
                "enum": [],
                "range": []
            },
            "signature": "torch.distributed.send(tensor,dst,group=<objectobject>,tag=0)",
            "descp": "tag(int, optional) - Tag to match send with remote recv"
        },
        {
            "fname": "torch.onnx.export.yaml",
            "arg": "output_names",
            "prompt": "Signature: torch.distributed.recv(tensor,src=None,group=<objectobject>,tag=0)\nParameter description: tensor(Tensor) - Tensor to fill with received data.\nConstraints:\ndtype:Null\nstructure:tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.div(input,other,out=None)\nParameter description: out(Tensor, optional) - the output tensor.\nConstraints:\ndtype:Null\nstructure:tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.nn.TransformerDecoder(decoder_layer,num_layers,norm=None)\nParameter description: num_layers - the number of sub-decoder-layers in the decoder (required).\nConstraints:\ndtype:int\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:[0,inf)\n\n###\n\nSignature: torch.nn.quantized.functional.interpolate(input,size=None,scale_factor=None,mode=nearest,align_corners=None)\nParameter description: align_corners(bool, optional) - Geometrically, we consider the pixels of the input and output as squares rather than points. If set to `True`, the input and output tensors are aligned by the center points of their corner pixels, preserving the values at the corner pixels. If set to `False`, the input and output tensors are aligned by the corner points of their corner pixels, and the interpolation uses edge value padding for out-of-boundary values, making this operation independent of input size when `scale_factor` is kept the same. This only has an effect when `mode` is `'bilinear'`. Default: `False`\nConstraints:\ndtype:bool\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.gt(input,other,out=None)\nParameter description: out(Tensor, optional) - the output tensor that must be a BoolTensor\nConstraints:\ndtype:booltensor\nstructure:tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.empty_like(input,dtype=None,layout=None,device=None,requires_grad=False,memory_format=torch.preserve_format)\nParameter description: dtype(`torch.dtype`, optional) - the desired data type of returned Tensor. Default: if `None`, defaults to the dtype of `input`.\nConstraints:\ndtype:torch.dtype\nstructure:Null\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.nn.RNN(*args,**kwargs,input_size=None,hidden_size=None,num_layers=None,nonlinearity=None,bias=None,batch_first=None,dropout=None,bidirectional=None)\nParameter description: hidden_size - The number of features in the hidden state h\nConstraints:\ndtype:int\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:[0,inf)\n\n###\n\nSignature: torch.nn.functional.avg_pool1d(input,kernel_size,stride=None,padding=0,ceil_mode=False,count_include_pad=True)\nParameter description: padding - implicit zero paddings on both sides of the input. Can be a single number or a tuple (padW,). Default: 0\nConstraints:\ndtype:bool;int\nstructure:tuple\nshape:Null\nndim:0;1\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.autograd.grad(outputs,inputs,grad_outputs=None,retain_graph=None,create_graph=False,only_inputs=True,allow_unused=False)\nParameter description: outputs(sequence of Tensor) - outputs of the differentiated function.\nConstraints:\ndtype:Null\nstructure:sequence;tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.repeat_interleave(input,repeats,dim=None)\nParameter description: dim(int, optional) - The dimension along which to repeat values.By default, use the flattened input array, and return a flat outputarray.\nConstraints:\ndtype:int\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.onnx.export(model,args,f,export_params=True,verbose=False,training=False,input_names=None,output_names=None,aten=False,export_raw_ir=False,operator_export_type=None,opset_version=None,_retain_param_name=True,do_constant_folding=True,example_outputs=None,strip_doc_string=True,dynamic_axes=None,keep_initializers_as_inputs=None,custom_opsets=None,enable_onnx_checker=True,use_external_data_format=False)\nParameter description: output_names(list of strings, default empty list) - names to assign to the output nodes of the graph, in order\nConstraints:",
            "constr": {
                "dtype": [
                    "string"
                ],
                "structure": [
                    "list"
                ],
                "shape": [],
                "ndim": [
                    "1"
                ],
                "enum": [],
                "range": []
            },
            "signature": "torch.onnx.export(model,args,f,export_params=True,verbose=False,training=False,input_names=None,output_names=None,aten=False,export_raw_ir=False,operator_export_type=None,opset_version=None,_retain_param_name=True,do_constant_folding=True,example_outputs=None,strip_doc_string=True,dynamic_axes=None,keep_initializers_as_inputs=None,custom_opsets=None,enable_onnx_checker=True,use_external_data_format=False)",
            "descp": "output_names(list of strings, default empty list) - names to assign to the output nodes of the graph, in order"
        },
        {
            "fname": "torch.nn.conv3d.yaml",
            "arg": "stride",
            "prompt": "Signature: torch.distributed.recv(tensor,src=None,group=<objectobject>,tag=0)\nParameter description: tensor(Tensor) - Tensor to fill with received data.\nConstraints:\ndtype:Null\nstructure:tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.div(input,other,out=None)\nParameter description: out(Tensor, optional) - the output tensor.\nConstraints:\ndtype:Null\nstructure:tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.nn.TransformerDecoder(decoder_layer,num_layers,norm=None)\nParameter description: num_layers - the number of sub-decoder-layers in the decoder (required).\nConstraints:\ndtype:int\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:[0,inf)\n\n###\n\nSignature: torch.nn.quantized.functional.interpolate(input,size=None,scale_factor=None,mode=nearest,align_corners=None)\nParameter description: align_corners(bool, optional) - Geometrically, we consider the pixels of the input and output as squares rather than points. If set to `True`, the input and output tensors are aligned by the center points of their corner pixels, preserving the values at the corner pixels. If set to `False`, the input and output tensors are aligned by the corner points of their corner pixels, and the interpolation uses edge value padding for out-of-boundary values, making this operation independent of input size when `scale_factor` is kept the same. This only has an effect when `mode` is `'bilinear'`. Default: `False`\nConstraints:\ndtype:bool\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.gt(input,other,out=None)\nParameter description: out(Tensor, optional) - the output tensor that must be a BoolTensor\nConstraints:\ndtype:booltensor\nstructure:tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.empty_like(input,dtype=None,layout=None,device=None,requires_grad=False,memory_format=torch.preserve_format)\nParameter description: dtype(`torch.dtype`, optional) - the desired data type of returned Tensor. Default: if `None`, defaults to the dtype of `input`.\nConstraints:\ndtype:torch.dtype\nstructure:Null\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.nn.RNN(*args,**kwargs,input_size=None,hidden_size=None,num_layers=None,nonlinearity=None,bias=None,batch_first=None,dropout=None,bidirectional=None)\nParameter description: hidden_size - The number of features in the hidden state h\nConstraints:\ndtype:int\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:[0,inf)\n\n###\n\nSignature: torch.nn.functional.avg_pool1d(input,kernel_size,stride=None,padding=0,ceil_mode=False,count_include_pad=True)\nParameter description: padding - implicit zero paddings on both sides of the input. Can be a single number or a tuple (padW,). Default: 0\nConstraints:\ndtype:bool;int\nstructure:tuple\nshape:Null\nndim:0;1\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.autograd.grad(outputs,inputs,grad_outputs=None,retain_graph=None,create_graph=False,only_inputs=True,allow_unused=False)\nParameter description: outputs(sequence of Tensor) - outputs of the differentiated function.\nConstraints:\ndtype:Null\nstructure:sequence;tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.repeat_interleave(input,repeats,dim=None)\nParameter description: dim(int, optional) - The dimension along which to repeat values.By default, use the flattened input array, and return a flat outputarray.\nConstraints:\ndtype:int\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.nn.Conv3d(in_channels,out_channels,kernel_size,stride=1,padding=0,dilation=1,groups=1,bias=True,padding_mode=zeros)\nParameter description: stride(int or tuple, optional) - Stride of the convolution. Default: 1\nConstraints:",
            "constr": {
                "dtype": [
                    "int"
                ],
                "structure": [
                    "tuple"
                ],
                "shape": [],
                "ndim": [
                    "0",
                    "1"
                ],
                "enum": [],
                "range": [
                    "[0,inf)"
                ]
            },
            "signature": "torch.nn.Conv3d(in_channels,out_channels,kernel_size,stride=1,padding=0,dilation=1,groups=1,bias=True,padding_mode=zeros)",
            "descp": "stride(int or tuple, optional) - Stride of the convolution. Default: 1"
        },
        {
            "fname": "torch.nn.functional.conv_transpose3d.yaml",
            "arg": "output_padding",
            "prompt": "Signature: torch.distributed.recv(tensor,src=None,group=<objectobject>,tag=0)\nParameter description: tensor(Tensor) - Tensor to fill with received data.\nConstraints:\ndtype:Null\nstructure:tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.div(input,other,out=None)\nParameter description: out(Tensor, optional) - the output tensor.\nConstraints:\ndtype:Null\nstructure:tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.nn.TransformerDecoder(decoder_layer,num_layers,norm=None)\nParameter description: num_layers - the number of sub-decoder-layers in the decoder (required).\nConstraints:\ndtype:int\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:[0,inf)\n\n###\n\nSignature: torch.nn.quantized.functional.interpolate(input,size=None,scale_factor=None,mode=nearest,align_corners=None)\nParameter description: align_corners(bool, optional) - Geometrically, we consider the pixels of the input and output as squares rather than points. If set to `True`, the input and output tensors are aligned by the center points of their corner pixels, preserving the values at the corner pixels. If set to `False`, the input and output tensors are aligned by the corner points of their corner pixels, and the interpolation uses edge value padding for out-of-boundary values, making this operation independent of input size when `scale_factor` is kept the same. This only has an effect when `mode` is `'bilinear'`. Default: `False`\nConstraints:\ndtype:bool\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.gt(input,other,out=None)\nParameter description: out(Tensor, optional) - the output tensor that must be a BoolTensor\nConstraints:\ndtype:booltensor\nstructure:tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.empty_like(input,dtype=None,layout=None,device=None,requires_grad=False,memory_format=torch.preserve_format)\nParameter description: dtype(`torch.dtype`, optional) - the desired data type of returned Tensor. Default: if `None`, defaults to the dtype of `input`.\nConstraints:\ndtype:torch.dtype\nstructure:Null\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.nn.RNN(*args,**kwargs,input_size=None,hidden_size=None,num_layers=None,nonlinearity=None,bias=None,batch_first=None,dropout=None,bidirectional=None)\nParameter description: hidden_size - The number of features in the hidden state h\nConstraints:\ndtype:int\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:[0,inf)\n\n###\n\nSignature: torch.nn.functional.avg_pool1d(input,kernel_size,stride=None,padding=0,ceil_mode=False,count_include_pad=True)\nParameter description: padding - implicit zero paddings on both sides of the input. Can be a single number or a tuple (padW,). Default: 0\nConstraints:\ndtype:bool;int\nstructure:tuple\nshape:Null\nndim:0;1\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.autograd.grad(outputs,inputs,grad_outputs=None,retain_graph=None,create_graph=False,only_inputs=True,allow_unused=False)\nParameter description: outputs(sequence of Tensor) - outputs of the differentiated function.\nConstraints:\ndtype:Null\nstructure:sequence;tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.repeat_interleave(input,repeats,dim=None)\nParameter description: dim(int, optional) - The dimension along which to repeat values.By default, use the flattened input array, and return a flat outputarray.\nConstraints:\ndtype:int\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.nn.functional.conv_transpose3d(input,weight,bias=None,stride=1,padding=0,output_padding=0,groups=1,dilation=1)\nParameter description: output_padding - additional size added to one side of each dimension in the output shape. Can be a single number or a tuple `(out_padT, out_padH, out_padW)`. Default: 0\nConstraints:",
            "constr": {
                "dtype": [
                    "numeric",
                    "int"
                ],
                "structure": [
                    "tuple"
                ],
                "shape": [],
                "ndim": [
                    "0",
                    "1"
                ],
                "enum": [],
                "range": [
                    "[0,inf)"
                ]
            },
            "signature": "torch.nn.functional.conv_transpose3d(input,weight,bias=None,stride=1,padding=0,output_padding=0,groups=1,dilation=1)",
            "descp": "output_padding - additional size added to one side of each dimension in the output shape. Can be a single number or a tuple `(out_padT, out_padH, out_padW)`. Default: 0"
        },
        {
            "fname": "torch.nn.utils.prune.random_structured.yaml",
            "arg": "amount",
            "prompt": "Signature: torch.distributed.recv(tensor,src=None,group=<objectobject>,tag=0)\nParameter description: tensor(Tensor) - Tensor to fill with received data.\nConstraints:\ndtype:Null\nstructure:tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.div(input,other,out=None)\nParameter description: out(Tensor, optional) - the output tensor.\nConstraints:\ndtype:Null\nstructure:tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.nn.TransformerDecoder(decoder_layer,num_layers,norm=None)\nParameter description: num_layers - the number of sub-decoder-layers in the decoder (required).\nConstraints:\ndtype:int\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:[0,inf)\n\n###\n\nSignature: torch.nn.quantized.functional.interpolate(input,size=None,scale_factor=None,mode=nearest,align_corners=None)\nParameter description: align_corners(bool, optional) - Geometrically, we consider the pixels of the input and output as squares rather than points. If set to `True`, the input and output tensors are aligned by the center points of their corner pixels, preserving the values at the corner pixels. If set to `False`, the input and output tensors are aligned by the corner points of their corner pixels, and the interpolation uses edge value padding for out-of-boundary values, making this operation independent of input size when `scale_factor` is kept the same. This only has an effect when `mode` is `'bilinear'`. Default: `False`\nConstraints:\ndtype:bool\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.gt(input,other,out=None)\nParameter description: out(Tensor, optional) - the output tensor that must be a BoolTensor\nConstraints:\ndtype:booltensor\nstructure:tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.empty_like(input,dtype=None,layout=None,device=None,requires_grad=False,memory_format=torch.preserve_format)\nParameter description: dtype(`torch.dtype`, optional) - the desired data type of returned Tensor. Default: if `None`, defaults to the dtype of `input`.\nConstraints:\ndtype:torch.dtype\nstructure:Null\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.nn.RNN(*args,**kwargs,input_size=None,hidden_size=None,num_layers=None,nonlinearity=None,bias=None,batch_first=None,dropout=None,bidirectional=None)\nParameter description: hidden_size - The number of features in the hidden state h\nConstraints:\ndtype:int\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:[0,inf)\n\n###\n\nSignature: torch.nn.functional.avg_pool1d(input,kernel_size,stride=None,padding=0,ceil_mode=False,count_include_pad=True)\nParameter description: padding - implicit zero paddings on both sides of the input. Can be a single number or a tuple (padW,). Default: 0\nConstraints:\ndtype:bool;int\nstructure:tuple\nshape:Null\nndim:0;1\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.autograd.grad(outputs,inputs,grad_outputs=None,retain_graph=None,create_graph=False,only_inputs=True,allow_unused=False)\nParameter description: outputs(sequence of Tensor) - outputs of the differentiated function.\nConstraints:\ndtype:Null\nstructure:sequence;tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.repeat_interleave(input,repeats,dim=None)\nParameter description: dim(int, optional) - The dimension along which to repeat values.By default, use the flattened input array, and return a flat outputarray.\nConstraints:\ndtype:int\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.nn.utils.prune.random_structured(module,name,amount,dim)\nParameter description: amount(int or float) - quantity of parameters to prune. If `float`, should be between 0.0 and 1.0 and represent the fraction of parameters to prune. If `int`, it represents the absolute number of parameters to prune.\nConstraints:",
            "constr": {
                "dtype": [
                    "float",
                    "int"
                ],
                "structure": [],
                "shape": [],
                "ndim": [
                    "0"
                ],
                "enum": [],
                "range": [
                    "[0,inf)"
                ]
            },
            "signature": "torch.nn.utils.prune.random_structured(module,name,amount,dim)",
            "descp": "amount(int or float) - quantity of parameters to prune. If `float`, should be between 0.0 and 1.0 and represent the fraction of parameters to prune. If `int`, it represents the absolute number of parameters to prune."
        },
        {
            "fname": "torch.nn.utils.prune.random_unstructured.yaml",
            "arg": "module",
            "prompt": "Signature: torch.distributed.recv(tensor,src=None,group=<objectobject>,tag=0)\nParameter description: tensor(Tensor) - Tensor to fill with received data.\nConstraints:\ndtype:Null\nstructure:tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.div(input,other,out=None)\nParameter description: out(Tensor, optional) - the output tensor.\nConstraints:\ndtype:Null\nstructure:tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.nn.TransformerDecoder(decoder_layer,num_layers,norm=None)\nParameter description: num_layers - the number of sub-decoder-layers in the decoder (required).\nConstraints:\ndtype:int\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:[0,inf)\n\n###\n\nSignature: torch.nn.quantized.functional.interpolate(input,size=None,scale_factor=None,mode=nearest,align_corners=None)\nParameter description: align_corners(bool, optional) - Geometrically, we consider the pixels of the input and output as squares rather than points. If set to `True`, the input and output tensors are aligned by the center points of their corner pixels, preserving the values at the corner pixels. If set to `False`, the input and output tensors are aligned by the corner points of their corner pixels, and the interpolation uses edge value padding for out-of-boundary values, making this operation independent of input size when `scale_factor` is kept the same. This only has an effect when `mode` is `'bilinear'`. Default: `False`\nConstraints:\ndtype:bool\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.gt(input,other,out=None)\nParameter description: out(Tensor, optional) - the output tensor that must be a BoolTensor\nConstraints:\ndtype:booltensor\nstructure:tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.empty_like(input,dtype=None,layout=None,device=None,requires_grad=False,memory_format=torch.preserve_format)\nParameter description: dtype(`torch.dtype`, optional) - the desired data type of returned Tensor. Default: if `None`, defaults to the dtype of `input`.\nConstraints:\ndtype:torch.dtype\nstructure:Null\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.nn.RNN(*args,**kwargs,input_size=None,hidden_size=None,num_layers=None,nonlinearity=None,bias=None,batch_first=None,dropout=None,bidirectional=None)\nParameter description: hidden_size - The number of features in the hidden state h\nConstraints:\ndtype:int\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:[0,inf)\n\n###\n\nSignature: torch.nn.functional.avg_pool1d(input,kernel_size,stride=None,padding=0,ceil_mode=False,count_include_pad=True)\nParameter description: padding - implicit zero paddings on both sides of the input. Can be a single number or a tuple (padW,). Default: 0\nConstraints:\ndtype:bool;int\nstructure:tuple\nshape:Null\nndim:0;1\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.autograd.grad(outputs,inputs,grad_outputs=None,retain_graph=None,create_graph=False,only_inputs=True,allow_unused=False)\nParameter description: outputs(sequence of Tensor) - outputs of the differentiated function.\nConstraints:\ndtype:Null\nstructure:sequence;tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.repeat_interleave(input,repeats,dim=None)\nParameter description: dim(int, optional) - The dimension along which to repeat values.By default, use the flattened input array, and return a flat outputarray.\nConstraints:\ndtype:int\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.nn.utils.prune.random_unstructured(module,name,amount)\nParameter description: module(nn.Module) - module containing the tensor to prune\nConstraints:",
            "constr": {
                "dtype": [],
                "structure": [
                    "tensor"
                ],
                "shape": [],
                "ndim": [],
                "enum": [],
                "range": []
            },
            "signature": "torch.nn.utils.prune.random_unstructured(module,name,amount)",
            "descp": "module(nn.Module) - module containing the tensor to prune"
        },
        {
            "fname": "torch.nn.rnn.yaml",
            "arg": "input_size",
            "prompt": "Signature: torch.distributed.recv(tensor,src=None,group=<objectobject>,tag=0)\nParameter description: tensor(Tensor) - Tensor to fill with received data.\nConstraints:\ndtype:Null\nstructure:tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.div(input,other,out=None)\nParameter description: out(Tensor, optional) - the output tensor.\nConstraints:\ndtype:Null\nstructure:tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.nn.TransformerDecoder(decoder_layer,num_layers,norm=None)\nParameter description: num_layers - the number of sub-decoder-layers in the decoder (required).\nConstraints:\ndtype:int\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:[0,inf)\n\n###\n\nSignature: torch.nn.quantized.functional.interpolate(input,size=None,scale_factor=None,mode=nearest,align_corners=None)\nParameter description: align_corners(bool, optional) - Geometrically, we consider the pixels of the input and output as squares rather than points. If set to `True`, the input and output tensors are aligned by the center points of their corner pixels, preserving the values at the corner pixels. If set to `False`, the input and output tensors are aligned by the corner points of their corner pixels, and the interpolation uses edge value padding for out-of-boundary values, making this operation independent of input size when `scale_factor` is kept the same. This only has an effect when `mode` is `'bilinear'`. Default: `False`\nConstraints:\ndtype:bool\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.gt(input,other,out=None)\nParameter description: out(Tensor, optional) - the output tensor that must be a BoolTensor\nConstraints:\ndtype:booltensor\nstructure:tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.empty_like(input,dtype=None,layout=None,device=None,requires_grad=False,memory_format=torch.preserve_format)\nParameter description: dtype(`torch.dtype`, optional) - the desired data type of returned Tensor. Default: if `None`, defaults to the dtype of `input`.\nConstraints:\ndtype:torch.dtype\nstructure:Null\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.nn.RNN(*args,**kwargs,input_size=None,hidden_size=None,num_layers=None,nonlinearity=None,bias=None,batch_first=None,dropout=None,bidirectional=None)\nParameter description: hidden_size - The number of features in the hidden state h\nConstraints:\ndtype:int\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:[0,inf)\n\n###\n\nSignature: torch.nn.functional.avg_pool1d(input,kernel_size,stride=None,padding=0,ceil_mode=False,count_include_pad=True)\nParameter description: padding - implicit zero paddings on both sides of the input. Can be a single number or a tuple (padW,). Default: 0\nConstraints:\ndtype:bool;int\nstructure:tuple\nshape:Null\nndim:0;1\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.autograd.grad(outputs,inputs,grad_outputs=None,retain_graph=None,create_graph=False,only_inputs=True,allow_unused=False)\nParameter description: outputs(sequence of Tensor) - outputs of the differentiated function.\nConstraints:\ndtype:Null\nstructure:sequence;tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.repeat_interleave(input,repeats,dim=None)\nParameter description: dim(int, optional) - The dimension along which to repeat values.By default, use the flattened input array, and return a flat outputarray.\nConstraints:\ndtype:int\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.nn.RNN(*args,**kwargs,input_size=None,hidden_size=None,num_layers=None,nonlinearity=None,bias=None,batch_first=None,dropout=None,bidirectional=None)\nParameter description: input_size - The number of expected features in the input x\nConstraints:",
            "constr": {
                "dtype": [
                    "int"
                ],
                "structure": [],
                "shape": [],
                "ndim": [
                    "0"
                ],
                "enum": [],
                "range": [
                    "[0,inf)"
                ]
            },
            "signature": "torch.nn.RNN(*args,**kwargs,input_size=None,hidden_size=None,num_layers=None,nonlinearity=None,bias=None,batch_first=None,dropout=None,bidirectional=None)",
            "descp": "input_size - The number of expected features in the input x"
        },
        {
            "fname": "torch.norm.yaml",
            "arg": "keepdim",
            "prompt": "Signature: torch.distributed.recv(tensor,src=None,group=<objectobject>,tag=0)\nParameter description: tensor(Tensor) - Tensor to fill with received data.\nConstraints:\ndtype:Null\nstructure:tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.div(input,other,out=None)\nParameter description: out(Tensor, optional) - the output tensor.\nConstraints:\ndtype:Null\nstructure:tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.nn.TransformerDecoder(decoder_layer,num_layers,norm=None)\nParameter description: num_layers - the number of sub-decoder-layers in the decoder (required).\nConstraints:\ndtype:int\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:[0,inf)\n\n###\n\nSignature: torch.nn.quantized.functional.interpolate(input,size=None,scale_factor=None,mode=nearest,align_corners=None)\nParameter description: align_corners(bool, optional) - Geometrically, we consider the pixels of the input and output as squares rather than points. If set to `True`, the input and output tensors are aligned by the center points of their corner pixels, preserving the values at the corner pixels. If set to `False`, the input and output tensors are aligned by the corner points of their corner pixels, and the interpolation uses edge value padding for out-of-boundary values, making this operation independent of input size when `scale_factor` is kept the same. This only has an effect when `mode` is `'bilinear'`. Default: `False`\nConstraints:\ndtype:bool\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.gt(input,other,out=None)\nParameter description: out(Tensor, optional) - the output tensor that must be a BoolTensor\nConstraints:\ndtype:booltensor\nstructure:tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.empty_like(input,dtype=None,layout=None,device=None,requires_grad=False,memory_format=torch.preserve_format)\nParameter description: dtype(`torch.dtype`, optional) - the desired data type of returned Tensor. Default: if `None`, defaults to the dtype of `input`.\nConstraints:\ndtype:torch.dtype\nstructure:Null\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.nn.RNN(*args,**kwargs,input_size=None,hidden_size=None,num_layers=None,nonlinearity=None,bias=None,batch_first=None,dropout=None,bidirectional=None)\nParameter description: hidden_size - The number of features in the hidden state h\nConstraints:\ndtype:int\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:[0,inf)\n\n###\n\nSignature: torch.nn.functional.avg_pool1d(input,kernel_size,stride=None,padding=0,ceil_mode=False,count_include_pad=True)\nParameter description: padding - implicit zero paddings on both sides of the input. Can be a single number or a tuple (padW,). Default: 0\nConstraints:\ndtype:bool;int\nstructure:tuple\nshape:Null\nndim:0;1\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.autograd.grad(outputs,inputs,grad_outputs=None,retain_graph=None,create_graph=False,only_inputs=True,allow_unused=False)\nParameter description: outputs(sequence of Tensor) - outputs of the differentiated function.\nConstraints:\ndtype:Null\nstructure:sequence;tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.repeat_interleave(input,repeats,dim=None)\nParameter description: dim(int, optional) - The dimension along which to repeat values.By default, use the flattened input array, and return a flat outputarray.\nConstraints:\ndtype:int\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.norm(input,p=fro,dim=None,keepdim=False,out=None,dtype=None)\nParameter description: keepdim(bool, optional) - whether the output tensors have `dim` retained or not. Ignored if `dim` = `None` and `out` = `None`. Default: `False`\nConstraints:",
            "constr": {
                "dtype": [
                    "bool"
                ],
                "structure": [],
                "shape": [],
                "ndim": [
                    "0"
                ],
                "enum": [],
                "range": []
            },
            "signature": "torch.norm(input,p=fro,dim=None,keepdim=False,out=None,dtype=None)",
            "descp": "keepdim(bool, optional) - whether the output tensors have `dim` retained or not. Ignored if `dim` = `None` and `out` = `None`. Default: `False`"
        },
        {
            "fname": "torch.cuda.manual_seed.yaml",
            "arg": "seed",
            "prompt": "Signature: torch.distributed.recv(tensor,src=None,group=<objectobject>,tag=0)\nParameter description: tensor(Tensor) - Tensor to fill with received data.\nConstraints:\ndtype:Null\nstructure:tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.div(input,other,out=None)\nParameter description: out(Tensor, optional) - the output tensor.\nConstraints:\ndtype:Null\nstructure:tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.nn.TransformerDecoder(decoder_layer,num_layers,norm=None)\nParameter description: num_layers - the number of sub-decoder-layers in the decoder (required).\nConstraints:\ndtype:int\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:[0,inf)\n\n###\n\nSignature: torch.nn.quantized.functional.interpolate(input,size=None,scale_factor=None,mode=nearest,align_corners=None)\nParameter description: align_corners(bool, optional) - Geometrically, we consider the pixels of the input and output as squares rather than points. If set to `True`, the input and output tensors are aligned by the center points of their corner pixels, preserving the values at the corner pixels. If set to `False`, the input and output tensors are aligned by the corner points of their corner pixels, and the interpolation uses edge value padding for out-of-boundary values, making this operation independent of input size when `scale_factor` is kept the same. This only has an effect when `mode` is `'bilinear'`. Default: `False`\nConstraints:\ndtype:bool\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.gt(input,other,out=None)\nParameter description: out(Tensor, optional) - the output tensor that must be a BoolTensor\nConstraints:\ndtype:booltensor\nstructure:tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.empty_like(input,dtype=None,layout=None,device=None,requires_grad=False,memory_format=torch.preserve_format)\nParameter description: dtype(`torch.dtype`, optional) - the desired data type of returned Tensor. Default: if `None`, defaults to the dtype of `input`.\nConstraints:\ndtype:torch.dtype\nstructure:Null\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.nn.RNN(*args,**kwargs,input_size=None,hidden_size=None,num_layers=None,nonlinearity=None,bias=None,batch_first=None,dropout=None,bidirectional=None)\nParameter description: hidden_size - The number of features in the hidden state h\nConstraints:\ndtype:int\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:[0,inf)\n\n###\n\nSignature: torch.nn.functional.avg_pool1d(input,kernel_size,stride=None,padding=0,ceil_mode=False,count_include_pad=True)\nParameter description: padding - implicit zero paddings on both sides of the input. Can be a single number or a tuple (padW,). Default: 0\nConstraints:\ndtype:bool;int\nstructure:tuple\nshape:Null\nndim:0;1\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.autograd.grad(outputs,inputs,grad_outputs=None,retain_graph=None,create_graph=False,only_inputs=True,allow_unused=False)\nParameter description: outputs(sequence of Tensor) - outputs of the differentiated function.\nConstraints:\ndtype:Null\nstructure:sequence;tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.repeat_interleave(input,repeats,dim=None)\nParameter description: dim(int, optional) - The dimension along which to repeat values.By default, use the flattened input array, and return a flat outputarray.\nConstraints:\ndtype:int\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.cuda.manual_seed(seed)\nParameter description: seed(int) - The desired seed.\nConstraints:",
            "constr": {
                "dtype": [
                    "int"
                ],
                "structure": [],
                "shape": [],
                "ndim": [],
                "enum": [],
                "range": []
            },
            "signature": "torch.cuda.manual_seed(seed)",
            "descp": "seed(int) - The desired seed."
        },
        {
            "fname": "torch.nn.functional.embedding.yaml",
            "arg": "padding_idx",
            "prompt": "Signature: torch.distributed.recv(tensor,src=None,group=<objectobject>,tag=0)\nParameter description: tensor(Tensor) - Tensor to fill with received data.\nConstraints:\ndtype:Null\nstructure:tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.div(input,other,out=None)\nParameter description: out(Tensor, optional) - the output tensor.\nConstraints:\ndtype:Null\nstructure:tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.nn.TransformerDecoder(decoder_layer,num_layers,norm=None)\nParameter description: num_layers - the number of sub-decoder-layers in the decoder (required).\nConstraints:\ndtype:int\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:[0,inf)\n\n###\n\nSignature: torch.nn.quantized.functional.interpolate(input,size=None,scale_factor=None,mode=nearest,align_corners=None)\nParameter description: align_corners(bool, optional) - Geometrically, we consider the pixels of the input and output as squares rather than points. If set to `True`, the input and output tensors are aligned by the center points of their corner pixels, preserving the values at the corner pixels. If set to `False`, the input and output tensors are aligned by the corner points of their corner pixels, and the interpolation uses edge value padding for out-of-boundary values, making this operation independent of input size when `scale_factor` is kept the same. This only has an effect when `mode` is `'bilinear'`. Default: `False`\nConstraints:\ndtype:bool\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.gt(input,other,out=None)\nParameter description: out(Tensor, optional) - the output tensor that must be a BoolTensor\nConstraints:\ndtype:booltensor\nstructure:tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.empty_like(input,dtype=None,layout=None,device=None,requires_grad=False,memory_format=torch.preserve_format)\nParameter description: dtype(`torch.dtype`, optional) - the desired data type of returned Tensor. Default: if `None`, defaults to the dtype of `input`.\nConstraints:\ndtype:torch.dtype\nstructure:Null\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.nn.RNN(*args,**kwargs,input_size=None,hidden_size=None,num_layers=None,nonlinearity=None,bias=None,batch_first=None,dropout=None,bidirectional=None)\nParameter description: hidden_size - The number of features in the hidden state h\nConstraints:\ndtype:int\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:[0,inf)\n\n###\n\nSignature: torch.nn.functional.avg_pool1d(input,kernel_size,stride=None,padding=0,ceil_mode=False,count_include_pad=True)\nParameter description: padding - implicit zero paddings on both sides of the input. Can be a single number or a tuple (padW,). Default: 0\nConstraints:\ndtype:bool;int\nstructure:tuple\nshape:Null\nndim:0;1\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.autograd.grad(outputs,inputs,grad_outputs=None,retain_graph=None,create_graph=False,only_inputs=True,allow_unused=False)\nParameter description: outputs(sequence of Tensor) - outputs of the differentiated function.\nConstraints:\ndtype:Null\nstructure:sequence;tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.repeat_interleave(input,repeats,dim=None)\nParameter description: dim(int, optional) - The dimension along which to repeat values.By default, use the flattened input array, and return a flat outputarray.\nConstraints:\ndtype:int\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.nn.functional.embedding(input,weight,padding_idx=None,max_norm=None,norm_type=2.0,scale_grad_by_freq=False,sparse=False)\nParameter description: padding_idx(int, optional) - If given, pads the output with the embedding vector at `padding_idx` (initialized to zeros) whenever it encounters the index.\nConstraints:",
            "constr": {
                "dtype": [
                    "int"
                ],
                "structure": [],
                "shape": [],
                "ndim": [],
                "enum": [],
                "range": []
            },
            "signature": "torch.nn.functional.embedding(input,weight,padding_idx=None,max_norm=None,norm_type=2.0,scale_grad_by_freq=False,sparse=False)",
            "descp": "padding_idx(int, optional) - If given, pads the output with the embedding vector at `padding_idx` (initialized to zeros) whenever it encounters the index."
        },
        {
            "fname": "torch.distributed.scatter.yaml",
            "arg": "tensor",
            "prompt": "Signature: torch.distributed.recv(tensor,src=None,group=<objectobject>,tag=0)\nParameter description: tensor(Tensor) - Tensor to fill with received data.\nConstraints:\ndtype:Null\nstructure:tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.div(input,other,out=None)\nParameter description: out(Tensor, optional) - the output tensor.\nConstraints:\ndtype:Null\nstructure:tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.nn.TransformerDecoder(decoder_layer,num_layers,norm=None)\nParameter description: num_layers - the number of sub-decoder-layers in the decoder (required).\nConstraints:\ndtype:int\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:[0,inf)\n\n###\n\nSignature: torch.nn.quantized.functional.interpolate(input,size=None,scale_factor=None,mode=nearest,align_corners=None)\nParameter description: align_corners(bool, optional) - Geometrically, we consider the pixels of the input and output as squares rather than points. If set to `True`, the input and output tensors are aligned by the center points of their corner pixels, preserving the values at the corner pixels. If set to `False`, the input and output tensors are aligned by the corner points of their corner pixels, and the interpolation uses edge value padding for out-of-boundary values, making this operation independent of input size when `scale_factor` is kept the same. This only has an effect when `mode` is `'bilinear'`. Default: `False`\nConstraints:\ndtype:bool\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.gt(input,other,out=None)\nParameter description: out(Tensor, optional) - the output tensor that must be a BoolTensor\nConstraints:\ndtype:booltensor\nstructure:tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.empty_like(input,dtype=None,layout=None,device=None,requires_grad=False,memory_format=torch.preserve_format)\nParameter description: dtype(`torch.dtype`, optional) - the desired data type of returned Tensor. Default: if `None`, defaults to the dtype of `input`.\nConstraints:\ndtype:torch.dtype\nstructure:Null\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.nn.RNN(*args,**kwargs,input_size=None,hidden_size=None,num_layers=None,nonlinearity=None,bias=None,batch_first=None,dropout=None,bidirectional=None)\nParameter description: hidden_size - The number of features in the hidden state h\nConstraints:\ndtype:int\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:[0,inf)\n\n###\n\nSignature: torch.nn.functional.avg_pool1d(input,kernel_size,stride=None,padding=0,ceil_mode=False,count_include_pad=True)\nParameter description: padding - implicit zero paddings on both sides of the input. Can be a single number or a tuple (padW,). Default: 0\nConstraints:\ndtype:bool;int\nstructure:tuple\nshape:Null\nndim:0;1\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.autograd.grad(outputs,inputs,grad_outputs=None,retain_graph=None,create_graph=False,only_inputs=True,allow_unused=False)\nParameter description: outputs(sequence of Tensor) - outputs of the differentiated function.\nConstraints:\ndtype:Null\nstructure:sequence;tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.repeat_interleave(input,repeats,dim=None)\nParameter description: dim(int, optional) - The dimension along which to repeat values.By default, use the flattened input array, and return a flat outputarray.\nConstraints:\ndtype:int\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.distributed.scatter(tensor,scatter_list=None,src=0,group=<objectobject>,async_op=False)\nParameter description: tensor(Tensor) - Output tensor.\nConstraints:",
            "constr": {
                "dtype": [],
                "structure": [
                    "tensor"
                ],
                "shape": [],
                "ndim": [],
                "enum": [],
                "range": []
            },
            "signature": "torch.distributed.scatter(tensor,scatter_list=None,src=0,group=<objectobject>,async_op=False)",
            "descp": "tensor(Tensor) - Output tensor."
        },
        {
            "fname": "torch.nn.maxunpool3d.yaml",
            "arg": "stride",
            "prompt": "Signature: torch.distributed.recv(tensor,src=None,group=<objectobject>,tag=0)\nParameter description: tensor(Tensor) - Tensor to fill with received data.\nConstraints:\ndtype:Null\nstructure:tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.div(input,other,out=None)\nParameter description: out(Tensor, optional) - the output tensor.\nConstraints:\ndtype:Null\nstructure:tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.nn.TransformerDecoder(decoder_layer,num_layers,norm=None)\nParameter description: num_layers - the number of sub-decoder-layers in the decoder (required).\nConstraints:\ndtype:int\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:[0,inf)\n\n###\n\nSignature: torch.nn.quantized.functional.interpolate(input,size=None,scale_factor=None,mode=nearest,align_corners=None)\nParameter description: align_corners(bool, optional) - Geometrically, we consider the pixels of the input and output as squares rather than points. If set to `True`, the input and output tensors are aligned by the center points of their corner pixels, preserving the values at the corner pixels. If set to `False`, the input and output tensors are aligned by the corner points of their corner pixels, and the interpolation uses edge value padding for out-of-boundary values, making this operation independent of input size when `scale_factor` is kept the same. This only has an effect when `mode` is `'bilinear'`. Default: `False`\nConstraints:\ndtype:bool\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.gt(input,other,out=None)\nParameter description: out(Tensor, optional) - the output tensor that must be a BoolTensor\nConstraints:\ndtype:booltensor\nstructure:tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.empty_like(input,dtype=None,layout=None,device=None,requires_grad=False,memory_format=torch.preserve_format)\nParameter description: dtype(`torch.dtype`, optional) - the desired data type of returned Tensor. Default: if `None`, defaults to the dtype of `input`.\nConstraints:\ndtype:torch.dtype\nstructure:Null\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.nn.RNN(*args,**kwargs,input_size=None,hidden_size=None,num_layers=None,nonlinearity=None,bias=None,batch_first=None,dropout=None,bidirectional=None)\nParameter description: hidden_size - The number of features in the hidden state h\nConstraints:\ndtype:int\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:[0,inf)\n\n###\n\nSignature: torch.nn.functional.avg_pool1d(input,kernel_size,stride=None,padding=0,ceil_mode=False,count_include_pad=True)\nParameter description: padding - implicit zero paddings on both sides of the input. Can be a single number or a tuple (padW,). Default: 0\nConstraints:\ndtype:bool;int\nstructure:tuple\nshape:Null\nndim:0;1\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.autograd.grad(outputs,inputs,grad_outputs=None,retain_graph=None,create_graph=False,only_inputs=True,allow_unused=False)\nParameter description: outputs(sequence of Tensor) - outputs of the differentiated function.\nConstraints:\ndtype:Null\nstructure:sequence;tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.repeat_interleave(input,repeats,dim=None)\nParameter description: dim(int, optional) - The dimension along which to repeat values.By default, use the flattened input array, and return a flat outputarray.\nConstraints:\ndtype:int\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.nn.MaxUnpool3d(kernel_size,stride=None,padding=0)\nParameter description: stride(int or tuple) - Stride of the max pooling window. It is set to `kernel_size` by default.\nConstraints:",
            "constr": {
                "dtype": [
                    "int"
                ],
                "structure": [
                    "tuple"
                ],
                "shape": [],
                "ndim": [],
                "enum": [],
                "range": [
                    "[0,inf)"
                ]
            },
            "signature": "torch.nn.MaxUnpool3d(kernel_size,stride=None,padding=0)",
            "descp": "stride(int or tuple) - Stride of the max pooling window. It is set to `kernel_size` by default."
        },
        {
            "fname": "torch.jit.save.yaml",
            "arg": "_extra_files",
            "prompt": "Signature: torch.distributed.recv(tensor,src=None,group=<objectobject>,tag=0)\nParameter description: tensor(Tensor) - Tensor to fill with received data.\nConstraints:\ndtype:Null\nstructure:tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.div(input,other,out=None)\nParameter description: out(Tensor, optional) - the output tensor.\nConstraints:\ndtype:Null\nstructure:tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.nn.TransformerDecoder(decoder_layer,num_layers,norm=None)\nParameter description: num_layers - the number of sub-decoder-layers in the decoder (required).\nConstraints:\ndtype:int\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:[0,inf)\n\n###\n\nSignature: torch.nn.quantized.functional.interpolate(input,size=None,scale_factor=None,mode=nearest,align_corners=None)\nParameter description: align_corners(bool, optional) - Geometrically, we consider the pixels of the input and output as squares rather than points. If set to `True`, the input and output tensors are aligned by the center points of their corner pixels, preserving the values at the corner pixels. If set to `False`, the input and output tensors are aligned by the corner points of their corner pixels, and the interpolation uses edge value padding for out-of-boundary values, making this operation independent of input size when `scale_factor` is kept the same. This only has an effect when `mode` is `'bilinear'`. Default: `False`\nConstraints:\ndtype:bool\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.gt(input,other,out=None)\nParameter description: out(Tensor, optional) - the output tensor that must be a BoolTensor\nConstraints:\ndtype:booltensor\nstructure:tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.empty_like(input,dtype=None,layout=None,device=None,requires_grad=False,memory_format=torch.preserve_format)\nParameter description: dtype(`torch.dtype`, optional) - the desired data type of returned Tensor. Default: if `None`, defaults to the dtype of `input`.\nConstraints:\ndtype:torch.dtype\nstructure:Null\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.nn.RNN(*args,**kwargs,input_size=None,hidden_size=None,num_layers=None,nonlinearity=None,bias=None,batch_first=None,dropout=None,bidirectional=None)\nParameter description: hidden_size - The number of features in the hidden state h\nConstraints:\ndtype:int\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:[0,inf)\n\n###\n\nSignature: torch.nn.functional.avg_pool1d(input,kernel_size,stride=None,padding=0,ceil_mode=False,count_include_pad=True)\nParameter description: padding - implicit zero paddings on both sides of the input. Can be a single number or a tuple (padW,). Default: 0\nConstraints:\ndtype:bool;int\nstructure:tuple\nshape:Null\nndim:0;1\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.autograd.grad(outputs,inputs,grad_outputs=None,retain_graph=None,create_graph=False,only_inputs=True,allow_unused=False)\nParameter description: outputs(sequence of Tensor) - outputs of the differentiated function.\nConstraints:\ndtype:Null\nstructure:sequence;tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.repeat_interleave(input,repeats,dim=None)\nParameter description: dim(int, optional) - The dimension along which to repeat values.By default, use the flattened input array, and return a flat outputarray.\nConstraints:\ndtype:int\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.jit.save(m,f,_extra_files=ExtraFilesMap{})\nParameter description: _extra_files - Map from filename to contents which will be stored as part of 'f'.\nConstraints:",
            "constr": {
                "dtype": [
                    "string"
                ],
                "structure": [],
                "shape": [],
                "ndim": [],
                "enum": [],
                "range": []
            },
            "signature": "torch.jit.save(m,f,_extra_files=ExtraFilesMap{})",
            "descp": "_extra_files - Map from filename to contents which will be stored as part of 'f'."
        },
        {
            "fname": "torch.tensordot.yaml",
            "arg": "dims",
            "prompt": "Signature: torch.distributed.recv(tensor,src=None,group=<objectobject>,tag=0)\nParameter description: tensor(Tensor) - Tensor to fill with received data.\nConstraints:\ndtype:Null\nstructure:tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.div(input,other,out=None)\nParameter description: out(Tensor, optional) - the output tensor.\nConstraints:\ndtype:Null\nstructure:tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.nn.TransformerDecoder(decoder_layer,num_layers,norm=None)\nParameter description: num_layers - the number of sub-decoder-layers in the decoder (required).\nConstraints:\ndtype:int\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:[0,inf)\n\n###\n\nSignature: torch.nn.quantized.functional.interpolate(input,size=None,scale_factor=None,mode=nearest,align_corners=None)\nParameter description: align_corners(bool, optional) - Geometrically, we consider the pixels of the input and output as squares rather than points. If set to `True`, the input and output tensors are aligned by the center points of their corner pixels, preserving the values at the corner pixels. If set to `False`, the input and output tensors are aligned by the corner points of their corner pixels, and the interpolation uses edge value padding for out-of-boundary values, making this operation independent of input size when `scale_factor` is kept the same. This only has an effect when `mode` is `'bilinear'`. Default: `False`\nConstraints:\ndtype:bool\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.gt(input,other,out=None)\nParameter description: out(Tensor, optional) - the output tensor that must be a BoolTensor\nConstraints:\ndtype:booltensor\nstructure:tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.empty_like(input,dtype=None,layout=None,device=None,requires_grad=False,memory_format=torch.preserve_format)\nParameter description: dtype(`torch.dtype`, optional) - the desired data type of returned Tensor. Default: if `None`, defaults to the dtype of `input`.\nConstraints:\ndtype:torch.dtype\nstructure:Null\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.nn.RNN(*args,**kwargs,input_size=None,hidden_size=None,num_layers=None,nonlinearity=None,bias=None,batch_first=None,dropout=None,bidirectional=None)\nParameter description: hidden_size - The number of features in the hidden state h\nConstraints:\ndtype:int\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:[0,inf)\n\n###\n\nSignature: torch.nn.functional.avg_pool1d(input,kernel_size,stride=None,padding=0,ceil_mode=False,count_include_pad=True)\nParameter description: padding - implicit zero paddings on both sides of the input. Can be a single number or a tuple (padW,). Default: 0\nConstraints:\ndtype:bool;int\nstructure:tuple\nshape:Null\nndim:0;1\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.autograd.grad(outputs,inputs,grad_outputs=None,retain_graph=None,create_graph=False,only_inputs=True,allow_unused=False)\nParameter description: outputs(sequence of Tensor) - outputs of the differentiated function.\nConstraints:\ndtype:Null\nstructure:sequence;tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.repeat_interleave(input,repeats,dim=None)\nParameter description: dim(int, optional) - The dimension along which to repeat values.By default, use the flattened input array, and return a flat outputarray.\nConstraints:\ndtype:int\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.tensordot(a,b,dims=2)\nParameter description: dims(int or tuple of two lists of python:integers) - number of dimensions to contract or explicit lists of dimensions for `a` and `b` respectively\nConstraints:",
            "constr": {
                "dtype": [
                    "int"
                ],
                "structure": [
                    "tuple",
                    "lists"
                ],
                "shape": [],
                "ndim": [
                    "0",
                    "1"
                ],
                "enum": [],
                "range": [
                    "[0,inf)"
                ]
            },
            "signature": "torch.tensordot(a,b,dims=2)",
            "descp": "dims(int or tuple of two lists of python:integers) - number of dimensions to contract or explicit lists of dimensions for `a` and `b` respectively"
        },
        {
            "fname": "torch.nn.instancenorm1d.yaml",
            "arg": "eps",
            "prompt": "Signature: torch.distributed.recv(tensor,src=None,group=<objectobject>,tag=0)\nParameter description: tensor(Tensor) - Tensor to fill with received data.\nConstraints:\ndtype:Null\nstructure:tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.div(input,other,out=None)\nParameter description: out(Tensor, optional) - the output tensor.\nConstraints:\ndtype:Null\nstructure:tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.nn.TransformerDecoder(decoder_layer,num_layers,norm=None)\nParameter description: num_layers - the number of sub-decoder-layers in the decoder (required).\nConstraints:\ndtype:int\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:[0,inf)\n\n###\n\nSignature: torch.nn.quantized.functional.interpolate(input,size=None,scale_factor=None,mode=nearest,align_corners=None)\nParameter description: align_corners(bool, optional) - Geometrically, we consider the pixels of the input and output as squares rather than points. If set to `True`, the input and output tensors are aligned by the center points of their corner pixels, preserving the values at the corner pixels. If set to `False`, the input and output tensors are aligned by the corner points of their corner pixels, and the interpolation uses edge value padding for out-of-boundary values, making this operation independent of input size when `scale_factor` is kept the same. This only has an effect when `mode` is `'bilinear'`. Default: `False`\nConstraints:\ndtype:bool\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.gt(input,other,out=None)\nParameter description: out(Tensor, optional) - the output tensor that must be a BoolTensor\nConstraints:\ndtype:booltensor\nstructure:tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.empty_like(input,dtype=None,layout=None,device=None,requires_grad=False,memory_format=torch.preserve_format)\nParameter description: dtype(`torch.dtype`, optional) - the desired data type of returned Tensor. Default: if `None`, defaults to the dtype of `input`.\nConstraints:\ndtype:torch.dtype\nstructure:Null\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.nn.RNN(*args,**kwargs,input_size=None,hidden_size=None,num_layers=None,nonlinearity=None,bias=None,batch_first=None,dropout=None,bidirectional=None)\nParameter description: hidden_size - The number of features in the hidden state h\nConstraints:\ndtype:int\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:[0,inf)\n\n###\n\nSignature: torch.nn.functional.avg_pool1d(input,kernel_size,stride=None,padding=0,ceil_mode=False,count_include_pad=True)\nParameter description: padding - implicit zero paddings on both sides of the input. Can be a single number or a tuple (padW,). Default: 0\nConstraints:\ndtype:bool;int\nstructure:tuple\nshape:Null\nndim:0;1\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.autograd.grad(outputs,inputs,grad_outputs=None,retain_graph=None,create_graph=False,only_inputs=True,allow_unused=False)\nParameter description: outputs(sequence of Tensor) - outputs of the differentiated function.\nConstraints:\ndtype:Null\nstructure:sequence;tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.repeat_interleave(input,repeats,dim=None)\nParameter description: dim(int, optional) - The dimension along which to repeat values.By default, use the flattened input array, and return a flat outputarray.\nConstraints:\ndtype:int\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.nn.InstanceNorm1d(num_features,eps=1e-05,momentum=0.1,affine=False,track_running_stats=False)\nParameter description: eps - a value added to the denominator for numerical stability. Default: 1e-5\nConstraints:",
            "constr": {
                "dtype": [
                    "numeric",
                    "float",
                    "int"
                ],
                "structure": [],
                "shape": [],
                "ndim": [
                    "0"
                ],
                "enum": [],
                "range": []
            },
            "signature": "torch.nn.InstanceNorm1d(num_features,eps=1e-05,momentum=0.1,affine=False,track_running_stats=False)",
            "descp": "eps - a value added to the denominator for numerical stability. Default: 1e-5"
        },
        {
            "fname": "torch.distributed.gather.yaml",
            "arg": "dst",
            "prompt": "Signature: torch.distributed.recv(tensor,src=None,group=<objectobject>,tag=0)\nParameter description: tensor(Tensor) - Tensor to fill with received data.\nConstraints:\ndtype:Null\nstructure:tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.div(input,other,out=None)\nParameter description: out(Tensor, optional) - the output tensor.\nConstraints:\ndtype:Null\nstructure:tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.nn.TransformerDecoder(decoder_layer,num_layers,norm=None)\nParameter description: num_layers - the number of sub-decoder-layers in the decoder (required).\nConstraints:\ndtype:int\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:[0,inf)\n\n###\n\nSignature: torch.nn.quantized.functional.interpolate(input,size=None,scale_factor=None,mode=nearest,align_corners=None)\nParameter description: align_corners(bool, optional) - Geometrically, we consider the pixels of the input and output as squares rather than points. If set to `True`, the input and output tensors are aligned by the center points of their corner pixels, preserving the values at the corner pixels. If set to `False`, the input and output tensors are aligned by the corner points of their corner pixels, and the interpolation uses edge value padding for out-of-boundary values, making this operation independent of input size when `scale_factor` is kept the same. This only has an effect when `mode` is `'bilinear'`. Default: `False`\nConstraints:\ndtype:bool\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.gt(input,other,out=None)\nParameter description: out(Tensor, optional) - the output tensor that must be a BoolTensor\nConstraints:\ndtype:booltensor\nstructure:tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.empty_like(input,dtype=None,layout=None,device=None,requires_grad=False,memory_format=torch.preserve_format)\nParameter description: dtype(`torch.dtype`, optional) - the desired data type of returned Tensor. Default: if `None`, defaults to the dtype of `input`.\nConstraints:\ndtype:torch.dtype\nstructure:Null\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.nn.RNN(*args,**kwargs,input_size=None,hidden_size=None,num_layers=None,nonlinearity=None,bias=None,batch_first=None,dropout=None,bidirectional=None)\nParameter description: hidden_size - The number of features in the hidden state h\nConstraints:\ndtype:int\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:[0,inf)\n\n###\n\nSignature: torch.nn.functional.avg_pool1d(input,kernel_size,stride=None,padding=0,ceil_mode=False,count_include_pad=True)\nParameter description: padding - implicit zero paddings on both sides of the input. Can be a single number or a tuple (padW,). Default: 0\nConstraints:\ndtype:bool;int\nstructure:tuple\nshape:Null\nndim:0;1\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.autograd.grad(outputs,inputs,grad_outputs=None,retain_graph=None,create_graph=False,only_inputs=True,allow_unused=False)\nParameter description: outputs(sequence of Tensor) - outputs of the differentiated function.\nConstraints:\ndtype:Null\nstructure:sequence;tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.repeat_interleave(input,repeats,dim=None)\nParameter description: dim(int, optional) - The dimension along which to repeat values.By default, use the flattened input array, and return a flat outputarray.\nConstraints:\ndtype:int\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.distributed.gather(tensor,gather_list=None,dst=0,group=<objectobject>,async_op=False)\nParameter description: dst(int, optional) - Destination rank (default is 0)\nConstraints:",
            "constr": {
                "dtype": [
                    "int"
                ],
                "structure": [],
                "shape": [],
                "ndim": [
                    "0"
                ],
                "enum": [],
                "range": []
            },
            "signature": "torch.distributed.gather(tensor,gather_list=None,dst=0,group=<objectobject>,async_op=False)",
            "descp": "dst(int, optional) - Destination rank (default is 0)"
        },
        {
            "fname": "torch.histc.yaml",
            "arg": "out",
            "prompt": "Signature: torch.distributed.recv(tensor,src=None,group=<objectobject>,tag=0)\nParameter description: tensor(Tensor) - Tensor to fill with received data.\nConstraints:\ndtype:Null\nstructure:tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.div(input,other,out=None)\nParameter description: out(Tensor, optional) - the output tensor.\nConstraints:\ndtype:Null\nstructure:tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.nn.TransformerDecoder(decoder_layer,num_layers,norm=None)\nParameter description: num_layers - the number of sub-decoder-layers in the decoder (required).\nConstraints:\ndtype:int\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:[0,inf)\n\n###\n\nSignature: torch.nn.quantized.functional.interpolate(input,size=None,scale_factor=None,mode=nearest,align_corners=None)\nParameter description: align_corners(bool, optional) - Geometrically, we consider the pixels of the input and output as squares rather than points. If set to `True`, the input and output tensors are aligned by the center points of their corner pixels, preserving the values at the corner pixels. If set to `False`, the input and output tensors are aligned by the corner points of their corner pixels, and the interpolation uses edge value padding for out-of-boundary values, making this operation independent of input size when `scale_factor` is kept the same. This only has an effect when `mode` is `'bilinear'`. Default: `False`\nConstraints:\ndtype:bool\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.gt(input,other,out=None)\nParameter description: out(Tensor, optional) - the output tensor that must be a BoolTensor\nConstraints:\ndtype:booltensor\nstructure:tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.empty_like(input,dtype=None,layout=None,device=None,requires_grad=False,memory_format=torch.preserve_format)\nParameter description: dtype(`torch.dtype`, optional) - the desired data type of returned Tensor. Default: if `None`, defaults to the dtype of `input`.\nConstraints:\ndtype:torch.dtype\nstructure:Null\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.nn.RNN(*args,**kwargs,input_size=None,hidden_size=None,num_layers=None,nonlinearity=None,bias=None,batch_first=None,dropout=None,bidirectional=None)\nParameter description: hidden_size - The number of features in the hidden state h\nConstraints:\ndtype:int\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:[0,inf)\n\n###\n\nSignature: torch.nn.functional.avg_pool1d(input,kernel_size,stride=None,padding=0,ceil_mode=False,count_include_pad=True)\nParameter description: padding - implicit zero paddings on both sides of the input. Can be a single number or a tuple (padW,). Default: 0\nConstraints:\ndtype:bool;int\nstructure:tuple\nshape:Null\nndim:0;1\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.autograd.grad(outputs,inputs,grad_outputs=None,retain_graph=None,create_graph=False,only_inputs=True,allow_unused=False)\nParameter description: outputs(sequence of Tensor) - outputs of the differentiated function.\nConstraints:\ndtype:Null\nstructure:sequence;tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.repeat_interleave(input,repeats,dim=None)\nParameter description: dim(int, optional) - The dimension along which to repeat values.By default, use the flattened input array, and return a flat outputarray.\nConstraints:\ndtype:int\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.histc(input,bins=100,min=0,max=0,out=None)\nParameter description: out(Tensor, optional) - the output tensor.\nConstraints:",
            "constr": {
                "dtype": [],
                "structure": [
                    "tensor"
                ],
                "shape": [],
                "ndim": [],
                "enum": [],
                "range": []
            },
            "signature": "torch.histc(input,bins=100,min=0,max=0,out=None)",
            "descp": "out(Tensor, optional) - the output tensor."
        },
        {
            "fname": "torch.sum2.yaml",
            "arg": "dim",
            "prompt": "Signature: torch.distributed.recv(tensor,src=None,group=<objectobject>,tag=0)\nParameter description: tensor(Tensor) - Tensor to fill with received data.\nConstraints:\ndtype:Null\nstructure:tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.div(input,other,out=None)\nParameter description: out(Tensor, optional) - the output tensor.\nConstraints:\ndtype:Null\nstructure:tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.nn.TransformerDecoder(decoder_layer,num_layers,norm=None)\nParameter description: num_layers - the number of sub-decoder-layers in the decoder (required).\nConstraints:\ndtype:int\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:[0,inf)\n\n###\n\nSignature: torch.nn.quantized.functional.interpolate(input,size=None,scale_factor=None,mode=nearest,align_corners=None)\nParameter description: align_corners(bool, optional) - Geometrically, we consider the pixels of the input and output as squares rather than points. If set to `True`, the input and output tensors are aligned by the center points of their corner pixels, preserving the values at the corner pixels. If set to `False`, the input and output tensors are aligned by the corner points of their corner pixels, and the interpolation uses edge value padding for out-of-boundary values, making this operation independent of input size when `scale_factor` is kept the same. This only has an effect when `mode` is `'bilinear'`. Default: `False`\nConstraints:\ndtype:bool\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.gt(input,other,out=None)\nParameter description: out(Tensor, optional) - the output tensor that must be a BoolTensor\nConstraints:\ndtype:booltensor\nstructure:tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.empty_like(input,dtype=None,layout=None,device=None,requires_grad=False,memory_format=torch.preserve_format)\nParameter description: dtype(`torch.dtype`, optional) - the desired data type of returned Tensor. Default: if `None`, defaults to the dtype of `input`.\nConstraints:\ndtype:torch.dtype\nstructure:Null\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.nn.RNN(*args,**kwargs,input_size=None,hidden_size=None,num_layers=None,nonlinearity=None,bias=None,batch_first=None,dropout=None,bidirectional=None)\nParameter description: hidden_size - The number of features in the hidden state h\nConstraints:\ndtype:int\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:[0,inf)\n\n###\n\nSignature: torch.nn.functional.avg_pool1d(input,kernel_size,stride=None,padding=0,ceil_mode=False,count_include_pad=True)\nParameter description: padding - implicit zero paddings on both sides of the input. Can be a single number or a tuple (padW,). Default: 0\nConstraints:\ndtype:bool;int\nstructure:tuple\nshape:Null\nndim:0;1\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.autograd.grad(outputs,inputs,grad_outputs=None,retain_graph=None,create_graph=False,only_inputs=True,allow_unused=False)\nParameter description: outputs(sequence of Tensor) - outputs of the differentiated function.\nConstraints:\ndtype:Null\nstructure:sequence;tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.repeat_interleave(input,repeats,dim=None)\nParameter description: dim(int, optional) - The dimension along which to repeat values.By default, use the flattened input array, and return a flat outputarray.\nConstraints:\ndtype:int\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.sum(input,dim,keepdim=False,dtype=None)\nParameter description: dim(int or tuple of python:ints) - the dimension or dimensions to reduce.\nConstraints:",
            "constr": {
                "dtype": [
                    "int"
                ],
                "structure": [
                    "tuple"
                ],
                "shape": [],
                "ndim": [
                    "0",
                    "1"
                ],
                "enum": [],
                "range": []
            },
            "signature": "torch.sum(input,dim,keepdim=False,dtype=None)",
            "descp": "dim(int or tuple of python:ints) - the dimension or dimensions to reduce."
        },
        {
            "fname": "torch.atan2.yaml",
            "arg": "other",
            "prompt": "Signature: torch.distributed.recv(tensor,src=None,group=<objectobject>,tag=0)\nParameter description: tensor(Tensor) - Tensor to fill with received data.\nConstraints:\ndtype:Null\nstructure:tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.div(input,other,out=None)\nParameter description: out(Tensor, optional) - the output tensor.\nConstraints:\ndtype:Null\nstructure:tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.nn.TransformerDecoder(decoder_layer,num_layers,norm=None)\nParameter description: num_layers - the number of sub-decoder-layers in the decoder (required).\nConstraints:\ndtype:int\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:[0,inf)\n\n###\n\nSignature: torch.nn.quantized.functional.interpolate(input,size=None,scale_factor=None,mode=nearest,align_corners=None)\nParameter description: align_corners(bool, optional) - Geometrically, we consider the pixels of the input and output as squares rather than points. If set to `True`, the input and output tensors are aligned by the center points of their corner pixels, preserving the values at the corner pixels. If set to `False`, the input and output tensors are aligned by the corner points of their corner pixels, and the interpolation uses edge value padding for out-of-boundary values, making this operation independent of input size when `scale_factor` is kept the same. This only has an effect when `mode` is `'bilinear'`. Default: `False`\nConstraints:\ndtype:bool\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.gt(input,other,out=None)\nParameter description: out(Tensor, optional) - the output tensor that must be a BoolTensor\nConstraints:\ndtype:booltensor\nstructure:tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.empty_like(input,dtype=None,layout=None,device=None,requires_grad=False,memory_format=torch.preserve_format)\nParameter description: dtype(`torch.dtype`, optional) - the desired data type of returned Tensor. Default: if `None`, defaults to the dtype of `input`.\nConstraints:\ndtype:torch.dtype\nstructure:Null\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.nn.RNN(*args,**kwargs,input_size=None,hidden_size=None,num_layers=None,nonlinearity=None,bias=None,batch_first=None,dropout=None,bidirectional=None)\nParameter description: hidden_size - The number of features in the hidden state h\nConstraints:\ndtype:int\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:[0,inf)\n\n###\n\nSignature: torch.nn.functional.avg_pool1d(input,kernel_size,stride=None,padding=0,ceil_mode=False,count_include_pad=True)\nParameter description: padding - implicit zero paddings on both sides of the input. Can be a single number or a tuple (padW,). Default: 0\nConstraints:\ndtype:bool;int\nstructure:tuple\nshape:Null\nndim:0;1\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.autograd.grad(outputs,inputs,grad_outputs=None,retain_graph=None,create_graph=False,only_inputs=True,allow_unused=False)\nParameter description: outputs(sequence of Tensor) - outputs of the differentiated function.\nConstraints:\ndtype:Null\nstructure:sequence;tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.repeat_interleave(input,repeats,dim=None)\nParameter description: dim(int, optional) - The dimension along which to repeat values.By default, use the flattened input array, and return a flat outputarray.\nConstraints:\ndtype:int\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.atan2(input,other,out=None)\nParameter description: other(Tensor) - the second input tensor\nConstraints:",
            "constr": {
                "dtype": [],
                "structure": [
                    "tensor"
                ],
                "shape": [],
                "ndim": [],
                "enum": [],
                "range": []
            },
            "signature": "torch.atan2(input,other,out=None)",
            "descp": "other(Tensor) - the second input tensor"
        },
        {
            "fname": "torch.empty_like.yaml",
            "arg": "input",
            "prompt": "Signature: torch.distributed.recv(tensor,src=None,group=<objectobject>,tag=0)\nParameter description: tensor(Tensor) - Tensor to fill with received data.\nConstraints:\ndtype:Null\nstructure:tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.div(input,other,out=None)\nParameter description: out(Tensor, optional) - the output tensor.\nConstraints:\ndtype:Null\nstructure:tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.nn.TransformerDecoder(decoder_layer,num_layers,norm=None)\nParameter description: num_layers - the number of sub-decoder-layers in the decoder (required).\nConstraints:\ndtype:int\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:[0,inf)\n\n###\n\nSignature: torch.nn.quantized.functional.interpolate(input,size=None,scale_factor=None,mode=nearest,align_corners=None)\nParameter description: align_corners(bool, optional) - Geometrically, we consider the pixels of the input and output as squares rather than points. If set to `True`, the input and output tensors are aligned by the center points of their corner pixels, preserving the values at the corner pixels. If set to `False`, the input and output tensors are aligned by the corner points of their corner pixels, and the interpolation uses edge value padding for out-of-boundary values, making this operation independent of input size when `scale_factor` is kept the same. This only has an effect when `mode` is `'bilinear'`. Default: `False`\nConstraints:\ndtype:bool\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.gt(input,other,out=None)\nParameter description: out(Tensor, optional) - the output tensor that must be a BoolTensor\nConstraints:\ndtype:booltensor\nstructure:tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.empty_like(input,dtype=None,layout=None,device=None,requires_grad=False,memory_format=torch.preserve_format)\nParameter description: dtype(`torch.dtype`, optional) - the desired data type of returned Tensor. Default: if `None`, defaults to the dtype of `input`.\nConstraints:\ndtype:torch.dtype\nstructure:Null\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.nn.RNN(*args,**kwargs,input_size=None,hidden_size=None,num_layers=None,nonlinearity=None,bias=None,batch_first=None,dropout=None,bidirectional=None)\nParameter description: hidden_size - The number of features in the hidden state h\nConstraints:\ndtype:int\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:[0,inf)\n\n###\n\nSignature: torch.nn.functional.avg_pool1d(input,kernel_size,stride=None,padding=0,ceil_mode=False,count_include_pad=True)\nParameter description: padding - implicit zero paddings on both sides of the input. Can be a single number or a tuple (padW,). Default: 0\nConstraints:\ndtype:bool;int\nstructure:tuple\nshape:Null\nndim:0;1\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.autograd.grad(outputs,inputs,grad_outputs=None,retain_graph=None,create_graph=False,only_inputs=True,allow_unused=False)\nParameter description: outputs(sequence of Tensor) - outputs of the differentiated function.\nConstraints:\ndtype:Null\nstructure:sequence;tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.repeat_interleave(input,repeats,dim=None)\nParameter description: dim(int, optional) - The dimension along which to repeat values.By default, use the flattened input array, and return a flat outputarray.\nConstraints:\ndtype:int\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.empty_like(input,dtype=None,layout=None,device=None,requires_grad=False,memory_format=torch.preserve_format)\nParameter description: input(Tensor) - the size of `input` will determine size of the output tensor.\nConstraints:",
            "constr": {
                "dtype": [
                    "int"
                ],
                "structure": [
                    "tensor"
                ],
                "shape": [],
                "ndim": [],
                "enum": [],
                "range": [
                    "[0,inf)"
                ]
            },
            "signature": "torch.empty_like(input,dtype=None,layout=None,device=None,requires_grad=False,memory_format=torch.preserve_format)",
            "descp": "input(Tensor) - the size of `input` will determine size of the output tensor."
        },
        {
            "fname": "torch.cat.yaml",
            "arg": "dim",
            "prompt": "Signature: torch.distributed.recv(tensor,src=None,group=<objectobject>,tag=0)\nParameter description: tensor(Tensor) - Tensor to fill with received data.\nConstraints:\ndtype:Null\nstructure:tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.div(input,other,out=None)\nParameter description: out(Tensor, optional) - the output tensor.\nConstraints:\ndtype:Null\nstructure:tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.nn.TransformerDecoder(decoder_layer,num_layers,norm=None)\nParameter description: num_layers - the number of sub-decoder-layers in the decoder (required).\nConstraints:\ndtype:int\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:[0,inf)\n\n###\n\nSignature: torch.nn.quantized.functional.interpolate(input,size=None,scale_factor=None,mode=nearest,align_corners=None)\nParameter description: align_corners(bool, optional) - Geometrically, we consider the pixels of the input and output as squares rather than points. If set to `True`, the input and output tensors are aligned by the center points of their corner pixels, preserving the values at the corner pixels. If set to `False`, the input and output tensors are aligned by the corner points of their corner pixels, and the interpolation uses edge value padding for out-of-boundary values, making this operation independent of input size when `scale_factor` is kept the same. This only has an effect when `mode` is `'bilinear'`. Default: `False`\nConstraints:\ndtype:bool\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.gt(input,other,out=None)\nParameter description: out(Tensor, optional) - the output tensor that must be a BoolTensor\nConstraints:\ndtype:booltensor\nstructure:tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.empty_like(input,dtype=None,layout=None,device=None,requires_grad=False,memory_format=torch.preserve_format)\nParameter description: dtype(`torch.dtype`, optional) - the desired data type of returned Tensor. Default: if `None`, defaults to the dtype of `input`.\nConstraints:\ndtype:torch.dtype\nstructure:Null\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.nn.RNN(*args,**kwargs,input_size=None,hidden_size=None,num_layers=None,nonlinearity=None,bias=None,batch_first=None,dropout=None,bidirectional=None)\nParameter description: hidden_size - The number of features in the hidden state h\nConstraints:\ndtype:int\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:[0,inf)\n\n###\n\nSignature: torch.nn.functional.avg_pool1d(input,kernel_size,stride=None,padding=0,ceil_mode=False,count_include_pad=True)\nParameter description: padding - implicit zero paddings on both sides of the input. Can be a single number or a tuple (padW,). Default: 0\nConstraints:\ndtype:bool;int\nstructure:tuple\nshape:Null\nndim:0;1\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.autograd.grad(outputs,inputs,grad_outputs=None,retain_graph=None,create_graph=False,only_inputs=True,allow_unused=False)\nParameter description: outputs(sequence of Tensor) - outputs of the differentiated function.\nConstraints:\ndtype:Null\nstructure:sequence;tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.repeat_interleave(input,repeats,dim=None)\nParameter description: dim(int, optional) - The dimension along which to repeat values.By default, use the flattened input array, and return a flat outputarray.\nConstraints:\ndtype:int\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.cat(tensors,dim=0,out=None)\nParameter description: dim(int, optional) - the dimension over which the tensors are concatenated\nConstraints:",
            "constr": {
                "dtype": [
                    "int"
                ],
                "structure": [],
                "shape": [],
                "ndim": [
                    "0"
                ],
                "enum": [],
                "range": []
            },
            "signature": "torch.cat(tensors,dim=0,out=None)",
            "descp": "dim(int, optional) - the dimension over which the tensors are concatenated"
        },
        {
            "fname": "torch.addmv.yaml",
            "arg": "out",
            "prompt": "Signature: torch.distributed.recv(tensor,src=None,group=<objectobject>,tag=0)\nParameter description: tensor(Tensor) - Tensor to fill with received data.\nConstraints:\ndtype:Null\nstructure:tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.div(input,other,out=None)\nParameter description: out(Tensor, optional) - the output tensor.\nConstraints:\ndtype:Null\nstructure:tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.nn.TransformerDecoder(decoder_layer,num_layers,norm=None)\nParameter description: num_layers - the number of sub-decoder-layers in the decoder (required).\nConstraints:\ndtype:int\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:[0,inf)\n\n###\n\nSignature: torch.nn.quantized.functional.interpolate(input,size=None,scale_factor=None,mode=nearest,align_corners=None)\nParameter description: align_corners(bool, optional) - Geometrically, we consider the pixels of the input and output as squares rather than points. If set to `True`, the input and output tensors are aligned by the center points of their corner pixels, preserving the values at the corner pixels. If set to `False`, the input and output tensors are aligned by the corner points of their corner pixels, and the interpolation uses edge value padding for out-of-boundary values, making this operation independent of input size when `scale_factor` is kept the same. This only has an effect when `mode` is `'bilinear'`. Default: `False`\nConstraints:\ndtype:bool\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.gt(input,other,out=None)\nParameter description: out(Tensor, optional) - the output tensor that must be a BoolTensor\nConstraints:\ndtype:booltensor\nstructure:tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.empty_like(input,dtype=None,layout=None,device=None,requires_grad=False,memory_format=torch.preserve_format)\nParameter description: dtype(`torch.dtype`, optional) - the desired data type of returned Tensor. Default: if `None`, defaults to the dtype of `input`.\nConstraints:\ndtype:torch.dtype\nstructure:Null\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.nn.RNN(*args,**kwargs,input_size=None,hidden_size=None,num_layers=None,nonlinearity=None,bias=None,batch_first=None,dropout=None,bidirectional=None)\nParameter description: hidden_size - The number of features in the hidden state h\nConstraints:\ndtype:int\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:[0,inf)\n\n###\n\nSignature: torch.nn.functional.avg_pool1d(input,kernel_size,stride=None,padding=0,ceil_mode=False,count_include_pad=True)\nParameter description: padding - implicit zero paddings on both sides of the input. Can be a single number or a tuple (padW,). Default: 0\nConstraints:\ndtype:bool;int\nstructure:tuple\nshape:Null\nndim:0;1\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.autograd.grad(outputs,inputs,grad_outputs=None,retain_graph=None,create_graph=False,only_inputs=True,allow_unused=False)\nParameter description: outputs(sequence of Tensor) - outputs of the differentiated function.\nConstraints:\ndtype:Null\nstructure:sequence;tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.repeat_interleave(input,repeats,dim=None)\nParameter description: dim(int, optional) - The dimension along which to repeat values.By default, use the flattened input array, and return a flat outputarray.\nConstraints:\ndtype:int\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.addmv(input,mat,vec,beta=1,alpha=1,out=None)\nParameter description: out(Tensor, optional) - the output tensor.\nConstraints:",
            "constr": {
                "dtype": [],
                "structure": [
                    "tensor"
                ],
                "shape": [],
                "ndim": [],
                "enum": [],
                "range": []
            },
            "signature": "torch.addmv(input,mat,vec,beta=1,alpha=1,out=None)",
            "descp": "out(Tensor, optional) - the output tensor."
        },
        {
            "fname": "torch.nn.softmax.yaml",
            "arg": "dim",
            "prompt": "Signature: torch.distributed.recv(tensor,src=None,group=<objectobject>,tag=0)\nParameter description: tensor(Tensor) - Tensor to fill with received data.\nConstraints:\ndtype:Null\nstructure:tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.div(input,other,out=None)\nParameter description: out(Tensor, optional) - the output tensor.\nConstraints:\ndtype:Null\nstructure:tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.nn.TransformerDecoder(decoder_layer,num_layers,norm=None)\nParameter description: num_layers - the number of sub-decoder-layers in the decoder (required).\nConstraints:\ndtype:int\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:[0,inf)\n\n###\n\nSignature: torch.nn.quantized.functional.interpolate(input,size=None,scale_factor=None,mode=nearest,align_corners=None)\nParameter description: align_corners(bool, optional) - Geometrically, we consider the pixels of the input and output as squares rather than points. If set to `True`, the input and output tensors are aligned by the center points of their corner pixels, preserving the values at the corner pixels. If set to `False`, the input and output tensors are aligned by the corner points of their corner pixels, and the interpolation uses edge value padding for out-of-boundary values, making this operation independent of input size when `scale_factor` is kept the same. This only has an effect when `mode` is `'bilinear'`. Default: `False`\nConstraints:\ndtype:bool\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.gt(input,other,out=None)\nParameter description: out(Tensor, optional) - the output tensor that must be a BoolTensor\nConstraints:\ndtype:booltensor\nstructure:tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.empty_like(input,dtype=None,layout=None,device=None,requires_grad=False,memory_format=torch.preserve_format)\nParameter description: dtype(`torch.dtype`, optional) - the desired data type of returned Tensor. Default: if `None`, defaults to the dtype of `input`.\nConstraints:\ndtype:torch.dtype\nstructure:Null\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.nn.RNN(*args,**kwargs,input_size=None,hidden_size=None,num_layers=None,nonlinearity=None,bias=None,batch_first=None,dropout=None,bidirectional=None)\nParameter description: hidden_size - The number of features in the hidden state h\nConstraints:\ndtype:int\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:[0,inf)\n\n###\n\nSignature: torch.nn.functional.avg_pool1d(input,kernel_size,stride=None,padding=0,ceil_mode=False,count_include_pad=True)\nParameter description: padding - implicit zero paddings on both sides of the input. Can be a single number or a tuple (padW,). Default: 0\nConstraints:\ndtype:bool;int\nstructure:tuple\nshape:Null\nndim:0;1\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.autograd.grad(outputs,inputs,grad_outputs=None,retain_graph=None,create_graph=False,only_inputs=True,allow_unused=False)\nParameter description: outputs(sequence of Tensor) - outputs of the differentiated function.\nConstraints:\ndtype:Null\nstructure:sequence;tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.repeat_interleave(input,repeats,dim=None)\nParameter description: dim(int, optional) - The dimension along which to repeat values.By default, use the flattened input array, and return a flat outputarray.\nConstraints:\ndtype:int\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.nn.Softmax(dim=None)\nParameter description: dim(int) - A dimension along which Softmax will be computed (so every slice along dim will sum to 1).\nConstraints:",
            "constr": {
                "dtype": [
                    "int"
                ],
                "structure": [],
                "shape": [],
                "ndim": [],
                "enum": [],
                "range": []
            },
            "signature": "torch.nn.Softmax(dim=None)",
            "descp": "dim(int) - A dimension along which Softmax will be computed (so every slice along dim will sum to 1)."
        },
        {
            "fname": "torch.var2.yaml",
            "arg": "keepdim",
            "prompt": "Signature: torch.distributed.recv(tensor,src=None,group=<objectobject>,tag=0)\nParameter description: tensor(Tensor) - Tensor to fill with received data.\nConstraints:\ndtype:Null\nstructure:tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.div(input,other,out=None)\nParameter description: out(Tensor, optional) - the output tensor.\nConstraints:\ndtype:Null\nstructure:tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.nn.TransformerDecoder(decoder_layer,num_layers,norm=None)\nParameter description: num_layers - the number of sub-decoder-layers in the decoder (required).\nConstraints:\ndtype:int\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:[0,inf)\n\n###\n\nSignature: torch.nn.quantized.functional.interpolate(input,size=None,scale_factor=None,mode=nearest,align_corners=None)\nParameter description: align_corners(bool, optional) - Geometrically, we consider the pixels of the input and output as squares rather than points. If set to `True`, the input and output tensors are aligned by the center points of their corner pixels, preserving the values at the corner pixels. If set to `False`, the input and output tensors are aligned by the corner points of their corner pixels, and the interpolation uses edge value padding for out-of-boundary values, making this operation independent of input size when `scale_factor` is kept the same. This only has an effect when `mode` is `'bilinear'`. Default: `False`\nConstraints:\ndtype:bool\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.gt(input,other,out=None)\nParameter description: out(Tensor, optional) - the output tensor that must be a BoolTensor\nConstraints:\ndtype:booltensor\nstructure:tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.empty_like(input,dtype=None,layout=None,device=None,requires_grad=False,memory_format=torch.preserve_format)\nParameter description: dtype(`torch.dtype`, optional) - the desired data type of returned Tensor. Default: if `None`, defaults to the dtype of `input`.\nConstraints:\ndtype:torch.dtype\nstructure:Null\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.nn.RNN(*args,**kwargs,input_size=None,hidden_size=None,num_layers=None,nonlinearity=None,bias=None,batch_first=None,dropout=None,bidirectional=None)\nParameter description: hidden_size - The number of features in the hidden state h\nConstraints:\ndtype:int\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:[0,inf)\n\n###\n\nSignature: torch.nn.functional.avg_pool1d(input,kernel_size,stride=None,padding=0,ceil_mode=False,count_include_pad=True)\nParameter description: padding - implicit zero paddings on both sides of the input. Can be a single number or a tuple (padW,). Default: 0\nConstraints:\ndtype:bool;int\nstructure:tuple\nshape:Null\nndim:0;1\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.autograd.grad(outputs,inputs,grad_outputs=None,retain_graph=None,create_graph=False,only_inputs=True,allow_unused=False)\nParameter description: outputs(sequence of Tensor) - outputs of the differentiated function.\nConstraints:\ndtype:Null\nstructure:sequence;tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.repeat_interleave(input,repeats,dim=None)\nParameter description: dim(int, optional) - The dimension along which to repeat values.By default, use the flattened input array, and return a flat outputarray.\nConstraints:\ndtype:int\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.var(input,dim,keepdim=False,unbiased=True,out=None)\nParameter description: keepdim(bool) - whether the output tensor has `dim` retained or not.\nConstraints:",
            "constr": {
                "dtype": [
                    "bool"
                ],
                "structure": [],
                "shape": [],
                "ndim": [
                    "0"
                ],
                "enum": [],
                "range": []
            },
            "signature": "torch.var(input,dim,keepdim=False,unbiased=True,out=None)",
            "descp": "keepdim(bool) - whether the output tensor has `dim` retained or not."
        },
        {
            "fname": "torch.nn.parallel.data_parallel.yaml",
            "arg": "device_ids",
            "prompt": "Signature: torch.distributed.recv(tensor,src=None,group=<objectobject>,tag=0)\nParameter description: tensor(Tensor) - Tensor to fill with received data.\nConstraints:\ndtype:Null\nstructure:tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.div(input,other,out=None)\nParameter description: out(Tensor, optional) - the output tensor.\nConstraints:\ndtype:Null\nstructure:tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.nn.TransformerDecoder(decoder_layer,num_layers,norm=None)\nParameter description: num_layers - the number of sub-decoder-layers in the decoder (required).\nConstraints:\ndtype:int\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:[0,inf)\n\n###\n\nSignature: torch.nn.quantized.functional.interpolate(input,size=None,scale_factor=None,mode=nearest,align_corners=None)\nParameter description: align_corners(bool, optional) - Geometrically, we consider the pixels of the input and output as squares rather than points. If set to `True`, the input and output tensors are aligned by the center points of their corner pixels, preserving the values at the corner pixels. If set to `False`, the input and output tensors are aligned by the corner points of their corner pixels, and the interpolation uses edge value padding for out-of-boundary values, making this operation independent of input size when `scale_factor` is kept the same. This only has an effect when `mode` is `'bilinear'`. Default: `False`\nConstraints:\ndtype:bool\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.gt(input,other,out=None)\nParameter description: out(Tensor, optional) - the output tensor that must be a BoolTensor\nConstraints:\ndtype:booltensor\nstructure:tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.empty_like(input,dtype=None,layout=None,device=None,requires_grad=False,memory_format=torch.preserve_format)\nParameter description: dtype(`torch.dtype`, optional) - the desired data type of returned Tensor. Default: if `None`, defaults to the dtype of `input`.\nConstraints:\ndtype:torch.dtype\nstructure:Null\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.nn.RNN(*args,**kwargs,input_size=None,hidden_size=None,num_layers=None,nonlinearity=None,bias=None,batch_first=None,dropout=None,bidirectional=None)\nParameter description: hidden_size - The number of features in the hidden state h\nConstraints:\ndtype:int\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:[0,inf)\n\n###\n\nSignature: torch.nn.functional.avg_pool1d(input,kernel_size,stride=None,padding=0,ceil_mode=False,count_include_pad=True)\nParameter description: padding - implicit zero paddings on both sides of the input. Can be a single number or a tuple (padW,). Default: 0\nConstraints:\ndtype:bool;int\nstructure:tuple\nshape:Null\nndim:0;1\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.autograd.grad(outputs,inputs,grad_outputs=None,retain_graph=None,create_graph=False,only_inputs=True,allow_unused=False)\nParameter description: outputs(sequence of Tensor) - outputs of the differentiated function.\nConstraints:\ndtype:Null\nstructure:sequence;tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.repeat_interleave(input,repeats,dim=None)\nParameter description: dim(int, optional) - The dimension along which to repeat values.By default, use the flattened input array, and return a flat outputarray.\nConstraints:\ndtype:int\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.nn.parallel.data_parallel(module,inputs,device_ids=None,output_device=None,dim=0,module_kwargs=None)\nParameter description: device_ids(list of python:int or torch.device) - GPU ids on which to replicate module\nConstraints:",
            "constr": {
                "dtype": [
                    "int"
                ],
                "structure": [
                    "list"
                ],
                "shape": [],
                "ndim": [],
                "enum": [],
                "range": [
                    "[0,inf)"
                ]
            },
            "signature": "torch.nn.parallel.data_parallel(module,inputs,device_ids=None,output_device=None,dim=0,module_kwargs=None)",
            "descp": "device_ids(list of python:int or torch.device) - GPU ids on which to replicate module"
        },
        {
            "fname": "torch.nn.transformerdecoderlayer.yaml",
            "arg": "dropout",
            "prompt": "Signature: torch.distributed.recv(tensor,src=None,group=<objectobject>,tag=0)\nParameter description: tensor(Tensor) - Tensor to fill with received data.\nConstraints:\ndtype:Null\nstructure:tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.div(input,other,out=None)\nParameter description: out(Tensor, optional) - the output tensor.\nConstraints:\ndtype:Null\nstructure:tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.nn.TransformerDecoder(decoder_layer,num_layers,norm=None)\nParameter description: num_layers - the number of sub-decoder-layers in the decoder (required).\nConstraints:\ndtype:int\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:[0,inf)\n\n###\n\nSignature: torch.nn.quantized.functional.interpolate(input,size=None,scale_factor=None,mode=nearest,align_corners=None)\nParameter description: align_corners(bool, optional) - Geometrically, we consider the pixels of the input and output as squares rather than points. If set to `True`, the input and output tensors are aligned by the center points of their corner pixels, preserving the values at the corner pixels. If set to `False`, the input and output tensors are aligned by the corner points of their corner pixels, and the interpolation uses edge value padding for out-of-boundary values, making this operation independent of input size when `scale_factor` is kept the same. This only has an effect when `mode` is `'bilinear'`. Default: `False`\nConstraints:\ndtype:bool\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.gt(input,other,out=None)\nParameter description: out(Tensor, optional) - the output tensor that must be a BoolTensor\nConstraints:\ndtype:booltensor\nstructure:tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.empty_like(input,dtype=None,layout=None,device=None,requires_grad=False,memory_format=torch.preserve_format)\nParameter description: dtype(`torch.dtype`, optional) - the desired data type of returned Tensor. Default: if `None`, defaults to the dtype of `input`.\nConstraints:\ndtype:torch.dtype\nstructure:Null\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.nn.RNN(*args,**kwargs,input_size=None,hidden_size=None,num_layers=None,nonlinearity=None,bias=None,batch_first=None,dropout=None,bidirectional=None)\nParameter description: hidden_size - The number of features in the hidden state h\nConstraints:\ndtype:int\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:[0,inf)\n\n###\n\nSignature: torch.nn.functional.avg_pool1d(input,kernel_size,stride=None,padding=0,ceil_mode=False,count_include_pad=True)\nParameter description: padding - implicit zero paddings on both sides of the input. Can be a single number or a tuple (padW,). Default: 0\nConstraints:\ndtype:bool;int\nstructure:tuple\nshape:Null\nndim:0;1\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.autograd.grad(outputs,inputs,grad_outputs=None,retain_graph=None,create_graph=False,only_inputs=True,allow_unused=False)\nParameter description: outputs(sequence of Tensor) - outputs of the differentiated function.\nConstraints:\ndtype:Null\nstructure:sequence;tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.repeat_interleave(input,repeats,dim=None)\nParameter description: dim(int, optional) - The dimension along which to repeat values.By default, use the flattened input array, and return a flat outputarray.\nConstraints:\ndtype:int\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.nn.TransformerDecoderLayer(d_model,nhead,dim_feedforward=2048,dropout=0.1,activation=relu)\nParameter description: dropout - the dropout value (default=0.1).\nConstraints:",
            "constr": {
                "dtype": [
                    "float"
                ],
                "structure": [],
                "shape": [],
                "ndim": [
                    "0"
                ],
                "enum": [],
                "range": []
            },
            "signature": "torch.nn.TransformerDecoderLayer(d_model,nhead,dim_feedforward=2048,dropout=0.1,activation=relu)",
            "descp": "dropout - the dropout value (default=0.1)."
        },
        {
            "fname": "torch.tensordot.yaml",
            "arg": "b",
            "prompt": "Signature: torch.distributed.recv(tensor,src=None,group=<objectobject>,tag=0)\nParameter description: tensor(Tensor) - Tensor to fill with received data.\nConstraints:\ndtype:Null\nstructure:tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.div(input,other,out=None)\nParameter description: out(Tensor, optional) - the output tensor.\nConstraints:\ndtype:Null\nstructure:tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.nn.TransformerDecoder(decoder_layer,num_layers,norm=None)\nParameter description: num_layers - the number of sub-decoder-layers in the decoder (required).\nConstraints:\ndtype:int\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:[0,inf)\n\n###\n\nSignature: torch.nn.quantized.functional.interpolate(input,size=None,scale_factor=None,mode=nearest,align_corners=None)\nParameter description: align_corners(bool, optional) - Geometrically, we consider the pixels of the input and output as squares rather than points. If set to `True`, the input and output tensors are aligned by the center points of their corner pixels, preserving the values at the corner pixels. If set to `False`, the input and output tensors are aligned by the corner points of their corner pixels, and the interpolation uses edge value padding for out-of-boundary values, making this operation independent of input size when `scale_factor` is kept the same. This only has an effect when `mode` is `'bilinear'`. Default: `False`\nConstraints:\ndtype:bool\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.gt(input,other,out=None)\nParameter description: out(Tensor, optional) - the output tensor that must be a BoolTensor\nConstraints:\ndtype:booltensor\nstructure:tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.empty_like(input,dtype=None,layout=None,device=None,requires_grad=False,memory_format=torch.preserve_format)\nParameter description: dtype(`torch.dtype`, optional) - the desired data type of returned Tensor. Default: if `None`, defaults to the dtype of `input`.\nConstraints:\ndtype:torch.dtype\nstructure:Null\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.nn.RNN(*args,**kwargs,input_size=None,hidden_size=None,num_layers=None,nonlinearity=None,bias=None,batch_first=None,dropout=None,bidirectional=None)\nParameter description: hidden_size - The number of features in the hidden state h\nConstraints:\ndtype:int\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:[0,inf)\n\n###\n\nSignature: torch.nn.functional.avg_pool1d(input,kernel_size,stride=None,padding=0,ceil_mode=False,count_include_pad=True)\nParameter description: padding - implicit zero paddings on both sides of the input. Can be a single number or a tuple (padW,). Default: 0\nConstraints:\ndtype:bool;int\nstructure:tuple\nshape:Null\nndim:0;1\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.autograd.grad(outputs,inputs,grad_outputs=None,retain_graph=None,create_graph=False,only_inputs=True,allow_unused=False)\nParameter description: outputs(sequence of Tensor) - outputs of the differentiated function.\nConstraints:\ndtype:Null\nstructure:sequence;tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.repeat_interleave(input,repeats,dim=None)\nParameter description: dim(int, optional) - The dimension along which to repeat values.By default, use the flattened input array, and return a flat outputarray.\nConstraints:\ndtype:int\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.tensordot(a,b,dims=2)\nParameter description: b(Tensor) - Right tensor to contract\nConstraints:",
            "constr": {
                "dtype": [],
                "structure": [
                    "tensor"
                ],
                "shape": [],
                "ndim": [],
                "enum": [],
                "range": []
            },
            "signature": "torch.tensordot(a,b,dims=2)",
            "descp": "b(Tensor) - Right tensor to contract"
        },
        {
            "fname": "torch.nn.embeddingbag.yaml",
            "arg": "norm_type",
            "prompt": "Signature: torch.distributed.recv(tensor,src=None,group=<objectobject>,tag=0)\nParameter description: tensor(Tensor) - Tensor to fill with received data.\nConstraints:\ndtype:Null\nstructure:tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.div(input,other,out=None)\nParameter description: out(Tensor, optional) - the output tensor.\nConstraints:\ndtype:Null\nstructure:tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.nn.TransformerDecoder(decoder_layer,num_layers,norm=None)\nParameter description: num_layers - the number of sub-decoder-layers in the decoder (required).\nConstraints:\ndtype:int\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:[0,inf)\n\n###\n\nSignature: torch.nn.quantized.functional.interpolate(input,size=None,scale_factor=None,mode=nearest,align_corners=None)\nParameter description: align_corners(bool, optional) - Geometrically, we consider the pixels of the input and output as squares rather than points. If set to `True`, the input and output tensors are aligned by the center points of their corner pixels, preserving the values at the corner pixels. If set to `False`, the input and output tensors are aligned by the corner points of their corner pixels, and the interpolation uses edge value padding for out-of-boundary values, making this operation independent of input size when `scale_factor` is kept the same. This only has an effect when `mode` is `'bilinear'`. Default: `False`\nConstraints:\ndtype:bool\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.gt(input,other,out=None)\nParameter description: out(Tensor, optional) - the output tensor that must be a BoolTensor\nConstraints:\ndtype:booltensor\nstructure:tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.empty_like(input,dtype=None,layout=None,device=None,requires_grad=False,memory_format=torch.preserve_format)\nParameter description: dtype(`torch.dtype`, optional) - the desired data type of returned Tensor. Default: if `None`, defaults to the dtype of `input`.\nConstraints:\ndtype:torch.dtype\nstructure:Null\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.nn.RNN(*args,**kwargs,input_size=None,hidden_size=None,num_layers=None,nonlinearity=None,bias=None,batch_first=None,dropout=None,bidirectional=None)\nParameter description: hidden_size - The number of features in the hidden state h\nConstraints:\ndtype:int\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:[0,inf)\n\n###\n\nSignature: torch.nn.functional.avg_pool1d(input,kernel_size,stride=None,padding=0,ceil_mode=False,count_include_pad=True)\nParameter description: padding - implicit zero paddings on both sides of the input. Can be a single number or a tuple (padW,). Default: 0\nConstraints:\ndtype:bool;int\nstructure:tuple\nshape:Null\nndim:0;1\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.autograd.grad(outputs,inputs,grad_outputs=None,retain_graph=None,create_graph=False,only_inputs=True,allow_unused=False)\nParameter description: outputs(sequence of Tensor) - outputs of the differentiated function.\nConstraints:\ndtype:Null\nstructure:sequence;tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.repeat_interleave(input,repeats,dim=None)\nParameter description: dim(int, optional) - The dimension along which to repeat values.By default, use the flattened input array, and return a flat outputarray.\nConstraints:\ndtype:int\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.nn.EmbeddingBag(num_embeddings,embedding_dim,max_norm=None,norm_type=2.0,scale_grad_by_freq=False,mode=mean,sparse=False,_weight=None,include_last_offset=False)\nParameter description: norm_type(float, optional) - The p of the p-norm to compute for the `max_norm` option. Default `2`.\nConstraints:",
            "constr": {
                "dtype": [
                    "float",
                    "int"
                ],
                "structure": [],
                "shape": [],
                "ndim": [
                    "0"
                ],
                "enum": [],
                "range": []
            },
            "signature": "torch.nn.EmbeddingBag(num_embeddings,embedding_dim,max_norm=None,norm_type=2.0,scale_grad_by_freq=False,mode=mean,sparse=False,_weight=None,include_last_offset=False)",
            "descp": "norm_type(float, optional) - The p of the p-norm to compute for the `max_norm` option. Default `2`."
        },
        {
            "fname": "torch.nn.quantized.functional.avg_pool2d.yaml",
            "arg": "ceil_mode",
            "prompt": "Signature: torch.distributed.recv(tensor,src=None,group=<objectobject>,tag=0)\nParameter description: tensor(Tensor) - Tensor to fill with received data.\nConstraints:\ndtype:Null\nstructure:tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.div(input,other,out=None)\nParameter description: out(Tensor, optional) - the output tensor.\nConstraints:\ndtype:Null\nstructure:tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.nn.TransformerDecoder(decoder_layer,num_layers,norm=None)\nParameter description: num_layers - the number of sub-decoder-layers in the decoder (required).\nConstraints:\ndtype:int\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:[0,inf)\n\n###\n\nSignature: torch.nn.quantized.functional.interpolate(input,size=None,scale_factor=None,mode=nearest,align_corners=None)\nParameter description: align_corners(bool, optional) - Geometrically, we consider the pixels of the input and output as squares rather than points. If set to `True`, the input and output tensors are aligned by the center points of their corner pixels, preserving the values at the corner pixels. If set to `False`, the input and output tensors are aligned by the corner points of their corner pixels, and the interpolation uses edge value padding for out-of-boundary values, making this operation independent of input size when `scale_factor` is kept the same. This only has an effect when `mode` is `'bilinear'`. Default: `False`\nConstraints:\ndtype:bool\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.gt(input,other,out=None)\nParameter description: out(Tensor, optional) - the output tensor that must be a BoolTensor\nConstraints:\ndtype:booltensor\nstructure:tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.empty_like(input,dtype=None,layout=None,device=None,requires_grad=False,memory_format=torch.preserve_format)\nParameter description: dtype(`torch.dtype`, optional) - the desired data type of returned Tensor. Default: if `None`, defaults to the dtype of `input`.\nConstraints:\ndtype:torch.dtype\nstructure:Null\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.nn.RNN(*args,**kwargs,input_size=None,hidden_size=None,num_layers=None,nonlinearity=None,bias=None,batch_first=None,dropout=None,bidirectional=None)\nParameter description: hidden_size - The number of features in the hidden state h\nConstraints:\ndtype:int\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:[0,inf)\n\n###\n\nSignature: torch.nn.functional.avg_pool1d(input,kernel_size,stride=None,padding=0,ceil_mode=False,count_include_pad=True)\nParameter description: padding - implicit zero paddings on both sides of the input. Can be a single number or a tuple (padW,). Default: 0\nConstraints:\ndtype:bool;int\nstructure:tuple\nshape:Null\nndim:0;1\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.autograd.grad(outputs,inputs,grad_outputs=None,retain_graph=None,create_graph=False,only_inputs=True,allow_unused=False)\nParameter description: outputs(sequence of Tensor) - outputs of the differentiated function.\nConstraints:\ndtype:Null\nstructure:sequence;tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.repeat_interleave(input,repeats,dim=None)\nParameter description: dim(int, optional) - The dimension along which to repeat values.By default, use the flattened input array, and return a flat outputarray.\nConstraints:\ndtype:int\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.nn.quantized.functional.avg_pool2d(input,kernel_size,stride=None,padding=0,ceil_mode=False,count_include_pad=True,divisor_override=None)\nParameter description: ceil_mode - when True, will use ceil instead of floor in the formula to compute the output shape. Default: `False`\nConstraints:",
            "constr": {
                "dtype": [
                    "bool"
                ],
                "structure": [],
                "shape": [],
                "ndim": [
                    "0"
                ],
                "enum": [],
                "range": []
            },
            "signature": "torch.nn.quantized.functional.avg_pool2d(input,kernel_size,stride=None,padding=0,ceil_mode=False,count_include_pad=True,divisor_override=None)",
            "descp": "ceil_mode - when True, will use ceil instead of floor in the formula to compute the output shape. Default: `False`"
        },
        {
            "fname": "torch.arange.yaml",
            "arg": "start",
            "prompt": "Signature: torch.distributed.recv(tensor,src=None,group=<objectobject>,tag=0)\nParameter description: tensor(Tensor) - Tensor to fill with received data.\nConstraints:\ndtype:Null\nstructure:tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.div(input,other,out=None)\nParameter description: out(Tensor, optional) - the output tensor.\nConstraints:\ndtype:Null\nstructure:tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.nn.TransformerDecoder(decoder_layer,num_layers,norm=None)\nParameter description: num_layers - the number of sub-decoder-layers in the decoder (required).\nConstraints:\ndtype:int\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:[0,inf)\n\n###\n\nSignature: torch.nn.quantized.functional.interpolate(input,size=None,scale_factor=None,mode=nearest,align_corners=None)\nParameter description: align_corners(bool, optional) - Geometrically, we consider the pixels of the input and output as squares rather than points. If set to `True`, the input and output tensors are aligned by the center points of their corner pixels, preserving the values at the corner pixels. If set to `False`, the input and output tensors are aligned by the corner points of their corner pixels, and the interpolation uses edge value padding for out-of-boundary values, making this operation independent of input size when `scale_factor` is kept the same. This only has an effect when `mode` is `'bilinear'`. Default: `False`\nConstraints:\ndtype:bool\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.gt(input,other,out=None)\nParameter description: out(Tensor, optional) - the output tensor that must be a BoolTensor\nConstraints:\ndtype:booltensor\nstructure:tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.empty_like(input,dtype=None,layout=None,device=None,requires_grad=False,memory_format=torch.preserve_format)\nParameter description: dtype(`torch.dtype`, optional) - the desired data type of returned Tensor. Default: if `None`, defaults to the dtype of `input`.\nConstraints:\ndtype:torch.dtype\nstructure:Null\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.nn.RNN(*args,**kwargs,input_size=None,hidden_size=None,num_layers=None,nonlinearity=None,bias=None,batch_first=None,dropout=None,bidirectional=None)\nParameter description: hidden_size - The number of features in the hidden state h\nConstraints:\ndtype:int\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:[0,inf)\n\n###\n\nSignature: torch.nn.functional.avg_pool1d(input,kernel_size,stride=None,padding=0,ceil_mode=False,count_include_pad=True)\nParameter description: padding - implicit zero paddings on both sides of the input. Can be a single number or a tuple (padW,). Default: 0\nConstraints:\ndtype:bool;int\nstructure:tuple\nshape:Null\nndim:0;1\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.autograd.grad(outputs,inputs,grad_outputs=None,retain_graph=None,create_graph=False,only_inputs=True,allow_unused=False)\nParameter description: outputs(sequence of Tensor) - outputs of the differentiated function.\nConstraints:\ndtype:Null\nstructure:sequence;tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.repeat_interleave(input,repeats,dim=None)\nParameter description: dim(int, optional) - The dimension along which to repeat values.By default, use the flattened input array, and return a flat outputarray.\nConstraints:\ndtype:int\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.arange(end,start=0,step=1,out=None,dtype=None,layout=torch.strided,device=None,requires_grad=False)\nParameter description: start(Number) - the starting value for the set of points. Default: `0`.\nConstraints:",
            "constr": {
                "dtype": [
                    "numeric",
                    "int"
                ],
                "structure": [],
                "shape": [],
                "ndim": [
                    "0"
                ],
                "enum": [],
                "range": []
            },
            "signature": "torch.arange(end,start=0,step=1,out=None,dtype=None,layout=torch.strided,device=None,requires_grad=False)",
            "descp": "start(Number) - the starting value for the set of points. Default: `0`."
        },
        {
            "fname": "torch.sparse.mm.yaml",
            "arg": "mat1",
            "prompt": "Signature: torch.distributed.recv(tensor,src=None,group=<objectobject>,tag=0)\nParameter description: tensor(Tensor) - Tensor to fill with received data.\nConstraints:\ndtype:Null\nstructure:tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.div(input,other,out=None)\nParameter description: out(Tensor, optional) - the output tensor.\nConstraints:\ndtype:Null\nstructure:tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.nn.TransformerDecoder(decoder_layer,num_layers,norm=None)\nParameter description: num_layers - the number of sub-decoder-layers in the decoder (required).\nConstraints:\ndtype:int\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:[0,inf)\n\n###\n\nSignature: torch.nn.quantized.functional.interpolate(input,size=None,scale_factor=None,mode=nearest,align_corners=None)\nParameter description: align_corners(bool, optional) - Geometrically, we consider the pixels of the input and output as squares rather than points. If set to `True`, the input and output tensors are aligned by the center points of their corner pixels, preserving the values at the corner pixels. If set to `False`, the input and output tensors are aligned by the corner points of their corner pixels, and the interpolation uses edge value padding for out-of-boundary values, making this operation independent of input size when `scale_factor` is kept the same. This only has an effect when `mode` is `'bilinear'`. Default: `False`\nConstraints:\ndtype:bool\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.gt(input,other,out=None)\nParameter description: out(Tensor, optional) - the output tensor that must be a BoolTensor\nConstraints:\ndtype:booltensor\nstructure:tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.empty_like(input,dtype=None,layout=None,device=None,requires_grad=False,memory_format=torch.preserve_format)\nParameter description: dtype(`torch.dtype`, optional) - the desired data type of returned Tensor. Default: if `None`, defaults to the dtype of `input`.\nConstraints:\ndtype:torch.dtype\nstructure:Null\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.nn.RNN(*args,**kwargs,input_size=None,hidden_size=None,num_layers=None,nonlinearity=None,bias=None,batch_first=None,dropout=None,bidirectional=None)\nParameter description: hidden_size - The number of features in the hidden state h\nConstraints:\ndtype:int\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:[0,inf)\n\n###\n\nSignature: torch.nn.functional.avg_pool1d(input,kernel_size,stride=None,padding=0,ceil_mode=False,count_include_pad=True)\nParameter description: padding - implicit zero paddings on both sides of the input. Can be a single number or a tuple (padW,). Default: 0\nConstraints:\ndtype:bool;int\nstructure:tuple\nshape:Null\nndim:0;1\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.autograd.grad(outputs,inputs,grad_outputs=None,retain_graph=None,create_graph=False,only_inputs=True,allow_unused=False)\nParameter description: outputs(sequence of Tensor) - outputs of the differentiated function.\nConstraints:\ndtype:Null\nstructure:sequence;tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.repeat_interleave(input,repeats,dim=None)\nParameter description: dim(int, optional) - The dimension along which to repeat values.By default, use the flattened input array, and return a flat outputarray.\nConstraints:\ndtype:int\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.sparse.mm(mat1,mat2)\nParameter description: mat1(SparseTensor) - the first sparse matrix to be multiplied\nConstraints:",
            "constr": {
                "dtype": [
                    "numeric"
                ],
                "structure": [
                    "sparsetensor"
                ],
                "shape": [],
                "ndim": [],
                "enum": [],
                "range": []
            },
            "signature": "torch.sparse.mm(mat1,mat2)",
            "descp": "mat1(SparseTensor) - the first sparse matrix to be multiplied"
        },
        {
            "fname": "torch.nn.nllloss.yaml",
            "arg": "size_average",
            "prompt": "Signature: torch.distributed.recv(tensor,src=None,group=<objectobject>,tag=0)\nParameter description: tensor(Tensor) - Tensor to fill with received data.\nConstraints:\ndtype:Null\nstructure:tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.div(input,other,out=None)\nParameter description: out(Tensor, optional) - the output tensor.\nConstraints:\ndtype:Null\nstructure:tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.nn.TransformerDecoder(decoder_layer,num_layers,norm=None)\nParameter description: num_layers - the number of sub-decoder-layers in the decoder (required).\nConstraints:\ndtype:int\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:[0,inf)\n\n###\n\nSignature: torch.nn.quantized.functional.interpolate(input,size=None,scale_factor=None,mode=nearest,align_corners=None)\nParameter description: align_corners(bool, optional) - Geometrically, we consider the pixels of the input and output as squares rather than points. If set to `True`, the input and output tensors are aligned by the center points of their corner pixels, preserving the values at the corner pixels. If set to `False`, the input and output tensors are aligned by the corner points of their corner pixels, and the interpolation uses edge value padding for out-of-boundary values, making this operation independent of input size when `scale_factor` is kept the same. This only has an effect when `mode` is `'bilinear'`. Default: `False`\nConstraints:\ndtype:bool\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.gt(input,other,out=None)\nParameter description: out(Tensor, optional) - the output tensor that must be a BoolTensor\nConstraints:\ndtype:booltensor\nstructure:tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.empty_like(input,dtype=None,layout=None,device=None,requires_grad=False,memory_format=torch.preserve_format)\nParameter description: dtype(`torch.dtype`, optional) - the desired data type of returned Tensor. Default: if `None`, defaults to the dtype of `input`.\nConstraints:\ndtype:torch.dtype\nstructure:Null\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.nn.RNN(*args,**kwargs,input_size=None,hidden_size=None,num_layers=None,nonlinearity=None,bias=None,batch_first=None,dropout=None,bidirectional=None)\nParameter description: hidden_size - The number of features in the hidden state h\nConstraints:\ndtype:int\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:[0,inf)\n\n###\n\nSignature: torch.nn.functional.avg_pool1d(input,kernel_size,stride=None,padding=0,ceil_mode=False,count_include_pad=True)\nParameter description: padding - implicit zero paddings on both sides of the input. Can be a single number or a tuple (padW,). Default: 0\nConstraints:\ndtype:bool;int\nstructure:tuple\nshape:Null\nndim:0;1\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.autograd.grad(outputs,inputs,grad_outputs=None,retain_graph=None,create_graph=False,only_inputs=True,allow_unused=False)\nParameter description: outputs(sequence of Tensor) - outputs of the differentiated function.\nConstraints:\ndtype:Null\nstructure:sequence;tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.repeat_interleave(input,repeats,dim=None)\nParameter description: dim(int, optional) - The dimension along which to repeat values.By default, use the flattened input array, and return a flat outputarray.\nConstraints:\ndtype:int\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.nn.NLLLoss(weight=None,size_average=None,ignore_index=-100,reduce=None,reduction=mean)\nParameter description: size_average(bool, optional) - Deprecated (see `reduction`). By default, the losses are averaged over each loss element in the batch. Note that for some losses, there are multiple elements per sample. If the field `size_average` is set to `False`, the losses are instead summed for each minibatch. Ignored when reduce is `False`. Default: `True`\nConstraints:",
            "constr": {
                "dtype": [
                    "bool"
                ],
                "structure": [],
                "shape": [],
                "ndim": [
                    "0"
                ],
                "enum": [],
                "range": []
            },
            "signature": "torch.nn.NLLLoss(weight=None,size_average=None,ignore_index=-100,reduce=None,reduction=mean)",
            "descp": "size_average(bool, optional) - Deprecated (see `reduction`). By default, the losses are averaged over each loss element in the batch. Note that for some losses, there are multiple elements per sample. If the field `size_average` is set to `False`, the losses are instead summed for each minibatch. Ignored when reduce is `False`. Default: `True`"
        },
        {
            "fname": "torch.nn.adaptivelogsoftmaxwithloss.yaml",
            "arg": "in_features",
            "prompt": "Signature: torch.distributed.recv(tensor,src=None,group=<objectobject>,tag=0)\nParameter description: tensor(Tensor) - Tensor to fill with received data.\nConstraints:\ndtype:Null\nstructure:tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.div(input,other,out=None)\nParameter description: out(Tensor, optional) - the output tensor.\nConstraints:\ndtype:Null\nstructure:tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.nn.TransformerDecoder(decoder_layer,num_layers,norm=None)\nParameter description: num_layers - the number of sub-decoder-layers in the decoder (required).\nConstraints:\ndtype:int\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:[0,inf)\n\n###\n\nSignature: torch.nn.quantized.functional.interpolate(input,size=None,scale_factor=None,mode=nearest,align_corners=None)\nParameter description: align_corners(bool, optional) - Geometrically, we consider the pixels of the input and output as squares rather than points. If set to `True`, the input and output tensors are aligned by the center points of their corner pixels, preserving the values at the corner pixels. If set to `False`, the input and output tensors are aligned by the corner points of their corner pixels, and the interpolation uses edge value padding for out-of-boundary values, making this operation independent of input size when `scale_factor` is kept the same. This only has an effect when `mode` is `'bilinear'`. Default: `False`\nConstraints:\ndtype:bool\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.gt(input,other,out=None)\nParameter description: out(Tensor, optional) - the output tensor that must be a BoolTensor\nConstraints:\ndtype:booltensor\nstructure:tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.empty_like(input,dtype=None,layout=None,device=None,requires_grad=False,memory_format=torch.preserve_format)\nParameter description: dtype(`torch.dtype`, optional) - the desired data type of returned Tensor. Default: if `None`, defaults to the dtype of `input`.\nConstraints:\ndtype:torch.dtype\nstructure:Null\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.nn.RNN(*args,**kwargs,input_size=None,hidden_size=None,num_layers=None,nonlinearity=None,bias=None,batch_first=None,dropout=None,bidirectional=None)\nParameter description: hidden_size - The number of features in the hidden state h\nConstraints:\ndtype:int\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:[0,inf)\n\n###\n\nSignature: torch.nn.functional.avg_pool1d(input,kernel_size,stride=None,padding=0,ceil_mode=False,count_include_pad=True)\nParameter description: padding - implicit zero paddings on both sides of the input. Can be a single number or a tuple (padW,). Default: 0\nConstraints:\ndtype:bool;int\nstructure:tuple\nshape:Null\nndim:0;1\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.autograd.grad(outputs,inputs,grad_outputs=None,retain_graph=None,create_graph=False,only_inputs=True,allow_unused=False)\nParameter description: outputs(sequence of Tensor) - outputs of the differentiated function.\nConstraints:\ndtype:Null\nstructure:sequence;tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.repeat_interleave(input,repeats,dim=None)\nParameter description: dim(int, optional) - The dimension along which to repeat values.By default, use the flattened input array, and return a flat outputarray.\nConstraints:\ndtype:int\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.nn.AdaptiveLogSoftmaxWithLoss(in_features,n_classes,cutoffs,div_value=4.0,head_bias=False)\nParameter description: in_features(int) - Number of features in the input tensor\nConstraints:",
            "constr": {
                "dtype": [
                    "int"
                ],
                "structure": [],
                "shape": [],
                "ndim": [
                    "0"
                ],
                "enum": [],
                "range": [
                    "[0,inf)"
                ]
            },
            "signature": "torch.nn.AdaptiveLogSoftmaxWithLoss(in_features,n_classes,cutoffs,div_value=4.0,head_bias=False)",
            "descp": "in_features(int) - Number of features in the input tensor"
        },
        {
            "fname": "torch.nn.conv1d.yaml",
            "arg": "kernel_size",
            "prompt": "Signature: torch.distributed.recv(tensor,src=None,group=<objectobject>,tag=0)\nParameter description: tensor(Tensor) - Tensor to fill with received data.\nConstraints:\ndtype:Null\nstructure:tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.div(input,other,out=None)\nParameter description: out(Tensor, optional) - the output tensor.\nConstraints:\ndtype:Null\nstructure:tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.nn.TransformerDecoder(decoder_layer,num_layers,norm=None)\nParameter description: num_layers - the number of sub-decoder-layers in the decoder (required).\nConstraints:\ndtype:int\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:[0,inf)\n\n###\n\nSignature: torch.nn.quantized.functional.interpolate(input,size=None,scale_factor=None,mode=nearest,align_corners=None)\nParameter description: align_corners(bool, optional) - Geometrically, we consider the pixels of the input and output as squares rather than points. If set to `True`, the input and output tensors are aligned by the center points of their corner pixels, preserving the values at the corner pixels. If set to `False`, the input and output tensors are aligned by the corner points of their corner pixels, and the interpolation uses edge value padding for out-of-boundary values, making this operation independent of input size when `scale_factor` is kept the same. This only has an effect when `mode` is `'bilinear'`. Default: `False`\nConstraints:\ndtype:bool\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.gt(input,other,out=None)\nParameter description: out(Tensor, optional) - the output tensor that must be a BoolTensor\nConstraints:\ndtype:booltensor\nstructure:tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.empty_like(input,dtype=None,layout=None,device=None,requires_grad=False,memory_format=torch.preserve_format)\nParameter description: dtype(`torch.dtype`, optional) - the desired data type of returned Tensor. Default: if `None`, defaults to the dtype of `input`.\nConstraints:\ndtype:torch.dtype\nstructure:Null\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.nn.RNN(*args,**kwargs,input_size=None,hidden_size=None,num_layers=None,nonlinearity=None,bias=None,batch_first=None,dropout=None,bidirectional=None)\nParameter description: hidden_size - The number of features in the hidden state h\nConstraints:\ndtype:int\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:[0,inf)\n\n###\n\nSignature: torch.nn.functional.avg_pool1d(input,kernel_size,stride=None,padding=0,ceil_mode=False,count_include_pad=True)\nParameter description: padding - implicit zero paddings on both sides of the input. Can be a single number or a tuple (padW,). Default: 0\nConstraints:\ndtype:bool;int\nstructure:tuple\nshape:Null\nndim:0;1\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.autograd.grad(outputs,inputs,grad_outputs=None,retain_graph=None,create_graph=False,only_inputs=True,allow_unused=False)\nParameter description: outputs(sequence of Tensor) - outputs of the differentiated function.\nConstraints:\ndtype:Null\nstructure:sequence;tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.repeat_interleave(input,repeats,dim=None)\nParameter description: dim(int, optional) - The dimension along which to repeat values.By default, use the flattened input array, and return a flat outputarray.\nConstraints:\ndtype:int\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.nn.Conv1d(in_channels,out_channels,kernel_size,stride=1,padding=0,dilation=1,groups=1,bias=True,padding_mode=zeros)\nParameter description: kernel_size(int or tuple) - Size of the convolving kernel\nConstraints:",
            "constr": {
                "dtype": [
                    "int"
                ],
                "structure": [
                    "tuple"
                ],
                "shape": [],
                "ndim": [],
                "enum": [],
                "range": [
                    "[0,inf)"
                ]
            },
            "signature": "torch.nn.Conv1d(in_channels,out_channels,kernel_size,stride=1,padding=0,dilation=1,groups=1,bias=True,padding_mode=zeros)",
            "descp": "kernel_size(int or tuple) - Size of the convolving kernel"
        },
        {
            "fname": "torch.dist.yaml",
            "arg": "input",
            "prompt": "Signature: torch.distributed.recv(tensor,src=None,group=<objectobject>,tag=0)\nParameter description: tensor(Tensor) - Tensor to fill with received data.\nConstraints:\ndtype:Null\nstructure:tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.div(input,other,out=None)\nParameter description: out(Tensor, optional) - the output tensor.\nConstraints:\ndtype:Null\nstructure:tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.nn.TransformerDecoder(decoder_layer,num_layers,norm=None)\nParameter description: num_layers - the number of sub-decoder-layers in the decoder (required).\nConstraints:\ndtype:int\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:[0,inf)\n\n###\n\nSignature: torch.nn.quantized.functional.interpolate(input,size=None,scale_factor=None,mode=nearest,align_corners=None)\nParameter description: align_corners(bool, optional) - Geometrically, we consider the pixels of the input and output as squares rather than points. If set to `True`, the input and output tensors are aligned by the center points of their corner pixels, preserving the values at the corner pixels. If set to `False`, the input and output tensors are aligned by the corner points of their corner pixels, and the interpolation uses edge value padding for out-of-boundary values, making this operation independent of input size when `scale_factor` is kept the same. This only has an effect when `mode` is `'bilinear'`. Default: `False`\nConstraints:\ndtype:bool\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.gt(input,other,out=None)\nParameter description: out(Tensor, optional) - the output tensor that must be a BoolTensor\nConstraints:\ndtype:booltensor\nstructure:tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.empty_like(input,dtype=None,layout=None,device=None,requires_grad=False,memory_format=torch.preserve_format)\nParameter description: dtype(`torch.dtype`, optional) - the desired data type of returned Tensor. Default: if `None`, defaults to the dtype of `input`.\nConstraints:\ndtype:torch.dtype\nstructure:Null\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.nn.RNN(*args,**kwargs,input_size=None,hidden_size=None,num_layers=None,nonlinearity=None,bias=None,batch_first=None,dropout=None,bidirectional=None)\nParameter description: hidden_size - The number of features in the hidden state h\nConstraints:\ndtype:int\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:[0,inf)\n\n###\n\nSignature: torch.nn.functional.avg_pool1d(input,kernel_size,stride=None,padding=0,ceil_mode=False,count_include_pad=True)\nParameter description: padding - implicit zero paddings on both sides of the input. Can be a single number or a tuple (padW,). Default: 0\nConstraints:\ndtype:bool;int\nstructure:tuple\nshape:Null\nndim:0;1\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.autograd.grad(outputs,inputs,grad_outputs=None,retain_graph=None,create_graph=False,only_inputs=True,allow_unused=False)\nParameter description: outputs(sequence of Tensor) - outputs of the differentiated function.\nConstraints:\ndtype:Null\nstructure:sequence;tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.repeat_interleave(input,repeats,dim=None)\nParameter description: dim(int, optional) - The dimension along which to repeat values.By default, use the flattened input array, and return a flat outputarray.\nConstraints:\ndtype:int\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.dist(input,other,p=2)\nParameter description: input(Tensor) - the input tensor.\nConstraints:",
            "constr": {
                "dtype": [],
                "structure": [
                    "tensor"
                ],
                "shape": [],
                "ndim": [],
                "enum": [],
                "range": []
            },
            "signature": "torch.dist(input,other,p=2)",
            "descp": "input(Tensor) - the input tensor."
        },
        {
            "fname": "torch.set_default_tensor_type.yaml",
            "arg": "t",
            "prompt": "Signature: torch.distributed.recv(tensor,src=None,group=<objectobject>,tag=0)\nParameter description: tensor(Tensor) - Tensor to fill with received data.\nConstraints:\ndtype:Null\nstructure:tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.div(input,other,out=None)\nParameter description: out(Tensor, optional) - the output tensor.\nConstraints:\ndtype:Null\nstructure:tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.nn.TransformerDecoder(decoder_layer,num_layers,norm=None)\nParameter description: num_layers - the number of sub-decoder-layers in the decoder (required).\nConstraints:\ndtype:int\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:[0,inf)\n\n###\n\nSignature: torch.nn.quantized.functional.interpolate(input,size=None,scale_factor=None,mode=nearest,align_corners=None)\nParameter description: align_corners(bool, optional) - Geometrically, we consider the pixels of the input and output as squares rather than points. If set to `True`, the input and output tensors are aligned by the center points of their corner pixels, preserving the values at the corner pixels. If set to `False`, the input and output tensors are aligned by the corner points of their corner pixels, and the interpolation uses edge value padding for out-of-boundary values, making this operation independent of input size when `scale_factor` is kept the same. This only has an effect when `mode` is `'bilinear'`. Default: `False`\nConstraints:\ndtype:bool\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.gt(input,other,out=None)\nParameter description: out(Tensor, optional) - the output tensor that must be a BoolTensor\nConstraints:\ndtype:booltensor\nstructure:tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.empty_like(input,dtype=None,layout=None,device=None,requires_grad=False,memory_format=torch.preserve_format)\nParameter description: dtype(`torch.dtype`, optional) - the desired data type of returned Tensor. Default: if `None`, defaults to the dtype of `input`.\nConstraints:\ndtype:torch.dtype\nstructure:Null\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.nn.RNN(*args,**kwargs,input_size=None,hidden_size=None,num_layers=None,nonlinearity=None,bias=None,batch_first=None,dropout=None,bidirectional=None)\nParameter description: hidden_size - The number of features in the hidden state h\nConstraints:\ndtype:int\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:[0,inf)\n\n###\n\nSignature: torch.nn.functional.avg_pool1d(input,kernel_size,stride=None,padding=0,ceil_mode=False,count_include_pad=True)\nParameter description: padding - implicit zero paddings on both sides of the input. Can be a single number or a tuple (padW,). Default: 0\nConstraints:\ndtype:bool;int\nstructure:tuple\nshape:Null\nndim:0;1\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.autograd.grad(outputs,inputs,grad_outputs=None,retain_graph=None,create_graph=False,only_inputs=True,allow_unused=False)\nParameter description: outputs(sequence of Tensor) - outputs of the differentiated function.\nConstraints:\ndtype:Null\nstructure:sequence;tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.repeat_interleave(input,repeats,dim=None)\nParameter description: dim(int, optional) - The dimension along which to repeat values.By default, use the flattened input array, and return a flat outputarray.\nConstraints:\ndtype:int\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.set_default_tensor_type(t)\nParameter description: t(type or string) - the floating point tensor type or its name\nConstraints:",
            "constr": {
                "dtype": [
                    "torch.dtype",
                    "string",
                    "float"
                ],
                "structure": [
                    "tensor"
                ],
                "shape": [],
                "ndim": [],
                "enum": [],
                "range": []
            },
            "signature": "torch.set_default_tensor_type(t)",
            "descp": "t(type or string) - the floating point tensor type or its name"
        },
        {
            "fname": "torch.set_rng_state.yaml",
            "arg": "new_state",
            "prompt": "Signature: torch.distributed.recv(tensor,src=None,group=<objectobject>,tag=0)\nParameter description: tensor(Tensor) - Tensor to fill with received data.\nConstraints:\ndtype:Null\nstructure:tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.div(input,other,out=None)\nParameter description: out(Tensor, optional) - the output tensor.\nConstraints:\ndtype:Null\nstructure:tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.nn.TransformerDecoder(decoder_layer,num_layers,norm=None)\nParameter description: num_layers - the number of sub-decoder-layers in the decoder (required).\nConstraints:\ndtype:int\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:[0,inf)\n\n###\n\nSignature: torch.nn.quantized.functional.interpolate(input,size=None,scale_factor=None,mode=nearest,align_corners=None)\nParameter description: align_corners(bool, optional) - Geometrically, we consider the pixels of the input and output as squares rather than points. If set to `True`, the input and output tensors are aligned by the center points of their corner pixels, preserving the values at the corner pixels. If set to `False`, the input and output tensors are aligned by the corner points of their corner pixels, and the interpolation uses edge value padding for out-of-boundary values, making this operation independent of input size when `scale_factor` is kept the same. This only has an effect when `mode` is `'bilinear'`. Default: `False`\nConstraints:\ndtype:bool\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.gt(input,other,out=None)\nParameter description: out(Tensor, optional) - the output tensor that must be a BoolTensor\nConstraints:\ndtype:booltensor\nstructure:tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.empty_like(input,dtype=None,layout=None,device=None,requires_grad=False,memory_format=torch.preserve_format)\nParameter description: dtype(`torch.dtype`, optional) - the desired data type of returned Tensor. Default: if `None`, defaults to the dtype of `input`.\nConstraints:\ndtype:torch.dtype\nstructure:Null\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.nn.RNN(*args,**kwargs,input_size=None,hidden_size=None,num_layers=None,nonlinearity=None,bias=None,batch_first=None,dropout=None,bidirectional=None)\nParameter description: hidden_size - The number of features in the hidden state h\nConstraints:\ndtype:int\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:[0,inf)\n\n###\n\nSignature: torch.nn.functional.avg_pool1d(input,kernel_size,stride=None,padding=0,ceil_mode=False,count_include_pad=True)\nParameter description: padding - implicit zero paddings on both sides of the input. Can be a single number or a tuple (padW,). Default: 0\nConstraints:\ndtype:bool;int\nstructure:tuple\nshape:Null\nndim:0;1\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.autograd.grad(outputs,inputs,grad_outputs=None,retain_graph=None,create_graph=False,only_inputs=True,allow_unused=False)\nParameter description: outputs(sequence of Tensor) - outputs of the differentiated function.\nConstraints:\ndtype:Null\nstructure:sequence;tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.repeat_interleave(input,repeats,dim=None)\nParameter description: dim(int, optional) - The dimension along which to repeat values.By default, use the flattened input array, and return a flat outputarray.\nConstraints:\ndtype:int\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.set_rng_state(new_state)\nParameter description: new_state(torch.ByteTensor) - The desired state\nConstraints:",
            "constr": {
                "dtype": [
                    "torch.bytetensor"
                ],
                "structure": [],
                "shape": [],
                "ndim": [],
                "enum": [],
                "range": []
            },
            "signature": "torch.set_rng_state(new_state)",
            "descp": "new_state(torch.ByteTensor) - The desired state"
        },
        {
            "fname": "torch.jit.trace.yaml",
            "arg": "check_trace",
            "prompt": "Signature: torch.distributed.recv(tensor,src=None,group=<objectobject>,tag=0)\nParameter description: tensor(Tensor) - Tensor to fill with received data.\nConstraints:\ndtype:Null\nstructure:tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.div(input,other,out=None)\nParameter description: out(Tensor, optional) - the output tensor.\nConstraints:\ndtype:Null\nstructure:tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.nn.TransformerDecoder(decoder_layer,num_layers,norm=None)\nParameter description: num_layers - the number of sub-decoder-layers in the decoder (required).\nConstraints:\ndtype:int\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:[0,inf)\n\n###\n\nSignature: torch.nn.quantized.functional.interpolate(input,size=None,scale_factor=None,mode=nearest,align_corners=None)\nParameter description: align_corners(bool, optional) - Geometrically, we consider the pixels of the input and output as squares rather than points. If set to `True`, the input and output tensors are aligned by the center points of their corner pixels, preserving the values at the corner pixels. If set to `False`, the input and output tensors are aligned by the corner points of their corner pixels, and the interpolation uses edge value padding for out-of-boundary values, making this operation independent of input size when `scale_factor` is kept the same. This only has an effect when `mode` is `'bilinear'`. Default: `False`\nConstraints:\ndtype:bool\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.gt(input,other,out=None)\nParameter description: out(Tensor, optional) - the output tensor that must be a BoolTensor\nConstraints:\ndtype:booltensor\nstructure:tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.empty_like(input,dtype=None,layout=None,device=None,requires_grad=False,memory_format=torch.preserve_format)\nParameter description: dtype(`torch.dtype`, optional) - the desired data type of returned Tensor. Default: if `None`, defaults to the dtype of `input`.\nConstraints:\ndtype:torch.dtype\nstructure:Null\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.nn.RNN(*args,**kwargs,input_size=None,hidden_size=None,num_layers=None,nonlinearity=None,bias=None,batch_first=None,dropout=None,bidirectional=None)\nParameter description: hidden_size - The number of features in the hidden state h\nConstraints:\ndtype:int\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:[0,inf)\n\n###\n\nSignature: torch.nn.functional.avg_pool1d(input,kernel_size,stride=None,padding=0,ceil_mode=False,count_include_pad=True)\nParameter description: padding - implicit zero paddings on both sides of the input. Can be a single number or a tuple (padW,). Default: 0\nConstraints:\ndtype:bool;int\nstructure:tuple\nshape:Null\nndim:0;1\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.autograd.grad(outputs,inputs,grad_outputs=None,retain_graph=None,create_graph=False,only_inputs=True,allow_unused=False)\nParameter description: outputs(sequence of Tensor) - outputs of the differentiated function.\nConstraints:\ndtype:Null\nstructure:sequence;tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.repeat_interleave(input,repeats,dim=None)\nParameter description: dim(int, optional) - The dimension along which to repeat values.By default, use the flattened input array, and return a flat outputarray.\nConstraints:\ndtype:int\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.jit.trace(func,example_inputs,optimize=None,check_trace=True,check_inputs=None,check_tolerance=1e-5)\nParameter description: check_trace(bool, optional) - Check if the same inputs run through traced code produce the same outputs. Default: `True`. You might want to disable this if, for example, your network contains non- deterministic ops or if you are sure that the network is correct despite a checker failure.\nConstraints:",
            "constr": {
                "dtype": [
                    "bool"
                ],
                "structure": [],
                "shape": [],
                "ndim": [
                    "0"
                ],
                "enum": [],
                "range": []
            },
            "signature": "torch.jit.trace(func,example_inputs,optimize=None,check_trace=True,check_inputs=None,check_tolerance=1e-5)",
            "descp": "check_trace(bool, optional) - Check if the same inputs run through traced code produce the same outputs. Default: `True`. You might want to disable this if, for example, your network contains non- deterministic ops or if you are sure that the network is correct despite a checker failure."
        },
        {
            "fname": "torch.nn.celu.yaml",
            "arg": "inplace",
            "prompt": "Signature: torch.distributed.recv(tensor,src=None,group=<objectobject>,tag=0)\nParameter description: tensor(Tensor) - Tensor to fill with received data.\nConstraints:\ndtype:Null\nstructure:tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.div(input,other,out=None)\nParameter description: out(Tensor, optional) - the output tensor.\nConstraints:\ndtype:Null\nstructure:tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.nn.TransformerDecoder(decoder_layer,num_layers,norm=None)\nParameter description: num_layers - the number of sub-decoder-layers in the decoder (required).\nConstraints:\ndtype:int\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:[0,inf)\n\n###\n\nSignature: torch.nn.quantized.functional.interpolate(input,size=None,scale_factor=None,mode=nearest,align_corners=None)\nParameter description: align_corners(bool, optional) - Geometrically, we consider the pixels of the input and output as squares rather than points. If set to `True`, the input and output tensors are aligned by the center points of their corner pixels, preserving the values at the corner pixels. If set to `False`, the input and output tensors are aligned by the corner points of their corner pixels, and the interpolation uses edge value padding for out-of-boundary values, making this operation independent of input size when `scale_factor` is kept the same. This only has an effect when `mode` is `'bilinear'`. Default: `False`\nConstraints:\ndtype:bool\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.gt(input,other,out=None)\nParameter description: out(Tensor, optional) - the output tensor that must be a BoolTensor\nConstraints:\ndtype:booltensor\nstructure:tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.empty_like(input,dtype=None,layout=None,device=None,requires_grad=False,memory_format=torch.preserve_format)\nParameter description: dtype(`torch.dtype`, optional) - the desired data type of returned Tensor. Default: if `None`, defaults to the dtype of `input`.\nConstraints:\ndtype:torch.dtype\nstructure:Null\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.nn.RNN(*args,**kwargs,input_size=None,hidden_size=None,num_layers=None,nonlinearity=None,bias=None,batch_first=None,dropout=None,bidirectional=None)\nParameter description: hidden_size - The number of features in the hidden state h\nConstraints:\ndtype:int\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:[0,inf)\n\n###\n\nSignature: torch.nn.functional.avg_pool1d(input,kernel_size,stride=None,padding=0,ceil_mode=False,count_include_pad=True)\nParameter description: padding - implicit zero paddings on both sides of the input. Can be a single number or a tuple (padW,). Default: 0\nConstraints:\ndtype:bool;int\nstructure:tuple\nshape:Null\nndim:0;1\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.autograd.grad(outputs,inputs,grad_outputs=None,retain_graph=None,create_graph=False,only_inputs=True,allow_unused=False)\nParameter description: outputs(sequence of Tensor) - outputs of the differentiated function.\nConstraints:\ndtype:Null\nstructure:sequence;tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.repeat_interleave(input,repeats,dim=None)\nParameter description: dim(int, optional) - The dimension along which to repeat values.By default, use the flattened input array, and return a flat outputarray.\nConstraints:\ndtype:int\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.nn.CELU(alpha=1.0,inplace=False)\nParameter description: inplace - can optionally do the operation in-place. Default: `False`\nConstraints:",
            "constr": {
                "dtype": [
                    "bool"
                ],
                "structure": [],
                "shape": [],
                "ndim": [
                    "0"
                ],
                "enum": [],
                "range": []
            },
            "signature": "torch.nn.CELU(alpha=1.0,inplace=False)",
            "descp": "inplace - can optionally do the operation in-place. Default: `False`"
        },
        {
            "fname": "torch.arange.yaml",
            "arg": "end",
            "prompt": "Signature: torch.distributed.recv(tensor,src=None,group=<objectobject>,tag=0)\nParameter description: tensor(Tensor) - Tensor to fill with received data.\nConstraints:\ndtype:Null\nstructure:tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.div(input,other,out=None)\nParameter description: out(Tensor, optional) - the output tensor.\nConstraints:\ndtype:Null\nstructure:tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.nn.TransformerDecoder(decoder_layer,num_layers,norm=None)\nParameter description: num_layers - the number of sub-decoder-layers in the decoder (required).\nConstraints:\ndtype:int\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:[0,inf)\n\n###\n\nSignature: torch.nn.quantized.functional.interpolate(input,size=None,scale_factor=None,mode=nearest,align_corners=None)\nParameter description: align_corners(bool, optional) - Geometrically, we consider the pixels of the input and output as squares rather than points. If set to `True`, the input and output tensors are aligned by the center points of their corner pixels, preserving the values at the corner pixels. If set to `False`, the input and output tensors are aligned by the corner points of their corner pixels, and the interpolation uses edge value padding for out-of-boundary values, making this operation independent of input size when `scale_factor` is kept the same. This only has an effect when `mode` is `'bilinear'`. Default: `False`\nConstraints:\ndtype:bool\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.gt(input,other,out=None)\nParameter description: out(Tensor, optional) - the output tensor that must be a BoolTensor\nConstraints:\ndtype:booltensor\nstructure:tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.empty_like(input,dtype=None,layout=None,device=None,requires_grad=False,memory_format=torch.preserve_format)\nParameter description: dtype(`torch.dtype`, optional) - the desired data type of returned Tensor. Default: if `None`, defaults to the dtype of `input`.\nConstraints:\ndtype:torch.dtype\nstructure:Null\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.nn.RNN(*args,**kwargs,input_size=None,hidden_size=None,num_layers=None,nonlinearity=None,bias=None,batch_first=None,dropout=None,bidirectional=None)\nParameter description: hidden_size - The number of features in the hidden state h\nConstraints:\ndtype:int\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:[0,inf)\n\n###\n\nSignature: torch.nn.functional.avg_pool1d(input,kernel_size,stride=None,padding=0,ceil_mode=False,count_include_pad=True)\nParameter description: padding - implicit zero paddings on both sides of the input. Can be a single number or a tuple (padW,). Default: 0\nConstraints:\ndtype:bool;int\nstructure:tuple\nshape:Null\nndim:0;1\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.autograd.grad(outputs,inputs,grad_outputs=None,retain_graph=None,create_graph=False,only_inputs=True,allow_unused=False)\nParameter description: outputs(sequence of Tensor) - outputs of the differentiated function.\nConstraints:\ndtype:Null\nstructure:sequence;tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.repeat_interleave(input,repeats,dim=None)\nParameter description: dim(int, optional) - The dimension along which to repeat values.By default, use the flattened input array, and return a flat outputarray.\nConstraints:\ndtype:int\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.arange(end,start=0,step=1,out=None,dtype=None,layout=torch.strided,device=None,requires_grad=False)\nParameter description: end(Number) - the ending value for the set of points\nConstraints:",
            "constr": {
                "dtype": [
                    "numeric"
                ],
                "structure": [],
                "shape": [],
                "ndim": [],
                "enum": [],
                "range": []
            },
            "signature": "torch.arange(end,start=0,step=1,out=None,dtype=None,layout=torch.strided,device=None,requires_grad=False)",
            "descp": "end(Number) - the ending value for the set of points"
        },
        {
            "fname": "torch.can_cast.yaml",
            "arg": "to",
            "prompt": "Signature: torch.distributed.recv(tensor,src=None,group=<objectobject>,tag=0)\nParameter description: tensor(Tensor) - Tensor to fill with received data.\nConstraints:\ndtype:Null\nstructure:tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.div(input,other,out=None)\nParameter description: out(Tensor, optional) - the output tensor.\nConstraints:\ndtype:Null\nstructure:tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.nn.TransformerDecoder(decoder_layer,num_layers,norm=None)\nParameter description: num_layers - the number of sub-decoder-layers in the decoder (required).\nConstraints:\ndtype:int\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:[0,inf)\n\n###\n\nSignature: torch.nn.quantized.functional.interpolate(input,size=None,scale_factor=None,mode=nearest,align_corners=None)\nParameter description: align_corners(bool, optional) - Geometrically, we consider the pixels of the input and output as squares rather than points. If set to `True`, the input and output tensors are aligned by the center points of their corner pixels, preserving the values at the corner pixels. If set to `False`, the input and output tensors are aligned by the corner points of their corner pixels, and the interpolation uses edge value padding for out-of-boundary values, making this operation independent of input size when `scale_factor` is kept the same. This only has an effect when `mode` is `'bilinear'`. Default: `False`\nConstraints:\ndtype:bool\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.gt(input,other,out=None)\nParameter description: out(Tensor, optional) - the output tensor that must be a BoolTensor\nConstraints:\ndtype:booltensor\nstructure:tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.empty_like(input,dtype=None,layout=None,device=None,requires_grad=False,memory_format=torch.preserve_format)\nParameter description: dtype(`torch.dtype`, optional) - the desired data type of returned Tensor. Default: if `None`, defaults to the dtype of `input`.\nConstraints:\ndtype:torch.dtype\nstructure:Null\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.nn.RNN(*args,**kwargs,input_size=None,hidden_size=None,num_layers=None,nonlinearity=None,bias=None,batch_first=None,dropout=None,bidirectional=None)\nParameter description: hidden_size - The number of features in the hidden state h\nConstraints:\ndtype:int\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:[0,inf)\n\n###\n\nSignature: torch.nn.functional.avg_pool1d(input,kernel_size,stride=None,padding=0,ceil_mode=False,count_include_pad=True)\nParameter description: padding - implicit zero paddings on both sides of the input. Can be a single number or a tuple (padW,). Default: 0\nConstraints:\ndtype:bool;int\nstructure:tuple\nshape:Null\nndim:0;1\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.autograd.grad(outputs,inputs,grad_outputs=None,retain_graph=None,create_graph=False,only_inputs=True,allow_unused=False)\nParameter description: outputs(sequence of Tensor) - outputs of the differentiated function.\nConstraints:\ndtype:Null\nstructure:sequence;tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.repeat_interleave(input,repeats,dim=None)\nParameter description: dim(int, optional) - The dimension along which to repeat values.By default, use the flattened input array, and return a flat outputarray.\nConstraints:\ndtype:int\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.can_cast(from,to)\nParameter description: to(dpython:type) - The target `torch.dtype`.\nConstraints:",
            "constr": {
                "dtype": [
                    "torch.dtype"
                ],
                "structure": [],
                "shape": [],
                "ndim": [],
                "enum": [],
                "range": []
            },
            "signature": "torch.can_cast(from,to)",
            "descp": "to(dpython:type) - The target `torch.dtype`."
        },
        {
            "fname": "torch.nn.functional.avg_pool1d.yaml",
            "arg": "stride",
            "prompt": "Signature: torch.distributed.recv(tensor,src=None,group=<objectobject>,tag=0)\nParameter description: tensor(Tensor) - Tensor to fill with received data.\nConstraints:\ndtype:Null\nstructure:tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.div(input,other,out=None)\nParameter description: out(Tensor, optional) - the output tensor.\nConstraints:\ndtype:Null\nstructure:tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.nn.TransformerDecoder(decoder_layer,num_layers,norm=None)\nParameter description: num_layers - the number of sub-decoder-layers in the decoder (required).\nConstraints:\ndtype:int\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:[0,inf)\n\n###\n\nSignature: torch.nn.quantized.functional.interpolate(input,size=None,scale_factor=None,mode=nearest,align_corners=None)\nParameter description: align_corners(bool, optional) - Geometrically, we consider the pixels of the input and output as squares rather than points. If set to `True`, the input and output tensors are aligned by the center points of their corner pixels, preserving the values at the corner pixels. If set to `False`, the input and output tensors are aligned by the corner points of their corner pixels, and the interpolation uses edge value padding for out-of-boundary values, making this operation independent of input size when `scale_factor` is kept the same. This only has an effect when `mode` is `'bilinear'`. Default: `False`\nConstraints:\ndtype:bool\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.gt(input,other,out=None)\nParameter description: out(Tensor, optional) - the output tensor that must be a BoolTensor\nConstraints:\ndtype:booltensor\nstructure:tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.empty_like(input,dtype=None,layout=None,device=None,requires_grad=False,memory_format=torch.preserve_format)\nParameter description: dtype(`torch.dtype`, optional) - the desired data type of returned Tensor. Default: if `None`, defaults to the dtype of `input`.\nConstraints:\ndtype:torch.dtype\nstructure:Null\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.nn.RNN(*args,**kwargs,input_size=None,hidden_size=None,num_layers=None,nonlinearity=None,bias=None,batch_first=None,dropout=None,bidirectional=None)\nParameter description: hidden_size - The number of features in the hidden state h\nConstraints:\ndtype:int\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:[0,inf)\n\n###\n\nSignature: torch.nn.functional.avg_pool1d(input,kernel_size,stride=None,padding=0,ceil_mode=False,count_include_pad=True)\nParameter description: padding - implicit zero paddings on both sides of the input. Can be a single number or a tuple (padW,). Default: 0\nConstraints:\ndtype:bool;int\nstructure:tuple\nshape:Null\nndim:0;1\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.autograd.grad(outputs,inputs,grad_outputs=None,retain_graph=None,create_graph=False,only_inputs=True,allow_unused=False)\nParameter description: outputs(sequence of Tensor) - outputs of the differentiated function.\nConstraints:\ndtype:Null\nstructure:sequence;tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.repeat_interleave(input,repeats,dim=None)\nParameter description: dim(int, optional) - The dimension along which to repeat values.By default, use the flattened input array, and return a flat outputarray.\nConstraints:\ndtype:int\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.nn.functional.avg_pool1d(input,kernel_size,stride=None,padding=0,ceil_mode=False,count_include_pad=True)\nParameter description: stride - the stride of the window. Can be a single number or a tuple (sW,). Default: `kernel_size`\nConstraints:",
            "constr": {
                "dtype": [
                    "numeric",
                    "int"
                ],
                "structure": [
                    "tuple"
                ],
                "shape": [],
                "ndim": [
                    "0",
                    "1"
                ],
                "enum": [],
                "range": [
                    "[0,inf)"
                ]
            },
            "signature": "torch.nn.functional.avg_pool1d(input,kernel_size,stride=None,padding=0,ceil_mode=False,count_include_pad=True)",
            "descp": "stride - the stride of the window. Can be a single number or a tuple (sW,). Default: `kernel_size`"
        },
        {
            "fname": "torch.nn.rrelu.yaml",
            "arg": "lower",
            "prompt": "Signature: torch.distributed.recv(tensor,src=None,group=<objectobject>,tag=0)\nParameter description: tensor(Tensor) - Tensor to fill with received data.\nConstraints:\ndtype:Null\nstructure:tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.div(input,other,out=None)\nParameter description: out(Tensor, optional) - the output tensor.\nConstraints:\ndtype:Null\nstructure:tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.nn.TransformerDecoder(decoder_layer,num_layers,norm=None)\nParameter description: num_layers - the number of sub-decoder-layers in the decoder (required).\nConstraints:\ndtype:int\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:[0,inf)\n\n###\n\nSignature: torch.nn.quantized.functional.interpolate(input,size=None,scale_factor=None,mode=nearest,align_corners=None)\nParameter description: align_corners(bool, optional) - Geometrically, we consider the pixels of the input and output as squares rather than points. If set to `True`, the input and output tensors are aligned by the center points of their corner pixels, preserving the values at the corner pixels. If set to `False`, the input and output tensors are aligned by the corner points of their corner pixels, and the interpolation uses edge value padding for out-of-boundary values, making this operation independent of input size when `scale_factor` is kept the same. This only has an effect when `mode` is `'bilinear'`. Default: `False`\nConstraints:\ndtype:bool\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.gt(input,other,out=None)\nParameter description: out(Tensor, optional) - the output tensor that must be a BoolTensor\nConstraints:\ndtype:booltensor\nstructure:tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.empty_like(input,dtype=None,layout=None,device=None,requires_grad=False,memory_format=torch.preserve_format)\nParameter description: dtype(`torch.dtype`, optional) - the desired data type of returned Tensor. Default: if `None`, defaults to the dtype of `input`.\nConstraints:\ndtype:torch.dtype\nstructure:Null\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.nn.RNN(*args,**kwargs,input_size=None,hidden_size=None,num_layers=None,nonlinearity=None,bias=None,batch_first=None,dropout=None,bidirectional=None)\nParameter description: hidden_size - The number of features in the hidden state h\nConstraints:\ndtype:int\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:[0,inf)\n\n###\n\nSignature: torch.nn.functional.avg_pool1d(input,kernel_size,stride=None,padding=0,ceil_mode=False,count_include_pad=True)\nParameter description: padding - implicit zero paddings on both sides of the input. Can be a single number or a tuple (padW,). Default: 0\nConstraints:\ndtype:bool;int\nstructure:tuple\nshape:Null\nndim:0;1\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.autograd.grad(outputs,inputs,grad_outputs=None,retain_graph=None,create_graph=False,only_inputs=True,allow_unused=False)\nParameter description: outputs(sequence of Tensor) - outputs of the differentiated function.\nConstraints:\ndtype:Null\nstructure:sequence;tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.repeat_interleave(input,repeats,dim=None)\nParameter description: dim(int, optional) - The dimension along which to repeat values.By default, use the flattened input array, and return a flat outputarray.\nConstraints:\ndtype:int\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.nn.RReLU(lower=0.125,upper=0.3333333333333333,inplace=False)\nParameter description: lower - lower bound of the uniform distribution. Default:  1/8 \nConstraints:",
            "constr": {
                "dtype": [
                    "numeric",
                    "float"
                ],
                "structure": [],
                "shape": [],
                "ndim": [
                    "0"
                ],
                "enum": [],
                "range": []
            },
            "signature": "torch.nn.RReLU(lower=0.125,upper=0.3333333333333333,inplace=False)",
            "descp": "lower - lower bound of the uniform distribution. Default:  1/8 "
        },
        {
            "fname": "torch.nn.functional.avg_pool2d.yaml",
            "arg": "stride",
            "prompt": "Signature: torch.distributed.recv(tensor,src=None,group=<objectobject>,tag=0)\nParameter description: tensor(Tensor) - Tensor to fill with received data.\nConstraints:\ndtype:Null\nstructure:tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.div(input,other,out=None)\nParameter description: out(Tensor, optional) - the output tensor.\nConstraints:\ndtype:Null\nstructure:tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.nn.TransformerDecoder(decoder_layer,num_layers,norm=None)\nParameter description: num_layers - the number of sub-decoder-layers in the decoder (required).\nConstraints:\ndtype:int\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:[0,inf)\n\n###\n\nSignature: torch.nn.quantized.functional.interpolate(input,size=None,scale_factor=None,mode=nearest,align_corners=None)\nParameter description: align_corners(bool, optional) - Geometrically, we consider the pixels of the input and output as squares rather than points. If set to `True`, the input and output tensors are aligned by the center points of their corner pixels, preserving the values at the corner pixels. If set to `False`, the input and output tensors are aligned by the corner points of their corner pixels, and the interpolation uses edge value padding for out-of-boundary values, making this operation independent of input size when `scale_factor` is kept the same. This only has an effect when `mode` is `'bilinear'`. Default: `False`\nConstraints:\ndtype:bool\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.gt(input,other,out=None)\nParameter description: out(Tensor, optional) - the output tensor that must be a BoolTensor\nConstraints:\ndtype:booltensor\nstructure:tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.empty_like(input,dtype=None,layout=None,device=None,requires_grad=False,memory_format=torch.preserve_format)\nParameter description: dtype(`torch.dtype`, optional) - the desired data type of returned Tensor. Default: if `None`, defaults to the dtype of `input`.\nConstraints:\ndtype:torch.dtype\nstructure:Null\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.nn.RNN(*args,**kwargs,input_size=None,hidden_size=None,num_layers=None,nonlinearity=None,bias=None,batch_first=None,dropout=None,bidirectional=None)\nParameter description: hidden_size - The number of features in the hidden state h\nConstraints:\ndtype:int\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:[0,inf)\n\n###\n\nSignature: torch.nn.functional.avg_pool1d(input,kernel_size,stride=None,padding=0,ceil_mode=False,count_include_pad=True)\nParameter description: padding - implicit zero paddings on both sides of the input. Can be a single number or a tuple (padW,). Default: 0\nConstraints:\ndtype:bool;int\nstructure:tuple\nshape:Null\nndim:0;1\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.autograd.grad(outputs,inputs,grad_outputs=None,retain_graph=None,create_graph=False,only_inputs=True,allow_unused=False)\nParameter description: outputs(sequence of Tensor) - outputs of the differentiated function.\nConstraints:\ndtype:Null\nstructure:sequence;tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.repeat_interleave(input,repeats,dim=None)\nParameter description: dim(int, optional) - The dimension along which to repeat values.By default, use the flattened input array, and return a flat outputarray.\nConstraints:\ndtype:int\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.nn.functional.avg_pool2d(input,kernel_size,stride=None,padding=0,ceil_mode=False,count_include_pad=True,divisor_override=None)\nParameter description: stride - stride of the pooling operation. Can be a single number or a tuple (sH, sW). Default: `kernel_size`\nConstraints:",
            "constr": {
                "dtype": [
                    "numeric",
                    "int"
                ],
                "structure": [
                    "tuple"
                ],
                "shape": [],
                "ndim": [
                    "0",
                    "1"
                ],
                "enum": [],
                "range": [
                    "[0,inf)"
                ]
            },
            "signature": "torch.nn.functional.avg_pool2d(input,kernel_size,stride=None,padding=0,ceil_mode=False,count_include_pad=True,divisor_override=None)",
            "descp": "stride - stride of the pooling operation. Can be a single number or a tuple (sH, sW). Default: `kernel_size`"
        },
        {
            "fname": "torch.nn.functional.dropout.yaml",
            "arg": "inplace",
            "prompt": "Signature: torch.distributed.recv(tensor,src=None,group=<objectobject>,tag=0)\nParameter description: tensor(Tensor) - Tensor to fill with received data.\nConstraints:\ndtype:Null\nstructure:tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.div(input,other,out=None)\nParameter description: out(Tensor, optional) - the output tensor.\nConstraints:\ndtype:Null\nstructure:tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.nn.TransformerDecoder(decoder_layer,num_layers,norm=None)\nParameter description: num_layers - the number of sub-decoder-layers in the decoder (required).\nConstraints:\ndtype:int\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:[0,inf)\n\n###\n\nSignature: torch.nn.quantized.functional.interpolate(input,size=None,scale_factor=None,mode=nearest,align_corners=None)\nParameter description: align_corners(bool, optional) - Geometrically, we consider the pixels of the input and output as squares rather than points. If set to `True`, the input and output tensors are aligned by the center points of their corner pixels, preserving the values at the corner pixels. If set to `False`, the input and output tensors are aligned by the corner points of their corner pixels, and the interpolation uses edge value padding for out-of-boundary values, making this operation independent of input size when `scale_factor` is kept the same. This only has an effect when `mode` is `'bilinear'`. Default: `False`\nConstraints:\ndtype:bool\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.gt(input,other,out=None)\nParameter description: out(Tensor, optional) - the output tensor that must be a BoolTensor\nConstraints:\ndtype:booltensor\nstructure:tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.empty_like(input,dtype=None,layout=None,device=None,requires_grad=False,memory_format=torch.preserve_format)\nParameter description: dtype(`torch.dtype`, optional) - the desired data type of returned Tensor. Default: if `None`, defaults to the dtype of `input`.\nConstraints:\ndtype:torch.dtype\nstructure:Null\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.nn.RNN(*args,**kwargs,input_size=None,hidden_size=None,num_layers=None,nonlinearity=None,bias=None,batch_first=None,dropout=None,bidirectional=None)\nParameter description: hidden_size - The number of features in the hidden state h\nConstraints:\ndtype:int\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:[0,inf)\n\n###\n\nSignature: torch.nn.functional.avg_pool1d(input,kernel_size,stride=None,padding=0,ceil_mode=False,count_include_pad=True)\nParameter description: padding - implicit zero paddings on both sides of the input. Can be a single number or a tuple (padW,). Default: 0\nConstraints:\ndtype:bool;int\nstructure:tuple\nshape:Null\nndim:0;1\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.autograd.grad(outputs,inputs,grad_outputs=None,retain_graph=None,create_graph=False,only_inputs=True,allow_unused=False)\nParameter description: outputs(sequence of Tensor) - outputs of the differentiated function.\nConstraints:\ndtype:Null\nstructure:sequence;tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.repeat_interleave(input,repeats,dim=None)\nParameter description: dim(int, optional) - The dimension along which to repeat values.By default, use the flattened input array, and return a flat outputarray.\nConstraints:\ndtype:int\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.nn.functional.dropout(input,p=0.5,training=True,inplace=False)\nParameter description: inplace - If set to `True`, will do this operation in-place. Default: `False`\nConstraints:",
            "constr": {
                "dtype": [
                    "bool"
                ],
                "structure": [],
                "shape": [],
                "ndim": [
                    "0"
                ],
                "enum": [],
                "range": []
            },
            "signature": "torch.nn.functional.dropout(input,p=0.5,training=True,inplace=False)",
            "descp": "inplace - If set to `True`, will do this operation in-place. Default: `False`"
        },
        {
            "fname": "torch.var2.yaml",
            "arg": "dim",
            "prompt": "Signature: torch.distributed.recv(tensor,src=None,group=<objectobject>,tag=0)\nParameter description: tensor(Tensor) - Tensor to fill with received data.\nConstraints:\ndtype:Null\nstructure:tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.div(input,other,out=None)\nParameter description: out(Tensor, optional) - the output tensor.\nConstraints:\ndtype:Null\nstructure:tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.nn.TransformerDecoder(decoder_layer,num_layers,norm=None)\nParameter description: num_layers - the number of sub-decoder-layers in the decoder (required).\nConstraints:\ndtype:int\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:[0,inf)\n\n###\n\nSignature: torch.nn.quantized.functional.interpolate(input,size=None,scale_factor=None,mode=nearest,align_corners=None)\nParameter description: align_corners(bool, optional) - Geometrically, we consider the pixels of the input and output as squares rather than points. If set to `True`, the input and output tensors are aligned by the center points of their corner pixels, preserving the values at the corner pixels. If set to `False`, the input and output tensors are aligned by the corner points of their corner pixels, and the interpolation uses edge value padding for out-of-boundary values, making this operation independent of input size when `scale_factor` is kept the same. This only has an effect when `mode` is `'bilinear'`. Default: `False`\nConstraints:\ndtype:bool\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.gt(input,other,out=None)\nParameter description: out(Tensor, optional) - the output tensor that must be a BoolTensor\nConstraints:\ndtype:booltensor\nstructure:tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.empty_like(input,dtype=None,layout=None,device=None,requires_grad=False,memory_format=torch.preserve_format)\nParameter description: dtype(`torch.dtype`, optional) - the desired data type of returned Tensor. Default: if `None`, defaults to the dtype of `input`.\nConstraints:\ndtype:torch.dtype\nstructure:Null\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.nn.RNN(*args,**kwargs,input_size=None,hidden_size=None,num_layers=None,nonlinearity=None,bias=None,batch_first=None,dropout=None,bidirectional=None)\nParameter description: hidden_size - The number of features in the hidden state h\nConstraints:\ndtype:int\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:[0,inf)\n\n###\n\nSignature: torch.nn.functional.avg_pool1d(input,kernel_size,stride=None,padding=0,ceil_mode=False,count_include_pad=True)\nParameter description: padding - implicit zero paddings on both sides of the input. Can be a single number or a tuple (padW,). Default: 0\nConstraints:\ndtype:bool;int\nstructure:tuple\nshape:Null\nndim:0;1\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.autograd.grad(outputs,inputs,grad_outputs=None,retain_graph=None,create_graph=False,only_inputs=True,allow_unused=False)\nParameter description: outputs(sequence of Tensor) - outputs of the differentiated function.\nConstraints:\ndtype:Null\nstructure:sequence;tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.repeat_interleave(input,repeats,dim=None)\nParameter description: dim(int, optional) - The dimension along which to repeat values.By default, use the flattened input array, and return a flat outputarray.\nConstraints:\ndtype:int\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.var(input,dim,keepdim=False,unbiased=True,out=None)\nParameter description: dim(int or tuple of python:ints) - the dimension or dimensions to reduce.\nConstraints:",
            "constr": {
                "dtype": [
                    "int"
                ],
                "structure": [
                    "tuple"
                ],
                "shape": [],
                "ndim": [
                    "0",
                    "1"
                ],
                "enum": [],
                "range": []
            },
            "signature": "torch.var(input,dim,keepdim=False,unbiased=True,out=None)",
            "descp": "dim(int or tuple of python:ints) - the dimension or dimensions to reduce."
        },
        {
            "fname": "torch.nn.functional.poisson_nll_loss.yaml",
            "arg": "eps",
            "prompt": "Signature: torch.distributed.recv(tensor,src=None,group=<objectobject>,tag=0)\nParameter description: tensor(Tensor) - Tensor to fill with received data.\nConstraints:\ndtype:Null\nstructure:tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.div(input,other,out=None)\nParameter description: out(Tensor, optional) - the output tensor.\nConstraints:\ndtype:Null\nstructure:tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.nn.TransformerDecoder(decoder_layer,num_layers,norm=None)\nParameter description: num_layers - the number of sub-decoder-layers in the decoder (required).\nConstraints:\ndtype:int\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:[0,inf)\n\n###\n\nSignature: torch.nn.quantized.functional.interpolate(input,size=None,scale_factor=None,mode=nearest,align_corners=None)\nParameter description: align_corners(bool, optional) - Geometrically, we consider the pixels of the input and output as squares rather than points. If set to `True`, the input and output tensors are aligned by the center points of their corner pixels, preserving the values at the corner pixels. If set to `False`, the input and output tensors are aligned by the corner points of their corner pixels, and the interpolation uses edge value padding for out-of-boundary values, making this operation independent of input size when `scale_factor` is kept the same. This only has an effect when `mode` is `'bilinear'`. Default: `False`\nConstraints:\ndtype:bool\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.gt(input,other,out=None)\nParameter description: out(Tensor, optional) - the output tensor that must be a BoolTensor\nConstraints:\ndtype:booltensor\nstructure:tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.empty_like(input,dtype=None,layout=None,device=None,requires_grad=False,memory_format=torch.preserve_format)\nParameter description: dtype(`torch.dtype`, optional) - the desired data type of returned Tensor. Default: if `None`, defaults to the dtype of `input`.\nConstraints:\ndtype:torch.dtype\nstructure:Null\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.nn.RNN(*args,**kwargs,input_size=None,hidden_size=None,num_layers=None,nonlinearity=None,bias=None,batch_first=None,dropout=None,bidirectional=None)\nParameter description: hidden_size - The number of features in the hidden state h\nConstraints:\ndtype:int\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:[0,inf)\n\n###\n\nSignature: torch.nn.functional.avg_pool1d(input,kernel_size,stride=None,padding=0,ceil_mode=False,count_include_pad=True)\nParameter description: padding - implicit zero paddings on both sides of the input. Can be a single number or a tuple (padW,). Default: 0\nConstraints:\ndtype:bool;int\nstructure:tuple\nshape:Null\nndim:0;1\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.autograd.grad(outputs,inputs,grad_outputs=None,retain_graph=None,create_graph=False,only_inputs=True,allow_unused=False)\nParameter description: outputs(sequence of Tensor) - outputs of the differentiated function.\nConstraints:\ndtype:Null\nstructure:sequence;tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.repeat_interleave(input,repeats,dim=None)\nParameter description: dim(int, optional) - The dimension along which to repeat values.By default, use the flattened input array, and return a flat outputarray.\nConstraints:\ndtype:int\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.nn.functional.poisson_nll_loss(input,target,log_input=True,full=False,size_average=None,eps=1e-08,reduce=None,reduction=mean)\nParameter description: eps(float, optional) - Small value to avoid evaluation of  log(0)  when `log_input`=``False``. Default: 1e-8\nConstraints:",
            "constr": {
                "dtype": [
                    "numeric",
                    "float"
                ],
                "structure": [],
                "shape": [],
                "ndim": [
                    "0"
                ],
                "enum": [],
                "range": []
            },
            "signature": "torch.nn.functional.poisson_nll_loss(input,target,log_input=True,full=False,size_average=None,eps=1e-08,reduce=None,reduction=mean)",
            "descp": "eps(float, optional) - Small value to avoid evaluation of  log(0)  when `log_input`=``False``. Default: 1e-8"
        },
        {
            "fname": "torch.nn.functional.conv1d.yaml",
            "arg": "stride",
            "prompt": "Signature: torch.distributed.recv(tensor,src=None,group=<objectobject>,tag=0)\nParameter description: tensor(Tensor) - Tensor to fill with received data.\nConstraints:\ndtype:Null\nstructure:tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.div(input,other,out=None)\nParameter description: out(Tensor, optional) - the output tensor.\nConstraints:\ndtype:Null\nstructure:tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.nn.TransformerDecoder(decoder_layer,num_layers,norm=None)\nParameter description: num_layers - the number of sub-decoder-layers in the decoder (required).\nConstraints:\ndtype:int\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:[0,inf)\n\n###\n\nSignature: torch.nn.quantized.functional.interpolate(input,size=None,scale_factor=None,mode=nearest,align_corners=None)\nParameter description: align_corners(bool, optional) - Geometrically, we consider the pixels of the input and output as squares rather than points. If set to `True`, the input and output tensors are aligned by the center points of their corner pixels, preserving the values at the corner pixels. If set to `False`, the input and output tensors are aligned by the corner points of their corner pixels, and the interpolation uses edge value padding for out-of-boundary values, making this operation independent of input size when `scale_factor` is kept the same. This only has an effect when `mode` is `'bilinear'`. Default: `False`\nConstraints:\ndtype:bool\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.gt(input,other,out=None)\nParameter description: out(Tensor, optional) - the output tensor that must be a BoolTensor\nConstraints:\ndtype:booltensor\nstructure:tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.empty_like(input,dtype=None,layout=None,device=None,requires_grad=False,memory_format=torch.preserve_format)\nParameter description: dtype(`torch.dtype`, optional) - the desired data type of returned Tensor. Default: if `None`, defaults to the dtype of `input`.\nConstraints:\ndtype:torch.dtype\nstructure:Null\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.nn.RNN(*args,**kwargs,input_size=None,hidden_size=None,num_layers=None,nonlinearity=None,bias=None,batch_first=None,dropout=None,bidirectional=None)\nParameter description: hidden_size - The number of features in the hidden state h\nConstraints:\ndtype:int\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:[0,inf)\n\n###\n\nSignature: torch.nn.functional.avg_pool1d(input,kernel_size,stride=None,padding=0,ceil_mode=False,count_include_pad=True)\nParameter description: padding - implicit zero paddings on both sides of the input. Can be a single number or a tuple (padW,). Default: 0\nConstraints:\ndtype:bool;int\nstructure:tuple\nshape:Null\nndim:0;1\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.autograd.grad(outputs,inputs,grad_outputs=None,retain_graph=None,create_graph=False,only_inputs=True,allow_unused=False)\nParameter description: outputs(sequence of Tensor) - outputs of the differentiated function.\nConstraints:\ndtype:Null\nstructure:sequence;tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.repeat_interleave(input,repeats,dim=None)\nParameter description: dim(int, optional) - The dimension along which to repeat values.By default, use the flattened input array, and return a flat outputarray.\nConstraints:\ndtype:int\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.nn.functional.conv1d(input,weight,bias=None,stride=1,padding=0,dilation=1,groups=1)\nParameter description: stride - the stride of the convolving kernel. Can be a single number or a one-element tuple (sW,). Default: 1\nConstraints:",
            "constr": {
                "dtype": [
                    "numeric",
                    "int"
                ],
                "structure": [
                    "tuple"
                ],
                "shape": [
                    "[1]"
                ],
                "ndim": [
                    "0",
                    "1"
                ],
                "enum": [],
                "range": [
                    "[0,inf)"
                ]
            },
            "signature": "torch.nn.functional.conv1d(input,weight,bias=None,stride=1,padding=0,dilation=1,groups=1)",
            "descp": "stride - the stride of the convolving kernel. Can be a single number or a one-element tuple (sW,). Default: 1"
        },
        {
            "fname": "torch.autograd.functional.vjp.yaml",
            "arg": "strict",
            "prompt": "Signature: torch.distributed.recv(tensor,src=None,group=<objectobject>,tag=0)\nParameter description: tensor(Tensor) - Tensor to fill with received data.\nConstraints:\ndtype:Null\nstructure:tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.div(input,other,out=None)\nParameter description: out(Tensor, optional) - the output tensor.\nConstraints:\ndtype:Null\nstructure:tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.nn.TransformerDecoder(decoder_layer,num_layers,norm=None)\nParameter description: num_layers - the number of sub-decoder-layers in the decoder (required).\nConstraints:\ndtype:int\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:[0,inf)\n\n###\n\nSignature: torch.nn.quantized.functional.interpolate(input,size=None,scale_factor=None,mode=nearest,align_corners=None)\nParameter description: align_corners(bool, optional) - Geometrically, we consider the pixels of the input and output as squares rather than points. If set to `True`, the input and output tensors are aligned by the center points of their corner pixels, preserving the values at the corner pixels. If set to `False`, the input and output tensors are aligned by the corner points of their corner pixels, and the interpolation uses edge value padding for out-of-boundary values, making this operation independent of input size when `scale_factor` is kept the same. This only has an effect when `mode` is `'bilinear'`. Default: `False`\nConstraints:\ndtype:bool\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.gt(input,other,out=None)\nParameter description: out(Tensor, optional) - the output tensor that must be a BoolTensor\nConstraints:\ndtype:booltensor\nstructure:tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.empty_like(input,dtype=None,layout=None,device=None,requires_grad=False,memory_format=torch.preserve_format)\nParameter description: dtype(`torch.dtype`, optional) - the desired data type of returned Tensor. Default: if `None`, defaults to the dtype of `input`.\nConstraints:\ndtype:torch.dtype\nstructure:Null\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.nn.RNN(*args,**kwargs,input_size=None,hidden_size=None,num_layers=None,nonlinearity=None,bias=None,batch_first=None,dropout=None,bidirectional=None)\nParameter description: hidden_size - The number of features in the hidden state h\nConstraints:\ndtype:int\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:[0,inf)\n\n###\n\nSignature: torch.nn.functional.avg_pool1d(input,kernel_size,stride=None,padding=0,ceil_mode=False,count_include_pad=True)\nParameter description: padding - implicit zero paddings on both sides of the input. Can be a single number or a tuple (padW,). Default: 0\nConstraints:\ndtype:bool;int\nstructure:tuple\nshape:Null\nndim:0;1\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.autograd.grad(outputs,inputs,grad_outputs=None,retain_graph=None,create_graph=False,only_inputs=True,allow_unused=False)\nParameter description: outputs(sequence of Tensor) - outputs of the differentiated function.\nConstraints:\ndtype:Null\nstructure:sequence;tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.repeat_interleave(input,repeats,dim=None)\nParameter description: dim(int, optional) - The dimension along which to repeat values.By default, use the flattened input array, and return a flat outputarray.\nConstraints:\ndtype:int\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.autograd.functional.vjp(func,inputs,v=None,create_graph=False,strict=False)\nParameter description: strict(bool, optional) - If `True`, an error will be raised when we detect that there exists an input such that all the outputs are independent of it. If `False`, we return a Tensor of zeros as the vjp for said inputs, which is the expected mathematical value. Defaults to `False`.\nConstraints:",
            "constr": {
                "dtype": [
                    "bool"
                ],
                "structure": [],
                "shape": [],
                "ndim": [
                    "0"
                ],
                "enum": [],
                "range": []
            },
            "signature": "torch.autograd.functional.vjp(func,inputs,v=None,create_graph=False,strict=False)",
            "descp": "strict(bool, optional) - If `True`, an error will be raised when we detect that there exists an input such that all the outputs are independent of it. If `False`, we return a Tensor of zeros as the vjp for said inputs, which is the expected mathematical value. Defaults to `False`."
        },
        {
            "fname": "torch.nn.linear.yaml",
            "arg": "in_features",
            "prompt": "Signature: torch.distributed.recv(tensor,src=None,group=<objectobject>,tag=0)\nParameter description: tensor(Tensor) - Tensor to fill with received data.\nConstraints:\ndtype:Null\nstructure:tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.div(input,other,out=None)\nParameter description: out(Tensor, optional) - the output tensor.\nConstraints:\ndtype:Null\nstructure:tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.nn.TransformerDecoder(decoder_layer,num_layers,norm=None)\nParameter description: num_layers - the number of sub-decoder-layers in the decoder (required).\nConstraints:\ndtype:int\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:[0,inf)\n\n###\n\nSignature: torch.nn.quantized.functional.interpolate(input,size=None,scale_factor=None,mode=nearest,align_corners=None)\nParameter description: align_corners(bool, optional) - Geometrically, we consider the pixels of the input and output as squares rather than points. If set to `True`, the input and output tensors are aligned by the center points of their corner pixels, preserving the values at the corner pixels. If set to `False`, the input and output tensors are aligned by the corner points of their corner pixels, and the interpolation uses edge value padding for out-of-boundary values, making this operation independent of input size when `scale_factor` is kept the same. This only has an effect when `mode` is `'bilinear'`. Default: `False`\nConstraints:\ndtype:bool\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.gt(input,other,out=None)\nParameter description: out(Tensor, optional) - the output tensor that must be a BoolTensor\nConstraints:\ndtype:booltensor\nstructure:tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.empty_like(input,dtype=None,layout=None,device=None,requires_grad=False,memory_format=torch.preserve_format)\nParameter description: dtype(`torch.dtype`, optional) - the desired data type of returned Tensor. Default: if `None`, defaults to the dtype of `input`.\nConstraints:\ndtype:torch.dtype\nstructure:Null\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.nn.RNN(*args,**kwargs,input_size=None,hidden_size=None,num_layers=None,nonlinearity=None,bias=None,batch_first=None,dropout=None,bidirectional=None)\nParameter description: hidden_size - The number of features in the hidden state h\nConstraints:\ndtype:int\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:[0,inf)\n\n###\n\nSignature: torch.nn.functional.avg_pool1d(input,kernel_size,stride=None,padding=0,ceil_mode=False,count_include_pad=True)\nParameter description: padding - implicit zero paddings on both sides of the input. Can be a single number or a tuple (padW,). Default: 0\nConstraints:\ndtype:bool;int\nstructure:tuple\nshape:Null\nndim:0;1\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.autograd.grad(outputs,inputs,grad_outputs=None,retain_graph=None,create_graph=False,only_inputs=True,allow_unused=False)\nParameter description: outputs(sequence of Tensor) - outputs of the differentiated function.\nConstraints:\ndtype:Null\nstructure:sequence;tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.repeat_interleave(input,repeats,dim=None)\nParameter description: dim(int, optional) - The dimension along which to repeat values.By default, use the flattened input array, and return a flat outputarray.\nConstraints:\ndtype:int\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.nn.Linear(in_features,out_features,bias=True)\nParameter description: in_features - size of each input sample\nConstraints:",
            "constr": {
                "dtype": [
                    "int"
                ],
                "structure": [],
                "shape": [],
                "ndim": [],
                "enum": [],
                "range": [
                    "[0,inf)"
                ]
            },
            "signature": "torch.nn.Linear(in_features,out_features,bias=True)",
            "descp": "in_features - size of each input sample"
        },
        {
            "fname": "torch.nn.functional.cosine_similarity.yaml",
            "arg": "x1",
            "prompt": "Signature: torch.distributed.recv(tensor,src=None,group=<objectobject>,tag=0)\nParameter description: tensor(Tensor) - Tensor to fill with received data.\nConstraints:\ndtype:Null\nstructure:tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.div(input,other,out=None)\nParameter description: out(Tensor, optional) - the output tensor.\nConstraints:\ndtype:Null\nstructure:tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.nn.TransformerDecoder(decoder_layer,num_layers,norm=None)\nParameter description: num_layers - the number of sub-decoder-layers in the decoder (required).\nConstraints:\ndtype:int\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:[0,inf)\n\n###\n\nSignature: torch.nn.quantized.functional.interpolate(input,size=None,scale_factor=None,mode=nearest,align_corners=None)\nParameter description: align_corners(bool, optional) - Geometrically, we consider the pixels of the input and output as squares rather than points. If set to `True`, the input and output tensors are aligned by the center points of their corner pixels, preserving the values at the corner pixels. If set to `False`, the input and output tensors are aligned by the corner points of their corner pixels, and the interpolation uses edge value padding for out-of-boundary values, making this operation independent of input size when `scale_factor` is kept the same. This only has an effect when `mode` is `'bilinear'`. Default: `False`\nConstraints:\ndtype:bool\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.gt(input,other,out=None)\nParameter description: out(Tensor, optional) - the output tensor that must be a BoolTensor\nConstraints:\ndtype:booltensor\nstructure:tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.empty_like(input,dtype=None,layout=None,device=None,requires_grad=False,memory_format=torch.preserve_format)\nParameter description: dtype(`torch.dtype`, optional) - the desired data type of returned Tensor. Default: if `None`, defaults to the dtype of `input`.\nConstraints:\ndtype:torch.dtype\nstructure:Null\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.nn.RNN(*args,**kwargs,input_size=None,hidden_size=None,num_layers=None,nonlinearity=None,bias=None,batch_first=None,dropout=None,bidirectional=None)\nParameter description: hidden_size - The number of features in the hidden state h\nConstraints:\ndtype:int\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:[0,inf)\n\n###\n\nSignature: torch.nn.functional.avg_pool1d(input,kernel_size,stride=None,padding=0,ceil_mode=False,count_include_pad=True)\nParameter description: padding - implicit zero paddings on both sides of the input. Can be a single number or a tuple (padW,). Default: 0\nConstraints:\ndtype:bool;int\nstructure:tuple\nshape:Null\nndim:0;1\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.autograd.grad(outputs,inputs,grad_outputs=None,retain_graph=None,create_graph=False,only_inputs=True,allow_unused=False)\nParameter description: outputs(sequence of Tensor) - outputs of the differentiated function.\nConstraints:\ndtype:Null\nstructure:sequence;tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.repeat_interleave(input,repeats,dim=None)\nParameter description: dim(int, optional) - The dimension along which to repeat values.By default, use the flattened input array, and return a flat outputarray.\nConstraints:\ndtype:int\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.nn.functional.cosine_similarity(x1,x2,dim=1,eps=1e-8)\nParameter description: x1(Tensor) - First input.\nConstraints:",
            "constr": {
                "dtype": [],
                "structure": [
                    "tensor"
                ],
                "shape": [],
                "ndim": [],
                "enum": [],
                "range": []
            },
            "signature": "torch.nn.functional.cosine_similarity(x1,x2,dim=1,eps=1e-8)",
            "descp": "x1(Tensor) - First input."
        },
        {
            "fname": "torch.ones.yaml",
            "arg": "dtype",
            "prompt": "Signature: torch.distributed.recv(tensor,src=None,group=<objectobject>,tag=0)\nParameter description: tensor(Tensor) - Tensor to fill with received data.\nConstraints:\ndtype:Null\nstructure:tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.div(input,other,out=None)\nParameter description: out(Tensor, optional) - the output tensor.\nConstraints:\ndtype:Null\nstructure:tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.nn.TransformerDecoder(decoder_layer,num_layers,norm=None)\nParameter description: num_layers - the number of sub-decoder-layers in the decoder (required).\nConstraints:\ndtype:int\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:[0,inf)\n\n###\n\nSignature: torch.nn.quantized.functional.interpolate(input,size=None,scale_factor=None,mode=nearest,align_corners=None)\nParameter description: align_corners(bool, optional) - Geometrically, we consider the pixels of the input and output as squares rather than points. If set to `True`, the input and output tensors are aligned by the center points of their corner pixels, preserving the values at the corner pixels. If set to `False`, the input and output tensors are aligned by the corner points of their corner pixels, and the interpolation uses edge value padding for out-of-boundary values, making this operation independent of input size when `scale_factor` is kept the same. This only has an effect when `mode` is `'bilinear'`. Default: `False`\nConstraints:\ndtype:bool\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.gt(input,other,out=None)\nParameter description: out(Tensor, optional) - the output tensor that must be a BoolTensor\nConstraints:\ndtype:booltensor\nstructure:tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.empty_like(input,dtype=None,layout=None,device=None,requires_grad=False,memory_format=torch.preserve_format)\nParameter description: dtype(`torch.dtype`, optional) - the desired data type of returned Tensor. Default: if `None`, defaults to the dtype of `input`.\nConstraints:\ndtype:torch.dtype\nstructure:Null\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.nn.RNN(*args,**kwargs,input_size=None,hidden_size=None,num_layers=None,nonlinearity=None,bias=None,batch_first=None,dropout=None,bidirectional=None)\nParameter description: hidden_size - The number of features in the hidden state h\nConstraints:\ndtype:int\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:[0,inf)\n\n###\n\nSignature: torch.nn.functional.avg_pool1d(input,kernel_size,stride=None,padding=0,ceil_mode=False,count_include_pad=True)\nParameter description: padding - implicit zero paddings on both sides of the input. Can be a single number or a tuple (padW,). Default: 0\nConstraints:\ndtype:bool;int\nstructure:tuple\nshape:Null\nndim:0;1\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.autograd.grad(outputs,inputs,grad_outputs=None,retain_graph=None,create_graph=False,only_inputs=True,allow_unused=False)\nParameter description: outputs(sequence of Tensor) - outputs of the differentiated function.\nConstraints:\ndtype:Null\nstructure:sequence;tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.repeat_interleave(input,repeats,dim=None)\nParameter description: dim(int, optional) - The dimension along which to repeat values.By default, use the flattened input array, and return a flat outputarray.\nConstraints:\ndtype:int\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.ones(*size,out=None,dtype=None,layout=torch.strided,device=None,requires_grad=False)\nParameter description: dtype(`torch.dtype`, optional) - the desired data type of returned tensor. Default: if `None`, uses a global default (see `torch.set_default_tensor_type()`).\nConstraints:",
            "constr": {
                "dtype": [
                    "torch.dtype"
                ],
                "structure": [],
                "shape": [],
                "ndim": [],
                "enum": [],
                "range": []
            },
            "signature": "torch.ones(*size,out=None,dtype=None,layout=torch.strided,device=None,requires_grad=False)",
            "descp": "dtype(`torch.dtype`, optional) - the desired data type of returned tensor. Default: if `None`, uses a global default (see `torch.set_default_tensor_type()`)."
        },
        {
            "fname": "torch.sum2.yaml",
            "arg": "keepdim",
            "prompt": "Signature: torch.distributed.recv(tensor,src=None,group=<objectobject>,tag=0)\nParameter description: tensor(Tensor) - Tensor to fill with received data.\nConstraints:\ndtype:Null\nstructure:tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.div(input,other,out=None)\nParameter description: out(Tensor, optional) - the output tensor.\nConstraints:\ndtype:Null\nstructure:tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.nn.TransformerDecoder(decoder_layer,num_layers,norm=None)\nParameter description: num_layers - the number of sub-decoder-layers in the decoder (required).\nConstraints:\ndtype:int\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:[0,inf)\n\n###\n\nSignature: torch.nn.quantized.functional.interpolate(input,size=None,scale_factor=None,mode=nearest,align_corners=None)\nParameter description: align_corners(bool, optional) - Geometrically, we consider the pixels of the input and output as squares rather than points. If set to `True`, the input and output tensors are aligned by the center points of their corner pixels, preserving the values at the corner pixels. If set to `False`, the input and output tensors are aligned by the corner points of their corner pixels, and the interpolation uses edge value padding for out-of-boundary values, making this operation independent of input size when `scale_factor` is kept the same. This only has an effect when `mode` is `'bilinear'`. Default: `False`\nConstraints:\ndtype:bool\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.gt(input,other,out=None)\nParameter description: out(Tensor, optional) - the output tensor that must be a BoolTensor\nConstraints:\ndtype:booltensor\nstructure:tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.empty_like(input,dtype=None,layout=None,device=None,requires_grad=False,memory_format=torch.preserve_format)\nParameter description: dtype(`torch.dtype`, optional) - the desired data type of returned Tensor. Default: if `None`, defaults to the dtype of `input`.\nConstraints:\ndtype:torch.dtype\nstructure:Null\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.nn.RNN(*args,**kwargs,input_size=None,hidden_size=None,num_layers=None,nonlinearity=None,bias=None,batch_first=None,dropout=None,bidirectional=None)\nParameter description: hidden_size - The number of features in the hidden state h\nConstraints:\ndtype:int\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:[0,inf)\n\n###\n\nSignature: torch.nn.functional.avg_pool1d(input,kernel_size,stride=None,padding=0,ceil_mode=False,count_include_pad=True)\nParameter description: padding - implicit zero paddings on both sides of the input. Can be a single number or a tuple (padW,). Default: 0\nConstraints:\ndtype:bool;int\nstructure:tuple\nshape:Null\nndim:0;1\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.autograd.grad(outputs,inputs,grad_outputs=None,retain_graph=None,create_graph=False,only_inputs=True,allow_unused=False)\nParameter description: outputs(sequence of Tensor) - outputs of the differentiated function.\nConstraints:\ndtype:Null\nstructure:sequence;tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.repeat_interleave(input,repeats,dim=None)\nParameter description: dim(int, optional) - The dimension along which to repeat values.By default, use the flattened input array, and return a flat outputarray.\nConstraints:\ndtype:int\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.sum(input,dim,keepdim=False,dtype=None)\nParameter description: keepdim(bool) - whether the output tensor has `dim` retained or not.\nConstraints:",
            "constr": {
                "dtype": [
                    "bool"
                ],
                "structure": [],
                "shape": [],
                "ndim": [
                    "0"
                ],
                "enum": [],
                "range": []
            },
            "signature": "torch.sum(input,dim,keepdim=False,dtype=None)",
            "descp": "keepdim(bool) - whether the output tensor has `dim` retained or not."
        },
        {
            "fname": "torch.nn.conv1d.yaml",
            "arg": "dilation",
            "prompt": "Signature: torch.distributed.recv(tensor,src=None,group=<objectobject>,tag=0)\nParameter description: tensor(Tensor) - Tensor to fill with received data.\nConstraints:\ndtype:Null\nstructure:tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.div(input,other,out=None)\nParameter description: out(Tensor, optional) - the output tensor.\nConstraints:\ndtype:Null\nstructure:tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.nn.TransformerDecoder(decoder_layer,num_layers,norm=None)\nParameter description: num_layers - the number of sub-decoder-layers in the decoder (required).\nConstraints:\ndtype:int\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:[0,inf)\n\n###\n\nSignature: torch.nn.quantized.functional.interpolate(input,size=None,scale_factor=None,mode=nearest,align_corners=None)\nParameter description: align_corners(bool, optional) - Geometrically, we consider the pixels of the input and output as squares rather than points. If set to `True`, the input and output tensors are aligned by the center points of their corner pixels, preserving the values at the corner pixels. If set to `False`, the input and output tensors are aligned by the corner points of their corner pixels, and the interpolation uses edge value padding for out-of-boundary values, making this operation independent of input size when `scale_factor` is kept the same. This only has an effect when `mode` is `'bilinear'`. Default: `False`\nConstraints:\ndtype:bool\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.gt(input,other,out=None)\nParameter description: out(Tensor, optional) - the output tensor that must be a BoolTensor\nConstraints:\ndtype:booltensor\nstructure:tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.empty_like(input,dtype=None,layout=None,device=None,requires_grad=False,memory_format=torch.preserve_format)\nParameter description: dtype(`torch.dtype`, optional) - the desired data type of returned Tensor. Default: if `None`, defaults to the dtype of `input`.\nConstraints:\ndtype:torch.dtype\nstructure:Null\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.nn.RNN(*args,**kwargs,input_size=None,hidden_size=None,num_layers=None,nonlinearity=None,bias=None,batch_first=None,dropout=None,bidirectional=None)\nParameter description: hidden_size - The number of features in the hidden state h\nConstraints:\ndtype:int\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:[0,inf)\n\n###\n\nSignature: torch.nn.functional.avg_pool1d(input,kernel_size,stride=None,padding=0,ceil_mode=False,count_include_pad=True)\nParameter description: padding - implicit zero paddings on both sides of the input. Can be a single number or a tuple (padW,). Default: 0\nConstraints:\ndtype:bool;int\nstructure:tuple\nshape:Null\nndim:0;1\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.autograd.grad(outputs,inputs,grad_outputs=None,retain_graph=None,create_graph=False,only_inputs=True,allow_unused=False)\nParameter description: outputs(sequence of Tensor) - outputs of the differentiated function.\nConstraints:\ndtype:Null\nstructure:sequence;tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.repeat_interleave(input,repeats,dim=None)\nParameter description: dim(int, optional) - The dimension along which to repeat values.By default, use the flattened input array, and return a flat outputarray.\nConstraints:\ndtype:int\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.nn.Conv1d(in_channels,out_channels,kernel_size,stride=1,padding=0,dilation=1,groups=1,bias=True,padding_mode=zeros)\nParameter description: dilation(int or tuple, optional) - Spacing between kernel elements. Default: 1\nConstraints:",
            "constr": {
                "dtype": [
                    "int"
                ],
                "structure": [
                    "tuple"
                ],
                "shape": [],
                "ndim": [
                    "0",
                    "1"
                ],
                "enum": [],
                "range": []
            },
            "signature": "torch.nn.Conv1d(in_channels,out_channels,kernel_size,stride=1,padding=0,dilation=1,groups=1,bias=True,padding_mode=zeros)",
            "descp": "dilation(int or tuple, optional) - Spacing between kernel elements. Default: 1"
        },
        {
            "fname": "torch.nn.localresponsenorm.yaml",
            "arg": "beta",
            "prompt": "Signature: torch.distributed.recv(tensor,src=None,group=<objectobject>,tag=0)\nParameter description: tensor(Tensor) - Tensor to fill with received data.\nConstraints:\ndtype:Null\nstructure:tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.div(input,other,out=None)\nParameter description: out(Tensor, optional) - the output tensor.\nConstraints:\ndtype:Null\nstructure:tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.nn.TransformerDecoder(decoder_layer,num_layers,norm=None)\nParameter description: num_layers - the number of sub-decoder-layers in the decoder (required).\nConstraints:\ndtype:int\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:[0,inf)\n\n###\n\nSignature: torch.nn.quantized.functional.interpolate(input,size=None,scale_factor=None,mode=nearest,align_corners=None)\nParameter description: align_corners(bool, optional) - Geometrically, we consider the pixels of the input and output as squares rather than points. If set to `True`, the input and output tensors are aligned by the center points of their corner pixels, preserving the values at the corner pixels. If set to `False`, the input and output tensors are aligned by the corner points of their corner pixels, and the interpolation uses edge value padding for out-of-boundary values, making this operation independent of input size when `scale_factor` is kept the same. This only has an effect when `mode` is `'bilinear'`. Default: `False`\nConstraints:\ndtype:bool\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.gt(input,other,out=None)\nParameter description: out(Tensor, optional) - the output tensor that must be a BoolTensor\nConstraints:\ndtype:booltensor\nstructure:tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.empty_like(input,dtype=None,layout=None,device=None,requires_grad=False,memory_format=torch.preserve_format)\nParameter description: dtype(`torch.dtype`, optional) - the desired data type of returned Tensor. Default: if `None`, defaults to the dtype of `input`.\nConstraints:\ndtype:torch.dtype\nstructure:Null\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.nn.RNN(*args,**kwargs,input_size=None,hidden_size=None,num_layers=None,nonlinearity=None,bias=None,batch_first=None,dropout=None,bidirectional=None)\nParameter description: hidden_size - The number of features in the hidden state h\nConstraints:\ndtype:int\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:[0,inf)\n\n###\n\nSignature: torch.nn.functional.avg_pool1d(input,kernel_size,stride=None,padding=0,ceil_mode=False,count_include_pad=True)\nParameter description: padding - implicit zero paddings on both sides of the input. Can be a single number or a tuple (padW,). Default: 0\nConstraints:\ndtype:bool;int\nstructure:tuple\nshape:Null\nndim:0;1\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.autograd.grad(outputs,inputs,grad_outputs=None,retain_graph=None,create_graph=False,only_inputs=True,allow_unused=False)\nParameter description: outputs(sequence of Tensor) - outputs of the differentiated function.\nConstraints:\ndtype:Null\nstructure:sequence;tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.repeat_interleave(input,repeats,dim=None)\nParameter description: dim(int, optional) - The dimension along which to repeat values.By default, use the flattened input array, and return a flat outputarray.\nConstraints:\ndtype:int\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.nn.LocalResponseNorm(size,alpha=0.0001,beta=0.75,k=1.0)\nParameter description: beta - exponent. Default: 0.75\nConstraints:",
            "constr": {
                "dtype": [
                    "float"
                ],
                "structure": [],
                "shape": [],
                "ndim": [
                    "0"
                ],
                "enum": [],
                "range": []
            },
            "signature": "torch.nn.LocalResponseNorm(size,alpha=0.0001,beta=0.75,k=1.0)",
            "descp": "beta - exponent. Default: 0.75"
        },
        {
            "fname": "torch.nn.utils.prune.ln_structured.yaml",
            "arg": "name",
            "prompt": "Signature: torch.distributed.recv(tensor,src=None,group=<objectobject>,tag=0)\nParameter description: tensor(Tensor) - Tensor to fill with received data.\nConstraints:\ndtype:Null\nstructure:tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.div(input,other,out=None)\nParameter description: out(Tensor, optional) - the output tensor.\nConstraints:\ndtype:Null\nstructure:tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.nn.TransformerDecoder(decoder_layer,num_layers,norm=None)\nParameter description: num_layers - the number of sub-decoder-layers in the decoder (required).\nConstraints:\ndtype:int\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:[0,inf)\n\n###\n\nSignature: torch.nn.quantized.functional.interpolate(input,size=None,scale_factor=None,mode=nearest,align_corners=None)\nParameter description: align_corners(bool, optional) - Geometrically, we consider the pixels of the input and output as squares rather than points. If set to `True`, the input and output tensors are aligned by the center points of their corner pixels, preserving the values at the corner pixels. If set to `False`, the input and output tensors are aligned by the corner points of their corner pixels, and the interpolation uses edge value padding for out-of-boundary values, making this operation independent of input size when `scale_factor` is kept the same. This only has an effect when `mode` is `'bilinear'`. Default: `False`\nConstraints:\ndtype:bool\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.gt(input,other,out=None)\nParameter description: out(Tensor, optional) - the output tensor that must be a BoolTensor\nConstraints:\ndtype:booltensor\nstructure:tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.empty_like(input,dtype=None,layout=None,device=None,requires_grad=False,memory_format=torch.preserve_format)\nParameter description: dtype(`torch.dtype`, optional) - the desired data type of returned Tensor. Default: if `None`, defaults to the dtype of `input`.\nConstraints:\ndtype:torch.dtype\nstructure:Null\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.nn.RNN(*args,**kwargs,input_size=None,hidden_size=None,num_layers=None,nonlinearity=None,bias=None,batch_first=None,dropout=None,bidirectional=None)\nParameter description: hidden_size - The number of features in the hidden state h\nConstraints:\ndtype:int\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:[0,inf)\n\n###\n\nSignature: torch.nn.functional.avg_pool1d(input,kernel_size,stride=None,padding=0,ceil_mode=False,count_include_pad=True)\nParameter description: padding - implicit zero paddings on both sides of the input. Can be a single number or a tuple (padW,). Default: 0\nConstraints:\ndtype:bool;int\nstructure:tuple\nshape:Null\nndim:0;1\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.autograd.grad(outputs,inputs,grad_outputs=None,retain_graph=None,create_graph=False,only_inputs=True,allow_unused=False)\nParameter description: outputs(sequence of Tensor) - outputs of the differentiated function.\nConstraints:\ndtype:Null\nstructure:sequence;tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.repeat_interleave(input,repeats,dim=None)\nParameter description: dim(int, optional) - The dimension along which to repeat values.By default, use the flattened input array, and return a flat outputarray.\nConstraints:\ndtype:int\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.nn.utils.prune.ln_structured(module,name,amount,n,dim)\nParameter description: name(str) - parameter name within `module` on which pruning will act.\nConstraints:",
            "constr": {
                "dtype": [
                    "string",
                    "str"
                ],
                "structure": [],
                "shape": [],
                "ndim": [
                    "0"
                ],
                "enum": [],
                "range": []
            },
            "signature": "torch.nn.utils.prune.ln_structured(module,name,amount,n,dim)",
            "descp": "name(str) - parameter name within `module` on which pruning will act."
        },
        {
            "fname": "torch.ones.yaml",
            "arg": "out",
            "prompt": "Signature: torch.distributed.recv(tensor,src=None,group=<objectobject>,tag=0)\nParameter description: tensor(Tensor) - Tensor to fill with received data.\nConstraints:\ndtype:Null\nstructure:tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.div(input,other,out=None)\nParameter description: out(Tensor, optional) - the output tensor.\nConstraints:\ndtype:Null\nstructure:tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.nn.TransformerDecoder(decoder_layer,num_layers,norm=None)\nParameter description: num_layers - the number of sub-decoder-layers in the decoder (required).\nConstraints:\ndtype:int\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:[0,inf)\n\n###\n\nSignature: torch.nn.quantized.functional.interpolate(input,size=None,scale_factor=None,mode=nearest,align_corners=None)\nParameter description: align_corners(bool, optional) - Geometrically, we consider the pixels of the input and output as squares rather than points. If set to `True`, the input and output tensors are aligned by the center points of their corner pixels, preserving the values at the corner pixels. If set to `False`, the input and output tensors are aligned by the corner points of their corner pixels, and the interpolation uses edge value padding for out-of-boundary values, making this operation independent of input size when `scale_factor` is kept the same. This only has an effect when `mode` is `'bilinear'`. Default: `False`\nConstraints:\ndtype:bool\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.gt(input,other,out=None)\nParameter description: out(Tensor, optional) - the output tensor that must be a BoolTensor\nConstraints:\ndtype:booltensor\nstructure:tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.empty_like(input,dtype=None,layout=None,device=None,requires_grad=False,memory_format=torch.preserve_format)\nParameter description: dtype(`torch.dtype`, optional) - the desired data type of returned Tensor. Default: if `None`, defaults to the dtype of `input`.\nConstraints:\ndtype:torch.dtype\nstructure:Null\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.nn.RNN(*args,**kwargs,input_size=None,hidden_size=None,num_layers=None,nonlinearity=None,bias=None,batch_first=None,dropout=None,bidirectional=None)\nParameter description: hidden_size - The number of features in the hidden state h\nConstraints:\ndtype:int\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:[0,inf)\n\n###\n\nSignature: torch.nn.functional.avg_pool1d(input,kernel_size,stride=None,padding=0,ceil_mode=False,count_include_pad=True)\nParameter description: padding - implicit zero paddings on both sides of the input. Can be a single number or a tuple (padW,). Default: 0\nConstraints:\ndtype:bool;int\nstructure:tuple\nshape:Null\nndim:0;1\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.autograd.grad(outputs,inputs,grad_outputs=None,retain_graph=None,create_graph=False,only_inputs=True,allow_unused=False)\nParameter description: outputs(sequence of Tensor) - outputs of the differentiated function.\nConstraints:\ndtype:Null\nstructure:sequence;tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.repeat_interleave(input,repeats,dim=None)\nParameter description: dim(int, optional) - The dimension along which to repeat values.By default, use the flattened input array, and return a flat outputarray.\nConstraints:\ndtype:int\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.ones(*size,out=None,dtype=None,layout=torch.strided,device=None,requires_grad=False)\nParameter description: out(Tensor, optional) - the output tensor.\nConstraints:",
            "constr": {
                "dtype": [],
                "structure": [
                    "tensor"
                ],
                "shape": [],
                "ndim": [],
                "enum": [],
                "range": []
            },
            "signature": "torch.ones(*size,out=None,dtype=None,layout=torch.strided,device=None,requires_grad=False)",
            "descp": "out(Tensor, optional) - the output tensor."
        },
        {
            "fname": "torch.nn.bcewithlogitsloss.yaml",
            "arg": "pos_weight",
            "prompt": "Signature: torch.distributed.recv(tensor,src=None,group=<objectobject>,tag=0)\nParameter description: tensor(Tensor) - Tensor to fill with received data.\nConstraints:\ndtype:Null\nstructure:tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.div(input,other,out=None)\nParameter description: out(Tensor, optional) - the output tensor.\nConstraints:\ndtype:Null\nstructure:tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.nn.TransformerDecoder(decoder_layer,num_layers,norm=None)\nParameter description: num_layers - the number of sub-decoder-layers in the decoder (required).\nConstraints:\ndtype:int\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:[0,inf)\n\n###\n\nSignature: torch.nn.quantized.functional.interpolate(input,size=None,scale_factor=None,mode=nearest,align_corners=None)\nParameter description: align_corners(bool, optional) - Geometrically, we consider the pixels of the input and output as squares rather than points. If set to `True`, the input and output tensors are aligned by the center points of their corner pixels, preserving the values at the corner pixels. If set to `False`, the input and output tensors are aligned by the corner points of their corner pixels, and the interpolation uses edge value padding for out-of-boundary values, making this operation independent of input size when `scale_factor` is kept the same. This only has an effect when `mode` is `'bilinear'`. Default: `False`\nConstraints:\ndtype:bool\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.gt(input,other,out=None)\nParameter description: out(Tensor, optional) - the output tensor that must be a BoolTensor\nConstraints:\ndtype:booltensor\nstructure:tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.empty_like(input,dtype=None,layout=None,device=None,requires_grad=False,memory_format=torch.preserve_format)\nParameter description: dtype(`torch.dtype`, optional) - the desired data type of returned Tensor. Default: if `None`, defaults to the dtype of `input`.\nConstraints:\ndtype:torch.dtype\nstructure:Null\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.nn.RNN(*args,**kwargs,input_size=None,hidden_size=None,num_layers=None,nonlinearity=None,bias=None,batch_first=None,dropout=None,bidirectional=None)\nParameter description: hidden_size - The number of features in the hidden state h\nConstraints:\ndtype:int\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:[0,inf)\n\n###\n\nSignature: torch.nn.functional.avg_pool1d(input,kernel_size,stride=None,padding=0,ceil_mode=False,count_include_pad=True)\nParameter description: padding - implicit zero paddings on both sides of the input. Can be a single number or a tuple (padW,). Default: 0\nConstraints:\ndtype:bool;int\nstructure:tuple\nshape:Null\nndim:0;1\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.autograd.grad(outputs,inputs,grad_outputs=None,retain_graph=None,create_graph=False,only_inputs=True,allow_unused=False)\nParameter description: outputs(sequence of Tensor) - outputs of the differentiated function.\nConstraints:\ndtype:Null\nstructure:sequence;tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.repeat_interleave(input,repeats,dim=None)\nParameter description: dim(int, optional) - The dimension along which to repeat values.By default, use the flattened input array, and return a flat outputarray.\nConstraints:\ndtype:int\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.nn.BCEWithLogitsLoss(weight=None,size_average=None,reduce=None,reduction=mean,pos_weight=None)\nParameter description: pos_weight(Tensor, optional) - a weight of positive examples. Must be a vector with length equal to the number of classes.\nConstraints:",
            "constr": {
                "dtype": [
                    "int"
                ],
                "structure": [
                    "vector",
                    "tensor"
                ],
                "shape": [],
                "ndim": [],
                "enum": [],
                "range": [
                    "[0,inf)"
                ]
            },
            "signature": "torch.nn.BCEWithLogitsLoss(weight=None,size_average=None,reduce=None,reduction=mean,pos_weight=None)",
            "descp": "pos_weight(Tensor, optional) - a weight of positive examples. Must be a vector with length equal to the number of classes."
        },
        {
            "fname": "torch.median.yaml",
            "arg": "input",
            "prompt": "Signature: torch.distributed.recv(tensor,src=None,group=<objectobject>,tag=0)\nParameter description: tensor(Tensor) - Tensor to fill with received data.\nConstraints:\ndtype:Null\nstructure:tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.div(input,other,out=None)\nParameter description: out(Tensor, optional) - the output tensor.\nConstraints:\ndtype:Null\nstructure:tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.nn.TransformerDecoder(decoder_layer,num_layers,norm=None)\nParameter description: num_layers - the number of sub-decoder-layers in the decoder (required).\nConstraints:\ndtype:int\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:[0,inf)\n\n###\n\nSignature: torch.nn.quantized.functional.interpolate(input,size=None,scale_factor=None,mode=nearest,align_corners=None)\nParameter description: align_corners(bool, optional) - Geometrically, we consider the pixels of the input and output as squares rather than points. If set to `True`, the input and output tensors are aligned by the center points of their corner pixels, preserving the values at the corner pixels. If set to `False`, the input and output tensors are aligned by the corner points of their corner pixels, and the interpolation uses edge value padding for out-of-boundary values, making this operation independent of input size when `scale_factor` is kept the same. This only has an effect when `mode` is `'bilinear'`. Default: `False`\nConstraints:\ndtype:bool\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.gt(input,other,out=None)\nParameter description: out(Tensor, optional) - the output tensor that must be a BoolTensor\nConstraints:\ndtype:booltensor\nstructure:tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.empty_like(input,dtype=None,layout=None,device=None,requires_grad=False,memory_format=torch.preserve_format)\nParameter description: dtype(`torch.dtype`, optional) - the desired data type of returned Tensor. Default: if `None`, defaults to the dtype of `input`.\nConstraints:\ndtype:torch.dtype\nstructure:Null\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.nn.RNN(*args,**kwargs,input_size=None,hidden_size=None,num_layers=None,nonlinearity=None,bias=None,batch_first=None,dropout=None,bidirectional=None)\nParameter description: hidden_size - The number of features in the hidden state h\nConstraints:\ndtype:int\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:[0,inf)\n\n###\n\nSignature: torch.nn.functional.avg_pool1d(input,kernel_size,stride=None,padding=0,ceil_mode=False,count_include_pad=True)\nParameter description: padding - implicit zero paddings on both sides of the input. Can be a single number or a tuple (padW,). Default: 0\nConstraints:\ndtype:bool;int\nstructure:tuple\nshape:Null\nndim:0;1\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.autograd.grad(outputs,inputs,grad_outputs=None,retain_graph=None,create_graph=False,only_inputs=True,allow_unused=False)\nParameter description: outputs(sequence of Tensor) - outputs of the differentiated function.\nConstraints:\ndtype:Null\nstructure:sequence;tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.repeat_interleave(input,repeats,dim=None)\nParameter description: dim(int, optional) - The dimension along which to repeat values.By default, use the flattened input array, and return a flat outputarray.\nConstraints:\ndtype:int\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.median(input)\nParameter description: input(Tensor) - the input tensor.\nConstraints:",
            "constr": {
                "dtype": [],
                "structure": [
                    "tensor"
                ],
                "shape": [],
                "ndim": [],
                "enum": [],
                "range": []
            },
            "signature": "torch.median(input)",
            "descp": "input(Tensor) - the input tensor."
        },
        {
            "fname": "torch.stft.yaml",
            "arg": "n_fft",
            "prompt": "Signature: torch.distributed.recv(tensor,src=None,group=<objectobject>,tag=0)\nParameter description: tensor(Tensor) - Tensor to fill with received data.\nConstraints:\ndtype:Null\nstructure:tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.div(input,other,out=None)\nParameter description: out(Tensor, optional) - the output tensor.\nConstraints:\ndtype:Null\nstructure:tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.nn.TransformerDecoder(decoder_layer,num_layers,norm=None)\nParameter description: num_layers - the number of sub-decoder-layers in the decoder (required).\nConstraints:\ndtype:int\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:[0,inf)\n\n###\n\nSignature: torch.nn.quantized.functional.interpolate(input,size=None,scale_factor=None,mode=nearest,align_corners=None)\nParameter description: align_corners(bool, optional) - Geometrically, we consider the pixels of the input and output as squares rather than points. If set to `True`, the input and output tensors are aligned by the center points of their corner pixels, preserving the values at the corner pixels. If set to `False`, the input and output tensors are aligned by the corner points of their corner pixels, and the interpolation uses edge value padding for out-of-boundary values, making this operation independent of input size when `scale_factor` is kept the same. This only has an effect when `mode` is `'bilinear'`. Default: `False`\nConstraints:\ndtype:bool\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.gt(input,other,out=None)\nParameter description: out(Tensor, optional) - the output tensor that must be a BoolTensor\nConstraints:\ndtype:booltensor\nstructure:tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.empty_like(input,dtype=None,layout=None,device=None,requires_grad=False,memory_format=torch.preserve_format)\nParameter description: dtype(`torch.dtype`, optional) - the desired data type of returned Tensor. Default: if `None`, defaults to the dtype of `input`.\nConstraints:\ndtype:torch.dtype\nstructure:Null\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.nn.RNN(*args,**kwargs,input_size=None,hidden_size=None,num_layers=None,nonlinearity=None,bias=None,batch_first=None,dropout=None,bidirectional=None)\nParameter description: hidden_size - The number of features in the hidden state h\nConstraints:\ndtype:int\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:[0,inf)\n\n###\n\nSignature: torch.nn.functional.avg_pool1d(input,kernel_size,stride=None,padding=0,ceil_mode=False,count_include_pad=True)\nParameter description: padding - implicit zero paddings on both sides of the input. Can be a single number or a tuple (padW,). Default: 0\nConstraints:\ndtype:bool;int\nstructure:tuple\nshape:Null\nndim:0;1\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.autograd.grad(outputs,inputs,grad_outputs=None,retain_graph=None,create_graph=False,only_inputs=True,allow_unused=False)\nParameter description: outputs(sequence of Tensor) - outputs of the differentiated function.\nConstraints:\ndtype:Null\nstructure:sequence;tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.repeat_interleave(input,repeats,dim=None)\nParameter description: dim(int, optional) - The dimension along which to repeat values.By default, use the flattened input array, and return a flat outputarray.\nConstraints:\ndtype:int\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.stft(input,n_fft,hop_length=None,win_length=None,window=None,center=True,pad_mode=reflect,normalized=False,onesided=True)\nParameter description: n_fft(int) - size of Fourier transform\nConstraints:",
            "constr": {
                "dtype": [
                    "int"
                ],
                "structure": [],
                "shape": [],
                "ndim": [],
                "enum": [],
                "range": [
                    "[0,inf)"
                ]
            },
            "signature": "torch.stft(input,n_fft,hop_length=None,win_length=None,window=None,center=True,pad_mode=reflect,normalized=False,onesided=True)",
            "descp": "n_fft(int) - size of Fourier transform"
        },
        {
            "fname": "torch.mean.yaml",
            "arg": "input",
            "prompt": "Signature: torch.distributed.recv(tensor,src=None,group=<objectobject>,tag=0)\nParameter description: tensor(Tensor) - Tensor to fill with received data.\nConstraints:\ndtype:Null\nstructure:tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.div(input,other,out=None)\nParameter description: out(Tensor, optional) - the output tensor.\nConstraints:\ndtype:Null\nstructure:tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.nn.TransformerDecoder(decoder_layer,num_layers,norm=None)\nParameter description: num_layers - the number of sub-decoder-layers in the decoder (required).\nConstraints:\ndtype:int\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:[0,inf)\n\n###\n\nSignature: torch.nn.quantized.functional.interpolate(input,size=None,scale_factor=None,mode=nearest,align_corners=None)\nParameter description: align_corners(bool, optional) - Geometrically, we consider the pixels of the input and output as squares rather than points. If set to `True`, the input and output tensors are aligned by the center points of their corner pixels, preserving the values at the corner pixels. If set to `False`, the input and output tensors are aligned by the corner points of their corner pixels, and the interpolation uses edge value padding for out-of-boundary values, making this operation independent of input size when `scale_factor` is kept the same. This only has an effect when `mode` is `'bilinear'`. Default: `False`\nConstraints:\ndtype:bool\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.gt(input,other,out=None)\nParameter description: out(Tensor, optional) - the output tensor that must be a BoolTensor\nConstraints:\ndtype:booltensor\nstructure:tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.empty_like(input,dtype=None,layout=None,device=None,requires_grad=False,memory_format=torch.preserve_format)\nParameter description: dtype(`torch.dtype`, optional) - the desired data type of returned Tensor. Default: if `None`, defaults to the dtype of `input`.\nConstraints:\ndtype:torch.dtype\nstructure:Null\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.nn.RNN(*args,**kwargs,input_size=None,hidden_size=None,num_layers=None,nonlinearity=None,bias=None,batch_first=None,dropout=None,bidirectional=None)\nParameter description: hidden_size - The number of features in the hidden state h\nConstraints:\ndtype:int\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:[0,inf)\n\n###\n\nSignature: torch.nn.functional.avg_pool1d(input,kernel_size,stride=None,padding=0,ceil_mode=False,count_include_pad=True)\nParameter description: padding - implicit zero paddings on both sides of the input. Can be a single number or a tuple (padW,). Default: 0\nConstraints:\ndtype:bool;int\nstructure:tuple\nshape:Null\nndim:0;1\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.autograd.grad(outputs,inputs,grad_outputs=None,retain_graph=None,create_graph=False,only_inputs=True,allow_unused=False)\nParameter description: outputs(sequence of Tensor) - outputs of the differentiated function.\nConstraints:\ndtype:Null\nstructure:sequence;tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.repeat_interleave(input,repeats,dim=None)\nParameter description: dim(int, optional) - The dimension along which to repeat values.By default, use the flattened input array, and return a flat outputarray.\nConstraints:\ndtype:int\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.mean(input)\nParameter description: input(Tensor) - the input tensor.\nConstraints:",
            "constr": {
                "dtype": [],
                "structure": [
                    "tensor"
                ],
                "shape": [],
                "ndim": [],
                "enum": [],
                "range": []
            },
            "signature": "torch.mean(input)",
            "descp": "input(Tensor) - the input tensor."
        },
        {
            "fname": "torch.utils.checkpoint.checkpoint.yaml",
            "arg": "*args",
            "prompt": "Signature: torch.distributed.recv(tensor,src=None,group=<objectobject>,tag=0)\nParameter description: tensor(Tensor) - Tensor to fill with received data.\nConstraints:\ndtype:Null\nstructure:tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.div(input,other,out=None)\nParameter description: out(Tensor, optional) - the output tensor.\nConstraints:\ndtype:Null\nstructure:tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.nn.TransformerDecoder(decoder_layer,num_layers,norm=None)\nParameter description: num_layers - the number of sub-decoder-layers in the decoder (required).\nConstraints:\ndtype:int\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:[0,inf)\n\n###\n\nSignature: torch.nn.quantized.functional.interpolate(input,size=None,scale_factor=None,mode=nearest,align_corners=None)\nParameter description: align_corners(bool, optional) - Geometrically, we consider the pixels of the input and output as squares rather than points. If set to `True`, the input and output tensors are aligned by the center points of their corner pixels, preserving the values at the corner pixels. If set to `False`, the input and output tensors are aligned by the corner points of their corner pixels, and the interpolation uses edge value padding for out-of-boundary values, making this operation independent of input size when `scale_factor` is kept the same. This only has an effect when `mode` is `'bilinear'`. Default: `False`\nConstraints:\ndtype:bool\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.gt(input,other,out=None)\nParameter description: out(Tensor, optional) - the output tensor that must be a BoolTensor\nConstraints:\ndtype:booltensor\nstructure:tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.empty_like(input,dtype=None,layout=None,device=None,requires_grad=False,memory_format=torch.preserve_format)\nParameter description: dtype(`torch.dtype`, optional) - the desired data type of returned Tensor. Default: if `None`, defaults to the dtype of `input`.\nConstraints:\ndtype:torch.dtype\nstructure:Null\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.nn.RNN(*args,**kwargs,input_size=None,hidden_size=None,num_layers=None,nonlinearity=None,bias=None,batch_first=None,dropout=None,bidirectional=None)\nParameter description: hidden_size - The number of features in the hidden state h\nConstraints:\ndtype:int\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:[0,inf)\n\n###\n\nSignature: torch.nn.functional.avg_pool1d(input,kernel_size,stride=None,padding=0,ceil_mode=False,count_include_pad=True)\nParameter description: padding - implicit zero paddings on both sides of the input. Can be a single number or a tuple (padW,). Default: 0\nConstraints:\ndtype:bool;int\nstructure:tuple\nshape:Null\nndim:0;1\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.autograd.grad(outputs,inputs,grad_outputs=None,retain_graph=None,create_graph=False,only_inputs=True,allow_unused=False)\nParameter description: outputs(sequence of Tensor) - outputs of the differentiated function.\nConstraints:\ndtype:Null\nstructure:sequence;tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.repeat_interleave(input,repeats,dim=None)\nParameter description: dim(int, optional) - The dimension along which to repeat values.By default, use the flattened input array, and return a flat outputarray.\nConstraints:\ndtype:int\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.utils.checkpoint.checkpoint(function,*args,**kwargs,preserve_rng_state=None)\nParameter description: *args - tuple containing inputs to the `function`\nConstraints:",
            "constr": {
                "dtype": [],
                "structure": [
                    "tuple"
                ],
                "shape": [],
                "ndim": [],
                "enum": [],
                "range": []
            },
            "signature": "torch.utils.checkpoint.checkpoint(function,*args,**kwargs,preserve_rng_state=None)",
            "descp": "*args - tuple containing inputs to the `function`"
        },
        {
            "fname": "torch.clamp.yaml",
            "arg": "input",
            "prompt": "Signature: torch.distributed.recv(tensor,src=None,group=<objectobject>,tag=0)\nParameter description: tensor(Tensor) - Tensor to fill with received data.\nConstraints:\ndtype:Null\nstructure:tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.div(input,other,out=None)\nParameter description: out(Tensor, optional) - the output tensor.\nConstraints:\ndtype:Null\nstructure:tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.nn.TransformerDecoder(decoder_layer,num_layers,norm=None)\nParameter description: num_layers - the number of sub-decoder-layers in the decoder (required).\nConstraints:\ndtype:int\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:[0,inf)\n\n###\n\nSignature: torch.nn.quantized.functional.interpolate(input,size=None,scale_factor=None,mode=nearest,align_corners=None)\nParameter description: align_corners(bool, optional) - Geometrically, we consider the pixels of the input and output as squares rather than points. If set to `True`, the input and output tensors are aligned by the center points of their corner pixels, preserving the values at the corner pixels. If set to `False`, the input and output tensors are aligned by the corner points of their corner pixels, and the interpolation uses edge value padding for out-of-boundary values, making this operation independent of input size when `scale_factor` is kept the same. This only has an effect when `mode` is `'bilinear'`. Default: `False`\nConstraints:\ndtype:bool\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.gt(input,other,out=None)\nParameter description: out(Tensor, optional) - the output tensor that must be a BoolTensor\nConstraints:\ndtype:booltensor\nstructure:tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.empty_like(input,dtype=None,layout=None,device=None,requires_grad=False,memory_format=torch.preserve_format)\nParameter description: dtype(`torch.dtype`, optional) - the desired data type of returned Tensor. Default: if `None`, defaults to the dtype of `input`.\nConstraints:\ndtype:torch.dtype\nstructure:Null\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.nn.RNN(*args,**kwargs,input_size=None,hidden_size=None,num_layers=None,nonlinearity=None,bias=None,batch_first=None,dropout=None,bidirectional=None)\nParameter description: hidden_size - The number of features in the hidden state h\nConstraints:\ndtype:int\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:[0,inf)\n\n###\n\nSignature: torch.nn.functional.avg_pool1d(input,kernel_size,stride=None,padding=0,ceil_mode=False,count_include_pad=True)\nParameter description: padding - implicit zero paddings on both sides of the input. Can be a single number or a tuple (padW,). Default: 0\nConstraints:\ndtype:bool;int\nstructure:tuple\nshape:Null\nndim:0;1\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.autograd.grad(outputs,inputs,grad_outputs=None,retain_graph=None,create_graph=False,only_inputs=True,allow_unused=False)\nParameter description: outputs(sequence of Tensor) - outputs of the differentiated function.\nConstraints:\ndtype:Null\nstructure:sequence;tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.repeat_interleave(input,repeats,dim=None)\nParameter description: dim(int, optional) - The dimension along which to repeat values.By default, use the flattened input array, and return a flat outputarray.\nConstraints:\ndtype:int\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.clamp(input,min,max,out=None)\nParameter description: input(Tensor) - the input tensor.\nConstraints:",
            "constr": {
                "dtype": [],
                "structure": [
                    "tensor"
                ],
                "shape": [],
                "ndim": [],
                "enum": [],
                "range": []
            },
            "signature": "torch.clamp(input,min,max,out=None)",
            "descp": "input(Tensor) - the input tensor."
        },
        {
            "fname": "torch.round.yaml",
            "arg": "input",
            "prompt": "Signature: torch.distributed.recv(tensor,src=None,group=<objectobject>,tag=0)\nParameter description: tensor(Tensor) - Tensor to fill with received data.\nConstraints:\ndtype:Null\nstructure:tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.div(input,other,out=None)\nParameter description: out(Tensor, optional) - the output tensor.\nConstraints:\ndtype:Null\nstructure:tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.nn.TransformerDecoder(decoder_layer,num_layers,norm=None)\nParameter description: num_layers - the number of sub-decoder-layers in the decoder (required).\nConstraints:\ndtype:int\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:[0,inf)\n\n###\n\nSignature: torch.nn.quantized.functional.interpolate(input,size=None,scale_factor=None,mode=nearest,align_corners=None)\nParameter description: align_corners(bool, optional) - Geometrically, we consider the pixels of the input and output as squares rather than points. If set to `True`, the input and output tensors are aligned by the center points of their corner pixels, preserving the values at the corner pixels. If set to `False`, the input and output tensors are aligned by the corner points of their corner pixels, and the interpolation uses edge value padding for out-of-boundary values, making this operation independent of input size when `scale_factor` is kept the same. This only has an effect when `mode` is `'bilinear'`. Default: `False`\nConstraints:\ndtype:bool\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.gt(input,other,out=None)\nParameter description: out(Tensor, optional) - the output tensor that must be a BoolTensor\nConstraints:\ndtype:booltensor\nstructure:tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.empty_like(input,dtype=None,layout=None,device=None,requires_grad=False,memory_format=torch.preserve_format)\nParameter description: dtype(`torch.dtype`, optional) - the desired data type of returned Tensor. Default: if `None`, defaults to the dtype of `input`.\nConstraints:\ndtype:torch.dtype\nstructure:Null\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.nn.RNN(*args,**kwargs,input_size=None,hidden_size=None,num_layers=None,nonlinearity=None,bias=None,batch_first=None,dropout=None,bidirectional=None)\nParameter description: hidden_size - The number of features in the hidden state h\nConstraints:\ndtype:int\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:[0,inf)\n\n###\n\nSignature: torch.nn.functional.avg_pool1d(input,kernel_size,stride=None,padding=0,ceil_mode=False,count_include_pad=True)\nParameter description: padding - implicit zero paddings on both sides of the input. Can be a single number or a tuple (padW,). Default: 0\nConstraints:\ndtype:bool;int\nstructure:tuple\nshape:Null\nndim:0;1\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.autograd.grad(outputs,inputs,grad_outputs=None,retain_graph=None,create_graph=False,only_inputs=True,allow_unused=False)\nParameter description: outputs(sequence of Tensor) - outputs of the differentiated function.\nConstraints:\ndtype:Null\nstructure:sequence;tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.repeat_interleave(input,repeats,dim=None)\nParameter description: dim(int, optional) - The dimension along which to repeat values.By default, use the flattened input array, and return a flat outputarray.\nConstraints:\ndtype:int\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.round(input,out=None)\nParameter description: input(Tensor) - the input tensor.\nConstraints:",
            "constr": {
                "dtype": [],
                "structure": [
                    "tensor"
                ],
                "shape": [],
                "ndim": [],
                "enum": [],
                "range": []
            },
            "signature": "torch.round(input,out=None)",
            "descp": "input(Tensor) - the input tensor."
        },
        {
            "fname": "torch.cuda.comm.reduce_add.yaml",
            "arg": "inputs",
            "prompt": "Signature: torch.distributed.recv(tensor,src=None,group=<objectobject>,tag=0)\nParameter description: tensor(Tensor) - Tensor to fill with received data.\nConstraints:\ndtype:Null\nstructure:tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.div(input,other,out=None)\nParameter description: out(Tensor, optional) - the output tensor.\nConstraints:\ndtype:Null\nstructure:tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.nn.TransformerDecoder(decoder_layer,num_layers,norm=None)\nParameter description: num_layers - the number of sub-decoder-layers in the decoder (required).\nConstraints:\ndtype:int\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:[0,inf)\n\n###\n\nSignature: torch.nn.quantized.functional.interpolate(input,size=None,scale_factor=None,mode=nearest,align_corners=None)\nParameter description: align_corners(bool, optional) - Geometrically, we consider the pixels of the input and output as squares rather than points. If set to `True`, the input and output tensors are aligned by the center points of their corner pixels, preserving the values at the corner pixels. If set to `False`, the input and output tensors are aligned by the corner points of their corner pixels, and the interpolation uses edge value padding for out-of-boundary values, making this operation independent of input size when `scale_factor` is kept the same. This only has an effect when `mode` is `'bilinear'`. Default: `False`\nConstraints:\ndtype:bool\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.gt(input,other,out=None)\nParameter description: out(Tensor, optional) - the output tensor that must be a BoolTensor\nConstraints:\ndtype:booltensor\nstructure:tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.empty_like(input,dtype=None,layout=None,device=None,requires_grad=False,memory_format=torch.preserve_format)\nParameter description: dtype(`torch.dtype`, optional) - the desired data type of returned Tensor. Default: if `None`, defaults to the dtype of `input`.\nConstraints:\ndtype:torch.dtype\nstructure:Null\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.nn.RNN(*args,**kwargs,input_size=None,hidden_size=None,num_layers=None,nonlinearity=None,bias=None,batch_first=None,dropout=None,bidirectional=None)\nParameter description: hidden_size - The number of features in the hidden state h\nConstraints:\ndtype:int\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:[0,inf)\n\n###\n\nSignature: torch.nn.functional.avg_pool1d(input,kernel_size,stride=None,padding=0,ceil_mode=False,count_include_pad=True)\nParameter description: padding - implicit zero paddings on both sides of the input. Can be a single number or a tuple (padW,). Default: 0\nConstraints:\ndtype:bool;int\nstructure:tuple\nshape:Null\nndim:0;1\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.autograd.grad(outputs,inputs,grad_outputs=None,retain_graph=None,create_graph=False,only_inputs=True,allow_unused=False)\nParameter description: outputs(sequence of Tensor) - outputs of the differentiated function.\nConstraints:\ndtype:Null\nstructure:sequence;tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.repeat_interleave(input,repeats,dim=None)\nParameter description: dim(int, optional) - The dimension along which to repeat values.By default, use the flattened input array, and return a flat outputarray.\nConstraints:\ndtype:int\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.cuda.comm.reduce_add(inputs,destination=None)\nParameter description: inputs(Iterable[Tensor]) - an iterable of tensors to add.\nConstraints:",
            "constr": {
                "dtype": [],
                "structure": [
                    "iterable",
                    "tensor"
                ],
                "shape": [],
                "ndim": [],
                "enum": [],
                "range": []
            },
            "signature": "torch.cuda.comm.reduce_add(inputs,destination=None)",
            "descp": "inputs(Iterable[Tensor]) - an iterable of tensors to add."
        },
        {
            "fname": "torch.nn.poissonnllloss.yaml",
            "arg": "size_average",
            "prompt": "Signature: torch.distributed.recv(tensor,src=None,group=<objectobject>,tag=0)\nParameter description: tensor(Tensor) - Tensor to fill with received data.\nConstraints:\ndtype:Null\nstructure:tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.div(input,other,out=None)\nParameter description: out(Tensor, optional) - the output tensor.\nConstraints:\ndtype:Null\nstructure:tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.nn.TransformerDecoder(decoder_layer,num_layers,norm=None)\nParameter description: num_layers - the number of sub-decoder-layers in the decoder (required).\nConstraints:\ndtype:int\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:[0,inf)\n\n###\n\nSignature: torch.nn.quantized.functional.interpolate(input,size=None,scale_factor=None,mode=nearest,align_corners=None)\nParameter description: align_corners(bool, optional) - Geometrically, we consider the pixels of the input and output as squares rather than points. If set to `True`, the input and output tensors are aligned by the center points of their corner pixels, preserving the values at the corner pixels. If set to `False`, the input and output tensors are aligned by the corner points of their corner pixels, and the interpolation uses edge value padding for out-of-boundary values, making this operation independent of input size when `scale_factor` is kept the same. This only has an effect when `mode` is `'bilinear'`. Default: `False`\nConstraints:\ndtype:bool\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.gt(input,other,out=None)\nParameter description: out(Tensor, optional) - the output tensor that must be a BoolTensor\nConstraints:\ndtype:booltensor\nstructure:tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.empty_like(input,dtype=None,layout=None,device=None,requires_grad=False,memory_format=torch.preserve_format)\nParameter description: dtype(`torch.dtype`, optional) - the desired data type of returned Tensor. Default: if `None`, defaults to the dtype of `input`.\nConstraints:\ndtype:torch.dtype\nstructure:Null\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.nn.RNN(*args,**kwargs,input_size=None,hidden_size=None,num_layers=None,nonlinearity=None,bias=None,batch_first=None,dropout=None,bidirectional=None)\nParameter description: hidden_size - The number of features in the hidden state h\nConstraints:\ndtype:int\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:[0,inf)\n\n###\n\nSignature: torch.nn.functional.avg_pool1d(input,kernel_size,stride=None,padding=0,ceil_mode=False,count_include_pad=True)\nParameter description: padding - implicit zero paddings on both sides of the input. Can be a single number or a tuple (padW,). Default: 0\nConstraints:\ndtype:bool;int\nstructure:tuple\nshape:Null\nndim:0;1\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.autograd.grad(outputs,inputs,grad_outputs=None,retain_graph=None,create_graph=False,only_inputs=True,allow_unused=False)\nParameter description: outputs(sequence of Tensor) - outputs of the differentiated function.\nConstraints:\ndtype:Null\nstructure:sequence;tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.repeat_interleave(input,repeats,dim=None)\nParameter description: dim(int, optional) - The dimension along which to repeat values.By default, use the flattened input array, and return a flat outputarray.\nConstraints:\ndtype:int\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.nn.PoissonNLLLoss(log_input=True,full=False,size_average=None,eps=1e-08,reduce=None,reduction=mean)\nParameter description: size_average(bool, optional) - Deprecated (see `reduction`). By default, the losses are averaged over each loss element in the batch. Note that for some losses, there are multiple elements per sample. If the field `size_average` is set to `False`, the losses are instead summed for each minibatch. Ignored when reduce is `False`. Default: `True`\nConstraints:",
            "constr": {
                "dtype": [
                    "bool"
                ],
                "structure": [],
                "shape": [],
                "ndim": [
                    "0"
                ],
                "enum": [],
                "range": []
            },
            "signature": "torch.nn.PoissonNLLLoss(log_input=True,full=False,size_average=None,eps=1e-08,reduce=None,reduction=mean)",
            "descp": "size_average(bool, optional) - Deprecated (see `reduction`). By default, the losses are averaged over each loss element in the batch. Note that for some losses, there are multiple elements per sample. If the field `size_average` is set to `False`, the losses are instead summed for each minibatch. Ignored when reduce is `False`. Default: `True`"
        },
        {
            "fname": "torch.unsqueeze.yaml",
            "arg": "dim",
            "prompt": "Signature: torch.distributed.recv(tensor,src=None,group=<objectobject>,tag=0)\nParameter description: tensor(Tensor) - Tensor to fill with received data.\nConstraints:\ndtype:Null\nstructure:tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.div(input,other,out=None)\nParameter description: out(Tensor, optional) - the output tensor.\nConstraints:\ndtype:Null\nstructure:tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.nn.TransformerDecoder(decoder_layer,num_layers,norm=None)\nParameter description: num_layers - the number of sub-decoder-layers in the decoder (required).\nConstraints:\ndtype:int\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:[0,inf)\n\n###\n\nSignature: torch.nn.quantized.functional.interpolate(input,size=None,scale_factor=None,mode=nearest,align_corners=None)\nParameter description: align_corners(bool, optional) - Geometrically, we consider the pixels of the input and output as squares rather than points. If set to `True`, the input and output tensors are aligned by the center points of their corner pixels, preserving the values at the corner pixels. If set to `False`, the input and output tensors are aligned by the corner points of their corner pixels, and the interpolation uses edge value padding for out-of-boundary values, making this operation independent of input size when `scale_factor` is kept the same. This only has an effect when `mode` is `'bilinear'`. Default: `False`\nConstraints:\ndtype:bool\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.gt(input,other,out=None)\nParameter description: out(Tensor, optional) - the output tensor that must be a BoolTensor\nConstraints:\ndtype:booltensor\nstructure:tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.empty_like(input,dtype=None,layout=None,device=None,requires_grad=False,memory_format=torch.preserve_format)\nParameter description: dtype(`torch.dtype`, optional) - the desired data type of returned Tensor. Default: if `None`, defaults to the dtype of `input`.\nConstraints:\ndtype:torch.dtype\nstructure:Null\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.nn.RNN(*args,**kwargs,input_size=None,hidden_size=None,num_layers=None,nonlinearity=None,bias=None,batch_first=None,dropout=None,bidirectional=None)\nParameter description: hidden_size - The number of features in the hidden state h\nConstraints:\ndtype:int\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:[0,inf)\n\n###\n\nSignature: torch.nn.functional.avg_pool1d(input,kernel_size,stride=None,padding=0,ceil_mode=False,count_include_pad=True)\nParameter description: padding - implicit zero paddings on both sides of the input. Can be a single number or a tuple (padW,). Default: 0\nConstraints:\ndtype:bool;int\nstructure:tuple\nshape:Null\nndim:0;1\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.autograd.grad(outputs,inputs,grad_outputs=None,retain_graph=None,create_graph=False,only_inputs=True,allow_unused=False)\nParameter description: outputs(sequence of Tensor) - outputs of the differentiated function.\nConstraints:\ndtype:Null\nstructure:sequence;tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.repeat_interleave(input,repeats,dim=None)\nParameter description: dim(int, optional) - The dimension along which to repeat values.By default, use the flattened input array, and return a flat outputarray.\nConstraints:\ndtype:int\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.unsqueeze(input,dim)\nParameter description: dim(int) - the index at which to insert the singleton dimension\nConstraints:",
            "constr": {
                "dtype": [
                    "int"
                ],
                "structure": [],
                "shape": [],
                "ndim": [
                    "0"
                ],
                "enum": [],
                "range": []
            },
            "signature": "torch.unsqueeze(input,dim)",
            "descp": "dim(int) - the index at which to insert the singleton dimension"
        },
        {
            "fname": "torch.nn.functional.avg_pool1d.yaml",
            "arg": "input",
            "prompt": "Signature: torch.distributed.recv(tensor,src=None,group=<objectobject>,tag=0)\nParameter description: tensor(Tensor) - Tensor to fill with received data.\nConstraints:\ndtype:Null\nstructure:tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.div(input,other,out=None)\nParameter description: out(Tensor, optional) - the output tensor.\nConstraints:\ndtype:Null\nstructure:tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.nn.TransformerDecoder(decoder_layer,num_layers,norm=None)\nParameter description: num_layers - the number of sub-decoder-layers in the decoder (required).\nConstraints:\ndtype:int\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:[0,inf)\n\n###\n\nSignature: torch.nn.quantized.functional.interpolate(input,size=None,scale_factor=None,mode=nearest,align_corners=None)\nParameter description: align_corners(bool, optional) - Geometrically, we consider the pixels of the input and output as squares rather than points. If set to `True`, the input and output tensors are aligned by the center points of their corner pixels, preserving the values at the corner pixels. If set to `False`, the input and output tensors are aligned by the corner points of their corner pixels, and the interpolation uses edge value padding for out-of-boundary values, making this operation independent of input size when `scale_factor` is kept the same. This only has an effect when `mode` is `'bilinear'`. Default: `False`\nConstraints:\ndtype:bool\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.gt(input,other,out=None)\nParameter description: out(Tensor, optional) - the output tensor that must be a BoolTensor\nConstraints:\ndtype:booltensor\nstructure:tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.empty_like(input,dtype=None,layout=None,device=None,requires_grad=False,memory_format=torch.preserve_format)\nParameter description: dtype(`torch.dtype`, optional) - the desired data type of returned Tensor. Default: if `None`, defaults to the dtype of `input`.\nConstraints:\ndtype:torch.dtype\nstructure:Null\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.nn.RNN(*args,**kwargs,input_size=None,hidden_size=None,num_layers=None,nonlinearity=None,bias=None,batch_first=None,dropout=None,bidirectional=None)\nParameter description: hidden_size - The number of features in the hidden state h\nConstraints:\ndtype:int\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:[0,inf)\n\n###\n\nSignature: torch.nn.functional.avg_pool1d(input,kernel_size,stride=None,padding=0,ceil_mode=False,count_include_pad=True)\nParameter description: padding - implicit zero paddings on both sides of the input. Can be a single number or a tuple (padW,). Default: 0\nConstraints:\ndtype:bool;int\nstructure:tuple\nshape:Null\nndim:0;1\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.autograd.grad(outputs,inputs,grad_outputs=None,retain_graph=None,create_graph=False,only_inputs=True,allow_unused=False)\nParameter description: outputs(sequence of Tensor) - outputs of the differentiated function.\nConstraints:\ndtype:Null\nstructure:sequence;tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.repeat_interleave(input,repeats,dim=None)\nParameter description: dim(int, optional) - The dimension along which to repeat values.By default, use the flattened input array, and return a flat outputarray.\nConstraints:\ndtype:int\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.nn.functional.avg_pool1d(input,kernel_size,stride=None,padding=0,ceil_mode=False,count_include_pad=True)\nParameter description: input - input tensor of shape (minibatch , in _channels , iW) \nConstraints:",
            "constr": {
                "dtype": [],
                "structure": [
                    "tensor"
                ],
                "shape": [
                    "(minibatch , in _channels , iW)"
                ],
                "ndim": [],
                "enum": [],
                "range": []
            },
            "signature": "torch.nn.functional.avg_pool1d(input,kernel_size,stride=None,padding=0,ceil_mode=False,count_include_pad=True)",
            "descp": "input - input tensor of shape (minibatch , in _channels , iW) "
        },
        {
            "fname": "torch.nn.replicationpad1d.yaml",
            "arg": "padding",
            "prompt": "Signature: torch.distributed.recv(tensor,src=None,group=<objectobject>,tag=0)\nParameter description: tensor(Tensor) - Tensor to fill with received data.\nConstraints:\ndtype:Null\nstructure:tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.div(input,other,out=None)\nParameter description: out(Tensor, optional) - the output tensor.\nConstraints:\ndtype:Null\nstructure:tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.nn.TransformerDecoder(decoder_layer,num_layers,norm=None)\nParameter description: num_layers - the number of sub-decoder-layers in the decoder (required).\nConstraints:\ndtype:int\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:[0,inf)\n\n###\n\nSignature: torch.nn.quantized.functional.interpolate(input,size=None,scale_factor=None,mode=nearest,align_corners=None)\nParameter description: align_corners(bool, optional) - Geometrically, we consider the pixels of the input and output as squares rather than points. If set to `True`, the input and output tensors are aligned by the center points of their corner pixels, preserving the values at the corner pixels. If set to `False`, the input and output tensors are aligned by the corner points of their corner pixels, and the interpolation uses edge value padding for out-of-boundary values, making this operation independent of input size when `scale_factor` is kept the same. This only has an effect when `mode` is `'bilinear'`. Default: `False`\nConstraints:\ndtype:bool\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.gt(input,other,out=None)\nParameter description: out(Tensor, optional) - the output tensor that must be a BoolTensor\nConstraints:\ndtype:booltensor\nstructure:tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.empty_like(input,dtype=None,layout=None,device=None,requires_grad=False,memory_format=torch.preserve_format)\nParameter description: dtype(`torch.dtype`, optional) - the desired data type of returned Tensor. Default: if `None`, defaults to the dtype of `input`.\nConstraints:\ndtype:torch.dtype\nstructure:Null\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.nn.RNN(*args,**kwargs,input_size=None,hidden_size=None,num_layers=None,nonlinearity=None,bias=None,batch_first=None,dropout=None,bidirectional=None)\nParameter description: hidden_size - The number of features in the hidden state h\nConstraints:\ndtype:int\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:[0,inf)\n\n###\n\nSignature: torch.nn.functional.avg_pool1d(input,kernel_size,stride=None,padding=0,ceil_mode=False,count_include_pad=True)\nParameter description: padding - implicit zero paddings on both sides of the input. Can be a single number or a tuple (padW,). Default: 0\nConstraints:\ndtype:bool;int\nstructure:tuple\nshape:Null\nndim:0;1\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.autograd.grad(outputs,inputs,grad_outputs=None,retain_graph=None,create_graph=False,only_inputs=True,allow_unused=False)\nParameter description: outputs(sequence of Tensor) - outputs of the differentiated function.\nConstraints:\ndtype:Null\nstructure:sequence;tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.repeat_interleave(input,repeats,dim=None)\nParameter description: dim(int, optional) - The dimension along which to repeat values.By default, use the flattened input array, and return a flat outputarray.\nConstraints:\ndtype:int\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.nn.ReplicationPad1d(padding)\nParameter description: padding(int, tuple) - the size of the padding. If is int, uses the same padding in all boundaries. If a 2-tuple, uses (padding _left , padding _right )\nConstraints:",
            "constr": {
                "dtype": [
                    "int"
                ],
                "structure": [
                    "tuple"
                ],
                "shape": [
                    "[2]"
                ],
                "ndim": [],
                "enum": [],
                "range": [
                    "[0,inf)"
                ]
            },
            "signature": "torch.nn.ReplicationPad1d(padding)",
            "descp": "padding(int, tuple) - the size of the padding. If is int, uses the same padding in all boundaries. If a 2-tuple, uses (padding _left , padding _right )"
        },
        {
            "fname": "torch.einsum.yaml",
            "arg": "*operands",
            "prompt": "Signature: torch.distributed.recv(tensor,src=None,group=<objectobject>,tag=0)\nParameter description: tensor(Tensor) - Tensor to fill with received data.\nConstraints:\ndtype:Null\nstructure:tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.div(input,other,out=None)\nParameter description: out(Tensor, optional) - the output tensor.\nConstraints:\ndtype:Null\nstructure:tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.nn.TransformerDecoder(decoder_layer,num_layers,norm=None)\nParameter description: num_layers - the number of sub-decoder-layers in the decoder (required).\nConstraints:\ndtype:int\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:[0,inf)\n\n###\n\nSignature: torch.nn.quantized.functional.interpolate(input,size=None,scale_factor=None,mode=nearest,align_corners=None)\nParameter description: align_corners(bool, optional) - Geometrically, we consider the pixels of the input and output as squares rather than points. If set to `True`, the input and output tensors are aligned by the center points of their corner pixels, preserving the values at the corner pixels. If set to `False`, the input and output tensors are aligned by the corner points of their corner pixels, and the interpolation uses edge value padding for out-of-boundary values, making this operation independent of input size when `scale_factor` is kept the same. This only has an effect when `mode` is `'bilinear'`. Default: `False`\nConstraints:\ndtype:bool\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.gt(input,other,out=None)\nParameter description: out(Tensor, optional) - the output tensor that must be a BoolTensor\nConstraints:\ndtype:booltensor\nstructure:tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.empty_like(input,dtype=None,layout=None,device=None,requires_grad=False,memory_format=torch.preserve_format)\nParameter description: dtype(`torch.dtype`, optional) - the desired data type of returned Tensor. Default: if `None`, defaults to the dtype of `input`.\nConstraints:\ndtype:torch.dtype\nstructure:Null\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.nn.RNN(*args,**kwargs,input_size=None,hidden_size=None,num_layers=None,nonlinearity=None,bias=None,batch_first=None,dropout=None,bidirectional=None)\nParameter description: hidden_size - The number of features in the hidden state h\nConstraints:\ndtype:int\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:[0,inf)\n\n###\n\nSignature: torch.nn.functional.avg_pool1d(input,kernel_size,stride=None,padding=0,ceil_mode=False,count_include_pad=True)\nParameter description: padding - implicit zero paddings on both sides of the input. Can be a single number or a tuple (padW,). Default: 0\nConstraints:\ndtype:bool;int\nstructure:tuple\nshape:Null\nndim:0;1\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.autograd.grad(outputs,inputs,grad_outputs=None,retain_graph=None,create_graph=False,only_inputs=True,allow_unused=False)\nParameter description: outputs(sequence of Tensor) - outputs of the differentiated function.\nConstraints:\ndtype:Null\nstructure:sequence;tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.repeat_interleave(input,repeats,dim=None)\nParameter description: dim(int, optional) - The dimension along which to repeat values.By default, use the flattened input array, and return a flat outputarray.\nConstraints:\ndtype:int\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.einsum(equation,*operands)\nParameter description: *operands(Tensor) - The operands to compute the Einstein sum of.\nConstraints:",
            "constr": {
                "dtype": [],
                "structure": [
                    "tensor"
                ],
                "shape": [],
                "ndim": [],
                "enum": [],
                "range": []
            },
            "signature": "torch.einsum(equation,*operands)",
            "descp": "*operands(Tensor) - The operands to compute the Einstein sum of."
        },
        {
            "fname": "torch.normal2.yaml",
            "arg": "std",
            "prompt": "Signature: torch.distributed.recv(tensor,src=None,group=<objectobject>,tag=0)\nParameter description: tensor(Tensor) - Tensor to fill with received data.\nConstraints:\ndtype:Null\nstructure:tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.div(input,other,out=None)\nParameter description: out(Tensor, optional) - the output tensor.\nConstraints:\ndtype:Null\nstructure:tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.nn.TransformerDecoder(decoder_layer,num_layers,norm=None)\nParameter description: num_layers - the number of sub-decoder-layers in the decoder (required).\nConstraints:\ndtype:int\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:[0,inf)\n\n###\n\nSignature: torch.nn.quantized.functional.interpolate(input,size=None,scale_factor=None,mode=nearest,align_corners=None)\nParameter description: align_corners(bool, optional) - Geometrically, we consider the pixels of the input and output as squares rather than points. If set to `True`, the input and output tensors are aligned by the center points of their corner pixels, preserving the values at the corner pixels. If set to `False`, the input and output tensors are aligned by the corner points of their corner pixels, and the interpolation uses edge value padding for out-of-boundary values, making this operation independent of input size when `scale_factor` is kept the same. This only has an effect when `mode` is `'bilinear'`. Default: `False`\nConstraints:\ndtype:bool\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.gt(input,other,out=None)\nParameter description: out(Tensor, optional) - the output tensor that must be a BoolTensor\nConstraints:\ndtype:booltensor\nstructure:tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.empty_like(input,dtype=None,layout=None,device=None,requires_grad=False,memory_format=torch.preserve_format)\nParameter description: dtype(`torch.dtype`, optional) - the desired data type of returned Tensor. Default: if `None`, defaults to the dtype of `input`.\nConstraints:\ndtype:torch.dtype\nstructure:Null\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.nn.RNN(*args,**kwargs,input_size=None,hidden_size=None,num_layers=None,nonlinearity=None,bias=None,batch_first=None,dropout=None,bidirectional=None)\nParameter description: hidden_size - The number of features in the hidden state h\nConstraints:\ndtype:int\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:[0,inf)\n\n###\n\nSignature: torch.nn.functional.avg_pool1d(input,kernel_size,stride=None,padding=0,ceil_mode=False,count_include_pad=True)\nParameter description: padding - implicit zero paddings on both sides of the input. Can be a single number or a tuple (padW,). Default: 0\nConstraints:\ndtype:bool;int\nstructure:tuple\nshape:Null\nndim:0;1\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.autograd.grad(outputs,inputs,grad_outputs=None,retain_graph=None,create_graph=False,only_inputs=True,allow_unused=False)\nParameter description: outputs(sequence of Tensor) - outputs of the differentiated function.\nConstraints:\ndtype:Null\nstructure:sequence;tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.repeat_interleave(input,repeats,dim=None)\nParameter description: dim(int, optional) - The dimension along which to repeat values.By default, use the flattened input array, and return a flat outputarray.\nConstraints:\ndtype:int\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.normal(std,mean=0.0,out=None)\nParameter description: std(Tensor) - the tensor of per-element standard deviations\nConstraints:",
            "constr": {
                "dtype": [
                    "numeric"
                ],
                "structure": [
                    "tensor"
                ],
                "shape": [],
                "ndim": [],
                "enum": [],
                "range": []
            },
            "signature": "torch.normal(std,mean=0.0,out=None)",
            "descp": "std(Tensor) - the tensor of per-element standard deviations"
        },
        {
            "fname": "torch.nn.functional.affine_grid.yaml",
            "arg": "size",
            "prompt": "Signature: torch.distributed.recv(tensor,src=None,group=<objectobject>,tag=0)\nParameter description: tensor(Tensor) - Tensor to fill with received data.\nConstraints:\ndtype:Null\nstructure:tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.div(input,other,out=None)\nParameter description: out(Tensor, optional) - the output tensor.\nConstraints:\ndtype:Null\nstructure:tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.nn.TransformerDecoder(decoder_layer,num_layers,norm=None)\nParameter description: num_layers - the number of sub-decoder-layers in the decoder (required).\nConstraints:\ndtype:int\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:[0,inf)\n\n###\n\nSignature: torch.nn.quantized.functional.interpolate(input,size=None,scale_factor=None,mode=nearest,align_corners=None)\nParameter description: align_corners(bool, optional) - Geometrically, we consider the pixels of the input and output as squares rather than points. If set to `True`, the input and output tensors are aligned by the center points of their corner pixels, preserving the values at the corner pixels. If set to `False`, the input and output tensors are aligned by the corner points of their corner pixels, and the interpolation uses edge value padding for out-of-boundary values, making this operation independent of input size when `scale_factor` is kept the same. This only has an effect when `mode` is `'bilinear'`. Default: `False`\nConstraints:\ndtype:bool\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.gt(input,other,out=None)\nParameter description: out(Tensor, optional) - the output tensor that must be a BoolTensor\nConstraints:\ndtype:booltensor\nstructure:tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.empty_like(input,dtype=None,layout=None,device=None,requires_grad=False,memory_format=torch.preserve_format)\nParameter description: dtype(`torch.dtype`, optional) - the desired data type of returned Tensor. Default: if `None`, defaults to the dtype of `input`.\nConstraints:\ndtype:torch.dtype\nstructure:Null\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.nn.RNN(*args,**kwargs,input_size=None,hidden_size=None,num_layers=None,nonlinearity=None,bias=None,batch_first=None,dropout=None,bidirectional=None)\nParameter description: hidden_size - The number of features in the hidden state h\nConstraints:\ndtype:int\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:[0,inf)\n\n###\n\nSignature: torch.nn.functional.avg_pool1d(input,kernel_size,stride=None,padding=0,ceil_mode=False,count_include_pad=True)\nParameter description: padding - implicit zero paddings on both sides of the input. Can be a single number or a tuple (padW,). Default: 0\nConstraints:\ndtype:bool;int\nstructure:tuple\nshape:Null\nndim:0;1\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.autograd.grad(outputs,inputs,grad_outputs=None,retain_graph=None,create_graph=False,only_inputs=True,allow_unused=False)\nParameter description: outputs(sequence of Tensor) - outputs of the differentiated function.\nConstraints:\ndtype:Null\nstructure:sequence;tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.repeat_interleave(input,repeats,dim=None)\nParameter description: dim(int, optional) - The dimension along which to repeat values.By default, use the flattened input array, and return a flat outputarray.\nConstraints:\ndtype:int\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.nn.functional.affine_grid(theta,size,align_corners=None)\nParameter description: size(torch.Size) - the target output image size. (N  times C  times H  times W  for 2D or N  times C  times D  times H  times W  for 3D) Example: torch.Size((32, 3, 24, 24))\nConstraints:",
            "constr": {
                "dtype": [
                    "int"
                ],
                "structure": [],
                "shape": [],
                "ndim": [
                    "1"
                ],
                "enum": [],
                "range": [
                    "[0,inf)"
                ]
            },
            "signature": "torch.nn.functional.affine_grid(theta,size,align_corners=None)",
            "descp": "size(torch.Size) - the target output image size. (N  times C  times H  times W  for 2D or N  times C  times D  times H  times W  for 3D) Example: torch.Size((32, 3, 24, 24))"
        },
        {
            "fname": "torch.poisson.yaml",
            "arg": "input*",
            "prompt": "Signature: torch.distributed.recv(tensor,src=None,group=<objectobject>,tag=0)\nParameter description: tensor(Tensor) - Tensor to fill with received data.\nConstraints:\ndtype:Null\nstructure:tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.div(input,other,out=None)\nParameter description: out(Tensor, optional) - the output tensor.\nConstraints:\ndtype:Null\nstructure:tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.nn.TransformerDecoder(decoder_layer,num_layers,norm=None)\nParameter description: num_layers - the number of sub-decoder-layers in the decoder (required).\nConstraints:\ndtype:int\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:[0,inf)\n\n###\n\nSignature: torch.nn.quantized.functional.interpolate(input,size=None,scale_factor=None,mode=nearest,align_corners=None)\nParameter description: align_corners(bool, optional) - Geometrically, we consider the pixels of the input and output as squares rather than points. If set to `True`, the input and output tensors are aligned by the center points of their corner pixels, preserving the values at the corner pixels. If set to `False`, the input and output tensors are aligned by the corner points of their corner pixels, and the interpolation uses edge value padding for out-of-boundary values, making this operation independent of input size when `scale_factor` is kept the same. This only has an effect when `mode` is `'bilinear'`. Default: `False`\nConstraints:\ndtype:bool\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.gt(input,other,out=None)\nParameter description: out(Tensor, optional) - the output tensor that must be a BoolTensor\nConstraints:\ndtype:booltensor\nstructure:tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.empty_like(input,dtype=None,layout=None,device=None,requires_grad=False,memory_format=torch.preserve_format)\nParameter description: dtype(`torch.dtype`, optional) - the desired data type of returned Tensor. Default: if `None`, defaults to the dtype of `input`.\nConstraints:\ndtype:torch.dtype\nstructure:Null\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.nn.RNN(*args,**kwargs,input_size=None,hidden_size=None,num_layers=None,nonlinearity=None,bias=None,batch_first=None,dropout=None,bidirectional=None)\nParameter description: hidden_size - The number of features in the hidden state h\nConstraints:\ndtype:int\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:[0,inf)\n\n###\n\nSignature: torch.nn.functional.avg_pool1d(input,kernel_size,stride=None,padding=0,ceil_mode=False,count_include_pad=True)\nParameter description: padding - implicit zero paddings on both sides of the input. Can be a single number or a tuple (padW,). Default: 0\nConstraints:\ndtype:bool;int\nstructure:tuple\nshape:Null\nndim:0;1\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.autograd.grad(outputs,inputs,grad_outputs=None,retain_graph=None,create_graph=False,only_inputs=True,allow_unused=False)\nParameter description: outputs(sequence of Tensor) - outputs of the differentiated function.\nConstraints:\ndtype:Null\nstructure:sequence;tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.repeat_interleave(input,repeats,dim=None)\nParameter description: dim(int, optional) - The dimension along which to repeat values.By default, use the flattened input array, and return a flat outputarray.\nConstraints:\ndtype:int\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.poisson(input*,generator=None)\nParameter description: input*(Tensor) - the input tensor containing the rates of the Poisson distribution\nConstraints:",
            "constr": {
                "dtype": [],
                "structure": [
                    "tensor"
                ],
                "shape": [],
                "ndim": [],
                "enum": [],
                "range": [
                    "[0,1]"
                ]
            },
            "signature": "torch.poisson(input*,generator=None)",
            "descp": "input*(Tensor) - the input tensor containing the rates of the Poisson distribution"
        },
        {
            "fname": "torch.where.yaml",
            "arg": "x",
            "prompt": "Signature: torch.distributed.recv(tensor,src=None,group=<objectobject>,tag=0)\nParameter description: tensor(Tensor) - Tensor to fill with received data.\nConstraints:\ndtype:Null\nstructure:tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.div(input,other,out=None)\nParameter description: out(Tensor, optional) - the output tensor.\nConstraints:\ndtype:Null\nstructure:tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.nn.TransformerDecoder(decoder_layer,num_layers,norm=None)\nParameter description: num_layers - the number of sub-decoder-layers in the decoder (required).\nConstraints:\ndtype:int\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:[0,inf)\n\n###\n\nSignature: torch.nn.quantized.functional.interpolate(input,size=None,scale_factor=None,mode=nearest,align_corners=None)\nParameter description: align_corners(bool, optional) - Geometrically, we consider the pixels of the input and output as squares rather than points. If set to `True`, the input and output tensors are aligned by the center points of their corner pixels, preserving the values at the corner pixels. If set to `False`, the input and output tensors are aligned by the corner points of their corner pixels, and the interpolation uses edge value padding for out-of-boundary values, making this operation independent of input size when `scale_factor` is kept the same. This only has an effect when `mode` is `'bilinear'`. Default: `False`\nConstraints:\ndtype:bool\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.gt(input,other,out=None)\nParameter description: out(Tensor, optional) - the output tensor that must be a BoolTensor\nConstraints:\ndtype:booltensor\nstructure:tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.empty_like(input,dtype=None,layout=None,device=None,requires_grad=False,memory_format=torch.preserve_format)\nParameter description: dtype(`torch.dtype`, optional) - the desired data type of returned Tensor. Default: if `None`, defaults to the dtype of `input`.\nConstraints:\ndtype:torch.dtype\nstructure:Null\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.nn.RNN(*args,**kwargs,input_size=None,hidden_size=None,num_layers=None,nonlinearity=None,bias=None,batch_first=None,dropout=None,bidirectional=None)\nParameter description: hidden_size - The number of features in the hidden state h\nConstraints:\ndtype:int\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:[0,inf)\n\n###\n\nSignature: torch.nn.functional.avg_pool1d(input,kernel_size,stride=None,padding=0,ceil_mode=False,count_include_pad=True)\nParameter description: padding - implicit zero paddings on both sides of the input. Can be a single number or a tuple (padW,). Default: 0\nConstraints:\ndtype:bool;int\nstructure:tuple\nshape:Null\nndim:0;1\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.autograd.grad(outputs,inputs,grad_outputs=None,retain_graph=None,create_graph=False,only_inputs=True,allow_unused=False)\nParameter description: outputs(sequence of Tensor) - outputs of the differentiated function.\nConstraints:\ndtype:Null\nstructure:sequence;tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.repeat_interleave(input,repeats,dim=None)\nParameter description: dim(int, optional) - The dimension along which to repeat values.By default, use the flattened input array, and return a flat outputarray.\nConstraints:\ndtype:int\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.where(condition,x,y)\nParameter description: x(Tensor) - values selected at indices where `condition` is `True`\nConstraints:",
            "constr": {
                "dtype": [],
                "structure": [
                    "tensor"
                ],
                "shape": [],
                "ndim": [],
                "enum": [],
                "range": []
            },
            "signature": "torch.where(condition,x,y)",
            "descp": "x(Tensor) - values selected at indices where `condition` is `True`"
        },
        {
            "fname": "torch.nn.utils.rnn.pad_sequence.yaml",
            "arg": "padding_value",
            "prompt": "Signature: torch.distributed.recv(tensor,src=None,group=<objectobject>,tag=0)\nParameter description: tensor(Tensor) - Tensor to fill with received data.\nConstraints:\ndtype:Null\nstructure:tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.div(input,other,out=None)\nParameter description: out(Tensor, optional) - the output tensor.\nConstraints:\ndtype:Null\nstructure:tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.nn.TransformerDecoder(decoder_layer,num_layers,norm=None)\nParameter description: num_layers - the number of sub-decoder-layers in the decoder (required).\nConstraints:\ndtype:int\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:[0,inf)\n\n###\n\nSignature: torch.nn.quantized.functional.interpolate(input,size=None,scale_factor=None,mode=nearest,align_corners=None)\nParameter description: align_corners(bool, optional) - Geometrically, we consider the pixels of the input and output as squares rather than points. If set to `True`, the input and output tensors are aligned by the center points of their corner pixels, preserving the values at the corner pixels. If set to `False`, the input and output tensors are aligned by the corner points of their corner pixels, and the interpolation uses edge value padding for out-of-boundary values, making this operation independent of input size when `scale_factor` is kept the same. This only has an effect when `mode` is `'bilinear'`. Default: `False`\nConstraints:\ndtype:bool\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.gt(input,other,out=None)\nParameter description: out(Tensor, optional) - the output tensor that must be a BoolTensor\nConstraints:\ndtype:booltensor\nstructure:tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.empty_like(input,dtype=None,layout=None,device=None,requires_grad=False,memory_format=torch.preserve_format)\nParameter description: dtype(`torch.dtype`, optional) - the desired data type of returned Tensor. Default: if `None`, defaults to the dtype of `input`.\nConstraints:\ndtype:torch.dtype\nstructure:Null\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.nn.RNN(*args,**kwargs,input_size=None,hidden_size=None,num_layers=None,nonlinearity=None,bias=None,batch_first=None,dropout=None,bidirectional=None)\nParameter description: hidden_size - The number of features in the hidden state h\nConstraints:\ndtype:int\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:[0,inf)\n\n###\n\nSignature: torch.nn.functional.avg_pool1d(input,kernel_size,stride=None,padding=0,ceil_mode=False,count_include_pad=True)\nParameter description: padding - implicit zero paddings on both sides of the input. Can be a single number or a tuple (padW,). Default: 0\nConstraints:\ndtype:bool;int\nstructure:tuple\nshape:Null\nndim:0;1\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.autograd.grad(outputs,inputs,grad_outputs=None,retain_graph=None,create_graph=False,only_inputs=True,allow_unused=False)\nParameter description: outputs(sequence of Tensor) - outputs of the differentiated function.\nConstraints:\ndtype:Null\nstructure:sequence;tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.repeat_interleave(input,repeats,dim=None)\nParameter description: dim(int, optional) - The dimension along which to repeat values.By default, use the flattened input array, and return a flat outputarray.\nConstraints:\ndtype:int\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.nn.utils.rnn.pad_sequence(sequences,batch_first=False,padding_value=0)\nParameter description: padding_value(float, optional) - value for padded elements. Default: 0.\nConstraints:",
            "constr": {
                "dtype": [
                    "float",
                    "int"
                ],
                "structure": [],
                "shape": [],
                "ndim": [
                    "0"
                ],
                "enum": [],
                "range": []
            },
            "signature": "torch.nn.utils.rnn.pad_sequence(sequences,batch_first=False,padding_value=0)",
            "descp": "padding_value(float, optional) - value for padded elements. Default: 0."
        },
        {
            "fname": "torch.eye.yaml",
            "arg": "requires_grad",
            "prompt": "Signature: torch.distributed.recv(tensor,src=None,group=<objectobject>,tag=0)\nParameter description: tensor(Tensor) - Tensor to fill with received data.\nConstraints:\ndtype:Null\nstructure:tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.div(input,other,out=None)\nParameter description: out(Tensor, optional) - the output tensor.\nConstraints:\ndtype:Null\nstructure:tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.nn.TransformerDecoder(decoder_layer,num_layers,norm=None)\nParameter description: num_layers - the number of sub-decoder-layers in the decoder (required).\nConstraints:\ndtype:int\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:[0,inf)\n\n###\n\nSignature: torch.nn.quantized.functional.interpolate(input,size=None,scale_factor=None,mode=nearest,align_corners=None)\nParameter description: align_corners(bool, optional) - Geometrically, we consider the pixels of the input and output as squares rather than points. If set to `True`, the input and output tensors are aligned by the center points of their corner pixels, preserving the values at the corner pixels. If set to `False`, the input and output tensors are aligned by the corner points of their corner pixels, and the interpolation uses edge value padding for out-of-boundary values, making this operation independent of input size when `scale_factor` is kept the same. This only has an effect when `mode` is `'bilinear'`. Default: `False`\nConstraints:\ndtype:bool\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.gt(input,other,out=None)\nParameter description: out(Tensor, optional) - the output tensor that must be a BoolTensor\nConstraints:\ndtype:booltensor\nstructure:tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.empty_like(input,dtype=None,layout=None,device=None,requires_grad=False,memory_format=torch.preserve_format)\nParameter description: dtype(`torch.dtype`, optional) - the desired data type of returned Tensor. Default: if `None`, defaults to the dtype of `input`.\nConstraints:\ndtype:torch.dtype\nstructure:Null\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.nn.RNN(*args,**kwargs,input_size=None,hidden_size=None,num_layers=None,nonlinearity=None,bias=None,batch_first=None,dropout=None,bidirectional=None)\nParameter description: hidden_size - The number of features in the hidden state h\nConstraints:\ndtype:int\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:[0,inf)\n\n###\n\nSignature: torch.nn.functional.avg_pool1d(input,kernel_size,stride=None,padding=0,ceil_mode=False,count_include_pad=True)\nParameter description: padding - implicit zero paddings on both sides of the input. Can be a single number or a tuple (padW,). Default: 0\nConstraints:\ndtype:bool;int\nstructure:tuple\nshape:Null\nndim:0;1\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.autograd.grad(outputs,inputs,grad_outputs=None,retain_graph=None,create_graph=False,only_inputs=True,allow_unused=False)\nParameter description: outputs(sequence of Tensor) - outputs of the differentiated function.\nConstraints:\ndtype:Null\nstructure:sequence;tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.repeat_interleave(input,repeats,dim=None)\nParameter description: dim(int, optional) - The dimension along which to repeat values.By default, use the flattened input array, and return a flat outputarray.\nConstraints:\ndtype:int\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.eye(n,m=None,out=None,dtype=None,layout=torch.strided,device=None,requires_grad=False)\nParameter description: requires_grad(bool, optional) - If autograd should record operations on the returned tensor. Default: `False`.\nConstraints:",
            "constr": {
                "dtype": [
                    "bool"
                ],
                "structure": [],
                "shape": [],
                "ndim": [
                    "0"
                ],
                "enum": [],
                "range": []
            },
            "signature": "torch.eye(n,m=None,out=None,dtype=None,layout=torch.strided,device=None,requires_grad=False)",
            "descp": "requires_grad(bool, optional) - If autograd should record operations on the returned tensor. Default: `False`."
        },
        {
            "fname": "torch.nn.quantized.functional.conv3d.yaml",
            "arg": "groups",
            "prompt": "Signature: torch.distributed.recv(tensor,src=None,group=<objectobject>,tag=0)\nParameter description: tensor(Tensor) - Tensor to fill with received data.\nConstraints:\ndtype:Null\nstructure:tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.div(input,other,out=None)\nParameter description: out(Tensor, optional) - the output tensor.\nConstraints:\ndtype:Null\nstructure:tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.nn.TransformerDecoder(decoder_layer,num_layers,norm=None)\nParameter description: num_layers - the number of sub-decoder-layers in the decoder (required).\nConstraints:\ndtype:int\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:[0,inf)\n\n###\n\nSignature: torch.nn.quantized.functional.interpolate(input,size=None,scale_factor=None,mode=nearest,align_corners=None)\nParameter description: align_corners(bool, optional) - Geometrically, we consider the pixels of the input and output as squares rather than points. If set to `True`, the input and output tensors are aligned by the center points of their corner pixels, preserving the values at the corner pixels. If set to `False`, the input and output tensors are aligned by the corner points of their corner pixels, and the interpolation uses edge value padding for out-of-boundary values, making this operation independent of input size when `scale_factor` is kept the same. This only has an effect when `mode` is `'bilinear'`. Default: `False`\nConstraints:\ndtype:bool\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.gt(input,other,out=None)\nParameter description: out(Tensor, optional) - the output tensor that must be a BoolTensor\nConstraints:\ndtype:booltensor\nstructure:tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.empty_like(input,dtype=None,layout=None,device=None,requires_grad=False,memory_format=torch.preserve_format)\nParameter description: dtype(`torch.dtype`, optional) - the desired data type of returned Tensor. Default: if `None`, defaults to the dtype of `input`.\nConstraints:\ndtype:torch.dtype\nstructure:Null\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.nn.RNN(*args,**kwargs,input_size=None,hidden_size=None,num_layers=None,nonlinearity=None,bias=None,batch_first=None,dropout=None,bidirectional=None)\nParameter description: hidden_size - The number of features in the hidden state h\nConstraints:\ndtype:int\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:[0,inf)\n\n###\n\nSignature: torch.nn.functional.avg_pool1d(input,kernel_size,stride=None,padding=0,ceil_mode=False,count_include_pad=True)\nParameter description: padding - implicit zero paddings on both sides of the input. Can be a single number or a tuple (padW,). Default: 0\nConstraints:\ndtype:bool;int\nstructure:tuple\nshape:Null\nndim:0;1\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.autograd.grad(outputs,inputs,grad_outputs=None,retain_graph=None,create_graph=False,only_inputs=True,allow_unused=False)\nParameter description: outputs(sequence of Tensor) - outputs of the differentiated function.\nConstraints:\ndtype:Null\nstructure:sequence;tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.repeat_interleave(input,repeats,dim=None)\nParameter description: dim(int, optional) - The dimension along which to repeat values.By default, use the flattened input array, and return a flat outputarray.\nConstraints:\ndtype:int\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.nn.quantized.functional.conv3d(input,weight,bias,stride=1,padding=0,dilation=1,groups=1,padding_mode=zeros,scale=1.0,zero_point=0,dtype=torch.quint8)\nParameter description: groups - split input into groups, in _channels  should be divisible by the number of groups. Default: 1\nConstraints:",
            "constr": {
                "dtype": [
                    "int"
                ],
                "structure": [],
                "shape": [],
                "ndim": [
                    "0"
                ],
                "enum": [],
                "range": []
            },
            "signature": "torch.nn.quantized.functional.conv3d(input,weight,bias,stride=1,padding=0,dilation=1,groups=1,padding_mode=zeros,scale=1.0,zero_point=0,dtype=torch.quint8)",
            "descp": "groups - split input into groups, in _channels  should be divisible by the number of groups. Default: 1"
        },
        {
            "fname": "torch.nn.hardtanh.yaml",
            "arg": "inplace",
            "prompt": "Signature: torch.distributed.recv(tensor,src=None,group=<objectobject>,tag=0)\nParameter description: tensor(Tensor) - Tensor to fill with received data.\nConstraints:\ndtype:Null\nstructure:tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.div(input,other,out=None)\nParameter description: out(Tensor, optional) - the output tensor.\nConstraints:\ndtype:Null\nstructure:tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.nn.TransformerDecoder(decoder_layer,num_layers,norm=None)\nParameter description: num_layers - the number of sub-decoder-layers in the decoder (required).\nConstraints:\ndtype:int\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:[0,inf)\n\n###\n\nSignature: torch.nn.quantized.functional.interpolate(input,size=None,scale_factor=None,mode=nearest,align_corners=None)\nParameter description: align_corners(bool, optional) - Geometrically, we consider the pixels of the input and output as squares rather than points. If set to `True`, the input and output tensors are aligned by the center points of their corner pixels, preserving the values at the corner pixels. If set to `False`, the input and output tensors are aligned by the corner points of their corner pixels, and the interpolation uses edge value padding for out-of-boundary values, making this operation independent of input size when `scale_factor` is kept the same. This only has an effect when `mode` is `'bilinear'`. Default: `False`\nConstraints:\ndtype:bool\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.gt(input,other,out=None)\nParameter description: out(Tensor, optional) - the output tensor that must be a BoolTensor\nConstraints:\ndtype:booltensor\nstructure:tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.empty_like(input,dtype=None,layout=None,device=None,requires_grad=False,memory_format=torch.preserve_format)\nParameter description: dtype(`torch.dtype`, optional) - the desired data type of returned Tensor. Default: if `None`, defaults to the dtype of `input`.\nConstraints:\ndtype:torch.dtype\nstructure:Null\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.nn.RNN(*args,**kwargs,input_size=None,hidden_size=None,num_layers=None,nonlinearity=None,bias=None,batch_first=None,dropout=None,bidirectional=None)\nParameter description: hidden_size - The number of features in the hidden state h\nConstraints:\ndtype:int\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:[0,inf)\n\n###\n\nSignature: torch.nn.functional.avg_pool1d(input,kernel_size,stride=None,padding=0,ceil_mode=False,count_include_pad=True)\nParameter description: padding - implicit zero paddings on both sides of the input. Can be a single number or a tuple (padW,). Default: 0\nConstraints:\ndtype:bool;int\nstructure:tuple\nshape:Null\nndim:0;1\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.autograd.grad(outputs,inputs,grad_outputs=None,retain_graph=None,create_graph=False,only_inputs=True,allow_unused=False)\nParameter description: outputs(sequence of Tensor) - outputs of the differentiated function.\nConstraints:\ndtype:Null\nstructure:sequence;tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.repeat_interleave(input,repeats,dim=None)\nParameter description: dim(int, optional) - The dimension along which to repeat values.By default, use the flattened input array, and return a flat outputarray.\nConstraints:\ndtype:int\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.nn.Hardtanh(min_val=-1.0,max_val=1.0,inplace=False,min_value=None,max_value=None)\nParameter description: inplace - can optionally do the operation in-place. Default: `False`\nConstraints:",
            "constr": {
                "dtype": [
                    "bool"
                ],
                "structure": [],
                "shape": [],
                "ndim": [
                    "0"
                ],
                "enum": [],
                "range": []
            },
            "signature": "torch.nn.Hardtanh(min_val=-1.0,max_val=1.0,inplace=False,min_value=None,max_value=None)",
            "descp": "inplace - can optionally do the operation in-place. Default: `False`"
        },
        {
            "fname": "torch.set_grad_enabled.yaml",
            "arg": "mode",
            "prompt": "Signature: torch.distributed.recv(tensor,src=None,group=<objectobject>,tag=0)\nParameter description: tensor(Tensor) - Tensor to fill with received data.\nConstraints:\ndtype:Null\nstructure:tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.div(input,other,out=None)\nParameter description: out(Tensor, optional) - the output tensor.\nConstraints:\ndtype:Null\nstructure:tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.nn.TransformerDecoder(decoder_layer,num_layers,norm=None)\nParameter description: num_layers - the number of sub-decoder-layers in the decoder (required).\nConstraints:\ndtype:int\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:[0,inf)\n\n###\n\nSignature: torch.nn.quantized.functional.interpolate(input,size=None,scale_factor=None,mode=nearest,align_corners=None)\nParameter description: align_corners(bool, optional) - Geometrically, we consider the pixels of the input and output as squares rather than points. If set to `True`, the input and output tensors are aligned by the center points of their corner pixels, preserving the values at the corner pixels. If set to `False`, the input and output tensors are aligned by the corner points of their corner pixels, and the interpolation uses edge value padding for out-of-boundary values, making this operation independent of input size when `scale_factor` is kept the same. This only has an effect when `mode` is `'bilinear'`. Default: `False`\nConstraints:\ndtype:bool\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.gt(input,other,out=None)\nParameter description: out(Tensor, optional) - the output tensor that must be a BoolTensor\nConstraints:\ndtype:booltensor\nstructure:tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.empty_like(input,dtype=None,layout=None,device=None,requires_grad=False,memory_format=torch.preserve_format)\nParameter description: dtype(`torch.dtype`, optional) - the desired data type of returned Tensor. Default: if `None`, defaults to the dtype of `input`.\nConstraints:\ndtype:torch.dtype\nstructure:Null\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.nn.RNN(*args,**kwargs,input_size=None,hidden_size=None,num_layers=None,nonlinearity=None,bias=None,batch_first=None,dropout=None,bidirectional=None)\nParameter description: hidden_size - The number of features in the hidden state h\nConstraints:\ndtype:int\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:[0,inf)\n\n###\n\nSignature: torch.nn.functional.avg_pool1d(input,kernel_size,stride=None,padding=0,ceil_mode=False,count_include_pad=True)\nParameter description: padding - implicit zero paddings on both sides of the input. Can be a single number or a tuple (padW,). Default: 0\nConstraints:\ndtype:bool;int\nstructure:tuple\nshape:Null\nndim:0;1\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.autograd.grad(outputs,inputs,grad_outputs=None,retain_graph=None,create_graph=False,only_inputs=True,allow_unused=False)\nParameter description: outputs(sequence of Tensor) - outputs of the differentiated function.\nConstraints:\ndtype:Null\nstructure:sequence;tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.repeat_interleave(input,repeats,dim=None)\nParameter description: dim(int, optional) - The dimension along which to repeat values.By default, use the flattened input array, and return a flat outputarray.\nConstraints:\ndtype:int\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.set_grad_enabled(mode)\nParameter description: mode(bool) - Flag whether to enable grad (`True`), or disable (`False`). This can be used to conditionally enable gradients.\nConstraints:",
            "constr": {
                "dtype": [
                    "bool"
                ],
                "structure": [],
                "shape": [],
                "ndim": [
                    "0"
                ],
                "enum": [],
                "range": []
            },
            "signature": "torch.set_grad_enabled(mode)",
            "descp": "mode(bool) - Flag whether to enable grad (`True`), or disable (`False`). This can be used to conditionally enable gradients."
        },
        {
            "fname": "torch.nn.quantized.functional.conv2d.yaml",
            "arg": "input",
            "prompt": "Signature: torch.distributed.recv(tensor,src=None,group=<objectobject>,tag=0)\nParameter description: tensor(Tensor) - Tensor to fill with received data.\nConstraints:\ndtype:Null\nstructure:tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.div(input,other,out=None)\nParameter description: out(Tensor, optional) - the output tensor.\nConstraints:\ndtype:Null\nstructure:tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.nn.TransformerDecoder(decoder_layer,num_layers,norm=None)\nParameter description: num_layers - the number of sub-decoder-layers in the decoder (required).\nConstraints:\ndtype:int\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:[0,inf)\n\n###\n\nSignature: torch.nn.quantized.functional.interpolate(input,size=None,scale_factor=None,mode=nearest,align_corners=None)\nParameter description: align_corners(bool, optional) - Geometrically, we consider the pixels of the input and output as squares rather than points. If set to `True`, the input and output tensors are aligned by the center points of their corner pixels, preserving the values at the corner pixels. If set to `False`, the input and output tensors are aligned by the corner points of their corner pixels, and the interpolation uses edge value padding for out-of-boundary values, making this operation independent of input size when `scale_factor` is kept the same. This only has an effect when `mode` is `'bilinear'`. Default: `False`\nConstraints:\ndtype:bool\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.gt(input,other,out=None)\nParameter description: out(Tensor, optional) - the output tensor that must be a BoolTensor\nConstraints:\ndtype:booltensor\nstructure:tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.empty_like(input,dtype=None,layout=None,device=None,requires_grad=False,memory_format=torch.preserve_format)\nParameter description: dtype(`torch.dtype`, optional) - the desired data type of returned Tensor. Default: if `None`, defaults to the dtype of `input`.\nConstraints:\ndtype:torch.dtype\nstructure:Null\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.nn.RNN(*args,**kwargs,input_size=None,hidden_size=None,num_layers=None,nonlinearity=None,bias=None,batch_first=None,dropout=None,bidirectional=None)\nParameter description: hidden_size - The number of features in the hidden state h\nConstraints:\ndtype:int\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:[0,inf)\n\n###\n\nSignature: torch.nn.functional.avg_pool1d(input,kernel_size,stride=None,padding=0,ceil_mode=False,count_include_pad=True)\nParameter description: padding - implicit zero paddings on both sides of the input. Can be a single number or a tuple (padW,). Default: 0\nConstraints:\ndtype:bool;int\nstructure:tuple\nshape:Null\nndim:0;1\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.autograd.grad(outputs,inputs,grad_outputs=None,retain_graph=None,create_graph=False,only_inputs=True,allow_unused=False)\nParameter description: outputs(sequence of Tensor) - outputs of the differentiated function.\nConstraints:\ndtype:Null\nstructure:sequence;tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.repeat_interleave(input,repeats,dim=None)\nParameter description: dim(int, optional) - The dimension along which to repeat values.By default, use the flattened input array, and return a flat outputarray.\nConstraints:\ndtype:int\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.nn.quantized.functional.conv2d(input,weight,bias,stride=1,padding=0,dilation=1,groups=1,padding_mode=zeros,scale=1.0,zero_point=0,dtype=torch.quint8)\nParameter description: input - quantized input tensor of shape (minibatch , in _channels , iH , iW) \nConstraints:",
            "constr": {
                "dtype": [],
                "structure": [
                    "tensor"
                ],
                "shape": [
                    "(minibatch , in _channels , iH , iW)"
                ],
                "ndim": [],
                "enum": [],
                "range": []
            },
            "signature": "torch.nn.quantized.functional.conv2d(input,weight,bias,stride=1,padding=0,dilation=1,groups=1,padding_mode=zeros,scale=1.0,zero_point=0,dtype=torch.quint8)",
            "descp": "input - quantized input tensor of shape (minibatch , in _channels , iH , iW) "
        },
        {
            "fname": "torch.min2.yaml",
            "arg": "out",
            "prompt": "Signature: torch.distributed.recv(tensor,src=None,group=<objectobject>,tag=0)\nParameter description: tensor(Tensor) - Tensor to fill with received data.\nConstraints:\ndtype:Null\nstructure:tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.div(input,other,out=None)\nParameter description: out(Tensor, optional) - the output tensor.\nConstraints:\ndtype:Null\nstructure:tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.nn.TransformerDecoder(decoder_layer,num_layers,norm=None)\nParameter description: num_layers - the number of sub-decoder-layers in the decoder (required).\nConstraints:\ndtype:int\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:[0,inf)\n\n###\n\nSignature: torch.nn.quantized.functional.interpolate(input,size=None,scale_factor=None,mode=nearest,align_corners=None)\nParameter description: align_corners(bool, optional) - Geometrically, we consider the pixels of the input and output as squares rather than points. If set to `True`, the input and output tensors are aligned by the center points of their corner pixels, preserving the values at the corner pixels. If set to `False`, the input and output tensors are aligned by the corner points of their corner pixels, and the interpolation uses edge value padding for out-of-boundary values, making this operation independent of input size when `scale_factor` is kept the same. This only has an effect when `mode` is `'bilinear'`. Default: `False`\nConstraints:\ndtype:bool\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.gt(input,other,out=None)\nParameter description: out(Tensor, optional) - the output tensor that must be a BoolTensor\nConstraints:\ndtype:booltensor\nstructure:tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.empty_like(input,dtype=None,layout=None,device=None,requires_grad=False,memory_format=torch.preserve_format)\nParameter description: dtype(`torch.dtype`, optional) - the desired data type of returned Tensor. Default: if `None`, defaults to the dtype of `input`.\nConstraints:\ndtype:torch.dtype\nstructure:Null\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.nn.RNN(*args,**kwargs,input_size=None,hidden_size=None,num_layers=None,nonlinearity=None,bias=None,batch_first=None,dropout=None,bidirectional=None)\nParameter description: hidden_size - The number of features in the hidden state h\nConstraints:\ndtype:int\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:[0,inf)\n\n###\n\nSignature: torch.nn.functional.avg_pool1d(input,kernel_size,stride=None,padding=0,ceil_mode=False,count_include_pad=True)\nParameter description: padding - implicit zero paddings on both sides of the input. Can be a single number or a tuple (padW,). Default: 0\nConstraints:\ndtype:bool;int\nstructure:tuple\nshape:Null\nndim:0;1\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.autograd.grad(outputs,inputs,grad_outputs=None,retain_graph=None,create_graph=False,only_inputs=True,allow_unused=False)\nParameter description: outputs(sequence of Tensor) - outputs of the differentiated function.\nConstraints:\ndtype:Null\nstructure:sequence;tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.repeat_interleave(input,repeats,dim=None)\nParameter description: dim(int, optional) - The dimension along which to repeat values.By default, use the flattened input array, and return a flat outputarray.\nConstraints:\ndtype:int\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.min(input,dim,keepdim=False,out=None)\nParameter description: out(tuple, optional) - the tuple of two output tensors (min, min_indices)\nConstraints:",
            "constr": {
                "dtype": [],
                "structure": [
                    "tuple",
                    "tensor"
                ],
                "shape": [],
                "ndim": [],
                "enum": [],
                "range": []
            },
            "signature": "torch.min(input,dim,keepdim=False,out=None)",
            "descp": "out(tuple, optional) - the tuple of two output tensors (min, min_indices)"
        },
        {
            "fname": "torch.nn.functional.avg_pool2d.yaml",
            "arg": "count_include_pad",
            "prompt": "Signature: torch.distributed.recv(tensor,src=None,group=<objectobject>,tag=0)\nParameter description: tensor(Tensor) - Tensor to fill with received data.\nConstraints:\ndtype:Null\nstructure:tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.div(input,other,out=None)\nParameter description: out(Tensor, optional) - the output tensor.\nConstraints:\ndtype:Null\nstructure:tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.nn.TransformerDecoder(decoder_layer,num_layers,norm=None)\nParameter description: num_layers - the number of sub-decoder-layers in the decoder (required).\nConstraints:\ndtype:int\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:[0,inf)\n\n###\n\nSignature: torch.nn.quantized.functional.interpolate(input,size=None,scale_factor=None,mode=nearest,align_corners=None)\nParameter description: align_corners(bool, optional) - Geometrically, we consider the pixels of the input and output as squares rather than points. If set to `True`, the input and output tensors are aligned by the center points of their corner pixels, preserving the values at the corner pixels. If set to `False`, the input and output tensors are aligned by the corner points of their corner pixels, and the interpolation uses edge value padding for out-of-boundary values, making this operation independent of input size when `scale_factor` is kept the same. This only has an effect when `mode` is `'bilinear'`. Default: `False`\nConstraints:\ndtype:bool\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.gt(input,other,out=None)\nParameter description: out(Tensor, optional) - the output tensor that must be a BoolTensor\nConstraints:\ndtype:booltensor\nstructure:tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.empty_like(input,dtype=None,layout=None,device=None,requires_grad=False,memory_format=torch.preserve_format)\nParameter description: dtype(`torch.dtype`, optional) - the desired data type of returned Tensor. Default: if `None`, defaults to the dtype of `input`.\nConstraints:\ndtype:torch.dtype\nstructure:Null\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.nn.RNN(*args,**kwargs,input_size=None,hidden_size=None,num_layers=None,nonlinearity=None,bias=None,batch_first=None,dropout=None,bidirectional=None)\nParameter description: hidden_size - The number of features in the hidden state h\nConstraints:\ndtype:int\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:[0,inf)\n\n###\n\nSignature: torch.nn.functional.avg_pool1d(input,kernel_size,stride=None,padding=0,ceil_mode=False,count_include_pad=True)\nParameter description: padding - implicit zero paddings on both sides of the input. Can be a single number or a tuple (padW,). Default: 0\nConstraints:\ndtype:bool;int\nstructure:tuple\nshape:Null\nndim:0;1\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.autograd.grad(outputs,inputs,grad_outputs=None,retain_graph=None,create_graph=False,only_inputs=True,allow_unused=False)\nParameter description: outputs(sequence of Tensor) - outputs of the differentiated function.\nConstraints:\ndtype:Null\nstructure:sequence;tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.repeat_interleave(input,repeats,dim=None)\nParameter description: dim(int, optional) - The dimension along which to repeat values.By default, use the flattened input array, and return a flat outputarray.\nConstraints:\ndtype:int\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.nn.functional.avg_pool2d(input,kernel_size,stride=None,padding=0,ceil_mode=False,count_include_pad=True,divisor_override=None)\nParameter description: count_include_pad - when True, will include the zero-padding in the averaging calculation. Default: `True`\nConstraints:",
            "constr": {
                "dtype": [
                    "bool"
                ],
                "structure": [],
                "shape": [],
                "ndim": [
                    "0"
                ],
                "enum": [],
                "range": []
            },
            "signature": "torch.nn.functional.avg_pool2d(input,kernel_size,stride=None,padding=0,ceil_mode=False,count_include_pad=True,divisor_override=None)",
            "descp": "count_include_pad - when True, will include the zero-padding in the averaging calculation. Default: `True`"
        },
        {
            "fname": "torch.ifft.yaml",
            "arg": "normalized",
            "prompt": "Signature: torch.distributed.recv(tensor,src=None,group=<objectobject>,tag=0)\nParameter description: tensor(Tensor) - Tensor to fill with received data.\nConstraints:\ndtype:Null\nstructure:tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.div(input,other,out=None)\nParameter description: out(Tensor, optional) - the output tensor.\nConstraints:\ndtype:Null\nstructure:tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.nn.TransformerDecoder(decoder_layer,num_layers,norm=None)\nParameter description: num_layers - the number of sub-decoder-layers in the decoder (required).\nConstraints:\ndtype:int\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:[0,inf)\n\n###\n\nSignature: torch.nn.quantized.functional.interpolate(input,size=None,scale_factor=None,mode=nearest,align_corners=None)\nParameter description: align_corners(bool, optional) - Geometrically, we consider the pixels of the input and output as squares rather than points. If set to `True`, the input and output tensors are aligned by the center points of their corner pixels, preserving the values at the corner pixels. If set to `False`, the input and output tensors are aligned by the corner points of their corner pixels, and the interpolation uses edge value padding for out-of-boundary values, making this operation independent of input size when `scale_factor` is kept the same. This only has an effect when `mode` is `'bilinear'`. Default: `False`\nConstraints:\ndtype:bool\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.gt(input,other,out=None)\nParameter description: out(Tensor, optional) - the output tensor that must be a BoolTensor\nConstraints:\ndtype:booltensor\nstructure:tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.empty_like(input,dtype=None,layout=None,device=None,requires_grad=False,memory_format=torch.preserve_format)\nParameter description: dtype(`torch.dtype`, optional) - the desired data type of returned Tensor. Default: if `None`, defaults to the dtype of `input`.\nConstraints:\ndtype:torch.dtype\nstructure:Null\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.nn.RNN(*args,**kwargs,input_size=None,hidden_size=None,num_layers=None,nonlinearity=None,bias=None,batch_first=None,dropout=None,bidirectional=None)\nParameter description: hidden_size - The number of features in the hidden state h\nConstraints:\ndtype:int\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:[0,inf)\n\n###\n\nSignature: torch.nn.functional.avg_pool1d(input,kernel_size,stride=None,padding=0,ceil_mode=False,count_include_pad=True)\nParameter description: padding - implicit zero paddings on both sides of the input. Can be a single number or a tuple (padW,). Default: 0\nConstraints:\ndtype:bool;int\nstructure:tuple\nshape:Null\nndim:0;1\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.autograd.grad(outputs,inputs,grad_outputs=None,retain_graph=None,create_graph=False,only_inputs=True,allow_unused=False)\nParameter description: outputs(sequence of Tensor) - outputs of the differentiated function.\nConstraints:\ndtype:Null\nstructure:sequence;tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.repeat_interleave(input,repeats,dim=None)\nParameter description: dim(int, optional) - The dimension along which to repeat values.By default, use the flattened input array, and return a flat outputarray.\nConstraints:\ndtype:int\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.ifft(input,signal_ndim,normalized=False)\nParameter description: normalized(bool, optional) - controls whether to return normalized results. Default: `False`\nConstraints:",
            "constr": {
                "dtype": [
                    "bool"
                ],
                "structure": [],
                "shape": [],
                "ndim": [
                    "0"
                ],
                "enum": [],
                "range": []
            },
            "signature": "torch.ifft(input,signal_ndim,normalized=False)",
            "descp": "normalized(bool, optional) - controls whether to return normalized results. Default: `False`"
        },
        {
            "fname": "torch.nn.functional.avg_pool1d.yaml",
            "arg": "ceil_mode",
            "prompt": "Signature: torch.distributed.recv(tensor,src=None,group=<objectobject>,tag=0)\nParameter description: tensor(Tensor) - Tensor to fill with received data.\nConstraints:\ndtype:Null\nstructure:tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.div(input,other,out=None)\nParameter description: out(Tensor, optional) - the output tensor.\nConstraints:\ndtype:Null\nstructure:tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.nn.TransformerDecoder(decoder_layer,num_layers,norm=None)\nParameter description: num_layers - the number of sub-decoder-layers in the decoder (required).\nConstraints:\ndtype:int\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:[0,inf)\n\n###\n\nSignature: torch.nn.quantized.functional.interpolate(input,size=None,scale_factor=None,mode=nearest,align_corners=None)\nParameter description: align_corners(bool, optional) - Geometrically, we consider the pixels of the input and output as squares rather than points. If set to `True`, the input and output tensors are aligned by the center points of their corner pixels, preserving the values at the corner pixels. If set to `False`, the input and output tensors are aligned by the corner points of their corner pixels, and the interpolation uses edge value padding for out-of-boundary values, making this operation independent of input size when `scale_factor` is kept the same. This only has an effect when `mode` is `'bilinear'`. Default: `False`\nConstraints:\ndtype:bool\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.gt(input,other,out=None)\nParameter description: out(Tensor, optional) - the output tensor that must be a BoolTensor\nConstraints:\ndtype:booltensor\nstructure:tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.empty_like(input,dtype=None,layout=None,device=None,requires_grad=False,memory_format=torch.preserve_format)\nParameter description: dtype(`torch.dtype`, optional) - the desired data type of returned Tensor. Default: if `None`, defaults to the dtype of `input`.\nConstraints:\ndtype:torch.dtype\nstructure:Null\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.nn.RNN(*args,**kwargs,input_size=None,hidden_size=None,num_layers=None,nonlinearity=None,bias=None,batch_first=None,dropout=None,bidirectional=None)\nParameter description: hidden_size - The number of features in the hidden state h\nConstraints:\ndtype:int\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:[0,inf)\n\n###\n\nSignature: torch.nn.functional.avg_pool1d(input,kernel_size,stride=None,padding=0,ceil_mode=False,count_include_pad=True)\nParameter description: padding - implicit zero paddings on both sides of the input. Can be a single number or a tuple (padW,). Default: 0\nConstraints:\ndtype:bool;int\nstructure:tuple\nshape:Null\nndim:0;1\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.autograd.grad(outputs,inputs,grad_outputs=None,retain_graph=None,create_graph=False,only_inputs=True,allow_unused=False)\nParameter description: outputs(sequence of Tensor) - outputs of the differentiated function.\nConstraints:\ndtype:Null\nstructure:sequence;tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.repeat_interleave(input,repeats,dim=None)\nParameter description: dim(int, optional) - The dimension along which to repeat values.By default, use the flattened input array, and return a flat outputarray.\nConstraints:\ndtype:int\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.nn.functional.avg_pool1d(input,kernel_size,stride=None,padding=0,ceil_mode=False,count_include_pad=True)\nParameter description: ceil_mode - when True, will use ceil instead of floor to compute the output shape. Default: `False`\nConstraints:",
            "constr": {
                "dtype": [
                    "bool"
                ],
                "structure": [],
                "shape": [],
                "ndim": [
                    "0"
                ],
                "enum": [],
                "range": []
            },
            "signature": "torch.nn.functional.avg_pool1d(input,kernel_size,stride=None,padding=0,ceil_mode=False,count_include_pad=True)",
            "descp": "ceil_mode - when True, will use ceil instead of floor to compute the output shape. Default: `False`"
        },
        {
            "fname": "torch.nn.quantized.functional.avg_pool2d.yaml",
            "arg": "kernel_size",
            "prompt": "Signature: torch.distributed.recv(tensor,src=None,group=<objectobject>,tag=0)\nParameter description: tensor(Tensor) - Tensor to fill with received data.\nConstraints:\ndtype:Null\nstructure:tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.div(input,other,out=None)\nParameter description: out(Tensor, optional) - the output tensor.\nConstraints:\ndtype:Null\nstructure:tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.nn.TransformerDecoder(decoder_layer,num_layers,norm=None)\nParameter description: num_layers - the number of sub-decoder-layers in the decoder (required).\nConstraints:\ndtype:int\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:[0,inf)\n\n###\n\nSignature: torch.nn.quantized.functional.interpolate(input,size=None,scale_factor=None,mode=nearest,align_corners=None)\nParameter description: align_corners(bool, optional) - Geometrically, we consider the pixels of the input and output as squares rather than points. If set to `True`, the input and output tensors are aligned by the center points of their corner pixels, preserving the values at the corner pixels. If set to `False`, the input and output tensors are aligned by the corner points of their corner pixels, and the interpolation uses edge value padding for out-of-boundary values, making this operation independent of input size when `scale_factor` is kept the same. This only has an effect when `mode` is `'bilinear'`. Default: `False`\nConstraints:\ndtype:bool\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.gt(input,other,out=None)\nParameter description: out(Tensor, optional) - the output tensor that must be a BoolTensor\nConstraints:\ndtype:booltensor\nstructure:tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.empty_like(input,dtype=None,layout=None,device=None,requires_grad=False,memory_format=torch.preserve_format)\nParameter description: dtype(`torch.dtype`, optional) - the desired data type of returned Tensor. Default: if `None`, defaults to the dtype of `input`.\nConstraints:\ndtype:torch.dtype\nstructure:Null\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.nn.RNN(*args,**kwargs,input_size=None,hidden_size=None,num_layers=None,nonlinearity=None,bias=None,batch_first=None,dropout=None,bidirectional=None)\nParameter description: hidden_size - The number of features in the hidden state h\nConstraints:\ndtype:int\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:[0,inf)\n\n###\n\nSignature: torch.nn.functional.avg_pool1d(input,kernel_size,stride=None,padding=0,ceil_mode=False,count_include_pad=True)\nParameter description: padding - implicit zero paddings on both sides of the input. Can be a single number or a tuple (padW,). Default: 0\nConstraints:\ndtype:bool;int\nstructure:tuple\nshape:Null\nndim:0;1\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.autograd.grad(outputs,inputs,grad_outputs=None,retain_graph=None,create_graph=False,only_inputs=True,allow_unused=False)\nParameter description: outputs(sequence of Tensor) - outputs of the differentiated function.\nConstraints:\ndtype:Null\nstructure:sequence;tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.repeat_interleave(input,repeats,dim=None)\nParameter description: dim(int, optional) - The dimension along which to repeat values.By default, use the flattened input array, and return a flat outputarray.\nConstraints:\ndtype:int\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.nn.quantized.functional.avg_pool2d(input,kernel_size,stride=None,padding=0,ceil_mode=False,count_include_pad=True,divisor_override=None)\nParameter description: kernel_size - size of the pooling region. Can be a single number or a tuple (kH, kW)\nConstraints:",
            "constr": {
                "dtype": [
                    "int"
                ],
                "structure": [
                    "tuple"
                ],
                "shape": [],
                "ndim": [
                    "0",
                    "1"
                ],
                "enum": [],
                "range": [
                    "[0,inf)"
                ]
            },
            "signature": "torch.nn.quantized.functional.avg_pool2d(input,kernel_size,stride=None,padding=0,ceil_mode=False,count_include_pad=True,divisor_override=None)",
            "descp": "kernel_size - size of the pooling region. Can be a single number or a tuple (kH, kW)"
        },
        {
            "fname": "torch.nn.poissonnllloss.yaml",
            "arg": "eps",
            "prompt": "Signature: torch.distributed.recv(tensor,src=None,group=<objectobject>,tag=0)\nParameter description: tensor(Tensor) - Tensor to fill with received data.\nConstraints:\ndtype:Null\nstructure:tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.div(input,other,out=None)\nParameter description: out(Tensor, optional) - the output tensor.\nConstraints:\ndtype:Null\nstructure:tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.nn.TransformerDecoder(decoder_layer,num_layers,norm=None)\nParameter description: num_layers - the number of sub-decoder-layers in the decoder (required).\nConstraints:\ndtype:int\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:[0,inf)\n\n###\n\nSignature: torch.nn.quantized.functional.interpolate(input,size=None,scale_factor=None,mode=nearest,align_corners=None)\nParameter description: align_corners(bool, optional) - Geometrically, we consider the pixels of the input and output as squares rather than points. If set to `True`, the input and output tensors are aligned by the center points of their corner pixels, preserving the values at the corner pixels. If set to `False`, the input and output tensors are aligned by the corner points of their corner pixels, and the interpolation uses edge value padding for out-of-boundary values, making this operation independent of input size when `scale_factor` is kept the same. This only has an effect when `mode` is `'bilinear'`. Default: `False`\nConstraints:\ndtype:bool\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.gt(input,other,out=None)\nParameter description: out(Tensor, optional) - the output tensor that must be a BoolTensor\nConstraints:\ndtype:booltensor\nstructure:tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.empty_like(input,dtype=None,layout=None,device=None,requires_grad=False,memory_format=torch.preserve_format)\nParameter description: dtype(`torch.dtype`, optional) - the desired data type of returned Tensor. Default: if `None`, defaults to the dtype of `input`.\nConstraints:\ndtype:torch.dtype\nstructure:Null\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.nn.RNN(*args,**kwargs,input_size=None,hidden_size=None,num_layers=None,nonlinearity=None,bias=None,batch_first=None,dropout=None,bidirectional=None)\nParameter description: hidden_size - The number of features in the hidden state h\nConstraints:\ndtype:int\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:[0,inf)\n\n###\n\nSignature: torch.nn.functional.avg_pool1d(input,kernel_size,stride=None,padding=0,ceil_mode=False,count_include_pad=True)\nParameter description: padding - implicit zero paddings on both sides of the input. Can be a single number or a tuple (padW,). Default: 0\nConstraints:\ndtype:bool;int\nstructure:tuple\nshape:Null\nndim:0;1\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.autograd.grad(outputs,inputs,grad_outputs=None,retain_graph=None,create_graph=False,only_inputs=True,allow_unused=False)\nParameter description: outputs(sequence of Tensor) - outputs of the differentiated function.\nConstraints:\ndtype:Null\nstructure:sequence;tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.repeat_interleave(input,repeats,dim=None)\nParameter description: dim(int, optional) - The dimension along which to repeat values.By default, use the flattened input array, and return a flat outputarray.\nConstraints:\ndtype:int\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.nn.PoissonNLLLoss(log_input=True,full=False,size_average=None,eps=1e-08,reduce=None,reduction=mean)\nParameter description: eps(float, optional) - Small value to avoid evaluation of  log(0)  when `log_input = False`. Default: 1e-8\nConstraints:",
            "constr": {
                "dtype": [
                    "numeric",
                    "float"
                ],
                "structure": [],
                "shape": [],
                "ndim": [
                    "0"
                ],
                "enum": [],
                "range": []
            },
            "signature": "torch.nn.PoissonNLLLoss(log_input=True,full=False,size_average=None,eps=1e-08,reduce=None,reduction=mean)",
            "descp": "eps(float, optional) - Small value to avoid evaluation of  log(0)  when `log_input = False`. Default: 1e-8"
        },
        {
            "fname": "torch.ne.yaml",
            "arg": "input",
            "prompt": "Signature: torch.distributed.recv(tensor,src=None,group=<objectobject>,tag=0)\nParameter description: tensor(Tensor) - Tensor to fill with received data.\nConstraints:\ndtype:Null\nstructure:tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.div(input,other,out=None)\nParameter description: out(Tensor, optional) - the output tensor.\nConstraints:\ndtype:Null\nstructure:tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.nn.TransformerDecoder(decoder_layer,num_layers,norm=None)\nParameter description: num_layers - the number of sub-decoder-layers in the decoder (required).\nConstraints:\ndtype:int\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:[0,inf)\n\n###\n\nSignature: torch.nn.quantized.functional.interpolate(input,size=None,scale_factor=None,mode=nearest,align_corners=None)\nParameter description: align_corners(bool, optional) - Geometrically, we consider the pixels of the input and output as squares rather than points. If set to `True`, the input and output tensors are aligned by the center points of their corner pixels, preserving the values at the corner pixels. If set to `False`, the input and output tensors are aligned by the corner points of their corner pixels, and the interpolation uses edge value padding for out-of-boundary values, making this operation independent of input size when `scale_factor` is kept the same. This only has an effect when `mode` is `'bilinear'`. Default: `False`\nConstraints:\ndtype:bool\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.gt(input,other,out=None)\nParameter description: out(Tensor, optional) - the output tensor that must be a BoolTensor\nConstraints:\ndtype:booltensor\nstructure:tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.empty_like(input,dtype=None,layout=None,device=None,requires_grad=False,memory_format=torch.preserve_format)\nParameter description: dtype(`torch.dtype`, optional) - the desired data type of returned Tensor. Default: if `None`, defaults to the dtype of `input`.\nConstraints:\ndtype:torch.dtype\nstructure:Null\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.nn.RNN(*args,**kwargs,input_size=None,hidden_size=None,num_layers=None,nonlinearity=None,bias=None,batch_first=None,dropout=None,bidirectional=None)\nParameter description: hidden_size - The number of features in the hidden state h\nConstraints:\ndtype:int\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:[0,inf)\n\n###\n\nSignature: torch.nn.functional.avg_pool1d(input,kernel_size,stride=None,padding=0,ceil_mode=False,count_include_pad=True)\nParameter description: padding - implicit zero paddings on both sides of the input. Can be a single number or a tuple (padW,). Default: 0\nConstraints:\ndtype:bool;int\nstructure:tuple\nshape:Null\nndim:0;1\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.autograd.grad(outputs,inputs,grad_outputs=None,retain_graph=None,create_graph=False,only_inputs=True,allow_unused=False)\nParameter description: outputs(sequence of Tensor) - outputs of the differentiated function.\nConstraints:\ndtype:Null\nstructure:sequence;tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.repeat_interleave(input,repeats,dim=None)\nParameter description: dim(int, optional) - The dimension along which to repeat values.By default, use the flattened input array, and return a flat outputarray.\nConstraints:\ndtype:int\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.ne(input,other,out=None)\nParameter description: input(Tensor) - the tensor to compare\nConstraints:",
            "constr": {
                "dtype": [],
                "structure": [
                    "tensor"
                ],
                "shape": [],
                "ndim": [],
                "enum": [],
                "range": []
            },
            "signature": "torch.ne(input,other,out=None)",
            "descp": "input(Tensor) - the tensor to compare"
        },
        {
            "fname": "torch.ger.yaml",
            "arg": "vec2",
            "prompt": "Signature: torch.distributed.recv(tensor,src=None,group=<objectobject>,tag=0)\nParameter description: tensor(Tensor) - Tensor to fill with received data.\nConstraints:\ndtype:Null\nstructure:tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.div(input,other,out=None)\nParameter description: out(Tensor, optional) - the output tensor.\nConstraints:\ndtype:Null\nstructure:tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.nn.TransformerDecoder(decoder_layer,num_layers,norm=None)\nParameter description: num_layers - the number of sub-decoder-layers in the decoder (required).\nConstraints:\ndtype:int\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:[0,inf)\n\n###\n\nSignature: torch.nn.quantized.functional.interpolate(input,size=None,scale_factor=None,mode=nearest,align_corners=None)\nParameter description: align_corners(bool, optional) - Geometrically, we consider the pixels of the input and output as squares rather than points. If set to `True`, the input and output tensors are aligned by the center points of their corner pixels, preserving the values at the corner pixels. If set to `False`, the input and output tensors are aligned by the corner points of their corner pixels, and the interpolation uses edge value padding for out-of-boundary values, making this operation independent of input size when `scale_factor` is kept the same. This only has an effect when `mode` is `'bilinear'`. Default: `False`\nConstraints:\ndtype:bool\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.gt(input,other,out=None)\nParameter description: out(Tensor, optional) - the output tensor that must be a BoolTensor\nConstraints:\ndtype:booltensor\nstructure:tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.empty_like(input,dtype=None,layout=None,device=None,requires_grad=False,memory_format=torch.preserve_format)\nParameter description: dtype(`torch.dtype`, optional) - the desired data type of returned Tensor. Default: if `None`, defaults to the dtype of `input`.\nConstraints:\ndtype:torch.dtype\nstructure:Null\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.nn.RNN(*args,**kwargs,input_size=None,hidden_size=None,num_layers=None,nonlinearity=None,bias=None,batch_first=None,dropout=None,bidirectional=None)\nParameter description: hidden_size - The number of features in the hidden state h\nConstraints:\ndtype:int\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:[0,inf)\n\n###\n\nSignature: torch.nn.functional.avg_pool1d(input,kernel_size,stride=None,padding=0,ceil_mode=False,count_include_pad=True)\nParameter description: padding - implicit zero paddings on both sides of the input. Can be a single number or a tuple (padW,). Default: 0\nConstraints:\ndtype:bool;int\nstructure:tuple\nshape:Null\nndim:0;1\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.autograd.grad(outputs,inputs,grad_outputs=None,retain_graph=None,create_graph=False,only_inputs=True,allow_unused=False)\nParameter description: outputs(sequence of Tensor) - outputs of the differentiated function.\nConstraints:\ndtype:Null\nstructure:sequence;tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.repeat_interleave(input,repeats,dim=None)\nParameter description: dim(int, optional) - The dimension along which to repeat values.By default, use the flattened input array, and return a flat outputarray.\nConstraints:\ndtype:int\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.ger(input,vec2,out=None)\nParameter description: vec2(Tensor) - 1-D input vector\nConstraints:",
            "constr": {
                "dtype": [],
                "structure": [
                    "vector",
                    "tensor"
                ],
                "shape": [],
                "ndim": [
                    "1"
                ],
                "enum": [],
                "range": []
            },
            "signature": "torch.ger(input,vec2,out=None)",
            "descp": "vec2(Tensor) - 1-D input vector"
        },
        {
            "fname": "torch.addbmm.yaml",
            "arg": "beta",
            "prompt": "Signature: torch.distributed.recv(tensor,src=None,group=<objectobject>,tag=0)\nParameter description: tensor(Tensor) - Tensor to fill with received data.\nConstraints:\ndtype:Null\nstructure:tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.div(input,other,out=None)\nParameter description: out(Tensor, optional) - the output tensor.\nConstraints:\ndtype:Null\nstructure:tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.nn.TransformerDecoder(decoder_layer,num_layers,norm=None)\nParameter description: num_layers - the number of sub-decoder-layers in the decoder (required).\nConstraints:\ndtype:int\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:[0,inf)\n\n###\n\nSignature: torch.nn.quantized.functional.interpolate(input,size=None,scale_factor=None,mode=nearest,align_corners=None)\nParameter description: align_corners(bool, optional) - Geometrically, we consider the pixels of the input and output as squares rather than points. If set to `True`, the input and output tensors are aligned by the center points of their corner pixels, preserving the values at the corner pixels. If set to `False`, the input and output tensors are aligned by the corner points of their corner pixels, and the interpolation uses edge value padding for out-of-boundary values, making this operation independent of input size when `scale_factor` is kept the same. This only has an effect when `mode` is `'bilinear'`. Default: `False`\nConstraints:\ndtype:bool\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.gt(input,other,out=None)\nParameter description: out(Tensor, optional) - the output tensor that must be a BoolTensor\nConstraints:\ndtype:booltensor\nstructure:tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.empty_like(input,dtype=None,layout=None,device=None,requires_grad=False,memory_format=torch.preserve_format)\nParameter description: dtype(`torch.dtype`, optional) - the desired data type of returned Tensor. Default: if `None`, defaults to the dtype of `input`.\nConstraints:\ndtype:torch.dtype\nstructure:Null\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.nn.RNN(*args,**kwargs,input_size=None,hidden_size=None,num_layers=None,nonlinearity=None,bias=None,batch_first=None,dropout=None,bidirectional=None)\nParameter description: hidden_size - The number of features in the hidden state h\nConstraints:\ndtype:int\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:[0,inf)\n\n###\n\nSignature: torch.nn.functional.avg_pool1d(input,kernel_size,stride=None,padding=0,ceil_mode=False,count_include_pad=True)\nParameter description: padding - implicit zero paddings on both sides of the input. Can be a single number or a tuple (padW,). Default: 0\nConstraints:\ndtype:bool;int\nstructure:tuple\nshape:Null\nndim:0;1\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.autograd.grad(outputs,inputs,grad_outputs=None,retain_graph=None,create_graph=False,only_inputs=True,allow_unused=False)\nParameter description: outputs(sequence of Tensor) - outputs of the differentiated function.\nConstraints:\ndtype:Null\nstructure:sequence;tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.repeat_interleave(input,repeats,dim=None)\nParameter description: dim(int, optional) - The dimension along which to repeat values.By default, use the flattened input array, and return a flat outputarray.\nConstraints:\ndtype:int\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.addbmm(input,batch1,batch2,beta=1,alpha=1,out=None)\nParameter description: beta(Number, optional) - multiplier for `input` ( beta )\nConstraints:",
            "constr": {
                "dtype": [
                    "numeric",
                    "int"
                ],
                "structure": [],
                "shape": [],
                "ndim": [
                    "0"
                ],
                "enum": [],
                "range": []
            },
            "signature": "torch.addbmm(input,batch1,batch2,beta=1,alpha=1,out=None)",
            "descp": "beta(Number, optional) - multiplier for `input` ( beta )"
        },
        {
            "fname": "torch.multinomial.yaml",
            "arg": "out",
            "prompt": "Signature: torch.distributed.recv(tensor,src=None,group=<objectobject>,tag=0)\nParameter description: tensor(Tensor) - Tensor to fill with received data.\nConstraints:\ndtype:Null\nstructure:tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.div(input,other,out=None)\nParameter description: out(Tensor, optional) - the output tensor.\nConstraints:\ndtype:Null\nstructure:tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.nn.TransformerDecoder(decoder_layer,num_layers,norm=None)\nParameter description: num_layers - the number of sub-decoder-layers in the decoder (required).\nConstraints:\ndtype:int\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:[0,inf)\n\n###\n\nSignature: torch.nn.quantized.functional.interpolate(input,size=None,scale_factor=None,mode=nearest,align_corners=None)\nParameter description: align_corners(bool, optional) - Geometrically, we consider the pixels of the input and output as squares rather than points. If set to `True`, the input and output tensors are aligned by the center points of their corner pixels, preserving the values at the corner pixels. If set to `False`, the input and output tensors are aligned by the corner points of their corner pixels, and the interpolation uses edge value padding for out-of-boundary values, making this operation independent of input size when `scale_factor` is kept the same. This only has an effect when `mode` is `'bilinear'`. Default: `False`\nConstraints:\ndtype:bool\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.gt(input,other,out=None)\nParameter description: out(Tensor, optional) - the output tensor that must be a BoolTensor\nConstraints:\ndtype:booltensor\nstructure:tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.empty_like(input,dtype=None,layout=None,device=None,requires_grad=False,memory_format=torch.preserve_format)\nParameter description: dtype(`torch.dtype`, optional) - the desired data type of returned Tensor. Default: if `None`, defaults to the dtype of `input`.\nConstraints:\ndtype:torch.dtype\nstructure:Null\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.nn.RNN(*args,**kwargs,input_size=None,hidden_size=None,num_layers=None,nonlinearity=None,bias=None,batch_first=None,dropout=None,bidirectional=None)\nParameter description: hidden_size - The number of features in the hidden state h\nConstraints:\ndtype:int\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:[0,inf)\n\n###\n\nSignature: torch.nn.functional.avg_pool1d(input,kernel_size,stride=None,padding=0,ceil_mode=False,count_include_pad=True)\nParameter description: padding - implicit zero paddings on both sides of the input. Can be a single number or a tuple (padW,). Default: 0\nConstraints:\ndtype:bool;int\nstructure:tuple\nshape:Null\nndim:0;1\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.autograd.grad(outputs,inputs,grad_outputs=None,retain_graph=None,create_graph=False,only_inputs=True,allow_unused=False)\nParameter description: outputs(sequence of Tensor) - outputs of the differentiated function.\nConstraints:\ndtype:Null\nstructure:sequence;tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.repeat_interleave(input,repeats,dim=None)\nParameter description: dim(int, optional) - The dimension along which to repeat values.By default, use the flattened input array, and return a flat outputarray.\nConstraints:\ndtype:int\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.multinomial(input,num_samples,replacement=False,generator=None,out=None)\nParameter description: out(Tensor, optional) - the output tensor.\nConstraints:",
            "constr": {
                "dtype": [],
                "structure": [
                    "tensor"
                ],
                "shape": [],
                "ndim": [],
                "enum": [],
                "range": []
            },
            "signature": "torch.multinomial(input,num_samples,replacement=False,generator=None,out=None)",
            "descp": "out(Tensor, optional) - the output tensor."
        },
        {
            "fname": "torch.nn.maxunpool2d.yaml",
            "arg": "stride",
            "prompt": "Signature: torch.distributed.recv(tensor,src=None,group=<objectobject>,tag=0)\nParameter description: tensor(Tensor) - Tensor to fill with received data.\nConstraints:\ndtype:Null\nstructure:tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.div(input,other,out=None)\nParameter description: out(Tensor, optional) - the output tensor.\nConstraints:\ndtype:Null\nstructure:tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.nn.TransformerDecoder(decoder_layer,num_layers,norm=None)\nParameter description: num_layers - the number of sub-decoder-layers in the decoder (required).\nConstraints:\ndtype:int\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:[0,inf)\n\n###\n\nSignature: torch.nn.quantized.functional.interpolate(input,size=None,scale_factor=None,mode=nearest,align_corners=None)\nParameter description: align_corners(bool, optional) - Geometrically, we consider the pixels of the input and output as squares rather than points. If set to `True`, the input and output tensors are aligned by the center points of their corner pixels, preserving the values at the corner pixels. If set to `False`, the input and output tensors are aligned by the corner points of their corner pixels, and the interpolation uses edge value padding for out-of-boundary values, making this operation independent of input size when `scale_factor` is kept the same. This only has an effect when `mode` is `'bilinear'`. Default: `False`\nConstraints:\ndtype:bool\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.gt(input,other,out=None)\nParameter description: out(Tensor, optional) - the output tensor that must be a BoolTensor\nConstraints:\ndtype:booltensor\nstructure:tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.empty_like(input,dtype=None,layout=None,device=None,requires_grad=False,memory_format=torch.preserve_format)\nParameter description: dtype(`torch.dtype`, optional) - the desired data type of returned Tensor. Default: if `None`, defaults to the dtype of `input`.\nConstraints:\ndtype:torch.dtype\nstructure:Null\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.nn.RNN(*args,**kwargs,input_size=None,hidden_size=None,num_layers=None,nonlinearity=None,bias=None,batch_first=None,dropout=None,bidirectional=None)\nParameter description: hidden_size - The number of features in the hidden state h\nConstraints:\ndtype:int\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:[0,inf)\n\n###\n\nSignature: torch.nn.functional.avg_pool1d(input,kernel_size,stride=None,padding=0,ceil_mode=False,count_include_pad=True)\nParameter description: padding - implicit zero paddings on both sides of the input. Can be a single number or a tuple (padW,). Default: 0\nConstraints:\ndtype:bool;int\nstructure:tuple\nshape:Null\nndim:0;1\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.autograd.grad(outputs,inputs,grad_outputs=None,retain_graph=None,create_graph=False,only_inputs=True,allow_unused=False)\nParameter description: outputs(sequence of Tensor) - outputs of the differentiated function.\nConstraints:\ndtype:Null\nstructure:sequence;tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.repeat_interleave(input,repeats,dim=None)\nParameter description: dim(int, optional) - The dimension along which to repeat values.By default, use the flattened input array, and return a flat outputarray.\nConstraints:\ndtype:int\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.nn.MaxUnpool2d(kernel_size,stride=None,padding=0)\nParameter description: stride(int or tuple) - Stride of the max pooling window. It is set to `kernel_size` by default.\nConstraints:",
            "constr": {
                "dtype": [
                    "int"
                ],
                "structure": [
                    "tuple"
                ],
                "shape": [],
                "ndim": [],
                "enum": [],
                "range": [
                    "[0,inf)"
                ]
            },
            "signature": "torch.nn.MaxUnpool2d(kernel_size,stride=None,padding=0)",
            "descp": "stride(int or tuple) - Stride of the max pooling window. It is set to `kernel_size` by default."
        },
        {
            "fname": "torch.nn.multilabelmarginloss.yaml",
            "arg": "size_average",
            "prompt": "Signature: torch.distributed.recv(tensor,src=None,group=<objectobject>,tag=0)\nParameter description: tensor(Tensor) - Tensor to fill with received data.\nConstraints:\ndtype:Null\nstructure:tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.div(input,other,out=None)\nParameter description: out(Tensor, optional) - the output tensor.\nConstraints:\ndtype:Null\nstructure:tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.nn.TransformerDecoder(decoder_layer,num_layers,norm=None)\nParameter description: num_layers - the number of sub-decoder-layers in the decoder (required).\nConstraints:\ndtype:int\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:[0,inf)\n\n###\n\nSignature: torch.nn.quantized.functional.interpolate(input,size=None,scale_factor=None,mode=nearest,align_corners=None)\nParameter description: align_corners(bool, optional) - Geometrically, we consider the pixels of the input and output as squares rather than points. If set to `True`, the input and output tensors are aligned by the center points of their corner pixels, preserving the values at the corner pixels. If set to `False`, the input and output tensors are aligned by the corner points of their corner pixels, and the interpolation uses edge value padding for out-of-boundary values, making this operation independent of input size when `scale_factor` is kept the same. This only has an effect when `mode` is `'bilinear'`. Default: `False`\nConstraints:\ndtype:bool\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.gt(input,other,out=None)\nParameter description: out(Tensor, optional) - the output tensor that must be a BoolTensor\nConstraints:\ndtype:booltensor\nstructure:tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.empty_like(input,dtype=None,layout=None,device=None,requires_grad=False,memory_format=torch.preserve_format)\nParameter description: dtype(`torch.dtype`, optional) - the desired data type of returned Tensor. Default: if `None`, defaults to the dtype of `input`.\nConstraints:\ndtype:torch.dtype\nstructure:Null\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.nn.RNN(*args,**kwargs,input_size=None,hidden_size=None,num_layers=None,nonlinearity=None,bias=None,batch_first=None,dropout=None,bidirectional=None)\nParameter description: hidden_size - The number of features in the hidden state h\nConstraints:\ndtype:int\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:[0,inf)\n\n###\n\nSignature: torch.nn.functional.avg_pool1d(input,kernel_size,stride=None,padding=0,ceil_mode=False,count_include_pad=True)\nParameter description: padding - implicit zero paddings on both sides of the input. Can be a single number or a tuple (padW,). Default: 0\nConstraints:\ndtype:bool;int\nstructure:tuple\nshape:Null\nndim:0;1\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.autograd.grad(outputs,inputs,grad_outputs=None,retain_graph=None,create_graph=False,only_inputs=True,allow_unused=False)\nParameter description: outputs(sequence of Tensor) - outputs of the differentiated function.\nConstraints:\ndtype:Null\nstructure:sequence;tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.repeat_interleave(input,repeats,dim=None)\nParameter description: dim(int, optional) - The dimension along which to repeat values.By default, use the flattened input array, and return a flat outputarray.\nConstraints:\ndtype:int\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.nn.MultiLabelMarginLoss(size_average=None,reduce=None,reduction=mean)\nParameter description: size_average(bool, optional) - Deprecated (see `reduction`). By default, the losses are averaged over each loss element in the batch. Note that for some losses, there are multiple elements per sample. If the field `size_average` is set to `False`, the losses are instead summed for each minibatch. Ignored when reduce is `False`. Default: `True`\nConstraints:",
            "constr": {
                "dtype": [
                    "bool"
                ],
                "structure": [],
                "shape": [],
                "ndim": [
                    "0"
                ],
                "enum": [],
                "range": []
            },
            "signature": "torch.nn.MultiLabelMarginLoss(size_average=None,reduce=None,reduction=mean)",
            "descp": "size_average(bool, optional) - Deprecated (see `reduction`). By default, the losses are averaged over each loss element in the batch. Note that for some losses, there are multiple elements per sample. If the field `size_average` is set to `False`, the losses are instead summed for each minibatch. Ignored when reduce is `False`. Default: `True`"
        },
        {
            "fname": "torch.matrix_power.yaml",
            "arg": "n",
            "prompt": "Signature: torch.distributed.recv(tensor,src=None,group=<objectobject>,tag=0)\nParameter description: tensor(Tensor) - Tensor to fill with received data.\nConstraints:\ndtype:Null\nstructure:tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.div(input,other,out=None)\nParameter description: out(Tensor, optional) - the output tensor.\nConstraints:\ndtype:Null\nstructure:tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.nn.TransformerDecoder(decoder_layer,num_layers,norm=None)\nParameter description: num_layers - the number of sub-decoder-layers in the decoder (required).\nConstraints:\ndtype:int\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:[0,inf)\n\n###\n\nSignature: torch.nn.quantized.functional.interpolate(input,size=None,scale_factor=None,mode=nearest,align_corners=None)\nParameter description: align_corners(bool, optional) - Geometrically, we consider the pixels of the input and output as squares rather than points. If set to `True`, the input and output tensors are aligned by the center points of their corner pixels, preserving the values at the corner pixels. If set to `False`, the input and output tensors are aligned by the corner points of their corner pixels, and the interpolation uses edge value padding for out-of-boundary values, making this operation independent of input size when `scale_factor` is kept the same. This only has an effect when `mode` is `'bilinear'`. Default: `False`\nConstraints:\ndtype:bool\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.gt(input,other,out=None)\nParameter description: out(Tensor, optional) - the output tensor that must be a BoolTensor\nConstraints:\ndtype:booltensor\nstructure:tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.empty_like(input,dtype=None,layout=None,device=None,requires_grad=False,memory_format=torch.preserve_format)\nParameter description: dtype(`torch.dtype`, optional) - the desired data type of returned Tensor. Default: if `None`, defaults to the dtype of `input`.\nConstraints:\ndtype:torch.dtype\nstructure:Null\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.nn.RNN(*args,**kwargs,input_size=None,hidden_size=None,num_layers=None,nonlinearity=None,bias=None,batch_first=None,dropout=None,bidirectional=None)\nParameter description: hidden_size - The number of features in the hidden state h\nConstraints:\ndtype:int\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:[0,inf)\n\n###\n\nSignature: torch.nn.functional.avg_pool1d(input,kernel_size,stride=None,padding=0,ceil_mode=False,count_include_pad=True)\nParameter description: padding - implicit zero paddings on both sides of the input. Can be a single number or a tuple (padW,). Default: 0\nConstraints:\ndtype:bool;int\nstructure:tuple\nshape:Null\nndim:0;1\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.autograd.grad(outputs,inputs,grad_outputs=None,retain_graph=None,create_graph=False,only_inputs=True,allow_unused=False)\nParameter description: outputs(sequence of Tensor) - outputs of the differentiated function.\nConstraints:\ndtype:Null\nstructure:sequence;tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.repeat_interleave(input,repeats,dim=None)\nParameter description: dim(int, optional) - The dimension along which to repeat values.By default, use the flattened input array, and return a flat outputarray.\nConstraints:\ndtype:int\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.matrix_power(input,n)\nParameter description: n(int) - the power to raise the matrix to\nConstraints:",
            "constr": {
                "dtype": [
                    "int"
                ],
                "structure": [],
                "shape": [],
                "ndim": [],
                "enum": [],
                "range": []
            },
            "signature": "torch.matrix_power(input,n)",
            "descp": "n(int) - the power to raise the matrix to"
        },
        {
            "fname": "torch.svd.yaml",
            "arg": "out",
            "prompt": "Signature: torch.distributed.recv(tensor,src=None,group=<objectobject>,tag=0)\nParameter description: tensor(Tensor) - Tensor to fill with received data.\nConstraints:\ndtype:Null\nstructure:tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.div(input,other,out=None)\nParameter description: out(Tensor, optional) - the output tensor.\nConstraints:\ndtype:Null\nstructure:tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.nn.TransformerDecoder(decoder_layer,num_layers,norm=None)\nParameter description: num_layers - the number of sub-decoder-layers in the decoder (required).\nConstraints:\ndtype:int\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:[0,inf)\n\n###\n\nSignature: torch.nn.quantized.functional.interpolate(input,size=None,scale_factor=None,mode=nearest,align_corners=None)\nParameter description: align_corners(bool, optional) - Geometrically, we consider the pixels of the input and output as squares rather than points. If set to `True`, the input and output tensors are aligned by the center points of their corner pixels, preserving the values at the corner pixels. If set to `False`, the input and output tensors are aligned by the corner points of their corner pixels, and the interpolation uses edge value padding for out-of-boundary values, making this operation independent of input size when `scale_factor` is kept the same. This only has an effect when `mode` is `'bilinear'`. Default: `False`\nConstraints:\ndtype:bool\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.gt(input,other,out=None)\nParameter description: out(Tensor, optional) - the output tensor that must be a BoolTensor\nConstraints:\ndtype:booltensor\nstructure:tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.empty_like(input,dtype=None,layout=None,device=None,requires_grad=False,memory_format=torch.preserve_format)\nParameter description: dtype(`torch.dtype`, optional) - the desired data type of returned Tensor. Default: if `None`, defaults to the dtype of `input`.\nConstraints:\ndtype:torch.dtype\nstructure:Null\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.nn.RNN(*args,**kwargs,input_size=None,hidden_size=None,num_layers=None,nonlinearity=None,bias=None,batch_first=None,dropout=None,bidirectional=None)\nParameter description: hidden_size - The number of features in the hidden state h\nConstraints:\ndtype:int\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:[0,inf)\n\n###\n\nSignature: torch.nn.functional.avg_pool1d(input,kernel_size,stride=None,padding=0,ceil_mode=False,count_include_pad=True)\nParameter description: padding - implicit zero paddings on both sides of the input. Can be a single number or a tuple (padW,). Default: 0\nConstraints:\ndtype:bool;int\nstructure:tuple\nshape:Null\nndim:0;1\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.autograd.grad(outputs,inputs,grad_outputs=None,retain_graph=None,create_graph=False,only_inputs=True,allow_unused=False)\nParameter description: outputs(sequence of Tensor) - outputs of the differentiated function.\nConstraints:\ndtype:Null\nstructure:sequence;tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.repeat_interleave(input,repeats,dim=None)\nParameter description: dim(int, optional) - The dimension along which to repeat values.By default, use the flattened input array, and return a flat outputarray.\nConstraints:\ndtype:int\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.svd(input,some=True,compute_uv=True,out=None)\nParameter description: out(tuple, optional) - the output tuple of tensors\nConstraints:",
            "constr": {
                "dtype": [],
                "structure": [
                    "tuple",
                    "tensor"
                ],
                "shape": [],
                "ndim": [],
                "enum": [],
                "range": []
            },
            "signature": "torch.svd(input,some=True,compute_uv=True,out=None)",
            "descp": "out(tuple, optional) - the output tuple of tensors"
        },
        {
            "fname": "torch.nn.instancenorm3d.yaml",
            "arg": "eps",
            "prompt": "Signature: torch.distributed.recv(tensor,src=None,group=<objectobject>,tag=0)\nParameter description: tensor(Tensor) - Tensor to fill with received data.\nConstraints:\ndtype:Null\nstructure:tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.div(input,other,out=None)\nParameter description: out(Tensor, optional) - the output tensor.\nConstraints:\ndtype:Null\nstructure:tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.nn.TransformerDecoder(decoder_layer,num_layers,norm=None)\nParameter description: num_layers - the number of sub-decoder-layers in the decoder (required).\nConstraints:\ndtype:int\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:[0,inf)\n\n###\n\nSignature: torch.nn.quantized.functional.interpolate(input,size=None,scale_factor=None,mode=nearest,align_corners=None)\nParameter description: align_corners(bool, optional) - Geometrically, we consider the pixels of the input and output as squares rather than points. If set to `True`, the input and output tensors are aligned by the center points of their corner pixels, preserving the values at the corner pixels. If set to `False`, the input and output tensors are aligned by the corner points of their corner pixels, and the interpolation uses edge value padding for out-of-boundary values, making this operation independent of input size when `scale_factor` is kept the same. This only has an effect when `mode` is `'bilinear'`. Default: `False`\nConstraints:\ndtype:bool\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.gt(input,other,out=None)\nParameter description: out(Tensor, optional) - the output tensor that must be a BoolTensor\nConstraints:\ndtype:booltensor\nstructure:tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.empty_like(input,dtype=None,layout=None,device=None,requires_grad=False,memory_format=torch.preserve_format)\nParameter description: dtype(`torch.dtype`, optional) - the desired data type of returned Tensor. Default: if `None`, defaults to the dtype of `input`.\nConstraints:\ndtype:torch.dtype\nstructure:Null\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.nn.RNN(*args,**kwargs,input_size=None,hidden_size=None,num_layers=None,nonlinearity=None,bias=None,batch_first=None,dropout=None,bidirectional=None)\nParameter description: hidden_size - The number of features in the hidden state h\nConstraints:\ndtype:int\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:[0,inf)\n\n###\n\nSignature: torch.nn.functional.avg_pool1d(input,kernel_size,stride=None,padding=0,ceil_mode=False,count_include_pad=True)\nParameter description: padding - implicit zero paddings on both sides of the input. Can be a single number or a tuple (padW,). Default: 0\nConstraints:\ndtype:bool;int\nstructure:tuple\nshape:Null\nndim:0;1\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.autograd.grad(outputs,inputs,grad_outputs=None,retain_graph=None,create_graph=False,only_inputs=True,allow_unused=False)\nParameter description: outputs(sequence of Tensor) - outputs of the differentiated function.\nConstraints:\ndtype:Null\nstructure:sequence;tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.repeat_interleave(input,repeats,dim=None)\nParameter description: dim(int, optional) - The dimension along which to repeat values.By default, use the flattened input array, and return a flat outputarray.\nConstraints:\ndtype:int\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.nn.InstanceNorm3d(num_features,eps=1e-05,momentum=0.1,affine=False,track_running_stats=False)\nParameter description: eps - a value added to the denominator for numerical stability. Default: 1e-5\nConstraints:",
            "constr": {
                "dtype": [
                    "bool",
                    "float"
                ],
                "structure": [],
                "shape": [],
                "ndim": [
                    "0"
                ],
                "enum": [],
                "range": []
            },
            "signature": "torch.nn.InstanceNorm3d(num_features,eps=1e-05,momentum=0.1,affine=False,track_running_stats=False)",
            "descp": "eps - a value added to the denominator for numerical stability. Default: 1e-5"
        },
        {
            "fname": "torch.nn.utils.rnn.pack_padded_sequence.yaml",
            "arg": "batch_first",
            "prompt": "Signature: torch.distributed.recv(tensor,src=None,group=<objectobject>,tag=0)\nParameter description: tensor(Tensor) - Tensor to fill with received data.\nConstraints:\ndtype:Null\nstructure:tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.div(input,other,out=None)\nParameter description: out(Tensor, optional) - the output tensor.\nConstraints:\ndtype:Null\nstructure:tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.nn.TransformerDecoder(decoder_layer,num_layers,norm=None)\nParameter description: num_layers - the number of sub-decoder-layers in the decoder (required).\nConstraints:\ndtype:int\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:[0,inf)\n\n###\n\nSignature: torch.nn.quantized.functional.interpolate(input,size=None,scale_factor=None,mode=nearest,align_corners=None)\nParameter description: align_corners(bool, optional) - Geometrically, we consider the pixels of the input and output as squares rather than points. If set to `True`, the input and output tensors are aligned by the center points of their corner pixels, preserving the values at the corner pixels. If set to `False`, the input and output tensors are aligned by the corner points of their corner pixels, and the interpolation uses edge value padding for out-of-boundary values, making this operation independent of input size when `scale_factor` is kept the same. This only has an effect when `mode` is `'bilinear'`. Default: `False`\nConstraints:\ndtype:bool\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.gt(input,other,out=None)\nParameter description: out(Tensor, optional) - the output tensor that must be a BoolTensor\nConstraints:\ndtype:booltensor\nstructure:tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.empty_like(input,dtype=None,layout=None,device=None,requires_grad=False,memory_format=torch.preserve_format)\nParameter description: dtype(`torch.dtype`, optional) - the desired data type of returned Tensor. Default: if `None`, defaults to the dtype of `input`.\nConstraints:\ndtype:torch.dtype\nstructure:Null\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.nn.RNN(*args,**kwargs,input_size=None,hidden_size=None,num_layers=None,nonlinearity=None,bias=None,batch_first=None,dropout=None,bidirectional=None)\nParameter description: hidden_size - The number of features in the hidden state h\nConstraints:\ndtype:int\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:[0,inf)\n\n###\n\nSignature: torch.nn.functional.avg_pool1d(input,kernel_size,stride=None,padding=0,ceil_mode=False,count_include_pad=True)\nParameter description: padding - implicit zero paddings on both sides of the input. Can be a single number or a tuple (padW,). Default: 0\nConstraints:\ndtype:bool;int\nstructure:tuple\nshape:Null\nndim:0;1\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.autograd.grad(outputs,inputs,grad_outputs=None,retain_graph=None,create_graph=False,only_inputs=True,allow_unused=False)\nParameter description: outputs(sequence of Tensor) - outputs of the differentiated function.\nConstraints:\ndtype:Null\nstructure:sequence;tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.repeat_interleave(input,repeats,dim=None)\nParameter description: dim(int, optional) - The dimension along which to repeat values.By default, use the flattened input array, and return a flat outputarray.\nConstraints:\ndtype:int\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.nn.utils.rnn.pack_padded_sequence(input,lengths,batch_first=False,enforce_sorted=True)\nParameter description: batch_first(bool, optional) - if `True`, the input is expected in `B x T x *` format.\nConstraints:",
            "constr": {
                "dtype": [
                    "bool"
                ],
                "structure": [],
                "shape": [],
                "ndim": [
                    "0"
                ],
                "enum": [],
                "range": []
            },
            "signature": "torch.nn.utils.rnn.pack_padded_sequence(input,lengths,batch_first=False,enforce_sorted=True)",
            "descp": "batch_first(bool, optional) - if `True`, the input is expected in `B x T x *` format."
        },
        {
            "fname": "torch.std.yaml",
            "arg": "unbiased",
            "prompt": "Signature: torch.distributed.recv(tensor,src=None,group=<objectobject>,tag=0)\nParameter description: tensor(Tensor) - Tensor to fill with received data.\nConstraints:\ndtype:Null\nstructure:tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.div(input,other,out=None)\nParameter description: out(Tensor, optional) - the output tensor.\nConstraints:\ndtype:Null\nstructure:tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.nn.TransformerDecoder(decoder_layer,num_layers,norm=None)\nParameter description: num_layers - the number of sub-decoder-layers in the decoder (required).\nConstraints:\ndtype:int\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:[0,inf)\n\n###\n\nSignature: torch.nn.quantized.functional.interpolate(input,size=None,scale_factor=None,mode=nearest,align_corners=None)\nParameter description: align_corners(bool, optional) - Geometrically, we consider the pixels of the input and output as squares rather than points. If set to `True`, the input and output tensors are aligned by the center points of their corner pixels, preserving the values at the corner pixels. If set to `False`, the input and output tensors are aligned by the corner points of their corner pixels, and the interpolation uses edge value padding for out-of-boundary values, making this operation independent of input size when `scale_factor` is kept the same. This only has an effect when `mode` is `'bilinear'`. Default: `False`\nConstraints:\ndtype:bool\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.gt(input,other,out=None)\nParameter description: out(Tensor, optional) - the output tensor that must be a BoolTensor\nConstraints:\ndtype:booltensor\nstructure:tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.empty_like(input,dtype=None,layout=None,device=None,requires_grad=False,memory_format=torch.preserve_format)\nParameter description: dtype(`torch.dtype`, optional) - the desired data type of returned Tensor. Default: if `None`, defaults to the dtype of `input`.\nConstraints:\ndtype:torch.dtype\nstructure:Null\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.nn.RNN(*args,**kwargs,input_size=None,hidden_size=None,num_layers=None,nonlinearity=None,bias=None,batch_first=None,dropout=None,bidirectional=None)\nParameter description: hidden_size - The number of features in the hidden state h\nConstraints:\ndtype:int\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:[0,inf)\n\n###\n\nSignature: torch.nn.functional.avg_pool1d(input,kernel_size,stride=None,padding=0,ceil_mode=False,count_include_pad=True)\nParameter description: padding - implicit zero paddings on both sides of the input. Can be a single number or a tuple (padW,). Default: 0\nConstraints:\ndtype:bool;int\nstructure:tuple\nshape:Null\nndim:0;1\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.autograd.grad(outputs,inputs,grad_outputs=None,retain_graph=None,create_graph=False,only_inputs=True,allow_unused=False)\nParameter description: outputs(sequence of Tensor) - outputs of the differentiated function.\nConstraints:\ndtype:Null\nstructure:sequence;tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.repeat_interleave(input,repeats,dim=None)\nParameter description: dim(int, optional) - The dimension along which to repeat values.By default, use the flattened input array, and return a flat outputarray.\nConstraints:\ndtype:int\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.std(input,unbiased=True)\nParameter description: unbiased(bool) - whether to use the unbiased estimation or not\nConstraints:",
            "constr": {
                "dtype": [
                    "bool"
                ],
                "structure": [],
                "shape": [],
                "ndim": [
                    "0"
                ],
                "enum": [],
                "range": []
            },
            "signature": "torch.std(input,unbiased=True)",
            "descp": "unbiased(bool) - whether to use the unbiased estimation or not"
        },
        {
            "fname": "torch.log1p.yaml",
            "arg": "out",
            "prompt": "Signature: torch.distributed.recv(tensor,src=None,group=<objectobject>,tag=0)\nParameter description: tensor(Tensor) - Tensor to fill with received data.\nConstraints:\ndtype:Null\nstructure:tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.div(input,other,out=None)\nParameter description: out(Tensor, optional) - the output tensor.\nConstraints:\ndtype:Null\nstructure:tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.nn.TransformerDecoder(decoder_layer,num_layers,norm=None)\nParameter description: num_layers - the number of sub-decoder-layers in the decoder (required).\nConstraints:\ndtype:int\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:[0,inf)\n\n###\n\nSignature: torch.nn.quantized.functional.interpolate(input,size=None,scale_factor=None,mode=nearest,align_corners=None)\nParameter description: align_corners(bool, optional) - Geometrically, we consider the pixels of the input and output as squares rather than points. If set to `True`, the input and output tensors are aligned by the center points of their corner pixels, preserving the values at the corner pixels. If set to `False`, the input and output tensors are aligned by the corner points of their corner pixels, and the interpolation uses edge value padding for out-of-boundary values, making this operation independent of input size when `scale_factor` is kept the same. This only has an effect when `mode` is `'bilinear'`. Default: `False`\nConstraints:\ndtype:bool\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.gt(input,other,out=None)\nParameter description: out(Tensor, optional) - the output tensor that must be a BoolTensor\nConstraints:\ndtype:booltensor\nstructure:tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.empty_like(input,dtype=None,layout=None,device=None,requires_grad=False,memory_format=torch.preserve_format)\nParameter description: dtype(`torch.dtype`, optional) - the desired data type of returned Tensor. Default: if `None`, defaults to the dtype of `input`.\nConstraints:\ndtype:torch.dtype\nstructure:Null\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.nn.RNN(*args,**kwargs,input_size=None,hidden_size=None,num_layers=None,nonlinearity=None,bias=None,batch_first=None,dropout=None,bidirectional=None)\nParameter description: hidden_size - The number of features in the hidden state h\nConstraints:\ndtype:int\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:[0,inf)\n\n###\n\nSignature: torch.nn.functional.avg_pool1d(input,kernel_size,stride=None,padding=0,ceil_mode=False,count_include_pad=True)\nParameter description: padding - implicit zero paddings on both sides of the input. Can be a single number or a tuple (padW,). Default: 0\nConstraints:\ndtype:bool;int\nstructure:tuple\nshape:Null\nndim:0;1\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.autograd.grad(outputs,inputs,grad_outputs=None,retain_graph=None,create_graph=False,only_inputs=True,allow_unused=False)\nParameter description: outputs(sequence of Tensor) - outputs of the differentiated function.\nConstraints:\ndtype:Null\nstructure:sequence;tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.repeat_interleave(input,repeats,dim=None)\nParameter description: dim(int, optional) - The dimension along which to repeat values.By default, use the flattened input array, and return a flat outputarray.\nConstraints:\ndtype:int\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.log1p(input,out=None)\nParameter description: out(Tensor, optional) - the output tensor.\nConstraints:",
            "constr": {
                "dtype": [],
                "structure": [
                    "tensor"
                ],
                "shape": [],
                "ndim": [],
                "enum": [],
                "range": []
            },
            "signature": "torch.log1p(input,out=None)",
            "descp": "out(Tensor, optional) - the output tensor."
        },
        {
            "fname": "torch.logspace.yaml",
            "arg": "start",
            "prompt": "Signature: torch.distributed.recv(tensor,src=None,group=<objectobject>,tag=0)\nParameter description: tensor(Tensor) - Tensor to fill with received data.\nConstraints:\ndtype:Null\nstructure:tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.div(input,other,out=None)\nParameter description: out(Tensor, optional) - the output tensor.\nConstraints:\ndtype:Null\nstructure:tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.nn.TransformerDecoder(decoder_layer,num_layers,norm=None)\nParameter description: num_layers - the number of sub-decoder-layers in the decoder (required).\nConstraints:\ndtype:int\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:[0,inf)\n\n###\n\nSignature: torch.nn.quantized.functional.interpolate(input,size=None,scale_factor=None,mode=nearest,align_corners=None)\nParameter description: align_corners(bool, optional) - Geometrically, we consider the pixels of the input and output as squares rather than points. If set to `True`, the input and output tensors are aligned by the center points of their corner pixels, preserving the values at the corner pixels. If set to `False`, the input and output tensors are aligned by the corner points of their corner pixels, and the interpolation uses edge value padding for out-of-boundary values, making this operation independent of input size when `scale_factor` is kept the same. This only has an effect when `mode` is `'bilinear'`. Default: `False`\nConstraints:\ndtype:bool\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.gt(input,other,out=None)\nParameter description: out(Tensor, optional) - the output tensor that must be a BoolTensor\nConstraints:\ndtype:booltensor\nstructure:tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.empty_like(input,dtype=None,layout=None,device=None,requires_grad=False,memory_format=torch.preserve_format)\nParameter description: dtype(`torch.dtype`, optional) - the desired data type of returned Tensor. Default: if `None`, defaults to the dtype of `input`.\nConstraints:\ndtype:torch.dtype\nstructure:Null\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.nn.RNN(*args,**kwargs,input_size=None,hidden_size=None,num_layers=None,nonlinearity=None,bias=None,batch_first=None,dropout=None,bidirectional=None)\nParameter description: hidden_size - The number of features in the hidden state h\nConstraints:\ndtype:int\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:[0,inf)\n\n###\n\nSignature: torch.nn.functional.avg_pool1d(input,kernel_size,stride=None,padding=0,ceil_mode=False,count_include_pad=True)\nParameter description: padding - implicit zero paddings on both sides of the input. Can be a single number or a tuple (padW,). Default: 0\nConstraints:\ndtype:bool;int\nstructure:tuple\nshape:Null\nndim:0;1\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.autograd.grad(outputs,inputs,grad_outputs=None,retain_graph=None,create_graph=False,only_inputs=True,allow_unused=False)\nParameter description: outputs(sequence of Tensor) - outputs of the differentiated function.\nConstraints:\ndtype:Null\nstructure:sequence;tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.repeat_interleave(input,repeats,dim=None)\nParameter description: dim(int, optional) - The dimension along which to repeat values.By default, use the flattened input array, and return a flat outputarray.\nConstraints:\ndtype:int\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.logspace(start,end,steps=100,base=10.0,out=None,dtype=None,layout=torch.strided,device=None,requires_grad=False)\nParameter description: start(float) - the starting value for the set of points\nConstraints:",
            "constr": {
                "dtype": [
                    "float"
                ],
                "structure": [],
                "shape": [],
                "ndim": [],
                "enum": [],
                "range": []
            },
            "signature": "torch.logspace(start,end,steps=100,base=10.0,out=None,dtype=None,layout=torch.strided,device=None,requires_grad=False)",
            "descp": "start(float) - the starting value for the set of points"
        },
        {
            "fname": "torch.conj.yaml",
            "arg": "out",
            "prompt": "Signature: torch.distributed.recv(tensor,src=None,group=<objectobject>,tag=0)\nParameter description: tensor(Tensor) - Tensor to fill with received data.\nConstraints:\ndtype:Null\nstructure:tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.div(input,other,out=None)\nParameter description: out(Tensor, optional) - the output tensor.\nConstraints:\ndtype:Null\nstructure:tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.nn.TransformerDecoder(decoder_layer,num_layers,norm=None)\nParameter description: num_layers - the number of sub-decoder-layers in the decoder (required).\nConstraints:\ndtype:int\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:[0,inf)\n\n###\n\nSignature: torch.nn.quantized.functional.interpolate(input,size=None,scale_factor=None,mode=nearest,align_corners=None)\nParameter description: align_corners(bool, optional) - Geometrically, we consider the pixels of the input and output as squares rather than points. If set to `True`, the input and output tensors are aligned by the center points of their corner pixels, preserving the values at the corner pixels. If set to `False`, the input and output tensors are aligned by the corner points of their corner pixels, and the interpolation uses edge value padding for out-of-boundary values, making this operation independent of input size when `scale_factor` is kept the same. This only has an effect when `mode` is `'bilinear'`. Default: `False`\nConstraints:\ndtype:bool\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.gt(input,other,out=None)\nParameter description: out(Tensor, optional) - the output tensor that must be a BoolTensor\nConstraints:\ndtype:booltensor\nstructure:tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.empty_like(input,dtype=None,layout=None,device=None,requires_grad=False,memory_format=torch.preserve_format)\nParameter description: dtype(`torch.dtype`, optional) - the desired data type of returned Tensor. Default: if `None`, defaults to the dtype of `input`.\nConstraints:\ndtype:torch.dtype\nstructure:Null\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.nn.RNN(*args,**kwargs,input_size=None,hidden_size=None,num_layers=None,nonlinearity=None,bias=None,batch_first=None,dropout=None,bidirectional=None)\nParameter description: hidden_size - The number of features in the hidden state h\nConstraints:\ndtype:int\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:[0,inf)\n\n###\n\nSignature: torch.nn.functional.avg_pool1d(input,kernel_size,stride=None,padding=0,ceil_mode=False,count_include_pad=True)\nParameter description: padding - implicit zero paddings on both sides of the input. Can be a single number or a tuple (padW,). Default: 0\nConstraints:\ndtype:bool;int\nstructure:tuple\nshape:Null\nndim:0;1\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.autograd.grad(outputs,inputs,grad_outputs=None,retain_graph=None,create_graph=False,only_inputs=True,allow_unused=False)\nParameter description: outputs(sequence of Tensor) - outputs of the differentiated function.\nConstraints:\ndtype:Null\nstructure:sequence;tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.repeat_interleave(input,repeats,dim=None)\nParameter description: dim(int, optional) - The dimension along which to repeat values.By default, use the flattened input array, and return a flat outputarray.\nConstraints:\ndtype:int\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.conj(input,out=None)\nParameter description: out(Tensor, optional) - the output tensor.\nConstraints:",
            "constr": {
                "dtype": [],
                "structure": [
                    "tensor"
                ],
                "shape": [],
                "ndim": [],
                "enum": [],
                "range": []
            },
            "signature": "torch.conj(input,out=None)",
            "descp": "out(Tensor, optional) - the output tensor."
        },
        {
            "fname": "torch.autograd.gradcheck.yaml",
            "arg": "inputs",
            "prompt": "Signature: torch.distributed.recv(tensor,src=None,group=<objectobject>,tag=0)\nParameter description: tensor(Tensor) - Tensor to fill with received data.\nConstraints:\ndtype:Null\nstructure:tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.div(input,other,out=None)\nParameter description: out(Tensor, optional) - the output tensor.\nConstraints:\ndtype:Null\nstructure:tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.nn.TransformerDecoder(decoder_layer,num_layers,norm=None)\nParameter description: num_layers - the number of sub-decoder-layers in the decoder (required).\nConstraints:\ndtype:int\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:[0,inf)\n\n###\n\nSignature: torch.nn.quantized.functional.interpolate(input,size=None,scale_factor=None,mode=nearest,align_corners=None)\nParameter description: align_corners(bool, optional) - Geometrically, we consider the pixels of the input and output as squares rather than points. If set to `True`, the input and output tensors are aligned by the center points of their corner pixels, preserving the values at the corner pixels. If set to `False`, the input and output tensors are aligned by the corner points of their corner pixels, and the interpolation uses edge value padding for out-of-boundary values, making this operation independent of input size when `scale_factor` is kept the same. This only has an effect when `mode` is `'bilinear'`. Default: `False`\nConstraints:\ndtype:bool\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.gt(input,other,out=None)\nParameter description: out(Tensor, optional) - the output tensor that must be a BoolTensor\nConstraints:\ndtype:booltensor\nstructure:tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.empty_like(input,dtype=None,layout=None,device=None,requires_grad=False,memory_format=torch.preserve_format)\nParameter description: dtype(`torch.dtype`, optional) - the desired data type of returned Tensor. Default: if `None`, defaults to the dtype of `input`.\nConstraints:\ndtype:torch.dtype\nstructure:Null\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.nn.RNN(*args,**kwargs,input_size=None,hidden_size=None,num_layers=None,nonlinearity=None,bias=None,batch_first=None,dropout=None,bidirectional=None)\nParameter description: hidden_size - The number of features in the hidden state h\nConstraints:\ndtype:int\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:[0,inf)\n\n###\n\nSignature: torch.nn.functional.avg_pool1d(input,kernel_size,stride=None,padding=0,ceil_mode=False,count_include_pad=True)\nParameter description: padding - implicit zero paddings on both sides of the input. Can be a single number or a tuple (padW,). Default: 0\nConstraints:\ndtype:bool;int\nstructure:tuple\nshape:Null\nndim:0;1\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.autograd.grad(outputs,inputs,grad_outputs=None,retain_graph=None,create_graph=False,only_inputs=True,allow_unused=False)\nParameter description: outputs(sequence of Tensor) - outputs of the differentiated function.\nConstraints:\ndtype:Null\nstructure:sequence;tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.repeat_interleave(input,repeats,dim=None)\nParameter description: dim(int, optional) - The dimension along which to repeat values.By default, use the flattened input array, and return a flat outputarray.\nConstraints:\ndtype:int\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.autograd.gradcheck(func,inputs,eps=1e-06,atol=1e-05,rtol=0.001,raise_exception=True,check_sparse_nnz=False,nondet_tol=0.0)\nParameter description: inputs(tuple of Tensor or Tensor) - inputs to the function\nConstraints:",
            "constr": {
                "dtype": [],
                "structure": [
                    "tuple",
                    "tensor"
                ],
                "shape": [],
                "ndim": [],
                "enum": [],
                "range": []
            },
            "signature": "torch.autograd.gradcheck(func,inputs,eps=1e-06,atol=1e-05,rtol=0.001,raise_exception=True,check_sparse_nnz=False,nondet_tol=0.0)",
            "descp": "inputs(tuple of Tensor or Tensor) - inputs to the function"
        },
        {
            "fname": "torch.nn.functional.conv_transpose2d.yaml",
            "arg": "weight",
            "prompt": "Signature: torch.distributed.recv(tensor,src=None,group=<objectobject>,tag=0)\nParameter description: tensor(Tensor) - Tensor to fill with received data.\nConstraints:\ndtype:Null\nstructure:tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.div(input,other,out=None)\nParameter description: out(Tensor, optional) - the output tensor.\nConstraints:\ndtype:Null\nstructure:tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.nn.TransformerDecoder(decoder_layer,num_layers,norm=None)\nParameter description: num_layers - the number of sub-decoder-layers in the decoder (required).\nConstraints:\ndtype:int\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:[0,inf)\n\n###\n\nSignature: torch.nn.quantized.functional.interpolate(input,size=None,scale_factor=None,mode=nearest,align_corners=None)\nParameter description: align_corners(bool, optional) - Geometrically, we consider the pixels of the input and output as squares rather than points. If set to `True`, the input and output tensors are aligned by the center points of their corner pixels, preserving the values at the corner pixels. If set to `False`, the input and output tensors are aligned by the corner points of their corner pixels, and the interpolation uses edge value padding for out-of-boundary values, making this operation independent of input size when `scale_factor` is kept the same. This only has an effect when `mode` is `'bilinear'`. Default: `False`\nConstraints:\ndtype:bool\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.gt(input,other,out=None)\nParameter description: out(Tensor, optional) - the output tensor that must be a BoolTensor\nConstraints:\ndtype:booltensor\nstructure:tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.empty_like(input,dtype=None,layout=None,device=None,requires_grad=False,memory_format=torch.preserve_format)\nParameter description: dtype(`torch.dtype`, optional) - the desired data type of returned Tensor. Default: if `None`, defaults to the dtype of `input`.\nConstraints:\ndtype:torch.dtype\nstructure:Null\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.nn.RNN(*args,**kwargs,input_size=None,hidden_size=None,num_layers=None,nonlinearity=None,bias=None,batch_first=None,dropout=None,bidirectional=None)\nParameter description: hidden_size - The number of features in the hidden state h\nConstraints:\ndtype:int\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:[0,inf)\n\n###\n\nSignature: torch.nn.functional.avg_pool1d(input,kernel_size,stride=None,padding=0,ceil_mode=False,count_include_pad=True)\nParameter description: padding - implicit zero paddings on both sides of the input. Can be a single number or a tuple (padW,). Default: 0\nConstraints:\ndtype:bool;int\nstructure:tuple\nshape:Null\nndim:0;1\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.autograd.grad(outputs,inputs,grad_outputs=None,retain_graph=None,create_graph=False,only_inputs=True,allow_unused=False)\nParameter description: outputs(sequence of Tensor) - outputs of the differentiated function.\nConstraints:\ndtype:Null\nstructure:sequence;tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.repeat_interleave(input,repeats,dim=None)\nParameter description: dim(int, optional) - The dimension along which to repeat values.By default, use the flattened input array, and return a flat outputarray.\nConstraints:\ndtype:int\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.nn.functional.conv_transpose2d(input,weight,bias=None,stride=1,padding=0,output_padding=0,groups=1,dilation=1)\nParameter description: weight - filters of shape (in _channels ,  out _channels/groups , kH , kW) \nConstraints:",
            "constr": {
                "dtype": [],
                "structure": [],
                "shape": [
                    "(in _channels , out _channels/groups , kH , kW)"
                ],
                "ndim": [],
                "enum": [],
                "range": []
            },
            "signature": "torch.nn.functional.conv_transpose2d(input,weight,bias=None,stride=1,padding=0,output_padding=0,groups=1,dilation=1)",
            "descp": "weight - filters of shape (in _channels ,  out _channels/groups , kH , kW) "
        },
        {
            "fname": "torch.eye.yaml",
            "arg": "out",
            "prompt": "Signature: torch.distributed.recv(tensor,src=None,group=<objectobject>,tag=0)\nParameter description: tensor(Tensor) - Tensor to fill with received data.\nConstraints:\ndtype:Null\nstructure:tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.div(input,other,out=None)\nParameter description: out(Tensor, optional) - the output tensor.\nConstraints:\ndtype:Null\nstructure:tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.nn.TransformerDecoder(decoder_layer,num_layers,norm=None)\nParameter description: num_layers - the number of sub-decoder-layers in the decoder (required).\nConstraints:\ndtype:int\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:[0,inf)\n\n###\n\nSignature: torch.nn.quantized.functional.interpolate(input,size=None,scale_factor=None,mode=nearest,align_corners=None)\nParameter description: align_corners(bool, optional) - Geometrically, we consider the pixels of the input and output as squares rather than points. If set to `True`, the input and output tensors are aligned by the center points of their corner pixels, preserving the values at the corner pixels. If set to `False`, the input and output tensors are aligned by the corner points of their corner pixels, and the interpolation uses edge value padding for out-of-boundary values, making this operation independent of input size when `scale_factor` is kept the same. This only has an effect when `mode` is `'bilinear'`. Default: `False`\nConstraints:\ndtype:bool\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.gt(input,other,out=None)\nParameter description: out(Tensor, optional) - the output tensor that must be a BoolTensor\nConstraints:\ndtype:booltensor\nstructure:tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.empty_like(input,dtype=None,layout=None,device=None,requires_grad=False,memory_format=torch.preserve_format)\nParameter description: dtype(`torch.dtype`, optional) - the desired data type of returned Tensor. Default: if `None`, defaults to the dtype of `input`.\nConstraints:\ndtype:torch.dtype\nstructure:Null\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.nn.RNN(*args,**kwargs,input_size=None,hidden_size=None,num_layers=None,nonlinearity=None,bias=None,batch_first=None,dropout=None,bidirectional=None)\nParameter description: hidden_size - The number of features in the hidden state h\nConstraints:\ndtype:int\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:[0,inf)\n\n###\n\nSignature: torch.nn.functional.avg_pool1d(input,kernel_size,stride=None,padding=0,ceil_mode=False,count_include_pad=True)\nParameter description: padding - implicit zero paddings on both sides of the input. Can be a single number or a tuple (padW,). Default: 0\nConstraints:\ndtype:bool;int\nstructure:tuple\nshape:Null\nndim:0;1\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.autograd.grad(outputs,inputs,grad_outputs=None,retain_graph=None,create_graph=False,only_inputs=True,allow_unused=False)\nParameter description: outputs(sequence of Tensor) - outputs of the differentiated function.\nConstraints:\ndtype:Null\nstructure:sequence;tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.repeat_interleave(input,repeats,dim=None)\nParameter description: dim(int, optional) - The dimension along which to repeat values.By default, use the flattened input array, and return a flat outputarray.\nConstraints:\ndtype:int\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.eye(n,m=None,out=None,dtype=None,layout=torch.strided,device=None,requires_grad=False)\nParameter description: out(Tensor, optional) - the output tensor.\nConstraints:",
            "constr": {
                "dtype": [],
                "structure": [
                    "tensor"
                ],
                "shape": [],
                "ndim": [],
                "enum": [],
                "range": []
            },
            "signature": "torch.eye(n,m=None,out=None,dtype=None,layout=torch.strided,device=None,requires_grad=False)",
            "descp": "out(Tensor, optional) - the output tensor."
        },
        {
            "fname": "torch.std2.yaml",
            "arg": "dim",
            "prompt": "Signature: torch.distributed.recv(tensor,src=None,group=<objectobject>,tag=0)\nParameter description: tensor(Tensor) - Tensor to fill with received data.\nConstraints:\ndtype:Null\nstructure:tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.div(input,other,out=None)\nParameter description: out(Tensor, optional) - the output tensor.\nConstraints:\ndtype:Null\nstructure:tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.nn.TransformerDecoder(decoder_layer,num_layers,norm=None)\nParameter description: num_layers - the number of sub-decoder-layers in the decoder (required).\nConstraints:\ndtype:int\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:[0,inf)\n\n###\n\nSignature: torch.nn.quantized.functional.interpolate(input,size=None,scale_factor=None,mode=nearest,align_corners=None)\nParameter description: align_corners(bool, optional) - Geometrically, we consider the pixels of the input and output as squares rather than points. If set to `True`, the input and output tensors are aligned by the center points of their corner pixels, preserving the values at the corner pixels. If set to `False`, the input and output tensors are aligned by the corner points of their corner pixels, and the interpolation uses edge value padding for out-of-boundary values, making this operation independent of input size when `scale_factor` is kept the same. This only has an effect when `mode` is `'bilinear'`. Default: `False`\nConstraints:\ndtype:bool\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.gt(input,other,out=None)\nParameter description: out(Tensor, optional) - the output tensor that must be a BoolTensor\nConstraints:\ndtype:booltensor\nstructure:tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.empty_like(input,dtype=None,layout=None,device=None,requires_grad=False,memory_format=torch.preserve_format)\nParameter description: dtype(`torch.dtype`, optional) - the desired data type of returned Tensor. Default: if `None`, defaults to the dtype of `input`.\nConstraints:\ndtype:torch.dtype\nstructure:Null\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.nn.RNN(*args,**kwargs,input_size=None,hidden_size=None,num_layers=None,nonlinearity=None,bias=None,batch_first=None,dropout=None,bidirectional=None)\nParameter description: hidden_size - The number of features in the hidden state h\nConstraints:\ndtype:int\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:[0,inf)\n\n###\n\nSignature: torch.nn.functional.avg_pool1d(input,kernel_size,stride=None,padding=0,ceil_mode=False,count_include_pad=True)\nParameter description: padding - implicit zero paddings on both sides of the input. Can be a single number or a tuple (padW,). Default: 0\nConstraints:\ndtype:bool;int\nstructure:tuple\nshape:Null\nndim:0;1\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.autograd.grad(outputs,inputs,grad_outputs=None,retain_graph=None,create_graph=False,only_inputs=True,allow_unused=False)\nParameter description: outputs(sequence of Tensor) - outputs of the differentiated function.\nConstraints:\ndtype:Null\nstructure:sequence;tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.repeat_interleave(input,repeats,dim=None)\nParameter description: dim(int, optional) - The dimension along which to repeat values.By default, use the flattened input array, and return a flat outputarray.\nConstraints:\ndtype:int\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.std(input,dim,unbiased=True,keepdim=False,out=None)\nParameter description: dim(int or tuple of python:ints) - the dimension or dimensions to reduce.\nConstraints:",
            "constr": {
                "dtype": [
                    "int"
                ],
                "structure": [
                    "tuple"
                ],
                "shape": [],
                "ndim": [
                    "0",
                    "1"
                ],
                "enum": [],
                "range": []
            },
            "signature": "torch.std(input,dim,unbiased=True,keepdim=False,out=None)",
            "descp": "dim(int or tuple of python:ints) - the dimension or dimensions to reduce."
        },
        {
            "fname": "torch.cuda.set_rng_state.yaml",
            "arg": "new_state",
            "prompt": "Signature: torch.distributed.recv(tensor,src=None,group=<objectobject>,tag=0)\nParameter description: tensor(Tensor) - Tensor to fill with received data.\nConstraints:\ndtype:Null\nstructure:tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.div(input,other,out=None)\nParameter description: out(Tensor, optional) - the output tensor.\nConstraints:\ndtype:Null\nstructure:tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.nn.TransformerDecoder(decoder_layer,num_layers,norm=None)\nParameter description: num_layers - the number of sub-decoder-layers in the decoder (required).\nConstraints:\ndtype:int\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:[0,inf)\n\n###\n\nSignature: torch.nn.quantized.functional.interpolate(input,size=None,scale_factor=None,mode=nearest,align_corners=None)\nParameter description: align_corners(bool, optional) - Geometrically, we consider the pixels of the input and output as squares rather than points. If set to `True`, the input and output tensors are aligned by the center points of their corner pixels, preserving the values at the corner pixels. If set to `False`, the input and output tensors are aligned by the corner points of their corner pixels, and the interpolation uses edge value padding for out-of-boundary values, making this operation independent of input size when `scale_factor` is kept the same. This only has an effect when `mode` is `'bilinear'`. Default: `False`\nConstraints:\ndtype:bool\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.gt(input,other,out=None)\nParameter description: out(Tensor, optional) - the output tensor that must be a BoolTensor\nConstraints:\ndtype:booltensor\nstructure:tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.empty_like(input,dtype=None,layout=None,device=None,requires_grad=False,memory_format=torch.preserve_format)\nParameter description: dtype(`torch.dtype`, optional) - the desired data type of returned Tensor. Default: if `None`, defaults to the dtype of `input`.\nConstraints:\ndtype:torch.dtype\nstructure:Null\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.nn.RNN(*args,**kwargs,input_size=None,hidden_size=None,num_layers=None,nonlinearity=None,bias=None,batch_first=None,dropout=None,bidirectional=None)\nParameter description: hidden_size - The number of features in the hidden state h\nConstraints:\ndtype:int\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:[0,inf)\n\n###\n\nSignature: torch.nn.functional.avg_pool1d(input,kernel_size,stride=None,padding=0,ceil_mode=False,count_include_pad=True)\nParameter description: padding - implicit zero paddings on both sides of the input. Can be a single number or a tuple (padW,). Default: 0\nConstraints:\ndtype:bool;int\nstructure:tuple\nshape:Null\nndim:0;1\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.autograd.grad(outputs,inputs,grad_outputs=None,retain_graph=None,create_graph=False,only_inputs=True,allow_unused=False)\nParameter description: outputs(sequence of Tensor) - outputs of the differentiated function.\nConstraints:\ndtype:Null\nstructure:sequence;tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.repeat_interleave(input,repeats,dim=None)\nParameter description: dim(int, optional) - The dimension along which to repeat values.By default, use the flattened input array, and return a flat outputarray.\nConstraints:\ndtype:int\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.cuda.set_rng_state(new_state,device=cuda)\nParameter description: new_state(torch.ByteTensor) - The desired state\nConstraints:",
            "constr": {
                "dtype": [
                    "torch.bytetensor"
                ],
                "structure": [],
                "shape": [],
                "ndim": [],
                "enum": [],
                "range": []
            },
            "signature": "torch.cuda.set_rng_state(new_state,device=cuda)",
            "descp": "new_state(torch.ByteTensor) - The desired state"
        },
        {
            "fname": "torch.nn.softshrink.yaml",
            "arg": "lambd",
            "prompt": "Signature: torch.distributed.recv(tensor,src=None,group=<objectobject>,tag=0)\nParameter description: tensor(Tensor) - Tensor to fill with received data.\nConstraints:\ndtype:Null\nstructure:tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.div(input,other,out=None)\nParameter description: out(Tensor, optional) - the output tensor.\nConstraints:\ndtype:Null\nstructure:tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.nn.TransformerDecoder(decoder_layer,num_layers,norm=None)\nParameter description: num_layers - the number of sub-decoder-layers in the decoder (required).\nConstraints:\ndtype:int\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:[0,inf)\n\n###\n\nSignature: torch.nn.quantized.functional.interpolate(input,size=None,scale_factor=None,mode=nearest,align_corners=None)\nParameter description: align_corners(bool, optional) - Geometrically, we consider the pixels of the input and output as squares rather than points. If set to `True`, the input and output tensors are aligned by the center points of their corner pixels, preserving the values at the corner pixels. If set to `False`, the input and output tensors are aligned by the corner points of their corner pixels, and the interpolation uses edge value padding for out-of-boundary values, making this operation independent of input size when `scale_factor` is kept the same. This only has an effect when `mode` is `'bilinear'`. Default: `False`\nConstraints:\ndtype:bool\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.gt(input,other,out=None)\nParameter description: out(Tensor, optional) - the output tensor that must be a BoolTensor\nConstraints:\ndtype:booltensor\nstructure:tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.empty_like(input,dtype=None,layout=None,device=None,requires_grad=False,memory_format=torch.preserve_format)\nParameter description: dtype(`torch.dtype`, optional) - the desired data type of returned Tensor. Default: if `None`, defaults to the dtype of `input`.\nConstraints:\ndtype:torch.dtype\nstructure:Null\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.nn.RNN(*args,**kwargs,input_size=None,hidden_size=None,num_layers=None,nonlinearity=None,bias=None,batch_first=None,dropout=None,bidirectional=None)\nParameter description: hidden_size - The number of features in the hidden state h\nConstraints:\ndtype:int\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:[0,inf)\n\n###\n\nSignature: torch.nn.functional.avg_pool1d(input,kernel_size,stride=None,padding=0,ceil_mode=False,count_include_pad=True)\nParameter description: padding - implicit zero paddings on both sides of the input. Can be a single number or a tuple (padW,). Default: 0\nConstraints:\ndtype:bool;int\nstructure:tuple\nshape:Null\nndim:0;1\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.autograd.grad(outputs,inputs,grad_outputs=None,retain_graph=None,create_graph=False,only_inputs=True,allow_unused=False)\nParameter description: outputs(sequence of Tensor) - outputs of the differentiated function.\nConstraints:\ndtype:Null\nstructure:sequence;tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.repeat_interleave(input,repeats,dim=None)\nParameter description: dim(int, optional) - The dimension along which to repeat values.By default, use the flattened input array, and return a flat outputarray.\nConstraints:\ndtype:int\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.nn.Softshrink(lambd=0.5)\nParameter description: lambd - the  lambda  (must be no less than zero) value for the Softshrink formulation. Default: 0.5\nConstraints:",
            "constr": {
                "dtype": [
                    "bool",
                    "float"
                ],
                "structure": [],
                "shape": [],
                "ndim": [
                    "0"
                ],
                "enum": [],
                "range": []
            },
            "signature": "torch.nn.Softshrink(lambd=0.5)",
            "descp": "lambd - the  lambda  (must be no less than zero) value for the Softshrink formulation. Default: 0.5"
        },
        {
            "fname": "torch.nn.functional.embedding.yaml",
            "arg": "sparse",
            "prompt": "Signature: torch.distributed.recv(tensor,src=None,group=<objectobject>,tag=0)\nParameter description: tensor(Tensor) - Tensor to fill with received data.\nConstraints:\ndtype:Null\nstructure:tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.div(input,other,out=None)\nParameter description: out(Tensor, optional) - the output tensor.\nConstraints:\ndtype:Null\nstructure:tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.nn.TransformerDecoder(decoder_layer,num_layers,norm=None)\nParameter description: num_layers - the number of sub-decoder-layers in the decoder (required).\nConstraints:\ndtype:int\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:[0,inf)\n\n###\n\nSignature: torch.nn.quantized.functional.interpolate(input,size=None,scale_factor=None,mode=nearest,align_corners=None)\nParameter description: align_corners(bool, optional) - Geometrically, we consider the pixels of the input and output as squares rather than points. If set to `True`, the input and output tensors are aligned by the center points of their corner pixels, preserving the values at the corner pixels. If set to `False`, the input and output tensors are aligned by the corner points of their corner pixels, and the interpolation uses edge value padding for out-of-boundary values, making this operation independent of input size when `scale_factor` is kept the same. This only has an effect when `mode` is `'bilinear'`. Default: `False`\nConstraints:\ndtype:bool\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.gt(input,other,out=None)\nParameter description: out(Tensor, optional) - the output tensor that must be a BoolTensor\nConstraints:\ndtype:booltensor\nstructure:tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.empty_like(input,dtype=None,layout=None,device=None,requires_grad=False,memory_format=torch.preserve_format)\nParameter description: dtype(`torch.dtype`, optional) - the desired data type of returned Tensor. Default: if `None`, defaults to the dtype of `input`.\nConstraints:\ndtype:torch.dtype\nstructure:Null\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.nn.RNN(*args,**kwargs,input_size=None,hidden_size=None,num_layers=None,nonlinearity=None,bias=None,batch_first=None,dropout=None,bidirectional=None)\nParameter description: hidden_size - The number of features in the hidden state h\nConstraints:\ndtype:int\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:[0,inf)\n\n###\n\nSignature: torch.nn.functional.avg_pool1d(input,kernel_size,stride=None,padding=0,ceil_mode=False,count_include_pad=True)\nParameter description: padding - implicit zero paddings on both sides of the input. Can be a single number or a tuple (padW,). Default: 0\nConstraints:\ndtype:bool;int\nstructure:tuple\nshape:Null\nndim:0;1\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.autograd.grad(outputs,inputs,grad_outputs=None,retain_graph=None,create_graph=False,only_inputs=True,allow_unused=False)\nParameter description: outputs(sequence of Tensor) - outputs of the differentiated function.\nConstraints:\ndtype:Null\nstructure:sequence;tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.repeat_interleave(input,repeats,dim=None)\nParameter description: dim(int, optional) - The dimension along which to repeat values.By default, use the flattened input array, and return a flat outputarray.\nConstraints:\ndtype:int\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.nn.functional.embedding(input,weight,padding_idx=None,max_norm=None,norm_type=2.0,scale_grad_by_freq=False,sparse=False)\nParameter description: sparse(bool, optional) - If `True`, gradient w.r.t. `weight` will be a sparse tensor. See Notes under `torch.nn.Embedding` for more details regarding sparse gradients.\nConstraints:",
            "constr": {
                "dtype": [
                    "bool"
                ],
                "structure": [],
                "shape": [],
                "ndim": [
                    "0"
                ],
                "enum": [],
                "range": []
            },
            "signature": "torch.nn.functional.embedding(input,weight,padding_idx=None,max_norm=None,norm_type=2.0,scale_grad_by_freq=False,sparse=False)",
            "descp": "sparse(bool, optional) - If `True`, gradient w.r.t. `weight` will be a sparse tensor. See Notes under `torch.nn.Embedding` for more details regarding sparse gradients."
        },
        {
            "fname": "torch.nn.grucell.yaml",
            "arg": "hidden_size",
            "prompt": "Signature: torch.distributed.recv(tensor,src=None,group=<objectobject>,tag=0)\nParameter description: tensor(Tensor) - Tensor to fill with received data.\nConstraints:\ndtype:Null\nstructure:tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.div(input,other,out=None)\nParameter description: out(Tensor, optional) - the output tensor.\nConstraints:\ndtype:Null\nstructure:tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.nn.TransformerDecoder(decoder_layer,num_layers,norm=None)\nParameter description: num_layers - the number of sub-decoder-layers in the decoder (required).\nConstraints:\ndtype:int\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:[0,inf)\n\n###\n\nSignature: torch.nn.quantized.functional.interpolate(input,size=None,scale_factor=None,mode=nearest,align_corners=None)\nParameter description: align_corners(bool, optional) - Geometrically, we consider the pixels of the input and output as squares rather than points. If set to `True`, the input and output tensors are aligned by the center points of their corner pixels, preserving the values at the corner pixels. If set to `False`, the input and output tensors are aligned by the corner points of their corner pixels, and the interpolation uses edge value padding for out-of-boundary values, making this operation independent of input size when `scale_factor` is kept the same. This only has an effect when `mode` is `'bilinear'`. Default: `False`\nConstraints:\ndtype:bool\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.gt(input,other,out=None)\nParameter description: out(Tensor, optional) - the output tensor that must be a BoolTensor\nConstraints:\ndtype:booltensor\nstructure:tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.empty_like(input,dtype=None,layout=None,device=None,requires_grad=False,memory_format=torch.preserve_format)\nParameter description: dtype(`torch.dtype`, optional) - the desired data type of returned Tensor. Default: if `None`, defaults to the dtype of `input`.\nConstraints:\ndtype:torch.dtype\nstructure:Null\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.nn.RNN(*args,**kwargs,input_size=None,hidden_size=None,num_layers=None,nonlinearity=None,bias=None,batch_first=None,dropout=None,bidirectional=None)\nParameter description: hidden_size - The number of features in the hidden state h\nConstraints:\ndtype:int\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:[0,inf)\n\n###\n\nSignature: torch.nn.functional.avg_pool1d(input,kernel_size,stride=None,padding=0,ceil_mode=False,count_include_pad=True)\nParameter description: padding - implicit zero paddings on both sides of the input. Can be a single number or a tuple (padW,). Default: 0\nConstraints:\ndtype:bool;int\nstructure:tuple\nshape:Null\nndim:0;1\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.autograd.grad(outputs,inputs,grad_outputs=None,retain_graph=None,create_graph=False,only_inputs=True,allow_unused=False)\nParameter description: outputs(sequence of Tensor) - outputs of the differentiated function.\nConstraints:\ndtype:Null\nstructure:sequence;tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.repeat_interleave(input,repeats,dim=None)\nParameter description: dim(int, optional) - The dimension along which to repeat values.By default, use the flattened input array, and return a flat outputarray.\nConstraints:\ndtype:int\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.nn.GRUCell(input_size,hidden_size,bias=True)\nParameter description: hidden_size - The number of features in the hidden state h\nConstraints:",
            "constr": {
                "dtype": [
                    "int"
                ],
                "structure": [],
                "shape": [],
                "ndim": [
                    "0"
                ],
                "enum": [],
                "range": [
                    "[0,inf)"
                ]
            },
            "signature": "torch.nn.GRUCell(input_size,hidden_size,bias=True)",
            "descp": "hidden_size - The number of features in the hidden state h"
        },
        {
            "fname": "torch.bitwise_not.yaml",
            "arg": "input",
            "prompt": "Signature: torch.distributed.recv(tensor,src=None,group=<objectobject>,tag=0)\nParameter description: tensor(Tensor) - Tensor to fill with received data.\nConstraints:\ndtype:Null\nstructure:tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.div(input,other,out=None)\nParameter description: out(Tensor, optional) - the output tensor.\nConstraints:\ndtype:Null\nstructure:tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.nn.TransformerDecoder(decoder_layer,num_layers,norm=None)\nParameter description: num_layers - the number of sub-decoder-layers in the decoder (required).\nConstraints:\ndtype:int\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:[0,inf)\n\n###\n\nSignature: torch.nn.quantized.functional.interpolate(input,size=None,scale_factor=None,mode=nearest,align_corners=None)\nParameter description: align_corners(bool, optional) - Geometrically, we consider the pixels of the input and output as squares rather than points. If set to `True`, the input and output tensors are aligned by the center points of their corner pixels, preserving the values at the corner pixels. If set to `False`, the input and output tensors are aligned by the corner points of their corner pixels, and the interpolation uses edge value padding for out-of-boundary values, making this operation independent of input size when `scale_factor` is kept the same. This only has an effect when `mode` is `'bilinear'`. Default: `False`\nConstraints:\ndtype:bool\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.gt(input,other,out=None)\nParameter description: out(Tensor, optional) - the output tensor that must be a BoolTensor\nConstraints:\ndtype:booltensor\nstructure:tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.empty_like(input,dtype=None,layout=None,device=None,requires_grad=False,memory_format=torch.preserve_format)\nParameter description: dtype(`torch.dtype`, optional) - the desired data type of returned Tensor. Default: if `None`, defaults to the dtype of `input`.\nConstraints:\ndtype:torch.dtype\nstructure:Null\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.nn.RNN(*args,**kwargs,input_size=None,hidden_size=None,num_layers=None,nonlinearity=None,bias=None,batch_first=None,dropout=None,bidirectional=None)\nParameter description: hidden_size - The number of features in the hidden state h\nConstraints:\ndtype:int\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:[0,inf)\n\n###\n\nSignature: torch.nn.functional.avg_pool1d(input,kernel_size,stride=None,padding=0,ceil_mode=False,count_include_pad=True)\nParameter description: padding - implicit zero paddings on both sides of the input. Can be a single number or a tuple (padW,). Default: 0\nConstraints:\ndtype:bool;int\nstructure:tuple\nshape:Null\nndim:0;1\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.autograd.grad(outputs,inputs,grad_outputs=None,retain_graph=None,create_graph=False,only_inputs=True,allow_unused=False)\nParameter description: outputs(sequence of Tensor) - outputs of the differentiated function.\nConstraints:\ndtype:Null\nstructure:sequence;tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.repeat_interleave(input,repeats,dim=None)\nParameter description: dim(int, optional) - The dimension along which to repeat values.By default, use the flattened input array, and return a flat outputarray.\nConstraints:\ndtype:int\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.bitwise_not(input,out=None)\nParameter description: input(Tensor) - the input tensor.\nConstraints:",
            "constr": {
                "dtype": [],
                "structure": [
                    "tensor"
                ],
                "shape": [],
                "ndim": [],
                "enum": [],
                "range": []
            },
            "signature": "torch.bitwise_not(input,out=None)",
            "descp": "input(Tensor) - the input tensor."
        },
        {
            "fname": "torch.rand.yaml",
            "arg": "*size",
            "prompt": "Signature: torch.distributed.recv(tensor,src=None,group=<objectobject>,tag=0)\nParameter description: tensor(Tensor) - Tensor to fill with received data.\nConstraints:\ndtype:Null\nstructure:tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.div(input,other,out=None)\nParameter description: out(Tensor, optional) - the output tensor.\nConstraints:\ndtype:Null\nstructure:tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.nn.TransformerDecoder(decoder_layer,num_layers,norm=None)\nParameter description: num_layers - the number of sub-decoder-layers in the decoder (required).\nConstraints:\ndtype:int\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:[0,inf)\n\n###\n\nSignature: torch.nn.quantized.functional.interpolate(input,size=None,scale_factor=None,mode=nearest,align_corners=None)\nParameter description: align_corners(bool, optional) - Geometrically, we consider the pixels of the input and output as squares rather than points. If set to `True`, the input and output tensors are aligned by the center points of their corner pixels, preserving the values at the corner pixels. If set to `False`, the input and output tensors are aligned by the corner points of their corner pixels, and the interpolation uses edge value padding for out-of-boundary values, making this operation independent of input size when `scale_factor` is kept the same. This only has an effect when `mode` is `'bilinear'`. Default: `False`\nConstraints:\ndtype:bool\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.gt(input,other,out=None)\nParameter description: out(Tensor, optional) - the output tensor that must be a BoolTensor\nConstraints:\ndtype:booltensor\nstructure:tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.empty_like(input,dtype=None,layout=None,device=None,requires_grad=False,memory_format=torch.preserve_format)\nParameter description: dtype(`torch.dtype`, optional) - the desired data type of returned Tensor. Default: if `None`, defaults to the dtype of `input`.\nConstraints:\ndtype:torch.dtype\nstructure:Null\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.nn.RNN(*args,**kwargs,input_size=None,hidden_size=None,num_layers=None,nonlinearity=None,bias=None,batch_first=None,dropout=None,bidirectional=None)\nParameter description: hidden_size - The number of features in the hidden state h\nConstraints:\ndtype:int\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:[0,inf)\n\n###\n\nSignature: torch.nn.functional.avg_pool1d(input,kernel_size,stride=None,padding=0,ceil_mode=False,count_include_pad=True)\nParameter description: padding - implicit zero paddings on both sides of the input. Can be a single number or a tuple (padW,). Default: 0\nConstraints:\ndtype:bool;int\nstructure:tuple\nshape:Null\nndim:0;1\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.autograd.grad(outputs,inputs,grad_outputs=None,retain_graph=None,create_graph=False,only_inputs=True,allow_unused=False)\nParameter description: outputs(sequence of Tensor) - outputs of the differentiated function.\nConstraints:\ndtype:Null\nstructure:sequence;tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.repeat_interleave(input,repeats,dim=None)\nParameter description: dim(int, optional) - The dimension along which to repeat values.By default, use the flattened input array, and return a flat outputarray.\nConstraints:\ndtype:int\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.rand(*size,out=None,dtype=None,layout=torch.strided,device=None,requires_grad=False)\nParameter description: *size(int...) - a sequence of integers defining the shape of the output tensor. Can be a variable number of arguments or a collection like a list or tuple.\nConstraints:",
            "constr": {
                "dtype": [
                    "integer",
                    "int"
                ],
                "structure": [
                    "list",
                    "sequence",
                    "tuple"
                ],
                "shape": [],
                "ndim": [
                    "0",
                    "1"
                ],
                "enum": [],
                "range": [
                    "[0,inf)"
                ]
            },
            "signature": "torch.rand(*size,out=None,dtype=None,layout=torch.strided,device=None,requires_grad=False)",
            "descp": "*size(int...) - a sequence of integers defining the shape of the output tensor. Can be a variable number of arguments or a collection like a list or tuple."
        },
        {
            "fname": "torch.ormqr.yaml",
            "arg": "input3",
            "prompt": "Signature: torch.distributed.recv(tensor,src=None,group=<objectobject>,tag=0)\nParameter description: tensor(Tensor) - Tensor to fill with received data.\nConstraints:\ndtype:Null\nstructure:tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.div(input,other,out=None)\nParameter description: out(Tensor, optional) - the output tensor.\nConstraints:\ndtype:Null\nstructure:tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.nn.TransformerDecoder(decoder_layer,num_layers,norm=None)\nParameter description: num_layers - the number of sub-decoder-layers in the decoder (required).\nConstraints:\ndtype:int\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:[0,inf)\n\n###\n\nSignature: torch.nn.quantized.functional.interpolate(input,size=None,scale_factor=None,mode=nearest,align_corners=None)\nParameter description: align_corners(bool, optional) - Geometrically, we consider the pixels of the input and output as squares rather than points. If set to `True`, the input and output tensors are aligned by the center points of their corner pixels, preserving the values at the corner pixels. If set to `False`, the input and output tensors are aligned by the corner points of their corner pixels, and the interpolation uses edge value padding for out-of-boundary values, making this operation independent of input size when `scale_factor` is kept the same. This only has an effect when `mode` is `'bilinear'`. Default: `False`\nConstraints:\ndtype:bool\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.gt(input,other,out=None)\nParameter description: out(Tensor, optional) - the output tensor that must be a BoolTensor\nConstraints:\ndtype:booltensor\nstructure:tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.empty_like(input,dtype=None,layout=None,device=None,requires_grad=False,memory_format=torch.preserve_format)\nParameter description: dtype(`torch.dtype`, optional) - the desired data type of returned Tensor. Default: if `None`, defaults to the dtype of `input`.\nConstraints:\ndtype:torch.dtype\nstructure:Null\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.nn.RNN(*args,**kwargs,input_size=None,hidden_size=None,num_layers=None,nonlinearity=None,bias=None,batch_first=None,dropout=None,bidirectional=None)\nParameter description: hidden_size - The number of features in the hidden state h\nConstraints:\ndtype:int\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:[0,inf)\n\n###\n\nSignature: torch.nn.functional.avg_pool1d(input,kernel_size,stride=None,padding=0,ceil_mode=False,count_include_pad=True)\nParameter description: padding - implicit zero paddings on both sides of the input. Can be a single number or a tuple (padW,). Default: 0\nConstraints:\ndtype:bool;int\nstructure:tuple\nshape:Null\nndim:0;1\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.autograd.grad(outputs,inputs,grad_outputs=None,retain_graph=None,create_graph=False,only_inputs=True,allow_unused=False)\nParameter description: outputs(sequence of Tensor) - outputs of the differentiated function.\nConstraints:\ndtype:Null\nstructure:sequence;tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.repeat_interleave(input,repeats,dim=None)\nParameter description: dim(int, optional) - The dimension along which to repeat values.By default, use the flattened input array, and return a flat outputarray.\nConstraints:\ndtype:int\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.ormqr(input,input2,input3,left=True,transpose=False)\nParameter description: input3(Tensor) - the matrix to be multiplied.\nConstraints:",
            "constr": {
                "dtype": [
                    "numeric"
                ],
                "structure": [
                    "tensor"
                ],
                "shape": [],
                "ndim": [],
                "enum": [],
                "range": []
            },
            "signature": "torch.ormqr(input,input2,input3,left=True,transpose=False)",
            "descp": "input3(Tensor) - the matrix to be multiplied."
        },
        {
            "fname": "torch.nn.upsamplingnearest2d.yaml",
            "arg": "scale_factor",
            "prompt": "Signature: torch.distributed.recv(tensor,src=None,group=<objectobject>,tag=0)\nParameter description: tensor(Tensor) - Tensor to fill with received data.\nConstraints:\ndtype:Null\nstructure:tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.div(input,other,out=None)\nParameter description: out(Tensor, optional) - the output tensor.\nConstraints:\ndtype:Null\nstructure:tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.nn.TransformerDecoder(decoder_layer,num_layers,norm=None)\nParameter description: num_layers - the number of sub-decoder-layers in the decoder (required).\nConstraints:\ndtype:int\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:[0,inf)\n\n###\n\nSignature: torch.nn.quantized.functional.interpolate(input,size=None,scale_factor=None,mode=nearest,align_corners=None)\nParameter description: align_corners(bool, optional) - Geometrically, we consider the pixels of the input and output as squares rather than points. If set to `True`, the input and output tensors are aligned by the center points of their corner pixels, preserving the values at the corner pixels. If set to `False`, the input and output tensors are aligned by the corner points of their corner pixels, and the interpolation uses edge value padding for out-of-boundary values, making this operation independent of input size when `scale_factor` is kept the same. This only has an effect when `mode` is `'bilinear'`. Default: `False`\nConstraints:\ndtype:bool\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.gt(input,other,out=None)\nParameter description: out(Tensor, optional) - the output tensor that must be a BoolTensor\nConstraints:\ndtype:booltensor\nstructure:tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.empty_like(input,dtype=None,layout=None,device=None,requires_grad=False,memory_format=torch.preserve_format)\nParameter description: dtype(`torch.dtype`, optional) - the desired data type of returned Tensor. Default: if `None`, defaults to the dtype of `input`.\nConstraints:\ndtype:torch.dtype\nstructure:Null\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.nn.RNN(*args,**kwargs,input_size=None,hidden_size=None,num_layers=None,nonlinearity=None,bias=None,batch_first=None,dropout=None,bidirectional=None)\nParameter description: hidden_size - The number of features in the hidden state h\nConstraints:\ndtype:int\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:[0,inf)\n\n###\n\nSignature: torch.nn.functional.avg_pool1d(input,kernel_size,stride=None,padding=0,ceil_mode=False,count_include_pad=True)\nParameter description: padding - implicit zero paddings on both sides of the input. Can be a single number or a tuple (padW,). Default: 0\nConstraints:\ndtype:bool;int\nstructure:tuple\nshape:Null\nndim:0;1\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.autograd.grad(outputs,inputs,grad_outputs=None,retain_graph=None,create_graph=False,only_inputs=True,allow_unused=False)\nParameter description: outputs(sequence of Tensor) - outputs of the differentiated function.\nConstraints:\ndtype:Null\nstructure:sequence;tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.repeat_interleave(input,repeats,dim=None)\nParameter description: dim(int, optional) - The dimension along which to repeat values.By default, use the flattened input array, and return a flat outputarray.\nConstraints:\ndtype:int\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.nn.UpsamplingNearest2d(size=None,scale_factor=None)\nParameter description: scale_factor(float or Tuple[float, float], optional) - multiplier for spatial size.\nConstraints:",
            "constr": {
                "dtype": [
                    "float"
                ],
                "structure": [
                    "tuple"
                ],
                "shape": [],
                "ndim": [
                    "0",
                    "1"
                ],
                "enum": [],
                "range": []
            },
            "signature": "torch.nn.UpsamplingNearest2d(size=None,scale_factor=None)",
            "descp": "scale_factor(float or Tuple[float, float], optional) - multiplier for spatial size."
        },
        {
            "fname": "torch.nn.functional.avg_pool3d.yaml",
            "arg": "input",
            "prompt": "Signature: torch.distributed.recv(tensor,src=None,group=<objectobject>,tag=0)\nParameter description: tensor(Tensor) - Tensor to fill with received data.\nConstraints:\ndtype:Null\nstructure:tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.div(input,other,out=None)\nParameter description: out(Tensor, optional) - the output tensor.\nConstraints:\ndtype:Null\nstructure:tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.nn.TransformerDecoder(decoder_layer,num_layers,norm=None)\nParameter description: num_layers - the number of sub-decoder-layers in the decoder (required).\nConstraints:\ndtype:int\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:[0,inf)\n\n###\n\nSignature: torch.nn.quantized.functional.interpolate(input,size=None,scale_factor=None,mode=nearest,align_corners=None)\nParameter description: align_corners(bool, optional) - Geometrically, we consider the pixels of the input and output as squares rather than points. If set to `True`, the input and output tensors are aligned by the center points of their corner pixels, preserving the values at the corner pixels. If set to `False`, the input and output tensors are aligned by the corner points of their corner pixels, and the interpolation uses edge value padding for out-of-boundary values, making this operation independent of input size when `scale_factor` is kept the same. This only has an effect when `mode` is `'bilinear'`. Default: `False`\nConstraints:\ndtype:bool\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.gt(input,other,out=None)\nParameter description: out(Tensor, optional) - the output tensor that must be a BoolTensor\nConstraints:\ndtype:booltensor\nstructure:tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.empty_like(input,dtype=None,layout=None,device=None,requires_grad=False,memory_format=torch.preserve_format)\nParameter description: dtype(`torch.dtype`, optional) - the desired data type of returned Tensor. Default: if `None`, defaults to the dtype of `input`.\nConstraints:\ndtype:torch.dtype\nstructure:Null\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.nn.RNN(*args,**kwargs,input_size=None,hidden_size=None,num_layers=None,nonlinearity=None,bias=None,batch_first=None,dropout=None,bidirectional=None)\nParameter description: hidden_size - The number of features in the hidden state h\nConstraints:\ndtype:int\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:[0,inf)\n\n###\n\nSignature: torch.nn.functional.avg_pool1d(input,kernel_size,stride=None,padding=0,ceil_mode=False,count_include_pad=True)\nParameter description: padding - implicit zero paddings on both sides of the input. Can be a single number or a tuple (padW,). Default: 0\nConstraints:\ndtype:bool;int\nstructure:tuple\nshape:Null\nndim:0;1\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.autograd.grad(outputs,inputs,grad_outputs=None,retain_graph=None,create_graph=False,only_inputs=True,allow_unused=False)\nParameter description: outputs(sequence of Tensor) - outputs of the differentiated function.\nConstraints:\ndtype:Null\nstructure:sequence;tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.repeat_interleave(input,repeats,dim=None)\nParameter description: dim(int, optional) - The dimension along which to repeat values.By default, use the flattened input array, and return a flat outputarray.\nConstraints:\ndtype:int\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.nn.functional.avg_pool3d(input,kernel_size,stride=None,padding=0,ceil_mode=False,count_include_pad=True,divisor_override=None)\nParameter description: input - input tensor (minibatch , in _channels , iT  times iH , iW) \nConstraints:",
            "constr": {
                "dtype": [],
                "structure": [
                    "tensor"
                ],
                "shape": [
                    "(minibatch , in _channels , iT times iH , iW)"
                ],
                "ndim": [],
                "enum": [],
                "range": []
            },
            "signature": "torch.nn.functional.avg_pool3d(input,kernel_size,stride=None,padding=0,ceil_mode=False,count_include_pad=True,divisor_override=None)",
            "descp": "input - input tensor (minibatch , in _channels , iT  times iH , iW) "
        },
        {
            "fname": "torch.unique.yaml",
            "arg": "return_inverse",
            "prompt": "Signature: torch.distributed.recv(tensor,src=None,group=<objectobject>,tag=0)\nParameter description: tensor(Tensor) - Tensor to fill with received data.\nConstraints:\ndtype:Null\nstructure:tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.div(input,other,out=None)\nParameter description: out(Tensor, optional) - the output tensor.\nConstraints:\ndtype:Null\nstructure:tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.nn.TransformerDecoder(decoder_layer,num_layers,norm=None)\nParameter description: num_layers - the number of sub-decoder-layers in the decoder (required).\nConstraints:\ndtype:int\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:[0,inf)\n\n###\n\nSignature: torch.nn.quantized.functional.interpolate(input,size=None,scale_factor=None,mode=nearest,align_corners=None)\nParameter description: align_corners(bool, optional) - Geometrically, we consider the pixels of the input and output as squares rather than points. If set to `True`, the input and output tensors are aligned by the center points of their corner pixels, preserving the values at the corner pixels. If set to `False`, the input and output tensors are aligned by the corner points of their corner pixels, and the interpolation uses edge value padding for out-of-boundary values, making this operation independent of input size when `scale_factor` is kept the same. This only has an effect when `mode` is `'bilinear'`. Default: `False`\nConstraints:\ndtype:bool\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.gt(input,other,out=None)\nParameter description: out(Tensor, optional) - the output tensor that must be a BoolTensor\nConstraints:\ndtype:booltensor\nstructure:tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.empty_like(input,dtype=None,layout=None,device=None,requires_grad=False,memory_format=torch.preserve_format)\nParameter description: dtype(`torch.dtype`, optional) - the desired data type of returned Tensor. Default: if `None`, defaults to the dtype of `input`.\nConstraints:\ndtype:torch.dtype\nstructure:Null\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.nn.RNN(*args,**kwargs,input_size=None,hidden_size=None,num_layers=None,nonlinearity=None,bias=None,batch_first=None,dropout=None,bidirectional=None)\nParameter description: hidden_size - The number of features in the hidden state h\nConstraints:\ndtype:int\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:[0,inf)\n\n###\n\nSignature: torch.nn.functional.avg_pool1d(input,kernel_size,stride=None,padding=0,ceil_mode=False,count_include_pad=True)\nParameter description: padding - implicit zero paddings on both sides of the input. Can be a single number or a tuple (padW,). Default: 0\nConstraints:\ndtype:bool;int\nstructure:tuple\nshape:Null\nndim:0;1\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.autograd.grad(outputs,inputs,grad_outputs=None,retain_graph=None,create_graph=False,only_inputs=True,allow_unused=False)\nParameter description: outputs(sequence of Tensor) - outputs of the differentiated function.\nConstraints:\ndtype:Null\nstructure:sequence;tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.repeat_interleave(input,repeats,dim=None)\nParameter description: dim(int, optional) - The dimension along which to repeat values.By default, use the flattened input array, and return a flat outputarray.\nConstraints:\ndtype:int\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.unique(input,sorted=True,return_inverse=False,return_counts=False,dim=None)\nParameter description: return_inverse(bool) - Whether to also return the indices for where elements in the original input ended up in the returned unique list.\nConstraints:",
            "constr": {
                "dtype": [
                    "bool"
                ],
                "structure": [],
                "shape": [],
                "ndim": [
                    "0"
                ],
                "enum": [],
                "range": []
            },
            "signature": "torch.unique(input,sorted=True,return_inverse=False,return_counts=False,dim=None)",
            "descp": "return_inverse(bool) - Whether to also return the indices for where elements in the original input ended up in the returned unique list."
        },
        {
            "fname": "torch.normal2.yaml",
            "arg": "out",
            "prompt": "Signature: torch.distributed.recv(tensor,src=None,group=<objectobject>,tag=0)\nParameter description: tensor(Tensor) - Tensor to fill with received data.\nConstraints:\ndtype:Null\nstructure:tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.div(input,other,out=None)\nParameter description: out(Tensor, optional) - the output tensor.\nConstraints:\ndtype:Null\nstructure:tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.nn.TransformerDecoder(decoder_layer,num_layers,norm=None)\nParameter description: num_layers - the number of sub-decoder-layers in the decoder (required).\nConstraints:\ndtype:int\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:[0,inf)\n\n###\n\nSignature: torch.nn.quantized.functional.interpolate(input,size=None,scale_factor=None,mode=nearest,align_corners=None)\nParameter description: align_corners(bool, optional) - Geometrically, we consider the pixels of the input and output as squares rather than points. If set to `True`, the input and output tensors are aligned by the center points of their corner pixels, preserving the values at the corner pixels. If set to `False`, the input and output tensors are aligned by the corner points of their corner pixels, and the interpolation uses edge value padding for out-of-boundary values, making this operation independent of input size when `scale_factor` is kept the same. This only has an effect when `mode` is `'bilinear'`. Default: `False`\nConstraints:\ndtype:bool\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.gt(input,other,out=None)\nParameter description: out(Tensor, optional) - the output tensor that must be a BoolTensor\nConstraints:\ndtype:booltensor\nstructure:tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.empty_like(input,dtype=None,layout=None,device=None,requires_grad=False,memory_format=torch.preserve_format)\nParameter description: dtype(`torch.dtype`, optional) - the desired data type of returned Tensor. Default: if `None`, defaults to the dtype of `input`.\nConstraints:\ndtype:torch.dtype\nstructure:Null\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.nn.RNN(*args,**kwargs,input_size=None,hidden_size=None,num_layers=None,nonlinearity=None,bias=None,batch_first=None,dropout=None,bidirectional=None)\nParameter description: hidden_size - The number of features in the hidden state h\nConstraints:\ndtype:int\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:[0,inf)\n\n###\n\nSignature: torch.nn.functional.avg_pool1d(input,kernel_size,stride=None,padding=0,ceil_mode=False,count_include_pad=True)\nParameter description: padding - implicit zero paddings on both sides of the input. Can be a single number or a tuple (padW,). Default: 0\nConstraints:\ndtype:bool;int\nstructure:tuple\nshape:Null\nndim:0;1\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.autograd.grad(outputs,inputs,grad_outputs=None,retain_graph=None,create_graph=False,only_inputs=True,allow_unused=False)\nParameter description: outputs(sequence of Tensor) - outputs of the differentiated function.\nConstraints:\ndtype:Null\nstructure:sequence;tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.repeat_interleave(input,repeats,dim=None)\nParameter description: dim(int, optional) - The dimension along which to repeat values.By default, use the flattened input array, and return a flat outputarray.\nConstraints:\ndtype:int\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.normal(std,mean=0.0,out=None)\nParameter description: out(Tensor, optional) - the output tensor.\nConstraints:",
            "constr": {
                "dtype": [],
                "structure": [
                    "tensor"
                ],
                "shape": [],
                "ndim": [],
                "enum": [],
                "range": []
            },
            "signature": "torch.normal(std,mean=0.0,out=None)",
            "descp": "out(Tensor, optional) - the output tensor."
        },
        {
            "fname": "torch.distributed.all_reduce.yaml",
            "arg": "tensor",
            "prompt": "Signature: torch.distributed.recv(tensor,src=None,group=<objectobject>,tag=0)\nParameter description: tensor(Tensor) - Tensor to fill with received data.\nConstraints:\ndtype:Null\nstructure:tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.div(input,other,out=None)\nParameter description: out(Tensor, optional) - the output tensor.\nConstraints:\ndtype:Null\nstructure:tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.nn.TransformerDecoder(decoder_layer,num_layers,norm=None)\nParameter description: num_layers - the number of sub-decoder-layers in the decoder (required).\nConstraints:\ndtype:int\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:[0,inf)\n\n###\n\nSignature: torch.nn.quantized.functional.interpolate(input,size=None,scale_factor=None,mode=nearest,align_corners=None)\nParameter description: align_corners(bool, optional) - Geometrically, we consider the pixels of the input and output as squares rather than points. If set to `True`, the input and output tensors are aligned by the center points of their corner pixels, preserving the values at the corner pixels. If set to `False`, the input and output tensors are aligned by the corner points of their corner pixels, and the interpolation uses edge value padding for out-of-boundary values, making this operation independent of input size when `scale_factor` is kept the same. This only has an effect when `mode` is `'bilinear'`. Default: `False`\nConstraints:\ndtype:bool\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.gt(input,other,out=None)\nParameter description: out(Tensor, optional) - the output tensor that must be a BoolTensor\nConstraints:\ndtype:booltensor\nstructure:tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.empty_like(input,dtype=None,layout=None,device=None,requires_grad=False,memory_format=torch.preserve_format)\nParameter description: dtype(`torch.dtype`, optional) - the desired data type of returned Tensor. Default: if `None`, defaults to the dtype of `input`.\nConstraints:\ndtype:torch.dtype\nstructure:Null\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.nn.RNN(*args,**kwargs,input_size=None,hidden_size=None,num_layers=None,nonlinearity=None,bias=None,batch_first=None,dropout=None,bidirectional=None)\nParameter description: hidden_size - The number of features in the hidden state h\nConstraints:\ndtype:int\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:[0,inf)\n\n###\n\nSignature: torch.nn.functional.avg_pool1d(input,kernel_size,stride=None,padding=0,ceil_mode=False,count_include_pad=True)\nParameter description: padding - implicit zero paddings on both sides of the input. Can be a single number or a tuple (padW,). Default: 0\nConstraints:\ndtype:bool;int\nstructure:tuple\nshape:Null\nndim:0;1\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.autograd.grad(outputs,inputs,grad_outputs=None,retain_graph=None,create_graph=False,only_inputs=True,allow_unused=False)\nParameter description: outputs(sequence of Tensor) - outputs of the differentiated function.\nConstraints:\ndtype:Null\nstructure:sequence;tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.repeat_interleave(input,repeats,dim=None)\nParameter description: dim(int, optional) - The dimension along which to repeat values.By default, use the flattened input array, and return a flat outputarray.\nConstraints:\ndtype:int\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.distributed.all_reduce(tensor,op=ReduceOp.SUM,group=<objectobject>,async_op=False)\nParameter description: tensor(Tensor) - Input and output of the collective. The function operates in-place.\nConstraints:",
            "constr": {
                "dtype": [],
                "structure": [
                    "tensor"
                ],
                "shape": [],
                "ndim": [],
                "enum": [],
                "range": []
            },
            "signature": "torch.distributed.all_reduce(tensor,op=ReduceOp.SUM,group=<objectobject>,async_op=False)",
            "descp": "tensor(Tensor) - Input and output of the collective. The function operates in-place."
        },
        {
            "fname": "torch.nn.convtranspose2d.yaml",
            "arg": "out_channels",
            "prompt": "Signature: torch.distributed.recv(tensor,src=None,group=<objectobject>,tag=0)\nParameter description: tensor(Tensor) - Tensor to fill with received data.\nConstraints:\ndtype:Null\nstructure:tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.div(input,other,out=None)\nParameter description: out(Tensor, optional) - the output tensor.\nConstraints:\ndtype:Null\nstructure:tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.nn.TransformerDecoder(decoder_layer,num_layers,norm=None)\nParameter description: num_layers - the number of sub-decoder-layers in the decoder (required).\nConstraints:\ndtype:int\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:[0,inf)\n\n###\n\nSignature: torch.nn.quantized.functional.interpolate(input,size=None,scale_factor=None,mode=nearest,align_corners=None)\nParameter description: align_corners(bool, optional) - Geometrically, we consider the pixels of the input and output as squares rather than points. If set to `True`, the input and output tensors are aligned by the center points of their corner pixels, preserving the values at the corner pixels. If set to `False`, the input and output tensors are aligned by the corner points of their corner pixels, and the interpolation uses edge value padding for out-of-boundary values, making this operation independent of input size when `scale_factor` is kept the same. This only has an effect when `mode` is `'bilinear'`. Default: `False`\nConstraints:\ndtype:bool\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.gt(input,other,out=None)\nParameter description: out(Tensor, optional) - the output tensor that must be a BoolTensor\nConstraints:\ndtype:booltensor\nstructure:tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.empty_like(input,dtype=None,layout=None,device=None,requires_grad=False,memory_format=torch.preserve_format)\nParameter description: dtype(`torch.dtype`, optional) - the desired data type of returned Tensor. Default: if `None`, defaults to the dtype of `input`.\nConstraints:\ndtype:torch.dtype\nstructure:Null\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.nn.RNN(*args,**kwargs,input_size=None,hidden_size=None,num_layers=None,nonlinearity=None,bias=None,batch_first=None,dropout=None,bidirectional=None)\nParameter description: hidden_size - The number of features in the hidden state h\nConstraints:\ndtype:int\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:[0,inf)\n\n###\n\nSignature: torch.nn.functional.avg_pool1d(input,kernel_size,stride=None,padding=0,ceil_mode=False,count_include_pad=True)\nParameter description: padding - implicit zero paddings on both sides of the input. Can be a single number or a tuple (padW,). Default: 0\nConstraints:\ndtype:bool;int\nstructure:tuple\nshape:Null\nndim:0;1\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.autograd.grad(outputs,inputs,grad_outputs=None,retain_graph=None,create_graph=False,only_inputs=True,allow_unused=False)\nParameter description: outputs(sequence of Tensor) - outputs of the differentiated function.\nConstraints:\ndtype:Null\nstructure:sequence;tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.repeat_interleave(input,repeats,dim=None)\nParameter description: dim(int, optional) - The dimension along which to repeat values.By default, use the flattened input array, and return a flat outputarray.\nConstraints:\ndtype:int\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.nn.ConvTranspose2d(in_channels,out_channels,kernel_size,stride=1,padding=0,output_padding=0,groups=1,bias=True,dilation=1,padding_mode=zeros)\nParameter description: out_channels(int) - Number of channels produced by the convolution\nConstraints:",
            "constr": {
                "dtype": [
                    "int"
                ],
                "structure": [],
                "shape": [],
                "ndim": [
                    "0"
                ],
                "enum": [],
                "range": [
                    "[0,inf)"
                ]
            },
            "signature": "torch.nn.ConvTranspose2d(in_channels,out_channels,kernel_size,stride=1,padding=0,output_padding=0,groups=1,bias=True,dilation=1,padding_mode=zeros)",
            "descp": "out_channels(int) - Number of channels produced by the convolution"
        },
        {
            "fname": "torch.jit.load.yaml",
            "arg": "map_location",
            "prompt": "Signature: torch.distributed.recv(tensor,src=None,group=<objectobject>,tag=0)\nParameter description: tensor(Tensor) - Tensor to fill with received data.\nConstraints:\ndtype:Null\nstructure:tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.div(input,other,out=None)\nParameter description: out(Tensor, optional) - the output tensor.\nConstraints:\ndtype:Null\nstructure:tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.nn.TransformerDecoder(decoder_layer,num_layers,norm=None)\nParameter description: num_layers - the number of sub-decoder-layers in the decoder (required).\nConstraints:\ndtype:int\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:[0,inf)\n\n###\n\nSignature: torch.nn.quantized.functional.interpolate(input,size=None,scale_factor=None,mode=nearest,align_corners=None)\nParameter description: align_corners(bool, optional) - Geometrically, we consider the pixels of the input and output as squares rather than points. If set to `True`, the input and output tensors are aligned by the center points of their corner pixels, preserving the values at the corner pixels. If set to `False`, the input and output tensors are aligned by the corner points of their corner pixels, and the interpolation uses edge value padding for out-of-boundary values, making this operation independent of input size when `scale_factor` is kept the same. This only has an effect when `mode` is `'bilinear'`. Default: `False`\nConstraints:\ndtype:bool\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.gt(input,other,out=None)\nParameter description: out(Tensor, optional) - the output tensor that must be a BoolTensor\nConstraints:\ndtype:booltensor\nstructure:tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.empty_like(input,dtype=None,layout=None,device=None,requires_grad=False,memory_format=torch.preserve_format)\nParameter description: dtype(`torch.dtype`, optional) - the desired data type of returned Tensor. Default: if `None`, defaults to the dtype of `input`.\nConstraints:\ndtype:torch.dtype\nstructure:Null\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.nn.RNN(*args,**kwargs,input_size=None,hidden_size=None,num_layers=None,nonlinearity=None,bias=None,batch_first=None,dropout=None,bidirectional=None)\nParameter description: hidden_size - The number of features in the hidden state h\nConstraints:\ndtype:int\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:[0,inf)\n\n###\n\nSignature: torch.nn.functional.avg_pool1d(input,kernel_size,stride=None,padding=0,ceil_mode=False,count_include_pad=True)\nParameter description: padding - implicit zero paddings on both sides of the input. Can be a single number or a tuple (padW,). Default: 0\nConstraints:\ndtype:bool;int\nstructure:tuple\nshape:Null\nndim:0;1\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.autograd.grad(outputs,inputs,grad_outputs=None,retain_graph=None,create_graph=False,only_inputs=True,allow_unused=False)\nParameter description: outputs(sequence of Tensor) - outputs of the differentiated function.\nConstraints:\ndtype:Null\nstructure:sequence;tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.repeat_interleave(input,repeats,dim=None)\nParameter description: dim(int, optional) - The dimension along which to repeat values.By default, use the flattened input array, and return a flat outputarray.\nConstraints:\ndtype:int\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.jit.load(f,map_location=None,_extra_files=ExtraFilesMap{})\nParameter description: map_location(string or torch.device) - A simplified version of `map_location` in `torch.save` used to dynamically remap storages to an alternative set of devices.\nConstraints:",
            "constr": {
                "dtype": [
                    "string"
                ],
                "structure": [],
                "shape": [],
                "ndim": [],
                "enum": [],
                "range": []
            },
            "signature": "torch.jit.load(f,map_location=None,_extra_files=ExtraFilesMap{})",
            "descp": "map_location(string or torch.device) - A simplified version of `map_location` in `torch.save` used to dynamically remap storages to an alternative set of devices."
        },
        {
            "fname": "torch.cumsum.yaml",
            "arg": "dim",
            "prompt": "Signature: torch.distributed.recv(tensor,src=None,group=<objectobject>,tag=0)\nParameter description: tensor(Tensor) - Tensor to fill with received data.\nConstraints:\ndtype:Null\nstructure:tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.div(input,other,out=None)\nParameter description: out(Tensor, optional) - the output tensor.\nConstraints:\ndtype:Null\nstructure:tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.nn.TransformerDecoder(decoder_layer,num_layers,norm=None)\nParameter description: num_layers - the number of sub-decoder-layers in the decoder (required).\nConstraints:\ndtype:int\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:[0,inf)\n\n###\n\nSignature: torch.nn.quantized.functional.interpolate(input,size=None,scale_factor=None,mode=nearest,align_corners=None)\nParameter description: align_corners(bool, optional) - Geometrically, we consider the pixels of the input and output as squares rather than points. If set to `True`, the input and output tensors are aligned by the center points of their corner pixels, preserving the values at the corner pixels. If set to `False`, the input and output tensors are aligned by the corner points of their corner pixels, and the interpolation uses edge value padding for out-of-boundary values, making this operation independent of input size when `scale_factor` is kept the same. This only has an effect when `mode` is `'bilinear'`. Default: `False`\nConstraints:\ndtype:bool\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.gt(input,other,out=None)\nParameter description: out(Tensor, optional) - the output tensor that must be a BoolTensor\nConstraints:\ndtype:booltensor\nstructure:tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.empty_like(input,dtype=None,layout=None,device=None,requires_grad=False,memory_format=torch.preserve_format)\nParameter description: dtype(`torch.dtype`, optional) - the desired data type of returned Tensor. Default: if `None`, defaults to the dtype of `input`.\nConstraints:\ndtype:torch.dtype\nstructure:Null\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.nn.RNN(*args,**kwargs,input_size=None,hidden_size=None,num_layers=None,nonlinearity=None,bias=None,batch_first=None,dropout=None,bidirectional=None)\nParameter description: hidden_size - The number of features in the hidden state h\nConstraints:\ndtype:int\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:[0,inf)\n\n###\n\nSignature: torch.nn.functional.avg_pool1d(input,kernel_size,stride=None,padding=0,ceil_mode=False,count_include_pad=True)\nParameter description: padding - implicit zero paddings on both sides of the input. Can be a single number or a tuple (padW,). Default: 0\nConstraints:\ndtype:bool;int\nstructure:tuple\nshape:Null\nndim:0;1\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.autograd.grad(outputs,inputs,grad_outputs=None,retain_graph=None,create_graph=False,only_inputs=True,allow_unused=False)\nParameter description: outputs(sequence of Tensor) - outputs of the differentiated function.\nConstraints:\ndtype:Null\nstructure:sequence;tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.repeat_interleave(input,repeats,dim=None)\nParameter description: dim(int, optional) - The dimension along which to repeat values.By default, use the flattened input array, and return a flat outputarray.\nConstraints:\ndtype:int\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.cumsum(input,dim,out=None,dtype=None)\nParameter description: dim(int) - the dimension to do the operation over\nConstraints:",
            "constr": {
                "dtype": [
                    "int"
                ],
                "structure": [],
                "shape": [],
                "ndim": [
                    "0"
                ],
                "enum": [],
                "range": []
            },
            "signature": "torch.cumsum(input,dim,out=None,dtype=None)",
            "descp": "dim(int) - the dimension to do the operation over"
        },
        {
            "fname": "torch.acos.yaml",
            "arg": "input",
            "prompt": "Signature: torch.distributed.recv(tensor,src=None,group=<objectobject>,tag=0)\nParameter description: tensor(Tensor) - Tensor to fill with received data.\nConstraints:\ndtype:Null\nstructure:tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.div(input,other,out=None)\nParameter description: out(Tensor, optional) - the output tensor.\nConstraints:\ndtype:Null\nstructure:tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.nn.TransformerDecoder(decoder_layer,num_layers,norm=None)\nParameter description: num_layers - the number of sub-decoder-layers in the decoder (required).\nConstraints:\ndtype:int\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:[0,inf)\n\n###\n\nSignature: torch.nn.quantized.functional.interpolate(input,size=None,scale_factor=None,mode=nearest,align_corners=None)\nParameter description: align_corners(bool, optional) - Geometrically, we consider the pixels of the input and output as squares rather than points. If set to `True`, the input and output tensors are aligned by the center points of their corner pixels, preserving the values at the corner pixels. If set to `False`, the input and output tensors are aligned by the corner points of their corner pixels, and the interpolation uses edge value padding for out-of-boundary values, making this operation independent of input size when `scale_factor` is kept the same. This only has an effect when `mode` is `'bilinear'`. Default: `False`\nConstraints:\ndtype:bool\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.gt(input,other,out=None)\nParameter description: out(Tensor, optional) - the output tensor that must be a BoolTensor\nConstraints:\ndtype:booltensor\nstructure:tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.empty_like(input,dtype=None,layout=None,device=None,requires_grad=False,memory_format=torch.preserve_format)\nParameter description: dtype(`torch.dtype`, optional) - the desired data type of returned Tensor. Default: if `None`, defaults to the dtype of `input`.\nConstraints:\ndtype:torch.dtype\nstructure:Null\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.nn.RNN(*args,**kwargs,input_size=None,hidden_size=None,num_layers=None,nonlinearity=None,bias=None,batch_first=None,dropout=None,bidirectional=None)\nParameter description: hidden_size - The number of features in the hidden state h\nConstraints:\ndtype:int\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:[0,inf)\n\n###\n\nSignature: torch.nn.functional.avg_pool1d(input,kernel_size,stride=None,padding=0,ceil_mode=False,count_include_pad=True)\nParameter description: padding - implicit zero paddings on both sides of the input. Can be a single number or a tuple (padW,). Default: 0\nConstraints:\ndtype:bool;int\nstructure:tuple\nshape:Null\nndim:0;1\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.autograd.grad(outputs,inputs,grad_outputs=None,retain_graph=None,create_graph=False,only_inputs=True,allow_unused=False)\nParameter description: outputs(sequence of Tensor) - outputs of the differentiated function.\nConstraints:\ndtype:Null\nstructure:sequence;tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.repeat_interleave(input,repeats,dim=None)\nParameter description: dim(int, optional) - The dimension along which to repeat values.By default, use the flattened input array, and return a flat outputarray.\nConstraints:\ndtype:int\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.acos(input,out=None)\nParameter description: input(Tensor) - the input tensor.\nConstraints:",
            "constr": {
                "dtype": [],
                "structure": [
                    "tensor"
                ],
                "shape": [],
                "ndim": [],
                "enum": [],
                "range": []
            },
            "signature": "torch.acos(input,out=None)",
            "descp": "input(Tensor) - the input tensor."
        },
        {
            "fname": "torch.nn.upsample.yaml",
            "arg": "mode",
            "prompt": "Signature: torch.distributed.recv(tensor,src=None,group=<objectobject>,tag=0)\nParameter description: tensor(Tensor) - Tensor to fill with received data.\nConstraints:\ndtype:Null\nstructure:tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.div(input,other,out=None)\nParameter description: out(Tensor, optional) - the output tensor.\nConstraints:\ndtype:Null\nstructure:tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.nn.TransformerDecoder(decoder_layer,num_layers,norm=None)\nParameter description: num_layers - the number of sub-decoder-layers in the decoder (required).\nConstraints:\ndtype:int\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:[0,inf)\n\n###\n\nSignature: torch.nn.quantized.functional.interpolate(input,size=None,scale_factor=None,mode=nearest,align_corners=None)\nParameter description: align_corners(bool, optional) - Geometrically, we consider the pixels of the input and output as squares rather than points. If set to `True`, the input and output tensors are aligned by the center points of their corner pixels, preserving the values at the corner pixels. If set to `False`, the input and output tensors are aligned by the corner points of their corner pixels, and the interpolation uses edge value padding for out-of-boundary values, making this operation independent of input size when `scale_factor` is kept the same. This only has an effect when `mode` is `'bilinear'`. Default: `False`\nConstraints:\ndtype:bool\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.gt(input,other,out=None)\nParameter description: out(Tensor, optional) - the output tensor that must be a BoolTensor\nConstraints:\ndtype:booltensor\nstructure:tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.empty_like(input,dtype=None,layout=None,device=None,requires_grad=False,memory_format=torch.preserve_format)\nParameter description: dtype(`torch.dtype`, optional) - the desired data type of returned Tensor. Default: if `None`, defaults to the dtype of `input`.\nConstraints:\ndtype:torch.dtype\nstructure:Null\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.nn.RNN(*args,**kwargs,input_size=None,hidden_size=None,num_layers=None,nonlinearity=None,bias=None,batch_first=None,dropout=None,bidirectional=None)\nParameter description: hidden_size - The number of features in the hidden state h\nConstraints:\ndtype:int\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:[0,inf)\n\n###\n\nSignature: torch.nn.functional.avg_pool1d(input,kernel_size,stride=None,padding=0,ceil_mode=False,count_include_pad=True)\nParameter description: padding - implicit zero paddings on both sides of the input. Can be a single number or a tuple (padW,). Default: 0\nConstraints:\ndtype:bool;int\nstructure:tuple\nshape:Null\nndim:0;1\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.autograd.grad(outputs,inputs,grad_outputs=None,retain_graph=None,create_graph=False,only_inputs=True,allow_unused=False)\nParameter description: outputs(sequence of Tensor) - outputs of the differentiated function.\nConstraints:\ndtype:Null\nstructure:sequence;tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.repeat_interleave(input,repeats,dim=None)\nParameter description: dim(int, optional) - The dimension along which to repeat values.By default, use the flattened input array, and return a flat outputarray.\nConstraints:\ndtype:int\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.nn.Upsample(size=None,scale_factor=None,mode=nearest,align_corners=None)\nParameter description: mode(str, optional) - the upsampling algorithm: one of `'nearest'`, `'linear'`, `'bilinear'`, `'bicubic'` and `'trilinear'`. Default: `'nearest'`\nConstraints:",
            "constr": {
                "dtype": [
                    "string",
                    "str"
                ],
                "structure": [],
                "shape": [],
                "ndim": [],
                "enum": [
                    "trilinear",
                    "linear",
                    "bilinear",
                    "bicubic",
                    "nearest"
                ],
                "range": []
            },
            "signature": "torch.nn.Upsample(size=None,scale_factor=None,mode=nearest,align_corners=None)",
            "descp": "mode(str, optional) - the upsampling algorithm: one of `'nearest'`, `'linear'`, `'bilinear'`, `'bicubic'` and `'trilinear'`. Default: `'nearest'`"
        },
        {
            "fname": "torch.nn.maxpool3d.yaml",
            "arg": "stride",
            "prompt": "Signature: torch.distributed.recv(tensor,src=None,group=<objectobject>,tag=0)\nParameter description: tensor(Tensor) - Tensor to fill with received data.\nConstraints:\ndtype:Null\nstructure:tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.div(input,other,out=None)\nParameter description: out(Tensor, optional) - the output tensor.\nConstraints:\ndtype:Null\nstructure:tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.nn.TransformerDecoder(decoder_layer,num_layers,norm=None)\nParameter description: num_layers - the number of sub-decoder-layers in the decoder (required).\nConstraints:\ndtype:int\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:[0,inf)\n\n###\n\nSignature: torch.nn.quantized.functional.interpolate(input,size=None,scale_factor=None,mode=nearest,align_corners=None)\nParameter description: align_corners(bool, optional) - Geometrically, we consider the pixels of the input and output as squares rather than points. If set to `True`, the input and output tensors are aligned by the center points of their corner pixels, preserving the values at the corner pixels. If set to `False`, the input and output tensors are aligned by the corner points of their corner pixels, and the interpolation uses edge value padding for out-of-boundary values, making this operation independent of input size when `scale_factor` is kept the same. This only has an effect when `mode` is `'bilinear'`. Default: `False`\nConstraints:\ndtype:bool\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.gt(input,other,out=None)\nParameter description: out(Tensor, optional) - the output tensor that must be a BoolTensor\nConstraints:\ndtype:booltensor\nstructure:tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.empty_like(input,dtype=None,layout=None,device=None,requires_grad=False,memory_format=torch.preserve_format)\nParameter description: dtype(`torch.dtype`, optional) - the desired data type of returned Tensor. Default: if `None`, defaults to the dtype of `input`.\nConstraints:\ndtype:torch.dtype\nstructure:Null\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.nn.RNN(*args,**kwargs,input_size=None,hidden_size=None,num_layers=None,nonlinearity=None,bias=None,batch_first=None,dropout=None,bidirectional=None)\nParameter description: hidden_size - The number of features in the hidden state h\nConstraints:\ndtype:int\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:[0,inf)\n\n###\n\nSignature: torch.nn.functional.avg_pool1d(input,kernel_size,stride=None,padding=0,ceil_mode=False,count_include_pad=True)\nParameter description: padding - implicit zero paddings on both sides of the input. Can be a single number or a tuple (padW,). Default: 0\nConstraints:\ndtype:bool;int\nstructure:tuple\nshape:Null\nndim:0;1\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.autograd.grad(outputs,inputs,grad_outputs=None,retain_graph=None,create_graph=False,only_inputs=True,allow_unused=False)\nParameter description: outputs(sequence of Tensor) - outputs of the differentiated function.\nConstraints:\ndtype:Null\nstructure:sequence;tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.repeat_interleave(input,repeats,dim=None)\nParameter description: dim(int, optional) - The dimension along which to repeat values.By default, use the flattened input array, and return a flat outputarray.\nConstraints:\ndtype:int\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.nn.MaxPool3d(kernel_size,stride=None,padding=0,dilation=1,return_indices=False,ceil_mode=False)\nParameter description: stride - the stride of the window. Default value is `kernel_size`\nConstraints:",
            "constr": {
                "dtype": [
                    "int"
                ],
                "structure": [],
                "shape": [],
                "ndim": [],
                "enum": [],
                "range": [
                    "[0,inf)"
                ]
            },
            "signature": "torch.nn.MaxPool3d(kernel_size,stride=None,padding=0,dilation=1,return_indices=False,ceil_mode=False)",
            "descp": "stride - the stride of the window. Default value is `kernel_size`"
        },
        {
            "fname": "torch.sinh.yaml",
            "arg": "input",
            "prompt": "Signature: torch.distributed.recv(tensor,src=None,group=<objectobject>,tag=0)\nParameter description: tensor(Tensor) - Tensor to fill with received data.\nConstraints:\ndtype:Null\nstructure:tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.div(input,other,out=None)\nParameter description: out(Tensor, optional) - the output tensor.\nConstraints:\ndtype:Null\nstructure:tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.nn.TransformerDecoder(decoder_layer,num_layers,norm=None)\nParameter description: num_layers - the number of sub-decoder-layers in the decoder (required).\nConstraints:\ndtype:int\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:[0,inf)\n\n###\n\nSignature: torch.nn.quantized.functional.interpolate(input,size=None,scale_factor=None,mode=nearest,align_corners=None)\nParameter description: align_corners(bool, optional) - Geometrically, we consider the pixels of the input and output as squares rather than points. If set to `True`, the input and output tensors are aligned by the center points of their corner pixels, preserving the values at the corner pixels. If set to `False`, the input and output tensors are aligned by the corner points of their corner pixels, and the interpolation uses edge value padding for out-of-boundary values, making this operation independent of input size when `scale_factor` is kept the same. This only has an effect when `mode` is `'bilinear'`. Default: `False`\nConstraints:\ndtype:bool\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.gt(input,other,out=None)\nParameter description: out(Tensor, optional) - the output tensor that must be a BoolTensor\nConstraints:\ndtype:booltensor\nstructure:tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.empty_like(input,dtype=None,layout=None,device=None,requires_grad=False,memory_format=torch.preserve_format)\nParameter description: dtype(`torch.dtype`, optional) - the desired data type of returned Tensor. Default: if `None`, defaults to the dtype of `input`.\nConstraints:\ndtype:torch.dtype\nstructure:Null\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.nn.RNN(*args,**kwargs,input_size=None,hidden_size=None,num_layers=None,nonlinearity=None,bias=None,batch_first=None,dropout=None,bidirectional=None)\nParameter description: hidden_size - The number of features in the hidden state h\nConstraints:\ndtype:int\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:[0,inf)\n\n###\n\nSignature: torch.nn.functional.avg_pool1d(input,kernel_size,stride=None,padding=0,ceil_mode=False,count_include_pad=True)\nParameter description: padding - implicit zero paddings on both sides of the input. Can be a single number or a tuple (padW,). Default: 0\nConstraints:\ndtype:bool;int\nstructure:tuple\nshape:Null\nndim:0;1\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.autograd.grad(outputs,inputs,grad_outputs=None,retain_graph=None,create_graph=False,only_inputs=True,allow_unused=False)\nParameter description: outputs(sequence of Tensor) - outputs of the differentiated function.\nConstraints:\ndtype:Null\nstructure:sequence;tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.repeat_interleave(input,repeats,dim=None)\nParameter description: dim(int, optional) - The dimension along which to repeat values.By default, use the flattened input array, and return a flat outputarray.\nConstraints:\ndtype:int\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.sinh(input,out=None)\nParameter description: input(Tensor) - the input tensor.\nConstraints:",
            "constr": {
                "dtype": [],
                "structure": [
                    "tensor"
                ],
                "shape": [],
                "ndim": [],
                "enum": [],
                "range": []
            },
            "signature": "torch.sinh(input,out=None)",
            "descp": "input(Tensor) - the input tensor."
        },
        {
            "fname": "torch.nn.smoothl1loss.yaml",
            "arg": "reduction",
            "prompt": "Signature: torch.distributed.recv(tensor,src=None,group=<objectobject>,tag=0)\nParameter description: tensor(Tensor) - Tensor to fill with received data.\nConstraints:\ndtype:Null\nstructure:tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.div(input,other,out=None)\nParameter description: out(Tensor, optional) - the output tensor.\nConstraints:\ndtype:Null\nstructure:tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.nn.TransformerDecoder(decoder_layer,num_layers,norm=None)\nParameter description: num_layers - the number of sub-decoder-layers in the decoder (required).\nConstraints:\ndtype:int\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:[0,inf)\n\n###\n\nSignature: torch.nn.quantized.functional.interpolate(input,size=None,scale_factor=None,mode=nearest,align_corners=None)\nParameter description: align_corners(bool, optional) - Geometrically, we consider the pixels of the input and output as squares rather than points. If set to `True`, the input and output tensors are aligned by the center points of their corner pixels, preserving the values at the corner pixels. If set to `False`, the input and output tensors are aligned by the corner points of their corner pixels, and the interpolation uses edge value padding for out-of-boundary values, making this operation independent of input size when `scale_factor` is kept the same. This only has an effect when `mode` is `'bilinear'`. Default: `False`\nConstraints:\ndtype:bool\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.gt(input,other,out=None)\nParameter description: out(Tensor, optional) - the output tensor that must be a BoolTensor\nConstraints:\ndtype:booltensor\nstructure:tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.empty_like(input,dtype=None,layout=None,device=None,requires_grad=False,memory_format=torch.preserve_format)\nParameter description: dtype(`torch.dtype`, optional) - the desired data type of returned Tensor. Default: if `None`, defaults to the dtype of `input`.\nConstraints:\ndtype:torch.dtype\nstructure:Null\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.nn.RNN(*args,**kwargs,input_size=None,hidden_size=None,num_layers=None,nonlinearity=None,bias=None,batch_first=None,dropout=None,bidirectional=None)\nParameter description: hidden_size - The number of features in the hidden state h\nConstraints:\ndtype:int\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:[0,inf)\n\n###\n\nSignature: torch.nn.functional.avg_pool1d(input,kernel_size,stride=None,padding=0,ceil_mode=False,count_include_pad=True)\nParameter description: padding - implicit zero paddings on both sides of the input. Can be a single number or a tuple (padW,). Default: 0\nConstraints:\ndtype:bool;int\nstructure:tuple\nshape:Null\nndim:0;1\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.autograd.grad(outputs,inputs,grad_outputs=None,retain_graph=None,create_graph=False,only_inputs=True,allow_unused=False)\nParameter description: outputs(sequence of Tensor) - outputs of the differentiated function.\nConstraints:\ndtype:Null\nstructure:sequence;tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.repeat_interleave(input,repeats,dim=None)\nParameter description: dim(int, optional) - The dimension along which to repeat values.By default, use the flattened input array, and return a flat outputarray.\nConstraints:\ndtype:int\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.nn.SmoothL1Loss(size_average=None,reduce=None,reduction=mean)\nParameter description: reduction(string, optional) - Specifies the reduction to apply to the output: `'none'` | `'mean'` | `'sum'`. `'none'`: no reduction will be applied, `'mean'`: the sum of the output will be divided by the number of elements in the output, `'sum'`: the output will be summed. Note: `size_average` and `reduce` are in the process of being deprecated, and in the meantime, specifying either of those two args will override `reduction`. Default: `'mean'`\nConstraints:",
            "constr": {
                "dtype": [
                    "string"
                ],
                "structure": [],
                "shape": [],
                "ndim": [],
                "enum": [
                    "sum",
                    "mean",
                    "none"
                ],
                "range": []
            },
            "signature": "torch.nn.SmoothL1Loss(size_average=None,reduce=None,reduction=mean)",
            "descp": "reduction(string, optional) - Specifies the reduction to apply to the output: `'none'` | `'mean'` | `'sum'`. `'none'`: no reduction will be applied, `'mean'`: the sum of the output will be divided by the number of elements in the output, `'sum'`: the output will be summed. Note: `size_average` and `reduce` are in the process of being deprecated, and in the meantime, specifying either of those two args will override `reduction`. Default: `'mean'`"
        },
        {
            "fname": "torch.nn.utils.rnn.pack_sequence.yaml",
            "arg": "enforce_sorted",
            "prompt": "Signature: torch.distributed.recv(tensor,src=None,group=<objectobject>,tag=0)\nParameter description: tensor(Tensor) - Tensor to fill with received data.\nConstraints:\ndtype:Null\nstructure:tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.div(input,other,out=None)\nParameter description: out(Tensor, optional) - the output tensor.\nConstraints:\ndtype:Null\nstructure:tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.nn.TransformerDecoder(decoder_layer,num_layers,norm=None)\nParameter description: num_layers - the number of sub-decoder-layers in the decoder (required).\nConstraints:\ndtype:int\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:[0,inf)\n\n###\n\nSignature: torch.nn.quantized.functional.interpolate(input,size=None,scale_factor=None,mode=nearest,align_corners=None)\nParameter description: align_corners(bool, optional) - Geometrically, we consider the pixels of the input and output as squares rather than points. If set to `True`, the input and output tensors are aligned by the center points of their corner pixels, preserving the values at the corner pixels. If set to `False`, the input and output tensors are aligned by the corner points of their corner pixels, and the interpolation uses edge value padding for out-of-boundary values, making this operation independent of input size when `scale_factor` is kept the same. This only has an effect when `mode` is `'bilinear'`. Default: `False`\nConstraints:\ndtype:bool\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.gt(input,other,out=None)\nParameter description: out(Tensor, optional) - the output tensor that must be a BoolTensor\nConstraints:\ndtype:booltensor\nstructure:tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.empty_like(input,dtype=None,layout=None,device=None,requires_grad=False,memory_format=torch.preserve_format)\nParameter description: dtype(`torch.dtype`, optional) - the desired data type of returned Tensor. Default: if `None`, defaults to the dtype of `input`.\nConstraints:\ndtype:torch.dtype\nstructure:Null\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.nn.RNN(*args,**kwargs,input_size=None,hidden_size=None,num_layers=None,nonlinearity=None,bias=None,batch_first=None,dropout=None,bidirectional=None)\nParameter description: hidden_size - The number of features in the hidden state h\nConstraints:\ndtype:int\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:[0,inf)\n\n###\n\nSignature: torch.nn.functional.avg_pool1d(input,kernel_size,stride=None,padding=0,ceil_mode=False,count_include_pad=True)\nParameter description: padding - implicit zero paddings on both sides of the input. Can be a single number or a tuple (padW,). Default: 0\nConstraints:\ndtype:bool;int\nstructure:tuple\nshape:Null\nndim:0;1\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.autograd.grad(outputs,inputs,grad_outputs=None,retain_graph=None,create_graph=False,only_inputs=True,allow_unused=False)\nParameter description: outputs(sequence of Tensor) - outputs of the differentiated function.\nConstraints:\ndtype:Null\nstructure:sequence;tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.repeat_interleave(input,repeats,dim=None)\nParameter description: dim(int, optional) - The dimension along which to repeat values.By default, use the flattened input array, and return a flat outputarray.\nConstraints:\ndtype:int\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.nn.utils.rnn.pack_sequence(sequences,enforce_sorted=True)\nParameter description: enforce_sorted(bool, optional) - if `True`, checks that the input contains sequences sorted by length in a decreasing order. If `False`, this condition is not checked. Default: `True`.\nConstraints:",
            "constr": {
                "dtype": [
                    "bool"
                ],
                "structure": [],
                "shape": [],
                "ndim": [
                    "0"
                ],
                "enum": [],
                "range": []
            },
            "signature": "torch.nn.utils.rnn.pack_sequence(sequences,enforce_sorted=True)",
            "descp": "enforce_sorted(bool, optional) - if `True`, checks that the input contains sequences sorted by length in a decreasing order. If `False`, this condition is not checked. Default: `True`."
        },
        {
            "fname": "torch.nn.avgpool3d.yaml",
            "arg": "count_include_pad",
            "prompt": "Signature: torch.distributed.recv(tensor,src=None,group=<objectobject>,tag=0)\nParameter description: tensor(Tensor) - Tensor to fill with received data.\nConstraints:\ndtype:Null\nstructure:tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.div(input,other,out=None)\nParameter description: out(Tensor, optional) - the output tensor.\nConstraints:\ndtype:Null\nstructure:tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.nn.TransformerDecoder(decoder_layer,num_layers,norm=None)\nParameter description: num_layers - the number of sub-decoder-layers in the decoder (required).\nConstraints:\ndtype:int\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:[0,inf)\n\n###\n\nSignature: torch.nn.quantized.functional.interpolate(input,size=None,scale_factor=None,mode=nearest,align_corners=None)\nParameter description: align_corners(bool, optional) - Geometrically, we consider the pixels of the input and output as squares rather than points. If set to `True`, the input and output tensors are aligned by the center points of their corner pixels, preserving the values at the corner pixels. If set to `False`, the input and output tensors are aligned by the corner points of their corner pixels, and the interpolation uses edge value padding for out-of-boundary values, making this operation independent of input size when `scale_factor` is kept the same. This only has an effect when `mode` is `'bilinear'`. Default: `False`\nConstraints:\ndtype:bool\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.gt(input,other,out=None)\nParameter description: out(Tensor, optional) - the output tensor that must be a BoolTensor\nConstraints:\ndtype:booltensor\nstructure:tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.empty_like(input,dtype=None,layout=None,device=None,requires_grad=False,memory_format=torch.preserve_format)\nParameter description: dtype(`torch.dtype`, optional) - the desired data type of returned Tensor. Default: if `None`, defaults to the dtype of `input`.\nConstraints:\ndtype:torch.dtype\nstructure:Null\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.nn.RNN(*args,**kwargs,input_size=None,hidden_size=None,num_layers=None,nonlinearity=None,bias=None,batch_first=None,dropout=None,bidirectional=None)\nParameter description: hidden_size - The number of features in the hidden state h\nConstraints:\ndtype:int\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:[0,inf)\n\n###\n\nSignature: torch.nn.functional.avg_pool1d(input,kernel_size,stride=None,padding=0,ceil_mode=False,count_include_pad=True)\nParameter description: padding - implicit zero paddings on both sides of the input. Can be a single number or a tuple (padW,). Default: 0\nConstraints:\ndtype:bool;int\nstructure:tuple\nshape:Null\nndim:0;1\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.autograd.grad(outputs,inputs,grad_outputs=None,retain_graph=None,create_graph=False,only_inputs=True,allow_unused=False)\nParameter description: outputs(sequence of Tensor) - outputs of the differentiated function.\nConstraints:\ndtype:Null\nstructure:sequence;tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.repeat_interleave(input,repeats,dim=None)\nParameter description: dim(int, optional) - The dimension along which to repeat values.By default, use the flattened input array, and return a flat outputarray.\nConstraints:\ndtype:int\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.nn.AvgPool3d(kernel_size,stride=None,padding=0,ceil_mode=False,count_include_pad=True,divisor_override=None)\nParameter description: count_include_pad - when True, will include the zero-padding in the averaging calculation\nConstraints:",
            "constr": {
                "dtype": [
                    "bool"
                ],
                "structure": [],
                "shape": [],
                "ndim": [
                    "0"
                ],
                "enum": [],
                "range": []
            },
            "signature": "torch.nn.AvgPool3d(kernel_size,stride=None,padding=0,ceil_mode=False,count_include_pad=True,divisor_override=None)",
            "descp": "count_include_pad - when True, will include the zero-padding in the averaging calculation"
        },
        {
            "fname": "torch.hann_window.yaml",
            "arg": "periodic",
            "prompt": "Signature: torch.distributed.recv(tensor,src=None,group=<objectobject>,tag=0)\nParameter description: tensor(Tensor) - Tensor to fill with received data.\nConstraints:\ndtype:Null\nstructure:tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.div(input,other,out=None)\nParameter description: out(Tensor, optional) - the output tensor.\nConstraints:\ndtype:Null\nstructure:tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.nn.TransformerDecoder(decoder_layer,num_layers,norm=None)\nParameter description: num_layers - the number of sub-decoder-layers in the decoder (required).\nConstraints:\ndtype:int\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:[0,inf)\n\n###\n\nSignature: torch.nn.quantized.functional.interpolate(input,size=None,scale_factor=None,mode=nearest,align_corners=None)\nParameter description: align_corners(bool, optional) - Geometrically, we consider the pixels of the input and output as squares rather than points. If set to `True`, the input and output tensors are aligned by the center points of their corner pixels, preserving the values at the corner pixels. If set to `False`, the input and output tensors are aligned by the corner points of their corner pixels, and the interpolation uses edge value padding for out-of-boundary values, making this operation independent of input size when `scale_factor` is kept the same. This only has an effect when `mode` is `'bilinear'`. Default: `False`\nConstraints:\ndtype:bool\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.gt(input,other,out=None)\nParameter description: out(Tensor, optional) - the output tensor that must be a BoolTensor\nConstraints:\ndtype:booltensor\nstructure:tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.empty_like(input,dtype=None,layout=None,device=None,requires_grad=False,memory_format=torch.preserve_format)\nParameter description: dtype(`torch.dtype`, optional) - the desired data type of returned Tensor. Default: if `None`, defaults to the dtype of `input`.\nConstraints:\ndtype:torch.dtype\nstructure:Null\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.nn.RNN(*args,**kwargs,input_size=None,hidden_size=None,num_layers=None,nonlinearity=None,bias=None,batch_first=None,dropout=None,bidirectional=None)\nParameter description: hidden_size - The number of features in the hidden state h\nConstraints:\ndtype:int\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:[0,inf)\n\n###\n\nSignature: torch.nn.functional.avg_pool1d(input,kernel_size,stride=None,padding=0,ceil_mode=False,count_include_pad=True)\nParameter description: padding - implicit zero paddings on both sides of the input. Can be a single number or a tuple (padW,). Default: 0\nConstraints:\ndtype:bool;int\nstructure:tuple\nshape:Null\nndim:0;1\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.autograd.grad(outputs,inputs,grad_outputs=None,retain_graph=None,create_graph=False,only_inputs=True,allow_unused=False)\nParameter description: outputs(sequence of Tensor) - outputs of the differentiated function.\nConstraints:\ndtype:Null\nstructure:sequence;tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.repeat_interleave(input,repeats,dim=None)\nParameter description: dim(int, optional) - The dimension along which to repeat values.By default, use the flattened input array, and return a flat outputarray.\nConstraints:\ndtype:int\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.hann_window(window_length,periodic=True,dtype=None,layout=torch.strided,device=None,requires_grad=False)\nParameter description: periodic(bool, optional) - If True, returns a window to be used as periodic function. If False, return a symmetric window.\nConstraints:",
            "constr": {
                "dtype": [
                    "bool"
                ],
                "structure": [],
                "shape": [],
                "ndim": [
                    "0"
                ],
                "enum": [],
                "range": []
            },
            "signature": "torch.hann_window(window_length,periodic=True,dtype=None,layout=torch.strided,device=None,requires_grad=False)",
            "descp": "periodic(bool, optional) - If True, returns a window to be used as periodic function. If False, return a symmetric window."
        },
        {
            "fname": "torch.cuda.comm.gather.yaml",
            "arg": "destination",
            "prompt": "Signature: torch.distributed.recv(tensor,src=None,group=<objectobject>,tag=0)\nParameter description: tensor(Tensor) - Tensor to fill with received data.\nConstraints:\ndtype:Null\nstructure:tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.div(input,other,out=None)\nParameter description: out(Tensor, optional) - the output tensor.\nConstraints:\ndtype:Null\nstructure:tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.nn.TransformerDecoder(decoder_layer,num_layers,norm=None)\nParameter description: num_layers - the number of sub-decoder-layers in the decoder (required).\nConstraints:\ndtype:int\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:[0,inf)\n\n###\n\nSignature: torch.nn.quantized.functional.interpolate(input,size=None,scale_factor=None,mode=nearest,align_corners=None)\nParameter description: align_corners(bool, optional) - Geometrically, we consider the pixels of the input and output as squares rather than points. If set to `True`, the input and output tensors are aligned by the center points of their corner pixels, preserving the values at the corner pixels. If set to `False`, the input and output tensors are aligned by the corner points of their corner pixels, and the interpolation uses edge value padding for out-of-boundary values, making this operation independent of input size when `scale_factor` is kept the same. This only has an effect when `mode` is `'bilinear'`. Default: `False`\nConstraints:\ndtype:bool\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.gt(input,other,out=None)\nParameter description: out(Tensor, optional) - the output tensor that must be a BoolTensor\nConstraints:\ndtype:booltensor\nstructure:tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.empty_like(input,dtype=None,layout=None,device=None,requires_grad=False,memory_format=torch.preserve_format)\nParameter description: dtype(`torch.dtype`, optional) - the desired data type of returned Tensor. Default: if `None`, defaults to the dtype of `input`.\nConstraints:\ndtype:torch.dtype\nstructure:Null\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.nn.RNN(*args,**kwargs,input_size=None,hidden_size=None,num_layers=None,nonlinearity=None,bias=None,batch_first=None,dropout=None,bidirectional=None)\nParameter description: hidden_size - The number of features in the hidden state h\nConstraints:\ndtype:int\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:[0,inf)\n\n###\n\nSignature: torch.nn.functional.avg_pool1d(input,kernel_size,stride=None,padding=0,ceil_mode=False,count_include_pad=True)\nParameter description: padding - implicit zero paddings on both sides of the input. Can be a single number or a tuple (padW,). Default: 0\nConstraints:\ndtype:bool;int\nstructure:tuple\nshape:Null\nndim:0;1\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.autograd.grad(outputs,inputs,grad_outputs=None,retain_graph=None,create_graph=False,only_inputs=True,allow_unused=False)\nParameter description: outputs(sequence of Tensor) - outputs of the differentiated function.\nConstraints:\ndtype:Null\nstructure:sequence;tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.repeat_interleave(input,repeats,dim=None)\nParameter description: dim(int, optional) - The dimension along which to repeat values.By default, use the flattened input array, and return a flat outputarray.\nConstraints:\ndtype:int\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.cuda.comm.gather(tensors,dim=0,destination=None)\nParameter description: destination(int, optional) - output device (-1 means CPU, default: current device)\nConstraints:",
            "constr": {
                "dtype": [
                    "int"
                ],
                "structure": [],
                "shape": [],
                "ndim": [],
                "enum": [],
                "range": []
            },
            "signature": "torch.cuda.comm.gather(tensors,dim=0,destination=None)",
            "descp": "destination(int, optional) - output device (-1 means CPU, default: current device)"
        },
        {
            "fname": "torch.distributed.all_gather.yaml",
            "arg": "async_op",
            "prompt": "Signature: torch.distributed.recv(tensor,src=None,group=<objectobject>,tag=0)\nParameter description: tensor(Tensor) - Tensor to fill with received data.\nConstraints:\ndtype:Null\nstructure:tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.div(input,other,out=None)\nParameter description: out(Tensor, optional) - the output tensor.\nConstraints:\ndtype:Null\nstructure:tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.nn.TransformerDecoder(decoder_layer,num_layers,norm=None)\nParameter description: num_layers - the number of sub-decoder-layers in the decoder (required).\nConstraints:\ndtype:int\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:[0,inf)\n\n###\n\nSignature: torch.nn.quantized.functional.interpolate(input,size=None,scale_factor=None,mode=nearest,align_corners=None)\nParameter description: align_corners(bool, optional) - Geometrically, we consider the pixels of the input and output as squares rather than points. If set to `True`, the input and output tensors are aligned by the center points of their corner pixels, preserving the values at the corner pixels. If set to `False`, the input and output tensors are aligned by the corner points of their corner pixels, and the interpolation uses edge value padding for out-of-boundary values, making this operation independent of input size when `scale_factor` is kept the same. This only has an effect when `mode` is `'bilinear'`. Default: `False`\nConstraints:\ndtype:bool\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.gt(input,other,out=None)\nParameter description: out(Tensor, optional) - the output tensor that must be a BoolTensor\nConstraints:\ndtype:booltensor\nstructure:tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.empty_like(input,dtype=None,layout=None,device=None,requires_grad=False,memory_format=torch.preserve_format)\nParameter description: dtype(`torch.dtype`, optional) - the desired data type of returned Tensor. Default: if `None`, defaults to the dtype of `input`.\nConstraints:\ndtype:torch.dtype\nstructure:Null\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.nn.RNN(*args,**kwargs,input_size=None,hidden_size=None,num_layers=None,nonlinearity=None,bias=None,batch_first=None,dropout=None,bidirectional=None)\nParameter description: hidden_size - The number of features in the hidden state h\nConstraints:\ndtype:int\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:[0,inf)\n\n###\n\nSignature: torch.nn.functional.avg_pool1d(input,kernel_size,stride=None,padding=0,ceil_mode=False,count_include_pad=True)\nParameter description: padding - implicit zero paddings on both sides of the input. Can be a single number or a tuple (padW,). Default: 0\nConstraints:\ndtype:bool;int\nstructure:tuple\nshape:Null\nndim:0;1\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.autograd.grad(outputs,inputs,grad_outputs=None,retain_graph=None,create_graph=False,only_inputs=True,allow_unused=False)\nParameter description: outputs(sequence of Tensor) - outputs of the differentiated function.\nConstraints:\ndtype:Null\nstructure:sequence;tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.repeat_interleave(input,repeats,dim=None)\nParameter description: dim(int, optional) - The dimension along which to repeat values.By default, use the flattened input array, and return a flat outputarray.\nConstraints:\ndtype:int\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.distributed.all_gather(tensor_list,tensor,group=<objectobject>,async_op=False)\nParameter description: async_op(bool, optional) - Whether this op should be an async op\nConstraints:",
            "constr": {
                "dtype": [
                    "bool"
                ],
                "structure": [],
                "shape": [],
                "ndim": [
                    "0"
                ],
                "enum": [],
                "range": []
            },
            "signature": "torch.distributed.all_gather(tensor_list,tensor,group=<objectobject>,async_op=False)",
            "descp": "async_op(bool, optional) - Whether this op should be an async op"
        },
        {
            "fname": "torch.nn.upsample.yaml",
            "arg": "align_corners",
            "prompt": "Signature: torch.distributed.recv(tensor,src=None,group=<objectobject>,tag=0)\nParameter description: tensor(Tensor) - Tensor to fill with received data.\nConstraints:\ndtype:Null\nstructure:tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.div(input,other,out=None)\nParameter description: out(Tensor, optional) - the output tensor.\nConstraints:\ndtype:Null\nstructure:tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.nn.TransformerDecoder(decoder_layer,num_layers,norm=None)\nParameter description: num_layers - the number of sub-decoder-layers in the decoder (required).\nConstraints:\ndtype:int\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:[0,inf)\n\n###\n\nSignature: torch.nn.quantized.functional.interpolate(input,size=None,scale_factor=None,mode=nearest,align_corners=None)\nParameter description: align_corners(bool, optional) - Geometrically, we consider the pixels of the input and output as squares rather than points. If set to `True`, the input and output tensors are aligned by the center points of their corner pixels, preserving the values at the corner pixels. If set to `False`, the input and output tensors are aligned by the corner points of their corner pixels, and the interpolation uses edge value padding for out-of-boundary values, making this operation independent of input size when `scale_factor` is kept the same. This only has an effect when `mode` is `'bilinear'`. Default: `False`\nConstraints:\ndtype:bool\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.gt(input,other,out=None)\nParameter description: out(Tensor, optional) - the output tensor that must be a BoolTensor\nConstraints:\ndtype:booltensor\nstructure:tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.empty_like(input,dtype=None,layout=None,device=None,requires_grad=False,memory_format=torch.preserve_format)\nParameter description: dtype(`torch.dtype`, optional) - the desired data type of returned Tensor. Default: if `None`, defaults to the dtype of `input`.\nConstraints:\ndtype:torch.dtype\nstructure:Null\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.nn.RNN(*args,**kwargs,input_size=None,hidden_size=None,num_layers=None,nonlinearity=None,bias=None,batch_first=None,dropout=None,bidirectional=None)\nParameter description: hidden_size - The number of features in the hidden state h\nConstraints:\ndtype:int\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:[0,inf)\n\n###\n\nSignature: torch.nn.functional.avg_pool1d(input,kernel_size,stride=None,padding=0,ceil_mode=False,count_include_pad=True)\nParameter description: padding - implicit zero paddings on both sides of the input. Can be a single number or a tuple (padW,). Default: 0\nConstraints:\ndtype:bool;int\nstructure:tuple\nshape:Null\nndim:0;1\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.autograd.grad(outputs,inputs,grad_outputs=None,retain_graph=None,create_graph=False,only_inputs=True,allow_unused=False)\nParameter description: outputs(sequence of Tensor) - outputs of the differentiated function.\nConstraints:\ndtype:Null\nstructure:sequence;tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.repeat_interleave(input,repeats,dim=None)\nParameter description: dim(int, optional) - The dimension along which to repeat values.By default, use the flattened input array, and return a flat outputarray.\nConstraints:\ndtype:int\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.nn.Upsample(size=None,scale_factor=None,mode=nearest,align_corners=None)\nParameter description: align_corners(bool, optional) - if `True`, the corner pixels of the input and output tensors are aligned, and thus preserving the values at those pixels. This only has effect when `mode` is `'linear'`, `'bilinear'`, or `'trilinear'`. Default: `False`\nConstraints:",
            "constr": {
                "dtype": [
                    "bool"
                ],
                "structure": [],
                "shape": [],
                "ndim": [
                    "0"
                ],
                "enum": [],
                "range": []
            },
            "signature": "torch.nn.Upsample(size=None,scale_factor=None,mode=nearest,align_corners=None)",
            "descp": "align_corners(bool, optional) - if `True`, the corner pixels of the input and output tensors are aligned, and thus preserving the values at those pixels. This only has effect when `mode` is `'linear'`, `'bilinear'`, or `'trilinear'`. Default: `False`"
        },
        {
            "fname": "torch.cummin.yaml",
            "arg": "input",
            "prompt": "Signature: torch.distributed.recv(tensor,src=None,group=<objectobject>,tag=0)\nParameter description: tensor(Tensor) - Tensor to fill with received data.\nConstraints:\ndtype:Null\nstructure:tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.div(input,other,out=None)\nParameter description: out(Tensor, optional) - the output tensor.\nConstraints:\ndtype:Null\nstructure:tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.nn.TransformerDecoder(decoder_layer,num_layers,norm=None)\nParameter description: num_layers - the number of sub-decoder-layers in the decoder (required).\nConstraints:\ndtype:int\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:[0,inf)\n\n###\n\nSignature: torch.nn.quantized.functional.interpolate(input,size=None,scale_factor=None,mode=nearest,align_corners=None)\nParameter description: align_corners(bool, optional) - Geometrically, we consider the pixels of the input and output as squares rather than points. If set to `True`, the input and output tensors are aligned by the center points of their corner pixels, preserving the values at the corner pixels. If set to `False`, the input and output tensors are aligned by the corner points of their corner pixels, and the interpolation uses edge value padding for out-of-boundary values, making this operation independent of input size when `scale_factor` is kept the same. This only has an effect when `mode` is `'bilinear'`. Default: `False`\nConstraints:\ndtype:bool\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.gt(input,other,out=None)\nParameter description: out(Tensor, optional) - the output tensor that must be a BoolTensor\nConstraints:\ndtype:booltensor\nstructure:tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.empty_like(input,dtype=None,layout=None,device=None,requires_grad=False,memory_format=torch.preserve_format)\nParameter description: dtype(`torch.dtype`, optional) - the desired data type of returned Tensor. Default: if `None`, defaults to the dtype of `input`.\nConstraints:\ndtype:torch.dtype\nstructure:Null\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.nn.RNN(*args,**kwargs,input_size=None,hidden_size=None,num_layers=None,nonlinearity=None,bias=None,batch_first=None,dropout=None,bidirectional=None)\nParameter description: hidden_size - The number of features in the hidden state h\nConstraints:\ndtype:int\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:[0,inf)\n\n###\n\nSignature: torch.nn.functional.avg_pool1d(input,kernel_size,stride=None,padding=0,ceil_mode=False,count_include_pad=True)\nParameter description: padding - implicit zero paddings on both sides of the input. Can be a single number or a tuple (padW,). Default: 0\nConstraints:\ndtype:bool;int\nstructure:tuple\nshape:Null\nndim:0;1\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.autograd.grad(outputs,inputs,grad_outputs=None,retain_graph=None,create_graph=False,only_inputs=True,allow_unused=False)\nParameter description: outputs(sequence of Tensor) - outputs of the differentiated function.\nConstraints:\ndtype:Null\nstructure:sequence;tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.repeat_interleave(input,repeats,dim=None)\nParameter description: dim(int, optional) - The dimension along which to repeat values.By default, use the flattened input array, and return a flat outputarray.\nConstraints:\ndtype:int\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.cummin(input,dim,out=None)\nParameter description: input(Tensor) - the input tensor.\nConstraints:",
            "constr": {
                "dtype": [],
                "structure": [
                    "tensor"
                ],
                "shape": [],
                "ndim": [],
                "enum": [],
                "range": []
            },
            "signature": "torch.cummin(input,dim,out=None)",
            "descp": "input(Tensor) - the input tensor."
        },
        {
            "fname": "torch.nn.functional.nll_loss.yaml",
            "arg": "ignore_index",
            "prompt": "Signature: torch.distributed.recv(tensor,src=None,group=<objectobject>,tag=0)\nParameter description: tensor(Tensor) - Tensor to fill with received data.\nConstraints:\ndtype:Null\nstructure:tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.div(input,other,out=None)\nParameter description: out(Tensor, optional) - the output tensor.\nConstraints:\ndtype:Null\nstructure:tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.nn.TransformerDecoder(decoder_layer,num_layers,norm=None)\nParameter description: num_layers - the number of sub-decoder-layers in the decoder (required).\nConstraints:\ndtype:int\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:[0,inf)\n\n###\n\nSignature: torch.nn.quantized.functional.interpolate(input,size=None,scale_factor=None,mode=nearest,align_corners=None)\nParameter description: align_corners(bool, optional) - Geometrically, we consider the pixels of the input and output as squares rather than points. If set to `True`, the input and output tensors are aligned by the center points of their corner pixels, preserving the values at the corner pixels. If set to `False`, the input and output tensors are aligned by the corner points of their corner pixels, and the interpolation uses edge value padding for out-of-boundary values, making this operation independent of input size when `scale_factor` is kept the same. This only has an effect when `mode` is `'bilinear'`. Default: `False`\nConstraints:\ndtype:bool\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.gt(input,other,out=None)\nParameter description: out(Tensor, optional) - the output tensor that must be a BoolTensor\nConstraints:\ndtype:booltensor\nstructure:tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.empty_like(input,dtype=None,layout=None,device=None,requires_grad=False,memory_format=torch.preserve_format)\nParameter description: dtype(`torch.dtype`, optional) - the desired data type of returned Tensor. Default: if `None`, defaults to the dtype of `input`.\nConstraints:\ndtype:torch.dtype\nstructure:Null\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.nn.RNN(*args,**kwargs,input_size=None,hidden_size=None,num_layers=None,nonlinearity=None,bias=None,batch_first=None,dropout=None,bidirectional=None)\nParameter description: hidden_size - The number of features in the hidden state h\nConstraints:\ndtype:int\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:[0,inf)\n\n###\n\nSignature: torch.nn.functional.avg_pool1d(input,kernel_size,stride=None,padding=0,ceil_mode=False,count_include_pad=True)\nParameter description: padding - implicit zero paddings on both sides of the input. Can be a single number or a tuple (padW,). Default: 0\nConstraints:\ndtype:bool;int\nstructure:tuple\nshape:Null\nndim:0;1\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.autograd.grad(outputs,inputs,grad_outputs=None,retain_graph=None,create_graph=False,only_inputs=True,allow_unused=False)\nParameter description: outputs(sequence of Tensor) - outputs of the differentiated function.\nConstraints:\ndtype:Null\nstructure:sequence;tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.repeat_interleave(input,repeats,dim=None)\nParameter description: dim(int, optional) - The dimension along which to repeat values.By default, use the flattened input array, and return a flat outputarray.\nConstraints:\ndtype:int\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.nn.functional.nll_loss(input,target,weight=None,size_average=None,ignore_index=-100,reduce=None,reduction=mean)\nParameter description: ignore_index(int, optional) - Specifies a target value that is ignored and does not contribute to the input gradient. When `size_average` is `True`, the loss is averaged over non-ignored targets. Default: -100\nConstraints:",
            "constr": {
                "dtype": [
                    "bool",
                    "int"
                ],
                "structure": [],
                "shape": [],
                "ndim": [
                    "0"
                ],
                "enum": [],
                "range": []
            },
            "signature": "torch.nn.functional.nll_loss(input,target,weight=None,size_average=None,ignore_index=-100,reduce=None,reduction=mean)",
            "descp": "ignore_index(int, optional) - Specifies a target value that is ignored and does not contribute to the input gradient. When `size_average` is `True`, the loss is averaged over non-ignored targets. Default: -100"
        },
        {
            "fname": "torch.distributed.broadcast.yaml",
            "arg": "src",
            "prompt": "Signature: torch.distributed.recv(tensor,src=None,group=<objectobject>,tag=0)\nParameter description: tensor(Tensor) - Tensor to fill with received data.\nConstraints:\ndtype:Null\nstructure:tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.div(input,other,out=None)\nParameter description: out(Tensor, optional) - the output tensor.\nConstraints:\ndtype:Null\nstructure:tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.nn.TransformerDecoder(decoder_layer,num_layers,norm=None)\nParameter description: num_layers - the number of sub-decoder-layers in the decoder (required).\nConstraints:\ndtype:int\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:[0,inf)\n\n###\n\nSignature: torch.nn.quantized.functional.interpolate(input,size=None,scale_factor=None,mode=nearest,align_corners=None)\nParameter description: align_corners(bool, optional) - Geometrically, we consider the pixels of the input and output as squares rather than points. If set to `True`, the input and output tensors are aligned by the center points of their corner pixels, preserving the values at the corner pixels. If set to `False`, the input and output tensors are aligned by the corner points of their corner pixels, and the interpolation uses edge value padding for out-of-boundary values, making this operation independent of input size when `scale_factor` is kept the same. This only has an effect when `mode` is `'bilinear'`. Default: `False`\nConstraints:\ndtype:bool\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.gt(input,other,out=None)\nParameter description: out(Tensor, optional) - the output tensor that must be a BoolTensor\nConstraints:\ndtype:booltensor\nstructure:tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.empty_like(input,dtype=None,layout=None,device=None,requires_grad=False,memory_format=torch.preserve_format)\nParameter description: dtype(`torch.dtype`, optional) - the desired data type of returned Tensor. Default: if `None`, defaults to the dtype of `input`.\nConstraints:\ndtype:torch.dtype\nstructure:Null\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.nn.RNN(*args,**kwargs,input_size=None,hidden_size=None,num_layers=None,nonlinearity=None,bias=None,batch_first=None,dropout=None,bidirectional=None)\nParameter description: hidden_size - The number of features in the hidden state h\nConstraints:\ndtype:int\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:[0,inf)\n\n###\n\nSignature: torch.nn.functional.avg_pool1d(input,kernel_size,stride=None,padding=0,ceil_mode=False,count_include_pad=True)\nParameter description: padding - implicit zero paddings on both sides of the input. Can be a single number or a tuple (padW,). Default: 0\nConstraints:\ndtype:bool;int\nstructure:tuple\nshape:Null\nndim:0;1\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.autograd.grad(outputs,inputs,grad_outputs=None,retain_graph=None,create_graph=False,only_inputs=True,allow_unused=False)\nParameter description: outputs(sequence of Tensor) - outputs of the differentiated function.\nConstraints:\ndtype:Null\nstructure:sequence;tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.repeat_interleave(input,repeats,dim=None)\nParameter description: dim(int, optional) - The dimension along which to repeat values.By default, use the flattened input array, and return a flat outputarray.\nConstraints:\ndtype:int\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.distributed.broadcast(tensor,src,group=<objectobject>,async_op=False)\nParameter description: src(int) - Source rank.\nConstraints:",
            "constr": {
                "dtype": [
                    "int"
                ],
                "structure": [],
                "shape": [],
                "ndim": [],
                "enum": [],
                "range": []
            },
            "signature": "torch.distributed.broadcast(tensor,src,group=<objectobject>,async_op=False)",
            "descp": "src(int) - Source rank."
        },
        {
            "fname": "torch.quantization.propagate_qconfig_.yaml",
            "arg": "qconfig_dict",
            "prompt": "Signature: torch.distributed.recv(tensor,src=None,group=<objectobject>,tag=0)\nParameter description: tensor(Tensor) - Tensor to fill with received data.\nConstraints:\ndtype:Null\nstructure:tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.div(input,other,out=None)\nParameter description: out(Tensor, optional) - the output tensor.\nConstraints:\ndtype:Null\nstructure:tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.nn.TransformerDecoder(decoder_layer,num_layers,norm=None)\nParameter description: num_layers - the number of sub-decoder-layers in the decoder (required).\nConstraints:\ndtype:int\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:[0,inf)\n\n###\n\nSignature: torch.nn.quantized.functional.interpolate(input,size=None,scale_factor=None,mode=nearest,align_corners=None)\nParameter description: align_corners(bool, optional) - Geometrically, we consider the pixels of the input and output as squares rather than points. If set to `True`, the input and output tensors are aligned by the center points of their corner pixels, preserving the values at the corner pixels. If set to `False`, the input and output tensors are aligned by the corner points of their corner pixels, and the interpolation uses edge value padding for out-of-boundary values, making this operation independent of input size when `scale_factor` is kept the same. This only has an effect when `mode` is `'bilinear'`. Default: `False`\nConstraints:\ndtype:bool\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.gt(input,other,out=None)\nParameter description: out(Tensor, optional) - the output tensor that must be a BoolTensor\nConstraints:\ndtype:booltensor\nstructure:tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.empty_like(input,dtype=None,layout=None,device=None,requires_grad=False,memory_format=torch.preserve_format)\nParameter description: dtype(`torch.dtype`, optional) - the desired data type of returned Tensor. Default: if `None`, defaults to the dtype of `input`.\nConstraints:\ndtype:torch.dtype\nstructure:Null\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.nn.RNN(*args,**kwargs,input_size=None,hidden_size=None,num_layers=None,nonlinearity=None,bias=None,batch_first=None,dropout=None,bidirectional=None)\nParameter description: hidden_size - The number of features in the hidden state h\nConstraints:\ndtype:int\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:[0,inf)\n\n###\n\nSignature: torch.nn.functional.avg_pool1d(input,kernel_size,stride=None,padding=0,ceil_mode=False,count_include_pad=True)\nParameter description: padding - implicit zero paddings on both sides of the input. Can be a single number or a tuple (padW,). Default: 0\nConstraints:\ndtype:bool;int\nstructure:tuple\nshape:Null\nndim:0;1\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.autograd.grad(outputs,inputs,grad_outputs=None,retain_graph=None,create_graph=False,only_inputs=True,allow_unused=False)\nParameter description: outputs(sequence of Tensor) - outputs of the differentiated function.\nConstraints:\ndtype:Null\nstructure:sequence;tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.repeat_interleave(input,repeats,dim=None)\nParameter description: dim(int, optional) - The dimension along which to repeat values.By default, use the flattened input array, and return a flat outputarray.\nConstraints:\ndtype:int\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.quantization.propagate_qconfig_(module,qconfig_dict=None)\nParameter description: qconfig_dict - dictionary that maps from name or type of submodule to quantization configuration, qconfig applies to all submodules of a given module unless qconfig for the submodules are specified (when the submodule already has qconfig attribute)\nConstraints:",
            "constr": {
                "dtype": [],
                "structure": [
                    "dictionary"
                ],
                "shape": [],
                "ndim": [],
                "enum": [],
                "range": []
            },
            "signature": "torch.quantization.propagate_qconfig_(module,qconfig_dict=None)",
            "descp": "qconfig_dict - dictionary that maps from name or type of submodule to quantization configuration, qconfig applies to all submodules of a given module unless qconfig for the submodules are specified (when the submodule already has qconfig attribute)"
        },
        {
            "fname": "torch.nn.utils.prune.remove.yaml",
            "arg": "name",
            "prompt": "Signature: torch.distributed.recv(tensor,src=None,group=<objectobject>,tag=0)\nParameter description: tensor(Tensor) - Tensor to fill with received data.\nConstraints:\ndtype:Null\nstructure:tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.div(input,other,out=None)\nParameter description: out(Tensor, optional) - the output tensor.\nConstraints:\ndtype:Null\nstructure:tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.nn.TransformerDecoder(decoder_layer,num_layers,norm=None)\nParameter description: num_layers - the number of sub-decoder-layers in the decoder (required).\nConstraints:\ndtype:int\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:[0,inf)\n\n###\n\nSignature: torch.nn.quantized.functional.interpolate(input,size=None,scale_factor=None,mode=nearest,align_corners=None)\nParameter description: align_corners(bool, optional) - Geometrically, we consider the pixels of the input and output as squares rather than points. If set to `True`, the input and output tensors are aligned by the center points of their corner pixels, preserving the values at the corner pixels. If set to `False`, the input and output tensors are aligned by the corner points of their corner pixels, and the interpolation uses edge value padding for out-of-boundary values, making this operation independent of input size when `scale_factor` is kept the same. This only has an effect when `mode` is `'bilinear'`. Default: `False`\nConstraints:\ndtype:bool\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.gt(input,other,out=None)\nParameter description: out(Tensor, optional) - the output tensor that must be a BoolTensor\nConstraints:\ndtype:booltensor\nstructure:tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.empty_like(input,dtype=None,layout=None,device=None,requires_grad=False,memory_format=torch.preserve_format)\nParameter description: dtype(`torch.dtype`, optional) - the desired data type of returned Tensor. Default: if `None`, defaults to the dtype of `input`.\nConstraints:\ndtype:torch.dtype\nstructure:Null\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.nn.RNN(*args,**kwargs,input_size=None,hidden_size=None,num_layers=None,nonlinearity=None,bias=None,batch_first=None,dropout=None,bidirectional=None)\nParameter description: hidden_size - The number of features in the hidden state h\nConstraints:\ndtype:int\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:[0,inf)\n\n###\n\nSignature: torch.nn.functional.avg_pool1d(input,kernel_size,stride=None,padding=0,ceil_mode=False,count_include_pad=True)\nParameter description: padding - implicit zero paddings on both sides of the input. Can be a single number or a tuple (padW,). Default: 0\nConstraints:\ndtype:bool;int\nstructure:tuple\nshape:Null\nndim:0;1\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.autograd.grad(outputs,inputs,grad_outputs=None,retain_graph=None,create_graph=False,only_inputs=True,allow_unused=False)\nParameter description: outputs(sequence of Tensor) - outputs of the differentiated function.\nConstraints:\ndtype:Null\nstructure:sequence;tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.repeat_interleave(input,repeats,dim=None)\nParameter description: dim(int, optional) - The dimension along which to repeat values.By default, use the flattened input array, and return a flat outputarray.\nConstraints:\ndtype:int\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.nn.utils.prune.remove(module,name)\nParameter description: name(str) - parameter name within `module` on which pruning will act.\nConstraints:",
            "constr": {
                "dtype": [
                    "str"
                ],
                "structure": [],
                "shape": [],
                "ndim": [],
                "enum": [],
                "range": []
            },
            "signature": "torch.nn.utils.prune.remove(module,name)",
            "descp": "name(str) - parameter name within `module` on which pruning will act."
        },
        {
            "fname": "torch.randperm.yaml",
            "arg": "out",
            "prompt": "Signature: torch.distributed.recv(tensor,src=None,group=<objectobject>,tag=0)\nParameter description: tensor(Tensor) - Tensor to fill with received data.\nConstraints:\ndtype:Null\nstructure:tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.div(input,other,out=None)\nParameter description: out(Tensor, optional) - the output tensor.\nConstraints:\ndtype:Null\nstructure:tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.nn.TransformerDecoder(decoder_layer,num_layers,norm=None)\nParameter description: num_layers - the number of sub-decoder-layers in the decoder (required).\nConstraints:\ndtype:int\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:[0,inf)\n\n###\n\nSignature: torch.nn.quantized.functional.interpolate(input,size=None,scale_factor=None,mode=nearest,align_corners=None)\nParameter description: align_corners(bool, optional) - Geometrically, we consider the pixels of the input and output as squares rather than points. If set to `True`, the input and output tensors are aligned by the center points of their corner pixels, preserving the values at the corner pixels. If set to `False`, the input and output tensors are aligned by the corner points of their corner pixels, and the interpolation uses edge value padding for out-of-boundary values, making this operation independent of input size when `scale_factor` is kept the same. This only has an effect when `mode` is `'bilinear'`. Default: `False`\nConstraints:\ndtype:bool\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.gt(input,other,out=None)\nParameter description: out(Tensor, optional) - the output tensor that must be a BoolTensor\nConstraints:\ndtype:booltensor\nstructure:tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.empty_like(input,dtype=None,layout=None,device=None,requires_grad=False,memory_format=torch.preserve_format)\nParameter description: dtype(`torch.dtype`, optional) - the desired data type of returned Tensor. Default: if `None`, defaults to the dtype of `input`.\nConstraints:\ndtype:torch.dtype\nstructure:Null\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.nn.RNN(*args,**kwargs,input_size=None,hidden_size=None,num_layers=None,nonlinearity=None,bias=None,batch_first=None,dropout=None,bidirectional=None)\nParameter description: hidden_size - The number of features in the hidden state h\nConstraints:\ndtype:int\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:[0,inf)\n\n###\n\nSignature: torch.nn.functional.avg_pool1d(input,kernel_size,stride=None,padding=0,ceil_mode=False,count_include_pad=True)\nParameter description: padding - implicit zero paddings on both sides of the input. Can be a single number or a tuple (padW,). Default: 0\nConstraints:\ndtype:bool;int\nstructure:tuple\nshape:Null\nndim:0;1\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.autograd.grad(outputs,inputs,grad_outputs=None,retain_graph=None,create_graph=False,only_inputs=True,allow_unused=False)\nParameter description: outputs(sequence of Tensor) - outputs of the differentiated function.\nConstraints:\ndtype:Null\nstructure:sequence;tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.repeat_interleave(input,repeats,dim=None)\nParameter description: dim(int, optional) - The dimension along which to repeat values.By default, use the flattened input array, and return a flat outputarray.\nConstraints:\ndtype:int\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.randperm(n,out=None,dtype=torch.int64,layout=torch.strided,device=None,requires_grad=False)\nParameter description: out(Tensor, optional) - the output tensor.\nConstraints:",
            "constr": {
                "dtype": [],
                "structure": [
                    "tensor"
                ],
                "shape": [],
                "ndim": [],
                "enum": [],
                "range": []
            },
            "signature": "torch.randperm(n,out=None,dtype=torch.int64,layout=torch.strided,device=None,requires_grad=False)",
            "descp": "out(Tensor, optional) - the output tensor."
        },
        {
            "fname": "torch.eig.yaml",
            "arg": "out",
            "prompt": "Signature: torch.distributed.recv(tensor,src=None,group=<objectobject>,tag=0)\nParameter description: tensor(Tensor) - Tensor to fill with received data.\nConstraints:\ndtype:Null\nstructure:tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.div(input,other,out=None)\nParameter description: out(Tensor, optional) - the output tensor.\nConstraints:\ndtype:Null\nstructure:tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.nn.TransformerDecoder(decoder_layer,num_layers,norm=None)\nParameter description: num_layers - the number of sub-decoder-layers in the decoder (required).\nConstraints:\ndtype:int\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:[0,inf)\n\n###\n\nSignature: torch.nn.quantized.functional.interpolate(input,size=None,scale_factor=None,mode=nearest,align_corners=None)\nParameter description: align_corners(bool, optional) - Geometrically, we consider the pixels of the input and output as squares rather than points. If set to `True`, the input and output tensors are aligned by the center points of their corner pixels, preserving the values at the corner pixels. If set to `False`, the input and output tensors are aligned by the corner points of their corner pixels, and the interpolation uses edge value padding for out-of-boundary values, making this operation independent of input size when `scale_factor` is kept the same. This only has an effect when `mode` is `'bilinear'`. Default: `False`\nConstraints:\ndtype:bool\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.gt(input,other,out=None)\nParameter description: out(Tensor, optional) - the output tensor that must be a BoolTensor\nConstraints:\ndtype:booltensor\nstructure:tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.empty_like(input,dtype=None,layout=None,device=None,requires_grad=False,memory_format=torch.preserve_format)\nParameter description: dtype(`torch.dtype`, optional) - the desired data type of returned Tensor. Default: if `None`, defaults to the dtype of `input`.\nConstraints:\ndtype:torch.dtype\nstructure:Null\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.nn.RNN(*args,**kwargs,input_size=None,hidden_size=None,num_layers=None,nonlinearity=None,bias=None,batch_first=None,dropout=None,bidirectional=None)\nParameter description: hidden_size - The number of features in the hidden state h\nConstraints:\ndtype:int\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:[0,inf)\n\n###\n\nSignature: torch.nn.functional.avg_pool1d(input,kernel_size,stride=None,padding=0,ceil_mode=False,count_include_pad=True)\nParameter description: padding - implicit zero paddings on both sides of the input. Can be a single number or a tuple (padW,). Default: 0\nConstraints:\ndtype:bool;int\nstructure:tuple\nshape:Null\nndim:0;1\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.autograd.grad(outputs,inputs,grad_outputs=None,retain_graph=None,create_graph=False,only_inputs=True,allow_unused=False)\nParameter description: outputs(sequence of Tensor) - outputs of the differentiated function.\nConstraints:\ndtype:Null\nstructure:sequence;tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.repeat_interleave(input,repeats,dim=None)\nParameter description: dim(int, optional) - The dimension along which to repeat values.By default, use the flattened input array, and return a flat outputarray.\nConstraints:\ndtype:int\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.eig(input,eigenvectors=False,out=None)\nParameter description: out(tuple, optional) - the output tensors\nConstraints:",
            "constr": {
                "dtype": [],
                "structure": [
                    "tuple",
                    "tensor"
                ],
                "shape": [],
                "ndim": [],
                "enum": [],
                "range": []
            },
            "signature": "torch.eig(input,eigenvectors=False,out=None)",
            "descp": "out(tuple, optional) - the output tensors"
        },
        {
            "fname": "torch.nn.upsamplingbilinear2d.yaml",
            "arg": "size",
            "prompt": "Signature: torch.distributed.recv(tensor,src=None,group=<objectobject>,tag=0)\nParameter description: tensor(Tensor) - Tensor to fill with received data.\nConstraints:\ndtype:Null\nstructure:tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.div(input,other,out=None)\nParameter description: out(Tensor, optional) - the output tensor.\nConstraints:\ndtype:Null\nstructure:tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.nn.TransformerDecoder(decoder_layer,num_layers,norm=None)\nParameter description: num_layers - the number of sub-decoder-layers in the decoder (required).\nConstraints:\ndtype:int\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:[0,inf)\n\n###\n\nSignature: torch.nn.quantized.functional.interpolate(input,size=None,scale_factor=None,mode=nearest,align_corners=None)\nParameter description: align_corners(bool, optional) - Geometrically, we consider the pixels of the input and output as squares rather than points. If set to `True`, the input and output tensors are aligned by the center points of their corner pixels, preserving the values at the corner pixels. If set to `False`, the input and output tensors are aligned by the corner points of their corner pixels, and the interpolation uses edge value padding for out-of-boundary values, making this operation independent of input size when `scale_factor` is kept the same. This only has an effect when `mode` is `'bilinear'`. Default: `False`\nConstraints:\ndtype:bool\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.gt(input,other,out=None)\nParameter description: out(Tensor, optional) - the output tensor that must be a BoolTensor\nConstraints:\ndtype:booltensor\nstructure:tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.empty_like(input,dtype=None,layout=None,device=None,requires_grad=False,memory_format=torch.preserve_format)\nParameter description: dtype(`torch.dtype`, optional) - the desired data type of returned Tensor. Default: if `None`, defaults to the dtype of `input`.\nConstraints:\ndtype:torch.dtype\nstructure:Null\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.nn.RNN(*args,**kwargs,input_size=None,hidden_size=None,num_layers=None,nonlinearity=None,bias=None,batch_first=None,dropout=None,bidirectional=None)\nParameter description: hidden_size - The number of features in the hidden state h\nConstraints:\ndtype:int\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:[0,inf)\n\n###\n\nSignature: torch.nn.functional.avg_pool1d(input,kernel_size,stride=None,padding=0,ceil_mode=False,count_include_pad=True)\nParameter description: padding - implicit zero paddings on both sides of the input. Can be a single number or a tuple (padW,). Default: 0\nConstraints:\ndtype:bool;int\nstructure:tuple\nshape:Null\nndim:0;1\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.autograd.grad(outputs,inputs,grad_outputs=None,retain_graph=None,create_graph=False,only_inputs=True,allow_unused=False)\nParameter description: outputs(sequence of Tensor) - outputs of the differentiated function.\nConstraints:\ndtype:Null\nstructure:sequence;tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.repeat_interleave(input,repeats,dim=None)\nParameter description: dim(int, optional) - The dimension along which to repeat values.By default, use the flattened input array, and return a flat outputarray.\nConstraints:\ndtype:int\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.nn.UpsamplingBilinear2d(size=None,scale_factor=None)\nParameter description: size(int or Tuple[int, int], optional) - output spatial sizes\nConstraints:",
            "constr": {
                "dtype": [
                    "int"
                ],
                "structure": [
                    "tuple"
                ],
                "shape": [],
                "ndim": [
                    "0",
                    "1"
                ],
                "enum": [],
                "range": [
                    "[0,inf)"
                ]
            },
            "signature": "torch.nn.UpsamplingBilinear2d(size=None,scale_factor=None)",
            "descp": "size(int or Tuple[int, int], optional) - output spatial sizes"
        },
        {
            "fname": "torch.nn.functional.embedding.yaml",
            "arg": "norm_type",
            "prompt": "Signature: torch.distributed.recv(tensor,src=None,group=<objectobject>,tag=0)\nParameter description: tensor(Tensor) - Tensor to fill with received data.\nConstraints:\ndtype:Null\nstructure:tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.div(input,other,out=None)\nParameter description: out(Tensor, optional) - the output tensor.\nConstraints:\ndtype:Null\nstructure:tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.nn.TransformerDecoder(decoder_layer,num_layers,norm=None)\nParameter description: num_layers - the number of sub-decoder-layers in the decoder (required).\nConstraints:\ndtype:int\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:[0,inf)\n\n###\n\nSignature: torch.nn.quantized.functional.interpolate(input,size=None,scale_factor=None,mode=nearest,align_corners=None)\nParameter description: align_corners(bool, optional) - Geometrically, we consider the pixels of the input and output as squares rather than points. If set to `True`, the input and output tensors are aligned by the center points of their corner pixels, preserving the values at the corner pixels. If set to `False`, the input and output tensors are aligned by the corner points of their corner pixels, and the interpolation uses edge value padding for out-of-boundary values, making this operation independent of input size when `scale_factor` is kept the same. This only has an effect when `mode` is `'bilinear'`. Default: `False`\nConstraints:\ndtype:bool\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.gt(input,other,out=None)\nParameter description: out(Tensor, optional) - the output tensor that must be a BoolTensor\nConstraints:\ndtype:booltensor\nstructure:tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.empty_like(input,dtype=None,layout=None,device=None,requires_grad=False,memory_format=torch.preserve_format)\nParameter description: dtype(`torch.dtype`, optional) - the desired data type of returned Tensor. Default: if `None`, defaults to the dtype of `input`.\nConstraints:\ndtype:torch.dtype\nstructure:Null\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.nn.RNN(*args,**kwargs,input_size=None,hidden_size=None,num_layers=None,nonlinearity=None,bias=None,batch_first=None,dropout=None,bidirectional=None)\nParameter description: hidden_size - The number of features in the hidden state h\nConstraints:\ndtype:int\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:[0,inf)\n\n###\n\nSignature: torch.nn.functional.avg_pool1d(input,kernel_size,stride=None,padding=0,ceil_mode=False,count_include_pad=True)\nParameter description: padding - implicit zero paddings on both sides of the input. Can be a single number or a tuple (padW,). Default: 0\nConstraints:\ndtype:bool;int\nstructure:tuple\nshape:Null\nndim:0;1\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.autograd.grad(outputs,inputs,grad_outputs=None,retain_graph=None,create_graph=False,only_inputs=True,allow_unused=False)\nParameter description: outputs(sequence of Tensor) - outputs of the differentiated function.\nConstraints:\ndtype:Null\nstructure:sequence;tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.repeat_interleave(input,repeats,dim=None)\nParameter description: dim(int, optional) - The dimension along which to repeat values.By default, use the flattened input array, and return a flat outputarray.\nConstraints:\ndtype:int\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.nn.functional.embedding(input,weight,padding_idx=None,max_norm=None,norm_type=2.0,scale_grad_by_freq=False,sparse=False)\nParameter description: norm_type(float, optional) - The p of the p-norm to compute for the `max_norm` option. Default `2`.\nConstraints:",
            "constr": {
                "dtype": [
                    "float",
                    "bool"
                ],
                "structure": [],
                "shape": [],
                "ndim": [
                    "0"
                ],
                "enum": [],
                "range": []
            },
            "signature": "torch.nn.functional.embedding(input,weight,padding_idx=None,max_norm=None,norm_type=2.0,scale_grad_by_freq=False,sparse=False)",
            "descp": "norm_type(float, optional) - The p of the p-norm to compute for the `max_norm` option. Default `2`."
        },
        {
            "fname": "torch.nn.batchnorm3d.yaml",
            "arg": "affine",
            "prompt": "Signature: torch.distributed.recv(tensor,src=None,group=<objectobject>,tag=0)\nParameter description: tensor(Tensor) - Tensor to fill with received data.\nConstraints:\ndtype:Null\nstructure:tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.div(input,other,out=None)\nParameter description: out(Tensor, optional) - the output tensor.\nConstraints:\ndtype:Null\nstructure:tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.nn.TransformerDecoder(decoder_layer,num_layers,norm=None)\nParameter description: num_layers - the number of sub-decoder-layers in the decoder (required).\nConstraints:\ndtype:int\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:[0,inf)\n\n###\n\nSignature: torch.nn.quantized.functional.interpolate(input,size=None,scale_factor=None,mode=nearest,align_corners=None)\nParameter description: align_corners(bool, optional) - Geometrically, we consider the pixels of the input and output as squares rather than points. If set to `True`, the input and output tensors are aligned by the center points of their corner pixels, preserving the values at the corner pixels. If set to `False`, the input and output tensors are aligned by the corner points of their corner pixels, and the interpolation uses edge value padding for out-of-boundary values, making this operation independent of input size when `scale_factor` is kept the same. This only has an effect when `mode` is `'bilinear'`. Default: `False`\nConstraints:\ndtype:bool\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.gt(input,other,out=None)\nParameter description: out(Tensor, optional) - the output tensor that must be a BoolTensor\nConstraints:\ndtype:booltensor\nstructure:tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.empty_like(input,dtype=None,layout=None,device=None,requires_grad=False,memory_format=torch.preserve_format)\nParameter description: dtype(`torch.dtype`, optional) - the desired data type of returned Tensor. Default: if `None`, defaults to the dtype of `input`.\nConstraints:\ndtype:torch.dtype\nstructure:Null\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.nn.RNN(*args,**kwargs,input_size=None,hidden_size=None,num_layers=None,nonlinearity=None,bias=None,batch_first=None,dropout=None,bidirectional=None)\nParameter description: hidden_size - The number of features in the hidden state h\nConstraints:\ndtype:int\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:[0,inf)\n\n###\n\nSignature: torch.nn.functional.avg_pool1d(input,kernel_size,stride=None,padding=0,ceil_mode=False,count_include_pad=True)\nParameter description: padding - implicit zero paddings on both sides of the input. Can be a single number or a tuple (padW,). Default: 0\nConstraints:\ndtype:bool;int\nstructure:tuple\nshape:Null\nndim:0;1\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.autograd.grad(outputs,inputs,grad_outputs=None,retain_graph=None,create_graph=False,only_inputs=True,allow_unused=False)\nParameter description: outputs(sequence of Tensor) - outputs of the differentiated function.\nConstraints:\ndtype:Null\nstructure:sequence;tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.repeat_interleave(input,repeats,dim=None)\nParameter description: dim(int, optional) - The dimension along which to repeat values.By default, use the flattened input array, and return a flat outputarray.\nConstraints:\ndtype:int\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.nn.BatchNorm3d(num_features,eps=1e-05,momentum=0.1,affine=True,track_running_stats=True)\nParameter description: affine - a boolean value that when set to `True`, this module has learnable affine parameters. Default: `True`\nConstraints:",
            "constr": {
                "dtype": [
                    "boolean",
                    "bool"
                ],
                "structure": [],
                "shape": [],
                "ndim": [
                    "0"
                ],
                "enum": [],
                "range": []
            },
            "signature": "torch.nn.BatchNorm3d(num_features,eps=1e-05,momentum=0.1,affine=True,track_running_stats=True)",
            "descp": "affine - a boolean value that when set to `True`, this module has learnable affine parameters. Default: `True`"
        },
        {
            "fname": "torch.nn.conv2d.yaml",
            "arg": "padding_mode",
            "prompt": "Signature: torch.distributed.recv(tensor,src=None,group=<objectobject>,tag=0)\nParameter description: tensor(Tensor) - Tensor to fill with received data.\nConstraints:\ndtype:Null\nstructure:tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.div(input,other,out=None)\nParameter description: out(Tensor, optional) - the output tensor.\nConstraints:\ndtype:Null\nstructure:tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.nn.TransformerDecoder(decoder_layer,num_layers,norm=None)\nParameter description: num_layers - the number of sub-decoder-layers in the decoder (required).\nConstraints:\ndtype:int\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:[0,inf)\n\n###\n\nSignature: torch.nn.quantized.functional.interpolate(input,size=None,scale_factor=None,mode=nearest,align_corners=None)\nParameter description: align_corners(bool, optional) - Geometrically, we consider the pixels of the input and output as squares rather than points. If set to `True`, the input and output tensors are aligned by the center points of their corner pixels, preserving the values at the corner pixels. If set to `False`, the input and output tensors are aligned by the corner points of their corner pixels, and the interpolation uses edge value padding for out-of-boundary values, making this operation independent of input size when `scale_factor` is kept the same. This only has an effect when `mode` is `'bilinear'`. Default: `False`\nConstraints:\ndtype:bool\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.gt(input,other,out=None)\nParameter description: out(Tensor, optional) - the output tensor that must be a BoolTensor\nConstraints:\ndtype:booltensor\nstructure:tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.empty_like(input,dtype=None,layout=None,device=None,requires_grad=False,memory_format=torch.preserve_format)\nParameter description: dtype(`torch.dtype`, optional) - the desired data type of returned Tensor. Default: if `None`, defaults to the dtype of `input`.\nConstraints:\ndtype:torch.dtype\nstructure:Null\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.nn.RNN(*args,**kwargs,input_size=None,hidden_size=None,num_layers=None,nonlinearity=None,bias=None,batch_first=None,dropout=None,bidirectional=None)\nParameter description: hidden_size - The number of features in the hidden state h\nConstraints:\ndtype:int\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:[0,inf)\n\n###\n\nSignature: torch.nn.functional.avg_pool1d(input,kernel_size,stride=None,padding=0,ceil_mode=False,count_include_pad=True)\nParameter description: padding - implicit zero paddings on both sides of the input. Can be a single number or a tuple (padW,). Default: 0\nConstraints:\ndtype:bool;int\nstructure:tuple\nshape:Null\nndim:0;1\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.autograd.grad(outputs,inputs,grad_outputs=None,retain_graph=None,create_graph=False,only_inputs=True,allow_unused=False)\nParameter description: outputs(sequence of Tensor) - outputs of the differentiated function.\nConstraints:\ndtype:Null\nstructure:sequence;tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.repeat_interleave(input,repeats,dim=None)\nParameter description: dim(int, optional) - The dimension along which to repeat values.By default, use the flattened input array, and return a flat outputarray.\nConstraints:\ndtype:int\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.nn.Conv2d(in_channels,out_channels,kernel_size,stride=1,padding=0,dilation=1,groups=1,bias=True,padding_mode=zeros)\nParameter description: padding_mode(string, optional) - `'zeros'`, `'reflect'`, `'replicate'` or `'circular'`. Default: `'zeros'`\nConstraints:",
            "constr": {
                "dtype": [
                    "string"
                ],
                "structure": [],
                "shape": [],
                "ndim": [],
                "enum": [
                    "zeros",
                    "reflect",
                    "circular",
                    "replicate"
                ],
                "range": []
            },
            "signature": "torch.nn.Conv2d(in_channels,out_channels,kernel_size,stride=1,padding=0,dilation=1,groups=1,bias=True,padding_mode=zeros)",
            "descp": "padding_mode(string, optional) - `'zeros'`, `'reflect'`, `'replicate'` or `'circular'`. Default: `'zeros'`"
        },
        {
            "fname": "torch.sin.yaml",
            "arg": "out",
            "prompt": "Signature: torch.distributed.recv(tensor,src=None,group=<objectobject>,tag=0)\nParameter description: tensor(Tensor) - Tensor to fill with received data.\nConstraints:\ndtype:Null\nstructure:tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.div(input,other,out=None)\nParameter description: out(Tensor, optional) - the output tensor.\nConstraints:\ndtype:Null\nstructure:tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.nn.TransformerDecoder(decoder_layer,num_layers,norm=None)\nParameter description: num_layers - the number of sub-decoder-layers in the decoder (required).\nConstraints:\ndtype:int\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:[0,inf)\n\n###\n\nSignature: torch.nn.quantized.functional.interpolate(input,size=None,scale_factor=None,mode=nearest,align_corners=None)\nParameter description: align_corners(bool, optional) - Geometrically, we consider the pixels of the input and output as squares rather than points. If set to `True`, the input and output tensors are aligned by the center points of their corner pixels, preserving the values at the corner pixels. If set to `False`, the input and output tensors are aligned by the corner points of their corner pixels, and the interpolation uses edge value padding for out-of-boundary values, making this operation independent of input size when `scale_factor` is kept the same. This only has an effect when `mode` is `'bilinear'`. Default: `False`\nConstraints:\ndtype:bool\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.gt(input,other,out=None)\nParameter description: out(Tensor, optional) - the output tensor that must be a BoolTensor\nConstraints:\ndtype:booltensor\nstructure:tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.empty_like(input,dtype=None,layout=None,device=None,requires_grad=False,memory_format=torch.preserve_format)\nParameter description: dtype(`torch.dtype`, optional) - the desired data type of returned Tensor. Default: if `None`, defaults to the dtype of `input`.\nConstraints:\ndtype:torch.dtype\nstructure:Null\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.nn.RNN(*args,**kwargs,input_size=None,hidden_size=None,num_layers=None,nonlinearity=None,bias=None,batch_first=None,dropout=None,bidirectional=None)\nParameter description: hidden_size - The number of features in the hidden state h\nConstraints:\ndtype:int\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:[0,inf)\n\n###\n\nSignature: torch.nn.functional.avg_pool1d(input,kernel_size,stride=None,padding=0,ceil_mode=False,count_include_pad=True)\nParameter description: padding - implicit zero paddings on both sides of the input. Can be a single number or a tuple (padW,). Default: 0\nConstraints:\ndtype:bool;int\nstructure:tuple\nshape:Null\nndim:0;1\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.autograd.grad(outputs,inputs,grad_outputs=None,retain_graph=None,create_graph=False,only_inputs=True,allow_unused=False)\nParameter description: outputs(sequence of Tensor) - outputs of the differentiated function.\nConstraints:\ndtype:Null\nstructure:sequence;tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.repeat_interleave(input,repeats,dim=None)\nParameter description: dim(int, optional) - The dimension along which to repeat values.By default, use the flattened input array, and return a flat outputarray.\nConstraints:\ndtype:int\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.sin(input,out=None)\nParameter description: out(Tensor, optional) - the output tensor.\nConstraints:",
            "constr": {
                "dtype": [],
                "structure": [
                    "tensor"
                ],
                "shape": [],
                "ndim": [],
                "enum": [],
                "range": []
            },
            "signature": "torch.sin(input,out=None)",
            "descp": "out(Tensor, optional) - the output tensor."
        },
        {
            "fname": "torch.nn.convtranspose1d.yaml",
            "arg": "out_channels",
            "prompt": "Signature: torch.distributed.recv(tensor,src=None,group=<objectobject>,tag=0)\nParameter description: tensor(Tensor) - Tensor to fill with received data.\nConstraints:\ndtype:Null\nstructure:tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.div(input,other,out=None)\nParameter description: out(Tensor, optional) - the output tensor.\nConstraints:\ndtype:Null\nstructure:tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.nn.TransformerDecoder(decoder_layer,num_layers,norm=None)\nParameter description: num_layers - the number of sub-decoder-layers in the decoder (required).\nConstraints:\ndtype:int\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:[0,inf)\n\n###\n\nSignature: torch.nn.quantized.functional.interpolate(input,size=None,scale_factor=None,mode=nearest,align_corners=None)\nParameter description: align_corners(bool, optional) - Geometrically, we consider the pixels of the input and output as squares rather than points. If set to `True`, the input and output tensors are aligned by the center points of their corner pixels, preserving the values at the corner pixels. If set to `False`, the input and output tensors are aligned by the corner points of their corner pixels, and the interpolation uses edge value padding for out-of-boundary values, making this operation independent of input size when `scale_factor` is kept the same. This only has an effect when `mode` is `'bilinear'`. Default: `False`\nConstraints:\ndtype:bool\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.gt(input,other,out=None)\nParameter description: out(Tensor, optional) - the output tensor that must be a BoolTensor\nConstraints:\ndtype:booltensor\nstructure:tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.empty_like(input,dtype=None,layout=None,device=None,requires_grad=False,memory_format=torch.preserve_format)\nParameter description: dtype(`torch.dtype`, optional) - the desired data type of returned Tensor. Default: if `None`, defaults to the dtype of `input`.\nConstraints:\ndtype:torch.dtype\nstructure:Null\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.nn.RNN(*args,**kwargs,input_size=None,hidden_size=None,num_layers=None,nonlinearity=None,bias=None,batch_first=None,dropout=None,bidirectional=None)\nParameter description: hidden_size - The number of features in the hidden state h\nConstraints:\ndtype:int\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:[0,inf)\n\n###\n\nSignature: torch.nn.functional.avg_pool1d(input,kernel_size,stride=None,padding=0,ceil_mode=False,count_include_pad=True)\nParameter description: padding - implicit zero paddings on both sides of the input. Can be a single number or a tuple (padW,). Default: 0\nConstraints:\ndtype:bool;int\nstructure:tuple\nshape:Null\nndim:0;1\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.autograd.grad(outputs,inputs,grad_outputs=None,retain_graph=None,create_graph=False,only_inputs=True,allow_unused=False)\nParameter description: outputs(sequence of Tensor) - outputs of the differentiated function.\nConstraints:\ndtype:Null\nstructure:sequence;tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.repeat_interleave(input,repeats,dim=None)\nParameter description: dim(int, optional) - The dimension along which to repeat values.By default, use the flattened input array, and return a flat outputarray.\nConstraints:\ndtype:int\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.nn.ConvTranspose1d(in_channels,out_channels,kernel_size,stride=1,padding=0,output_padding=0,groups=1,bias=True,dilation=1,padding_mode=zeros)\nParameter description: out_channels(int) - Number of channels produced by the convolution\nConstraints:",
            "constr": {
                "dtype": [
                    "int"
                ],
                "structure": [],
                "shape": [],
                "ndim": [
                    "0"
                ],
                "enum": [],
                "range": [
                    "[0,inf)"
                ]
            },
            "signature": "torch.nn.ConvTranspose1d(in_channels,out_channels,kernel_size,stride=1,padding=0,output_padding=0,groups=1,bias=True,dilation=1,padding_mode=zeros)",
            "descp": "out_channels(int) - Number of channels produced by the convolution"
        },
        {
            "fname": "torch.sparse.addmm.yaml",
            "arg": "mat",
            "prompt": "Signature: torch.distributed.recv(tensor,src=None,group=<objectobject>,tag=0)\nParameter description: tensor(Tensor) - Tensor to fill with received data.\nConstraints:\ndtype:Null\nstructure:tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.div(input,other,out=None)\nParameter description: out(Tensor, optional) - the output tensor.\nConstraints:\ndtype:Null\nstructure:tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.nn.TransformerDecoder(decoder_layer,num_layers,norm=None)\nParameter description: num_layers - the number of sub-decoder-layers in the decoder (required).\nConstraints:\ndtype:int\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:[0,inf)\n\n###\n\nSignature: torch.nn.quantized.functional.interpolate(input,size=None,scale_factor=None,mode=nearest,align_corners=None)\nParameter description: align_corners(bool, optional) - Geometrically, we consider the pixels of the input and output as squares rather than points. If set to `True`, the input and output tensors are aligned by the center points of their corner pixels, preserving the values at the corner pixels. If set to `False`, the input and output tensors are aligned by the corner points of their corner pixels, and the interpolation uses edge value padding for out-of-boundary values, making this operation independent of input size when `scale_factor` is kept the same. This only has an effect when `mode` is `'bilinear'`. Default: `False`\nConstraints:\ndtype:bool\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.gt(input,other,out=None)\nParameter description: out(Tensor, optional) - the output tensor that must be a BoolTensor\nConstraints:\ndtype:booltensor\nstructure:tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.empty_like(input,dtype=None,layout=None,device=None,requires_grad=False,memory_format=torch.preserve_format)\nParameter description: dtype(`torch.dtype`, optional) - the desired data type of returned Tensor. Default: if `None`, defaults to the dtype of `input`.\nConstraints:\ndtype:torch.dtype\nstructure:Null\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.nn.RNN(*args,**kwargs,input_size=None,hidden_size=None,num_layers=None,nonlinearity=None,bias=None,batch_first=None,dropout=None,bidirectional=None)\nParameter description: hidden_size - The number of features in the hidden state h\nConstraints:\ndtype:int\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:[0,inf)\n\n###\n\nSignature: torch.nn.functional.avg_pool1d(input,kernel_size,stride=None,padding=0,ceil_mode=False,count_include_pad=True)\nParameter description: padding - implicit zero paddings on both sides of the input. Can be a single number or a tuple (padW,). Default: 0\nConstraints:\ndtype:bool;int\nstructure:tuple\nshape:Null\nndim:0;1\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.autograd.grad(outputs,inputs,grad_outputs=None,retain_graph=None,create_graph=False,only_inputs=True,allow_unused=False)\nParameter description: outputs(sequence of Tensor) - outputs of the differentiated function.\nConstraints:\ndtype:Null\nstructure:sequence;tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.repeat_interleave(input,repeats,dim=None)\nParameter description: dim(int, optional) - The dimension along which to repeat values.By default, use the flattened input array, and return a flat outputarray.\nConstraints:\ndtype:int\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.sparse.addmm(mat,mat1,mat2,beta=1,alpha=1)\nParameter description: mat(Tensor) - a dense matrix to be added\nConstraints:",
            "constr": {
                "dtype": [
                    "numeric"
                ],
                "structure": [
                    "tensor"
                ],
                "shape": [],
                "ndim": [],
                "enum": [],
                "range": []
            },
            "signature": "torch.sparse.addmm(mat,mat1,mat2,beta=1,alpha=1)",
            "descp": "mat(Tensor) - a dense matrix to be added"
        },
        {
            "fname": "torch.kthvalue.yaml",
            "arg": "dim",
            "prompt": "Signature: torch.distributed.recv(tensor,src=None,group=<objectobject>,tag=0)\nParameter description: tensor(Tensor) - Tensor to fill with received data.\nConstraints:\ndtype:Null\nstructure:tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.div(input,other,out=None)\nParameter description: out(Tensor, optional) - the output tensor.\nConstraints:\ndtype:Null\nstructure:tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.nn.TransformerDecoder(decoder_layer,num_layers,norm=None)\nParameter description: num_layers - the number of sub-decoder-layers in the decoder (required).\nConstraints:\ndtype:int\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:[0,inf)\n\n###\n\nSignature: torch.nn.quantized.functional.interpolate(input,size=None,scale_factor=None,mode=nearest,align_corners=None)\nParameter description: align_corners(bool, optional) - Geometrically, we consider the pixels of the input and output as squares rather than points. If set to `True`, the input and output tensors are aligned by the center points of their corner pixels, preserving the values at the corner pixels. If set to `False`, the input and output tensors are aligned by the corner points of their corner pixels, and the interpolation uses edge value padding for out-of-boundary values, making this operation independent of input size when `scale_factor` is kept the same. This only has an effect when `mode` is `'bilinear'`. Default: `False`\nConstraints:\ndtype:bool\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.gt(input,other,out=None)\nParameter description: out(Tensor, optional) - the output tensor that must be a BoolTensor\nConstraints:\ndtype:booltensor\nstructure:tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.empty_like(input,dtype=None,layout=None,device=None,requires_grad=False,memory_format=torch.preserve_format)\nParameter description: dtype(`torch.dtype`, optional) - the desired data type of returned Tensor. Default: if `None`, defaults to the dtype of `input`.\nConstraints:\ndtype:torch.dtype\nstructure:Null\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.nn.RNN(*args,**kwargs,input_size=None,hidden_size=None,num_layers=None,nonlinearity=None,bias=None,batch_first=None,dropout=None,bidirectional=None)\nParameter description: hidden_size - The number of features in the hidden state h\nConstraints:\ndtype:int\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:[0,inf)\n\n###\n\nSignature: torch.nn.functional.avg_pool1d(input,kernel_size,stride=None,padding=0,ceil_mode=False,count_include_pad=True)\nParameter description: padding - implicit zero paddings on both sides of the input. Can be a single number or a tuple (padW,). Default: 0\nConstraints:\ndtype:bool;int\nstructure:tuple\nshape:Null\nndim:0;1\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.autograd.grad(outputs,inputs,grad_outputs=None,retain_graph=None,create_graph=False,only_inputs=True,allow_unused=False)\nParameter description: outputs(sequence of Tensor) - outputs of the differentiated function.\nConstraints:\ndtype:Null\nstructure:sequence;tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.repeat_interleave(input,repeats,dim=None)\nParameter description: dim(int, optional) - The dimension along which to repeat values.By default, use the flattened input array, and return a flat outputarray.\nConstraints:\ndtype:int\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.kthvalue(input,k,dim=None,keepdim=False,out=None)\nParameter description: dim(int, optional) - the dimension to find the kth value along\nConstraints:",
            "constr": {
                "dtype": [
                    "int"
                ],
                "structure": [],
                "shape": [],
                "ndim": [
                    "0"
                ],
                "enum": [],
                "range": []
            },
            "signature": "torch.kthvalue(input,k,dim=None,keepdim=False,out=None)",
            "descp": "dim(int, optional) - the dimension to find the kth value along"
        },
        {
            "fname": "torch.fmod.yaml",
            "arg": "input",
            "prompt": "Signature: torch.distributed.recv(tensor,src=None,group=<objectobject>,tag=0)\nParameter description: tensor(Tensor) - Tensor to fill with received data.\nConstraints:\ndtype:Null\nstructure:tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.div(input,other,out=None)\nParameter description: out(Tensor, optional) - the output tensor.\nConstraints:\ndtype:Null\nstructure:tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.nn.TransformerDecoder(decoder_layer,num_layers,norm=None)\nParameter description: num_layers - the number of sub-decoder-layers in the decoder (required).\nConstraints:\ndtype:int\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:[0,inf)\n\n###\n\nSignature: torch.nn.quantized.functional.interpolate(input,size=None,scale_factor=None,mode=nearest,align_corners=None)\nParameter description: align_corners(bool, optional) - Geometrically, we consider the pixels of the input and output as squares rather than points. If set to `True`, the input and output tensors are aligned by the center points of their corner pixels, preserving the values at the corner pixels. If set to `False`, the input and output tensors are aligned by the corner points of their corner pixels, and the interpolation uses edge value padding for out-of-boundary values, making this operation independent of input size when `scale_factor` is kept the same. This only has an effect when `mode` is `'bilinear'`. Default: `False`\nConstraints:\ndtype:bool\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.gt(input,other,out=None)\nParameter description: out(Tensor, optional) - the output tensor that must be a BoolTensor\nConstraints:\ndtype:booltensor\nstructure:tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.empty_like(input,dtype=None,layout=None,device=None,requires_grad=False,memory_format=torch.preserve_format)\nParameter description: dtype(`torch.dtype`, optional) - the desired data type of returned Tensor. Default: if `None`, defaults to the dtype of `input`.\nConstraints:\ndtype:torch.dtype\nstructure:Null\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.nn.RNN(*args,**kwargs,input_size=None,hidden_size=None,num_layers=None,nonlinearity=None,bias=None,batch_first=None,dropout=None,bidirectional=None)\nParameter description: hidden_size - The number of features in the hidden state h\nConstraints:\ndtype:int\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:[0,inf)\n\n###\n\nSignature: torch.nn.functional.avg_pool1d(input,kernel_size,stride=None,padding=0,ceil_mode=False,count_include_pad=True)\nParameter description: padding - implicit zero paddings on both sides of the input. Can be a single number or a tuple (padW,). Default: 0\nConstraints:\ndtype:bool;int\nstructure:tuple\nshape:Null\nndim:0;1\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.autograd.grad(outputs,inputs,grad_outputs=None,retain_graph=None,create_graph=False,only_inputs=True,allow_unused=False)\nParameter description: outputs(sequence of Tensor) - outputs of the differentiated function.\nConstraints:\ndtype:Null\nstructure:sequence;tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.repeat_interleave(input,repeats,dim=None)\nParameter description: dim(int, optional) - The dimension along which to repeat values.By default, use the flattened input array, and return a flat outputarray.\nConstraints:\ndtype:int\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.fmod(input,other,out=None)\nParameter description: input(Tensor) - the dividend\nConstraints:",
            "constr": {
                "dtype": [],
                "structure": [
                    "tensor"
                ],
                "shape": [],
                "ndim": [],
                "enum": [],
                "range": []
            },
            "signature": "torch.fmod(input,other,out=None)",
            "descp": "input(Tensor) - the dividend"
        },
        {
            "fname": "torch.is_complex.yaml",
            "arg": "input",
            "prompt": "Signature: torch.distributed.recv(tensor,src=None,group=<objectobject>,tag=0)\nParameter description: tensor(Tensor) - Tensor to fill with received data.\nConstraints:\ndtype:Null\nstructure:tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.div(input,other,out=None)\nParameter description: out(Tensor, optional) - the output tensor.\nConstraints:\ndtype:Null\nstructure:tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.nn.TransformerDecoder(decoder_layer,num_layers,norm=None)\nParameter description: num_layers - the number of sub-decoder-layers in the decoder (required).\nConstraints:\ndtype:int\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:[0,inf)\n\n###\n\nSignature: torch.nn.quantized.functional.interpolate(input,size=None,scale_factor=None,mode=nearest,align_corners=None)\nParameter description: align_corners(bool, optional) - Geometrically, we consider the pixels of the input and output as squares rather than points. If set to `True`, the input and output tensors are aligned by the center points of their corner pixels, preserving the values at the corner pixels. If set to `False`, the input and output tensors are aligned by the corner points of their corner pixels, and the interpolation uses edge value padding for out-of-boundary values, making this operation independent of input size when `scale_factor` is kept the same. This only has an effect when `mode` is `'bilinear'`. Default: `False`\nConstraints:\ndtype:bool\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.gt(input,other,out=None)\nParameter description: out(Tensor, optional) - the output tensor that must be a BoolTensor\nConstraints:\ndtype:booltensor\nstructure:tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.empty_like(input,dtype=None,layout=None,device=None,requires_grad=False,memory_format=torch.preserve_format)\nParameter description: dtype(`torch.dtype`, optional) - the desired data type of returned Tensor. Default: if `None`, defaults to the dtype of `input`.\nConstraints:\ndtype:torch.dtype\nstructure:Null\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.nn.RNN(*args,**kwargs,input_size=None,hidden_size=None,num_layers=None,nonlinearity=None,bias=None,batch_first=None,dropout=None,bidirectional=None)\nParameter description: hidden_size - The number of features in the hidden state h\nConstraints:\ndtype:int\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:[0,inf)\n\n###\n\nSignature: torch.nn.functional.avg_pool1d(input,kernel_size,stride=None,padding=0,ceil_mode=False,count_include_pad=True)\nParameter description: padding - implicit zero paddings on both sides of the input. Can be a single number or a tuple (padW,). Default: 0\nConstraints:\ndtype:bool;int\nstructure:tuple\nshape:Null\nndim:0;1\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.autograd.grad(outputs,inputs,grad_outputs=None,retain_graph=None,create_graph=False,only_inputs=True,allow_unused=False)\nParameter description: outputs(sequence of Tensor) - outputs of the differentiated function.\nConstraints:\ndtype:Null\nstructure:sequence;tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.repeat_interleave(input,repeats,dim=None)\nParameter description: dim(int, optional) - The dimension along which to repeat values.By default, use the flattened input array, and return a flat outputarray.\nConstraints:\ndtype:int\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.is_complex(input)\nParameter description: input(Tensor) - the PyTorch tensor to test\nConstraints:",
            "constr": {
                "dtype": [],
                "structure": [
                    "tensor"
                ],
                "shape": [],
                "ndim": [],
                "enum": [],
                "range": []
            },
            "signature": "torch.is_complex(input)",
            "descp": "input(Tensor) - the PyTorch tensor to test"
        },
        {
            "fname": "torch.autograd.functional.vhp.yaml",
            "arg": "create_graph",
            "prompt": "Signature: torch.distributed.recv(tensor,src=None,group=<objectobject>,tag=0)\nParameter description: tensor(Tensor) - Tensor to fill with received data.\nConstraints:\ndtype:Null\nstructure:tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.div(input,other,out=None)\nParameter description: out(Tensor, optional) - the output tensor.\nConstraints:\ndtype:Null\nstructure:tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.nn.TransformerDecoder(decoder_layer,num_layers,norm=None)\nParameter description: num_layers - the number of sub-decoder-layers in the decoder (required).\nConstraints:\ndtype:int\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:[0,inf)\n\n###\n\nSignature: torch.nn.quantized.functional.interpolate(input,size=None,scale_factor=None,mode=nearest,align_corners=None)\nParameter description: align_corners(bool, optional) - Geometrically, we consider the pixels of the input and output as squares rather than points. If set to `True`, the input and output tensors are aligned by the center points of their corner pixels, preserving the values at the corner pixels. If set to `False`, the input and output tensors are aligned by the corner points of their corner pixels, and the interpolation uses edge value padding for out-of-boundary values, making this operation independent of input size when `scale_factor` is kept the same. This only has an effect when `mode` is `'bilinear'`. Default: `False`\nConstraints:\ndtype:bool\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.gt(input,other,out=None)\nParameter description: out(Tensor, optional) - the output tensor that must be a BoolTensor\nConstraints:\ndtype:booltensor\nstructure:tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.empty_like(input,dtype=None,layout=None,device=None,requires_grad=False,memory_format=torch.preserve_format)\nParameter description: dtype(`torch.dtype`, optional) - the desired data type of returned Tensor. Default: if `None`, defaults to the dtype of `input`.\nConstraints:\ndtype:torch.dtype\nstructure:Null\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.nn.RNN(*args,**kwargs,input_size=None,hidden_size=None,num_layers=None,nonlinearity=None,bias=None,batch_first=None,dropout=None,bidirectional=None)\nParameter description: hidden_size - The number of features in the hidden state h\nConstraints:\ndtype:int\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:[0,inf)\n\n###\n\nSignature: torch.nn.functional.avg_pool1d(input,kernel_size,stride=None,padding=0,ceil_mode=False,count_include_pad=True)\nParameter description: padding - implicit zero paddings on both sides of the input. Can be a single number or a tuple (padW,). Default: 0\nConstraints:\ndtype:bool;int\nstructure:tuple\nshape:Null\nndim:0;1\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.autograd.grad(outputs,inputs,grad_outputs=None,retain_graph=None,create_graph=False,only_inputs=True,allow_unused=False)\nParameter description: outputs(sequence of Tensor) - outputs of the differentiated function.\nConstraints:\ndtype:Null\nstructure:sequence;tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.repeat_interleave(input,repeats,dim=None)\nParameter description: dim(int, optional) - The dimension along which to repeat values.By default, use the flattened input array, and return a flat outputarray.\nConstraints:\ndtype:int\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.autograd.functional.vhp(func,inputs,v=None,create_graph=False,strict=False)\nParameter description: create_graph(bool, optional) - If `True`, both the output and result will be computed in a differentiable way. Note that when `strict` is `False`, the result can not require gradients or be disconnected from the inputs. Defaults to `False`.\nConstraints:",
            "constr": {
                "dtype": [
                    "bool"
                ],
                "structure": [],
                "shape": [],
                "ndim": [
                    "0"
                ],
                "enum": [],
                "range": []
            },
            "signature": "torch.autograd.functional.vhp(func,inputs,v=None,create_graph=False,strict=False)",
            "descp": "create_graph(bool, optional) - If `True`, both the output and result will be computed in a differentiable way. Note that when `strict` is `False`, the result can not require gradients or be disconnected from the inputs. Defaults to `False`."
        },
        {
            "fname": "torch.autograd.grad.yaml",
            "arg": "retain_graph",
            "prompt": "Signature: torch.distributed.recv(tensor,src=None,group=<objectobject>,tag=0)\nParameter description: tensor(Tensor) - Tensor to fill with received data.\nConstraints:\ndtype:Null\nstructure:tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.div(input,other,out=None)\nParameter description: out(Tensor, optional) - the output tensor.\nConstraints:\ndtype:Null\nstructure:tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.nn.TransformerDecoder(decoder_layer,num_layers,norm=None)\nParameter description: num_layers - the number of sub-decoder-layers in the decoder (required).\nConstraints:\ndtype:int\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:[0,inf)\n\n###\n\nSignature: torch.nn.quantized.functional.interpolate(input,size=None,scale_factor=None,mode=nearest,align_corners=None)\nParameter description: align_corners(bool, optional) - Geometrically, we consider the pixels of the input and output as squares rather than points. If set to `True`, the input and output tensors are aligned by the center points of their corner pixels, preserving the values at the corner pixels. If set to `False`, the input and output tensors are aligned by the corner points of their corner pixels, and the interpolation uses edge value padding for out-of-boundary values, making this operation independent of input size when `scale_factor` is kept the same. This only has an effect when `mode` is `'bilinear'`. Default: `False`\nConstraints:\ndtype:bool\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.gt(input,other,out=None)\nParameter description: out(Tensor, optional) - the output tensor that must be a BoolTensor\nConstraints:\ndtype:booltensor\nstructure:tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.empty_like(input,dtype=None,layout=None,device=None,requires_grad=False,memory_format=torch.preserve_format)\nParameter description: dtype(`torch.dtype`, optional) - the desired data type of returned Tensor. Default: if `None`, defaults to the dtype of `input`.\nConstraints:\ndtype:torch.dtype\nstructure:Null\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.nn.RNN(*args,**kwargs,input_size=None,hidden_size=None,num_layers=None,nonlinearity=None,bias=None,batch_first=None,dropout=None,bidirectional=None)\nParameter description: hidden_size - The number of features in the hidden state h\nConstraints:\ndtype:int\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:[0,inf)\n\n###\n\nSignature: torch.nn.functional.avg_pool1d(input,kernel_size,stride=None,padding=0,ceil_mode=False,count_include_pad=True)\nParameter description: padding - implicit zero paddings on both sides of the input. Can be a single number or a tuple (padW,). Default: 0\nConstraints:\ndtype:bool;int\nstructure:tuple\nshape:Null\nndim:0;1\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.autograd.grad(outputs,inputs,grad_outputs=None,retain_graph=None,create_graph=False,only_inputs=True,allow_unused=False)\nParameter description: outputs(sequence of Tensor) - outputs of the differentiated function.\nConstraints:\ndtype:Null\nstructure:sequence;tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.repeat_interleave(input,repeats,dim=None)\nParameter description: dim(int, optional) - The dimension along which to repeat values.By default, use the flattened input array, and return a flat outputarray.\nConstraints:\ndtype:int\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.autograd.grad(outputs,inputs,grad_outputs=None,retain_graph=None,create_graph=False,only_inputs=True,allow_unused=False)\nParameter description: retain_graph(bool, optional) - If `False`, the graph used to compute the grad will be freed. Note that in nearly all cases setting this option to `True` is not needed and often can be worked around in a much more efficient way. Defaults to the value of `create_graph`.\nConstraints:",
            "constr": {
                "dtype": [
                    "bool"
                ],
                "structure": [],
                "shape": [],
                "ndim": [
                    "0"
                ],
                "enum": [],
                "range": []
            },
            "signature": "torch.autograd.grad(outputs,inputs,grad_outputs=None,retain_graph=None,create_graph=False,only_inputs=True,allow_unused=False)",
            "descp": "retain_graph(bool, optional) - If `False`, the graph used to compute the grad will be freed. Note that in nearly all cases setting this option to `True` is not needed and often can be worked around in a much more efficient way. Defaults to the value of `create_graph`."
        },
        {
            "fname": "torch.nn.functional.adaptive_max_pool3d.yaml",
            "arg": "return_indices",
            "prompt": "Signature: torch.distributed.recv(tensor,src=None,group=<objectobject>,tag=0)\nParameter description: tensor(Tensor) - Tensor to fill with received data.\nConstraints:\ndtype:Null\nstructure:tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.div(input,other,out=None)\nParameter description: out(Tensor, optional) - the output tensor.\nConstraints:\ndtype:Null\nstructure:tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.nn.TransformerDecoder(decoder_layer,num_layers,norm=None)\nParameter description: num_layers - the number of sub-decoder-layers in the decoder (required).\nConstraints:\ndtype:int\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:[0,inf)\n\n###\n\nSignature: torch.nn.quantized.functional.interpolate(input,size=None,scale_factor=None,mode=nearest,align_corners=None)\nParameter description: align_corners(bool, optional) - Geometrically, we consider the pixels of the input and output as squares rather than points. If set to `True`, the input and output tensors are aligned by the center points of their corner pixels, preserving the values at the corner pixels. If set to `False`, the input and output tensors are aligned by the corner points of their corner pixels, and the interpolation uses edge value padding for out-of-boundary values, making this operation independent of input size when `scale_factor` is kept the same. This only has an effect when `mode` is `'bilinear'`. Default: `False`\nConstraints:\ndtype:bool\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.gt(input,other,out=None)\nParameter description: out(Tensor, optional) - the output tensor that must be a BoolTensor\nConstraints:\ndtype:booltensor\nstructure:tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.empty_like(input,dtype=None,layout=None,device=None,requires_grad=False,memory_format=torch.preserve_format)\nParameter description: dtype(`torch.dtype`, optional) - the desired data type of returned Tensor. Default: if `None`, defaults to the dtype of `input`.\nConstraints:\ndtype:torch.dtype\nstructure:Null\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.nn.RNN(*args,**kwargs,input_size=None,hidden_size=None,num_layers=None,nonlinearity=None,bias=None,batch_first=None,dropout=None,bidirectional=None)\nParameter description: hidden_size - The number of features in the hidden state h\nConstraints:\ndtype:int\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:[0,inf)\n\n###\n\nSignature: torch.nn.functional.avg_pool1d(input,kernel_size,stride=None,padding=0,ceil_mode=False,count_include_pad=True)\nParameter description: padding - implicit zero paddings on both sides of the input. Can be a single number or a tuple (padW,). Default: 0\nConstraints:\ndtype:bool;int\nstructure:tuple\nshape:Null\nndim:0;1\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.autograd.grad(outputs,inputs,grad_outputs=None,retain_graph=None,create_graph=False,only_inputs=True,allow_unused=False)\nParameter description: outputs(sequence of Tensor) - outputs of the differentiated function.\nConstraints:\ndtype:Null\nstructure:sequence;tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.repeat_interleave(input,repeats,dim=None)\nParameter description: dim(int, optional) - The dimension along which to repeat values.By default, use the flattened input array, and return a flat outputarray.\nConstraints:\ndtype:int\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.nn.functional.adaptive_max_pool3d(*args,**kwargs,output_size=None,return_indices=None)\nParameter description: return_indices - whether to return pooling indices. Default: `False`\nConstraints:",
            "constr": {
                "dtype": [
                    "bool"
                ],
                "structure": [],
                "shape": [],
                "ndim": [
                    "0"
                ],
                "enum": [],
                "range": []
            },
            "signature": "torch.nn.functional.adaptive_max_pool3d(*args,**kwargs,output_size=None,return_indices=None)",
            "descp": "return_indices - whether to return pooling indices. Default: `False`"
        },
        {
            "fname": "torch.nn.maxpool3d.yaml",
            "arg": "ceil_mode",
            "prompt": "Signature: torch.distributed.recv(tensor,src=None,group=<objectobject>,tag=0)\nParameter description: tensor(Tensor) - Tensor to fill with received data.\nConstraints:\ndtype:Null\nstructure:tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.div(input,other,out=None)\nParameter description: out(Tensor, optional) - the output tensor.\nConstraints:\ndtype:Null\nstructure:tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.nn.TransformerDecoder(decoder_layer,num_layers,norm=None)\nParameter description: num_layers - the number of sub-decoder-layers in the decoder (required).\nConstraints:\ndtype:int\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:[0,inf)\n\n###\n\nSignature: torch.nn.quantized.functional.interpolate(input,size=None,scale_factor=None,mode=nearest,align_corners=None)\nParameter description: align_corners(bool, optional) - Geometrically, we consider the pixels of the input and output as squares rather than points. If set to `True`, the input and output tensors are aligned by the center points of their corner pixels, preserving the values at the corner pixels. If set to `False`, the input and output tensors are aligned by the corner points of their corner pixels, and the interpolation uses edge value padding for out-of-boundary values, making this operation independent of input size when `scale_factor` is kept the same. This only has an effect when `mode` is `'bilinear'`. Default: `False`\nConstraints:\ndtype:bool\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.gt(input,other,out=None)\nParameter description: out(Tensor, optional) - the output tensor that must be a BoolTensor\nConstraints:\ndtype:booltensor\nstructure:tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.empty_like(input,dtype=None,layout=None,device=None,requires_grad=False,memory_format=torch.preserve_format)\nParameter description: dtype(`torch.dtype`, optional) - the desired data type of returned Tensor. Default: if `None`, defaults to the dtype of `input`.\nConstraints:\ndtype:torch.dtype\nstructure:Null\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.nn.RNN(*args,**kwargs,input_size=None,hidden_size=None,num_layers=None,nonlinearity=None,bias=None,batch_first=None,dropout=None,bidirectional=None)\nParameter description: hidden_size - The number of features in the hidden state h\nConstraints:\ndtype:int\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:[0,inf)\n\n###\n\nSignature: torch.nn.functional.avg_pool1d(input,kernel_size,stride=None,padding=0,ceil_mode=False,count_include_pad=True)\nParameter description: padding - implicit zero paddings on both sides of the input. Can be a single number or a tuple (padW,). Default: 0\nConstraints:\ndtype:bool;int\nstructure:tuple\nshape:Null\nndim:0;1\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.autograd.grad(outputs,inputs,grad_outputs=None,retain_graph=None,create_graph=False,only_inputs=True,allow_unused=False)\nParameter description: outputs(sequence of Tensor) - outputs of the differentiated function.\nConstraints:\ndtype:Null\nstructure:sequence;tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.repeat_interleave(input,repeats,dim=None)\nParameter description: dim(int, optional) - The dimension along which to repeat values.By default, use the flattened input array, and return a flat outputarray.\nConstraints:\ndtype:int\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.nn.MaxPool3d(kernel_size,stride=None,padding=0,dilation=1,return_indices=False,ceil_mode=False)\nParameter description: ceil_mode - when True, will use ceil instead of floor to compute the output shape\nConstraints:",
            "constr": {
                "dtype": [
                    "bool"
                ],
                "structure": [],
                "shape": [],
                "ndim": [
                    "0"
                ],
                "enum": [],
                "range": []
            },
            "signature": "torch.nn.MaxPool3d(kernel_size,stride=None,padding=0,dilation=1,return_indices=False,ceil_mode=False)",
            "descp": "ceil_mode - when True, will use ceil instead of floor to compute the output shape"
        },
        {
            "fname": "torch.chunk.yaml",
            "arg": "chunks",
            "prompt": "Signature: torch.distributed.recv(tensor,src=None,group=<objectobject>,tag=0)\nParameter description: tensor(Tensor) - Tensor to fill with received data.\nConstraints:\ndtype:Null\nstructure:tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.div(input,other,out=None)\nParameter description: out(Tensor, optional) - the output tensor.\nConstraints:\ndtype:Null\nstructure:tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.nn.TransformerDecoder(decoder_layer,num_layers,norm=None)\nParameter description: num_layers - the number of sub-decoder-layers in the decoder (required).\nConstraints:\ndtype:int\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:[0,inf)\n\n###\n\nSignature: torch.nn.quantized.functional.interpolate(input,size=None,scale_factor=None,mode=nearest,align_corners=None)\nParameter description: align_corners(bool, optional) - Geometrically, we consider the pixels of the input and output as squares rather than points. If set to `True`, the input and output tensors are aligned by the center points of their corner pixels, preserving the values at the corner pixels. If set to `False`, the input and output tensors are aligned by the corner points of their corner pixels, and the interpolation uses edge value padding for out-of-boundary values, making this operation independent of input size when `scale_factor` is kept the same. This only has an effect when `mode` is `'bilinear'`. Default: `False`\nConstraints:\ndtype:bool\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.gt(input,other,out=None)\nParameter description: out(Tensor, optional) - the output tensor that must be a BoolTensor\nConstraints:\ndtype:booltensor\nstructure:tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.empty_like(input,dtype=None,layout=None,device=None,requires_grad=False,memory_format=torch.preserve_format)\nParameter description: dtype(`torch.dtype`, optional) - the desired data type of returned Tensor. Default: if `None`, defaults to the dtype of `input`.\nConstraints:\ndtype:torch.dtype\nstructure:Null\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.nn.RNN(*args,**kwargs,input_size=None,hidden_size=None,num_layers=None,nonlinearity=None,bias=None,batch_first=None,dropout=None,bidirectional=None)\nParameter description: hidden_size - The number of features in the hidden state h\nConstraints:\ndtype:int\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:[0,inf)\n\n###\n\nSignature: torch.nn.functional.avg_pool1d(input,kernel_size,stride=None,padding=0,ceil_mode=False,count_include_pad=True)\nParameter description: padding - implicit zero paddings on both sides of the input. Can be a single number or a tuple (padW,). Default: 0\nConstraints:\ndtype:bool;int\nstructure:tuple\nshape:Null\nndim:0;1\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.autograd.grad(outputs,inputs,grad_outputs=None,retain_graph=None,create_graph=False,only_inputs=True,allow_unused=False)\nParameter description: outputs(sequence of Tensor) - outputs of the differentiated function.\nConstraints:\ndtype:Null\nstructure:sequence;tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.repeat_interleave(input,repeats,dim=None)\nParameter description: dim(int, optional) - The dimension along which to repeat values.By default, use the flattened input array, and return a flat outputarray.\nConstraints:\ndtype:int\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.chunk(input,chunks,dim=0)\nParameter description: chunks(int) - number of chunks to return\nConstraints:",
            "constr": {
                "dtype": [
                    "int"
                ],
                "structure": [],
                "shape": [],
                "ndim": [
                    "0"
                ],
                "enum": [],
                "range": [
                    "[0,inf)"
                ]
            },
            "signature": "torch.chunk(input,chunks,dim=0)",
            "descp": "chunks(int) - number of chunks to return"
        },
        {
            "fname": "torch.nn.multilabelsoftmarginloss.yaml",
            "arg": "reduction",
            "prompt": "Signature: torch.distributed.recv(tensor,src=None,group=<objectobject>,tag=0)\nParameter description: tensor(Tensor) - Tensor to fill with received data.\nConstraints:\ndtype:Null\nstructure:tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.div(input,other,out=None)\nParameter description: out(Tensor, optional) - the output tensor.\nConstraints:\ndtype:Null\nstructure:tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.nn.TransformerDecoder(decoder_layer,num_layers,norm=None)\nParameter description: num_layers - the number of sub-decoder-layers in the decoder (required).\nConstraints:\ndtype:int\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:[0,inf)\n\n###\n\nSignature: torch.nn.quantized.functional.interpolate(input,size=None,scale_factor=None,mode=nearest,align_corners=None)\nParameter description: align_corners(bool, optional) - Geometrically, we consider the pixels of the input and output as squares rather than points. If set to `True`, the input and output tensors are aligned by the center points of their corner pixels, preserving the values at the corner pixels. If set to `False`, the input and output tensors are aligned by the corner points of their corner pixels, and the interpolation uses edge value padding for out-of-boundary values, making this operation independent of input size when `scale_factor` is kept the same. This only has an effect when `mode` is `'bilinear'`. Default: `False`\nConstraints:\ndtype:bool\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.gt(input,other,out=None)\nParameter description: out(Tensor, optional) - the output tensor that must be a BoolTensor\nConstraints:\ndtype:booltensor\nstructure:tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.empty_like(input,dtype=None,layout=None,device=None,requires_grad=False,memory_format=torch.preserve_format)\nParameter description: dtype(`torch.dtype`, optional) - the desired data type of returned Tensor. Default: if `None`, defaults to the dtype of `input`.\nConstraints:\ndtype:torch.dtype\nstructure:Null\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.nn.RNN(*args,**kwargs,input_size=None,hidden_size=None,num_layers=None,nonlinearity=None,bias=None,batch_first=None,dropout=None,bidirectional=None)\nParameter description: hidden_size - The number of features in the hidden state h\nConstraints:\ndtype:int\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:[0,inf)\n\n###\n\nSignature: torch.nn.functional.avg_pool1d(input,kernel_size,stride=None,padding=0,ceil_mode=False,count_include_pad=True)\nParameter description: padding - implicit zero paddings on both sides of the input. Can be a single number or a tuple (padW,). Default: 0\nConstraints:\ndtype:bool;int\nstructure:tuple\nshape:Null\nndim:0;1\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.autograd.grad(outputs,inputs,grad_outputs=None,retain_graph=None,create_graph=False,only_inputs=True,allow_unused=False)\nParameter description: outputs(sequence of Tensor) - outputs of the differentiated function.\nConstraints:\ndtype:Null\nstructure:sequence;tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.repeat_interleave(input,repeats,dim=None)\nParameter description: dim(int, optional) - The dimension along which to repeat values.By default, use the flattened input array, and return a flat outputarray.\nConstraints:\ndtype:int\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.nn.MultiLabelSoftMarginLoss(weight=None,size_average=None,reduce=None,reduction=mean)\nParameter description: reduction(string, optional) - Specifies the reduction to apply to the output: `'none'` | `'mean'` | `'sum'`. `'none'`: no reduction will be applied, `'mean'`: the sum of the output will be divided by the number of elements in the output, `'sum'`: the output will be summed. Note: `size_average` and `reduce` are in the process of being deprecated, and in the meantime, specifying either of those two args will override `reduction`. Default: `'mean'`\nConstraints:",
            "constr": {
                "dtype": [
                    "string"
                ],
                "structure": [],
                "shape": [],
                "ndim": [],
                "enum": [
                    "sum",
                    "mean",
                    "none"
                ],
                "range": []
            },
            "signature": "torch.nn.MultiLabelSoftMarginLoss(weight=None,size_average=None,reduce=None,reduction=mean)",
            "descp": "reduction(string, optional) - Specifies the reduction to apply to the output: `'none'` | `'mean'` | `'sum'`. `'none'`: no reduction will be applied, `'mean'`: the sum of the output will be divided by the number of elements in the output, `'sum'`: the output will be summed. Note: `size_average` and `reduce` are in the process of being deprecated, and in the meantime, specifying either of those two args will override `reduction`. Default: `'mean'`"
        },
        {
            "fname": "torch.nn.multimarginloss.yaml",
            "arg": "weight",
            "prompt": "Signature: torch.distributed.recv(tensor,src=None,group=<objectobject>,tag=0)\nParameter description: tensor(Tensor) - Tensor to fill with received data.\nConstraints:\ndtype:Null\nstructure:tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.div(input,other,out=None)\nParameter description: out(Tensor, optional) - the output tensor.\nConstraints:\ndtype:Null\nstructure:tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.nn.TransformerDecoder(decoder_layer,num_layers,norm=None)\nParameter description: num_layers - the number of sub-decoder-layers in the decoder (required).\nConstraints:\ndtype:int\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:[0,inf)\n\n###\n\nSignature: torch.nn.quantized.functional.interpolate(input,size=None,scale_factor=None,mode=nearest,align_corners=None)\nParameter description: align_corners(bool, optional) - Geometrically, we consider the pixels of the input and output as squares rather than points. If set to `True`, the input and output tensors are aligned by the center points of their corner pixels, preserving the values at the corner pixels. If set to `False`, the input and output tensors are aligned by the corner points of their corner pixels, and the interpolation uses edge value padding for out-of-boundary values, making this operation independent of input size when `scale_factor` is kept the same. This only has an effect when `mode` is `'bilinear'`. Default: `False`\nConstraints:\ndtype:bool\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.gt(input,other,out=None)\nParameter description: out(Tensor, optional) - the output tensor that must be a BoolTensor\nConstraints:\ndtype:booltensor\nstructure:tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.empty_like(input,dtype=None,layout=None,device=None,requires_grad=False,memory_format=torch.preserve_format)\nParameter description: dtype(`torch.dtype`, optional) - the desired data type of returned Tensor. Default: if `None`, defaults to the dtype of `input`.\nConstraints:\ndtype:torch.dtype\nstructure:Null\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.nn.RNN(*args,**kwargs,input_size=None,hidden_size=None,num_layers=None,nonlinearity=None,bias=None,batch_first=None,dropout=None,bidirectional=None)\nParameter description: hidden_size - The number of features in the hidden state h\nConstraints:\ndtype:int\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:[0,inf)\n\n###\n\nSignature: torch.nn.functional.avg_pool1d(input,kernel_size,stride=None,padding=0,ceil_mode=False,count_include_pad=True)\nParameter description: padding - implicit zero paddings on both sides of the input. Can be a single number or a tuple (padW,). Default: 0\nConstraints:\ndtype:bool;int\nstructure:tuple\nshape:Null\nndim:0;1\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.autograd.grad(outputs,inputs,grad_outputs=None,retain_graph=None,create_graph=False,only_inputs=True,allow_unused=False)\nParameter description: outputs(sequence of Tensor) - outputs of the differentiated function.\nConstraints:\ndtype:Null\nstructure:sequence;tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.repeat_interleave(input,repeats,dim=None)\nParameter description: dim(int, optional) - The dimension along which to repeat values.By default, use the flattened input array, and return a flat outputarray.\nConstraints:\ndtype:int\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.nn.MultiMarginLoss(p=1,margin=1.0,weight=None,size_average=None,reduce=None,reduction=mean)\nParameter description: weight(Tensor, optional) - a manual rescaling weight given to each class. If given, it has to be a Tensor of size C. Otherwise, it is treated as if having all ones.\nConstraints:",
            "constr": {
                "dtype": [
                    "numeric"
                ],
                "structure": [
                    "tensor"
                ],
                "shape": [
                    "[C]"
                ],
                "ndim": [],
                "enum": [],
                "range": []
            },
            "signature": "torch.nn.MultiMarginLoss(p=1,margin=1.0,weight=None,size_average=None,reduce=None,reduction=mean)",
            "descp": "weight(Tensor, optional) - a manual rescaling weight given to each class. If given, it has to be a Tensor of size C. Otherwise, it is treated as if having all ones."
        },
        {
            "fname": "torch.distributed.new_group.yaml",
            "arg": "ranks",
            "prompt": "Signature: torch.distributed.recv(tensor,src=None,group=<objectobject>,tag=0)\nParameter description: tensor(Tensor) - Tensor to fill with received data.\nConstraints:\ndtype:Null\nstructure:tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.div(input,other,out=None)\nParameter description: out(Tensor, optional) - the output tensor.\nConstraints:\ndtype:Null\nstructure:tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.nn.TransformerDecoder(decoder_layer,num_layers,norm=None)\nParameter description: num_layers - the number of sub-decoder-layers in the decoder (required).\nConstraints:\ndtype:int\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:[0,inf)\n\n###\n\nSignature: torch.nn.quantized.functional.interpolate(input,size=None,scale_factor=None,mode=nearest,align_corners=None)\nParameter description: align_corners(bool, optional) - Geometrically, we consider the pixels of the input and output as squares rather than points. If set to `True`, the input and output tensors are aligned by the center points of their corner pixels, preserving the values at the corner pixels. If set to `False`, the input and output tensors are aligned by the corner points of their corner pixels, and the interpolation uses edge value padding for out-of-boundary values, making this operation independent of input size when `scale_factor` is kept the same. This only has an effect when `mode` is `'bilinear'`. Default: `False`\nConstraints:\ndtype:bool\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.gt(input,other,out=None)\nParameter description: out(Tensor, optional) - the output tensor that must be a BoolTensor\nConstraints:\ndtype:booltensor\nstructure:tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.empty_like(input,dtype=None,layout=None,device=None,requires_grad=False,memory_format=torch.preserve_format)\nParameter description: dtype(`torch.dtype`, optional) - the desired data type of returned Tensor. Default: if `None`, defaults to the dtype of `input`.\nConstraints:\ndtype:torch.dtype\nstructure:Null\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.nn.RNN(*args,**kwargs,input_size=None,hidden_size=None,num_layers=None,nonlinearity=None,bias=None,batch_first=None,dropout=None,bidirectional=None)\nParameter description: hidden_size - The number of features in the hidden state h\nConstraints:\ndtype:int\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:[0,inf)\n\n###\n\nSignature: torch.nn.functional.avg_pool1d(input,kernel_size,stride=None,padding=0,ceil_mode=False,count_include_pad=True)\nParameter description: padding - implicit zero paddings on both sides of the input. Can be a single number or a tuple (padW,). Default: 0\nConstraints:\ndtype:bool;int\nstructure:tuple\nshape:Null\nndim:0;1\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.autograd.grad(outputs,inputs,grad_outputs=None,retain_graph=None,create_graph=False,only_inputs=True,allow_unused=False)\nParameter description: outputs(sequence of Tensor) - outputs of the differentiated function.\nConstraints:\ndtype:Null\nstructure:sequence;tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.repeat_interleave(input,repeats,dim=None)\nParameter description: dim(int, optional) - The dimension along which to repeat values.By default, use the flattened input array, and return a flat outputarray.\nConstraints:\ndtype:int\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.distributed.new_group(ranks=None,timeout=datetime.timedelta(0,1800),backend=None)\nParameter description: ranks(list[int]) - List of ranks of group members.\nConstraints:",
            "constr": {
                "dtype": [
                    "int"
                ],
                "structure": [
                    "list"
                ],
                "shape": [],
                "ndim": [],
                "enum": [],
                "range": []
            },
            "signature": "torch.distributed.new_group(ranks=None,timeout=datetime.timedelta(0,1800),backend=None)",
            "descp": "ranks(list[int]) - List of ranks of group members."
        },
        {
            "fname": "torch.std_mean.yaml",
            "arg": "input",
            "prompt": "Signature: torch.distributed.recv(tensor,src=None,group=<objectobject>,tag=0)\nParameter description: tensor(Tensor) - Tensor to fill with received data.\nConstraints:\ndtype:Null\nstructure:tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.div(input,other,out=None)\nParameter description: out(Tensor, optional) - the output tensor.\nConstraints:\ndtype:Null\nstructure:tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.nn.TransformerDecoder(decoder_layer,num_layers,norm=None)\nParameter description: num_layers - the number of sub-decoder-layers in the decoder (required).\nConstraints:\ndtype:int\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:[0,inf)\n\n###\n\nSignature: torch.nn.quantized.functional.interpolate(input,size=None,scale_factor=None,mode=nearest,align_corners=None)\nParameter description: align_corners(bool, optional) - Geometrically, we consider the pixels of the input and output as squares rather than points. If set to `True`, the input and output tensors are aligned by the center points of their corner pixels, preserving the values at the corner pixels. If set to `False`, the input and output tensors are aligned by the corner points of their corner pixels, and the interpolation uses edge value padding for out-of-boundary values, making this operation independent of input size when `scale_factor` is kept the same. This only has an effect when `mode` is `'bilinear'`. Default: `False`\nConstraints:\ndtype:bool\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.gt(input,other,out=None)\nParameter description: out(Tensor, optional) - the output tensor that must be a BoolTensor\nConstraints:\ndtype:booltensor\nstructure:tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.empty_like(input,dtype=None,layout=None,device=None,requires_grad=False,memory_format=torch.preserve_format)\nParameter description: dtype(`torch.dtype`, optional) - the desired data type of returned Tensor. Default: if `None`, defaults to the dtype of `input`.\nConstraints:\ndtype:torch.dtype\nstructure:Null\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.nn.RNN(*args,**kwargs,input_size=None,hidden_size=None,num_layers=None,nonlinearity=None,bias=None,batch_first=None,dropout=None,bidirectional=None)\nParameter description: hidden_size - The number of features in the hidden state h\nConstraints:\ndtype:int\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:[0,inf)\n\n###\n\nSignature: torch.nn.functional.avg_pool1d(input,kernel_size,stride=None,padding=0,ceil_mode=False,count_include_pad=True)\nParameter description: padding - implicit zero paddings on both sides of the input. Can be a single number or a tuple (padW,). Default: 0\nConstraints:\ndtype:bool;int\nstructure:tuple\nshape:Null\nndim:0;1\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.autograd.grad(outputs,inputs,grad_outputs=None,retain_graph=None,create_graph=False,only_inputs=True,allow_unused=False)\nParameter description: outputs(sequence of Tensor) - outputs of the differentiated function.\nConstraints:\ndtype:Null\nstructure:sequence;tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.repeat_interleave(input,repeats,dim=None)\nParameter description: dim(int, optional) - The dimension along which to repeat values.By default, use the flattened input array, and return a flat outputarray.\nConstraints:\ndtype:int\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.std_mean(input,unbiased=True)\nParameter description: input(Tensor) - the input tensor.\nConstraints:",
            "constr": {
                "dtype": [],
                "structure": [
                    "tensor"
                ],
                "shape": [],
                "ndim": [],
                "enum": [],
                "range": []
            },
            "signature": "torch.std_mean(input,unbiased=True)",
            "descp": "input(Tensor) - the input tensor."
        },
        {
            "fname": "torch.diag.yaml",
            "arg": "out",
            "prompt": "Signature: torch.distributed.recv(tensor,src=None,group=<objectobject>,tag=0)\nParameter description: tensor(Tensor) - Tensor to fill with received data.\nConstraints:\ndtype:Null\nstructure:tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.div(input,other,out=None)\nParameter description: out(Tensor, optional) - the output tensor.\nConstraints:\ndtype:Null\nstructure:tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.nn.TransformerDecoder(decoder_layer,num_layers,norm=None)\nParameter description: num_layers - the number of sub-decoder-layers in the decoder (required).\nConstraints:\ndtype:int\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:[0,inf)\n\n###\n\nSignature: torch.nn.quantized.functional.interpolate(input,size=None,scale_factor=None,mode=nearest,align_corners=None)\nParameter description: align_corners(bool, optional) - Geometrically, we consider the pixels of the input and output as squares rather than points. If set to `True`, the input and output tensors are aligned by the center points of their corner pixels, preserving the values at the corner pixels. If set to `False`, the input and output tensors are aligned by the corner points of their corner pixels, and the interpolation uses edge value padding for out-of-boundary values, making this operation independent of input size when `scale_factor` is kept the same. This only has an effect when `mode` is `'bilinear'`. Default: `False`\nConstraints:\ndtype:bool\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.gt(input,other,out=None)\nParameter description: out(Tensor, optional) - the output tensor that must be a BoolTensor\nConstraints:\ndtype:booltensor\nstructure:tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.empty_like(input,dtype=None,layout=None,device=None,requires_grad=False,memory_format=torch.preserve_format)\nParameter description: dtype(`torch.dtype`, optional) - the desired data type of returned Tensor. Default: if `None`, defaults to the dtype of `input`.\nConstraints:\ndtype:torch.dtype\nstructure:Null\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.nn.RNN(*args,**kwargs,input_size=None,hidden_size=None,num_layers=None,nonlinearity=None,bias=None,batch_first=None,dropout=None,bidirectional=None)\nParameter description: hidden_size - The number of features in the hidden state h\nConstraints:\ndtype:int\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:[0,inf)\n\n###\n\nSignature: torch.nn.functional.avg_pool1d(input,kernel_size,stride=None,padding=0,ceil_mode=False,count_include_pad=True)\nParameter description: padding - implicit zero paddings on both sides of the input. Can be a single number or a tuple (padW,). Default: 0\nConstraints:\ndtype:bool;int\nstructure:tuple\nshape:Null\nndim:0;1\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.autograd.grad(outputs,inputs,grad_outputs=None,retain_graph=None,create_graph=False,only_inputs=True,allow_unused=False)\nParameter description: outputs(sequence of Tensor) - outputs of the differentiated function.\nConstraints:\ndtype:Null\nstructure:sequence;tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.repeat_interleave(input,repeats,dim=None)\nParameter description: dim(int, optional) - The dimension along which to repeat values.By default, use the flattened input array, and return a flat outputarray.\nConstraints:\ndtype:int\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.diag(input,diagonal=0,out=None)\nParameter description: out(Tensor, optional) - the output tensor.\nConstraints:",
            "constr": {
                "dtype": [],
                "structure": [
                    "tensor"
                ],
                "shape": [],
                "ndim": [],
                "enum": [],
                "range": []
            },
            "signature": "torch.diag(input,diagonal=0,out=None)",
            "descp": "out(Tensor, optional) - the output tensor."
        },
        {
            "fname": "torch.narrow.yaml",
            "arg": "input",
            "prompt": "Signature: torch.distributed.recv(tensor,src=None,group=<objectobject>,tag=0)\nParameter description: tensor(Tensor) - Tensor to fill with received data.\nConstraints:\ndtype:Null\nstructure:tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.div(input,other,out=None)\nParameter description: out(Tensor, optional) - the output tensor.\nConstraints:\ndtype:Null\nstructure:tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.nn.TransformerDecoder(decoder_layer,num_layers,norm=None)\nParameter description: num_layers - the number of sub-decoder-layers in the decoder (required).\nConstraints:\ndtype:int\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:[0,inf)\n\n###\n\nSignature: torch.nn.quantized.functional.interpolate(input,size=None,scale_factor=None,mode=nearest,align_corners=None)\nParameter description: align_corners(bool, optional) - Geometrically, we consider the pixels of the input and output as squares rather than points. If set to `True`, the input and output tensors are aligned by the center points of their corner pixels, preserving the values at the corner pixels. If set to `False`, the input and output tensors are aligned by the corner points of their corner pixels, and the interpolation uses edge value padding for out-of-boundary values, making this operation independent of input size when `scale_factor` is kept the same. This only has an effect when `mode` is `'bilinear'`. Default: `False`\nConstraints:\ndtype:bool\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.gt(input,other,out=None)\nParameter description: out(Tensor, optional) - the output tensor that must be a BoolTensor\nConstraints:\ndtype:booltensor\nstructure:tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.empty_like(input,dtype=None,layout=None,device=None,requires_grad=False,memory_format=torch.preserve_format)\nParameter description: dtype(`torch.dtype`, optional) - the desired data type of returned Tensor. Default: if `None`, defaults to the dtype of `input`.\nConstraints:\ndtype:torch.dtype\nstructure:Null\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.nn.RNN(*args,**kwargs,input_size=None,hidden_size=None,num_layers=None,nonlinearity=None,bias=None,batch_first=None,dropout=None,bidirectional=None)\nParameter description: hidden_size - The number of features in the hidden state h\nConstraints:\ndtype:int\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:[0,inf)\n\n###\n\nSignature: torch.nn.functional.avg_pool1d(input,kernel_size,stride=None,padding=0,ceil_mode=False,count_include_pad=True)\nParameter description: padding - implicit zero paddings on both sides of the input. Can be a single number or a tuple (padW,). Default: 0\nConstraints:\ndtype:bool;int\nstructure:tuple\nshape:Null\nndim:0;1\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.autograd.grad(outputs,inputs,grad_outputs=None,retain_graph=None,create_graph=False,only_inputs=True,allow_unused=False)\nParameter description: outputs(sequence of Tensor) - outputs of the differentiated function.\nConstraints:\ndtype:Null\nstructure:sequence;tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.repeat_interleave(input,repeats,dim=None)\nParameter description: dim(int, optional) - The dimension along which to repeat values.By default, use the flattened input array, and return a flat outputarray.\nConstraints:\ndtype:int\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.narrow(input,dim,start,length)\nParameter description: input(Tensor) - the tensor to narrow\nConstraints:",
            "constr": {
                "dtype": [],
                "structure": [
                    "tensor"
                ],
                "shape": [],
                "ndim": [],
                "enum": [],
                "range": []
            },
            "signature": "torch.narrow(input,dim,start,length)",
            "descp": "input(Tensor) - the tensor to narrow"
        },
        {
            "fname": "torch.kthvalue.yaml",
            "arg": "keepdim",
            "prompt": "Signature: torch.distributed.recv(tensor,src=None,group=<objectobject>,tag=0)\nParameter description: tensor(Tensor) - Tensor to fill with received data.\nConstraints:\ndtype:Null\nstructure:tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.div(input,other,out=None)\nParameter description: out(Tensor, optional) - the output tensor.\nConstraints:\ndtype:Null\nstructure:tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.nn.TransformerDecoder(decoder_layer,num_layers,norm=None)\nParameter description: num_layers - the number of sub-decoder-layers in the decoder (required).\nConstraints:\ndtype:int\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:[0,inf)\n\n###\n\nSignature: torch.nn.quantized.functional.interpolate(input,size=None,scale_factor=None,mode=nearest,align_corners=None)\nParameter description: align_corners(bool, optional) - Geometrically, we consider the pixels of the input and output as squares rather than points. If set to `True`, the input and output tensors are aligned by the center points of their corner pixels, preserving the values at the corner pixels. If set to `False`, the input and output tensors are aligned by the corner points of their corner pixels, and the interpolation uses edge value padding for out-of-boundary values, making this operation independent of input size when `scale_factor` is kept the same. This only has an effect when `mode` is `'bilinear'`. Default: `False`\nConstraints:\ndtype:bool\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.gt(input,other,out=None)\nParameter description: out(Tensor, optional) - the output tensor that must be a BoolTensor\nConstraints:\ndtype:booltensor\nstructure:tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.empty_like(input,dtype=None,layout=None,device=None,requires_grad=False,memory_format=torch.preserve_format)\nParameter description: dtype(`torch.dtype`, optional) - the desired data type of returned Tensor. Default: if `None`, defaults to the dtype of `input`.\nConstraints:\ndtype:torch.dtype\nstructure:Null\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.nn.RNN(*args,**kwargs,input_size=None,hidden_size=None,num_layers=None,nonlinearity=None,bias=None,batch_first=None,dropout=None,bidirectional=None)\nParameter description: hidden_size - The number of features in the hidden state h\nConstraints:\ndtype:int\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:[0,inf)\n\n###\n\nSignature: torch.nn.functional.avg_pool1d(input,kernel_size,stride=None,padding=0,ceil_mode=False,count_include_pad=True)\nParameter description: padding - implicit zero paddings on both sides of the input. Can be a single number or a tuple (padW,). Default: 0\nConstraints:\ndtype:bool;int\nstructure:tuple\nshape:Null\nndim:0;1\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.autograd.grad(outputs,inputs,grad_outputs=None,retain_graph=None,create_graph=False,only_inputs=True,allow_unused=False)\nParameter description: outputs(sequence of Tensor) - outputs of the differentiated function.\nConstraints:\ndtype:Null\nstructure:sequence;tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.repeat_interleave(input,repeats,dim=None)\nParameter description: dim(int, optional) - The dimension along which to repeat values.By default, use the flattened input array, and return a flat outputarray.\nConstraints:\ndtype:int\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.kthvalue(input,k,dim=None,keepdim=False,out=None)\nParameter description: keepdim(bool) - whether the output tensor has `dim` retained or not.\nConstraints:",
            "constr": {
                "dtype": [
                    "bool"
                ],
                "structure": [],
                "shape": [],
                "ndim": [
                    "0"
                ],
                "enum": [],
                "range": []
            },
            "signature": "torch.kthvalue(input,k,dim=None,keepdim=False,out=None)",
            "descp": "keepdim(bool) - whether the output tensor has `dim` retained or not."
        },
        {
            "fname": "torch.nn.maxpool2d.yaml",
            "arg": "stride",
            "prompt": "Signature: torch.distributed.recv(tensor,src=None,group=<objectobject>,tag=0)\nParameter description: tensor(Tensor) - Tensor to fill with received data.\nConstraints:\ndtype:Null\nstructure:tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.div(input,other,out=None)\nParameter description: out(Tensor, optional) - the output tensor.\nConstraints:\ndtype:Null\nstructure:tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.nn.TransformerDecoder(decoder_layer,num_layers,norm=None)\nParameter description: num_layers - the number of sub-decoder-layers in the decoder (required).\nConstraints:\ndtype:int\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:[0,inf)\n\n###\n\nSignature: torch.nn.quantized.functional.interpolate(input,size=None,scale_factor=None,mode=nearest,align_corners=None)\nParameter description: align_corners(bool, optional) - Geometrically, we consider the pixels of the input and output as squares rather than points. If set to `True`, the input and output tensors are aligned by the center points of their corner pixels, preserving the values at the corner pixels. If set to `False`, the input and output tensors are aligned by the corner points of their corner pixels, and the interpolation uses edge value padding for out-of-boundary values, making this operation independent of input size when `scale_factor` is kept the same. This only has an effect when `mode` is `'bilinear'`. Default: `False`\nConstraints:\ndtype:bool\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.gt(input,other,out=None)\nParameter description: out(Tensor, optional) - the output tensor that must be a BoolTensor\nConstraints:\ndtype:booltensor\nstructure:tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.empty_like(input,dtype=None,layout=None,device=None,requires_grad=False,memory_format=torch.preserve_format)\nParameter description: dtype(`torch.dtype`, optional) - the desired data type of returned Tensor. Default: if `None`, defaults to the dtype of `input`.\nConstraints:\ndtype:torch.dtype\nstructure:Null\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.nn.RNN(*args,**kwargs,input_size=None,hidden_size=None,num_layers=None,nonlinearity=None,bias=None,batch_first=None,dropout=None,bidirectional=None)\nParameter description: hidden_size - The number of features in the hidden state h\nConstraints:\ndtype:int\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:[0,inf)\n\n###\n\nSignature: torch.nn.functional.avg_pool1d(input,kernel_size,stride=None,padding=0,ceil_mode=False,count_include_pad=True)\nParameter description: padding - implicit zero paddings on both sides of the input. Can be a single number or a tuple (padW,). Default: 0\nConstraints:\ndtype:bool;int\nstructure:tuple\nshape:Null\nndim:0;1\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.autograd.grad(outputs,inputs,grad_outputs=None,retain_graph=None,create_graph=False,only_inputs=True,allow_unused=False)\nParameter description: outputs(sequence of Tensor) - outputs of the differentiated function.\nConstraints:\ndtype:Null\nstructure:sequence;tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.repeat_interleave(input,repeats,dim=None)\nParameter description: dim(int, optional) - The dimension along which to repeat values.By default, use the flattened input array, and return a flat outputarray.\nConstraints:\ndtype:int\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.nn.MaxPool2d(kernel_size,stride=None,padding=0,dilation=1,return_indices=False,ceil_mode=False)\nParameter description: stride - the stride of the window. Default value is `kernel_size`\nConstraints:",
            "constr": {
                "dtype": [
                    "int"
                ],
                "structure": [],
                "shape": [],
                "ndim": [],
                "enum": [],
                "range": [
                    "[0,inf)"
                ]
            },
            "signature": "torch.nn.MaxPool2d(kernel_size,stride=None,padding=0,dilation=1,return_indices=False,ceil_mode=False)",
            "descp": "stride - the stride of the window. Default value is `kernel_size`"
        },
        {
            "fname": "torch.imag.yaml",
            "arg": "input",
            "prompt": "Signature: torch.distributed.recv(tensor,src=None,group=<objectobject>,tag=0)\nParameter description: tensor(Tensor) - Tensor to fill with received data.\nConstraints:\ndtype:Null\nstructure:tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.div(input,other,out=None)\nParameter description: out(Tensor, optional) - the output tensor.\nConstraints:\ndtype:Null\nstructure:tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.nn.TransformerDecoder(decoder_layer,num_layers,norm=None)\nParameter description: num_layers - the number of sub-decoder-layers in the decoder (required).\nConstraints:\ndtype:int\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:[0,inf)\n\n###\n\nSignature: torch.nn.quantized.functional.interpolate(input,size=None,scale_factor=None,mode=nearest,align_corners=None)\nParameter description: align_corners(bool, optional) - Geometrically, we consider the pixels of the input and output as squares rather than points. If set to `True`, the input and output tensors are aligned by the center points of their corner pixels, preserving the values at the corner pixels. If set to `False`, the input and output tensors are aligned by the corner points of their corner pixels, and the interpolation uses edge value padding for out-of-boundary values, making this operation independent of input size when `scale_factor` is kept the same. This only has an effect when `mode` is `'bilinear'`. Default: `False`\nConstraints:\ndtype:bool\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.gt(input,other,out=None)\nParameter description: out(Tensor, optional) - the output tensor that must be a BoolTensor\nConstraints:\ndtype:booltensor\nstructure:tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.empty_like(input,dtype=None,layout=None,device=None,requires_grad=False,memory_format=torch.preserve_format)\nParameter description: dtype(`torch.dtype`, optional) - the desired data type of returned Tensor. Default: if `None`, defaults to the dtype of `input`.\nConstraints:\ndtype:torch.dtype\nstructure:Null\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.nn.RNN(*args,**kwargs,input_size=None,hidden_size=None,num_layers=None,nonlinearity=None,bias=None,batch_first=None,dropout=None,bidirectional=None)\nParameter description: hidden_size - The number of features in the hidden state h\nConstraints:\ndtype:int\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:[0,inf)\n\n###\n\nSignature: torch.nn.functional.avg_pool1d(input,kernel_size,stride=None,padding=0,ceil_mode=False,count_include_pad=True)\nParameter description: padding - implicit zero paddings on both sides of the input. Can be a single number or a tuple (padW,). Default: 0\nConstraints:\ndtype:bool;int\nstructure:tuple\nshape:Null\nndim:0;1\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.autograd.grad(outputs,inputs,grad_outputs=None,retain_graph=None,create_graph=False,only_inputs=True,allow_unused=False)\nParameter description: outputs(sequence of Tensor) - outputs of the differentiated function.\nConstraints:\ndtype:Null\nstructure:sequence;tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.repeat_interleave(input,repeats,dim=None)\nParameter description: dim(int, optional) - The dimension along which to repeat values.By default, use the flattened input array, and return a flat outputarray.\nConstraints:\ndtype:int\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.imag(input,out=None)\nParameter description: input(Tensor) - the input tensor.\nConstraints:",
            "constr": {
                "dtype": [],
                "structure": [
                    "tensor"
                ],
                "shape": [],
                "ndim": [],
                "enum": [],
                "range": []
            },
            "signature": "torch.imag(input,out=None)",
            "descp": "input(Tensor) - the input tensor."
        },
        {
            "fname": "torch.nn.batchnorm2d.yaml",
            "arg": "track_running_stats",
            "prompt": "Signature: torch.distributed.recv(tensor,src=None,group=<objectobject>,tag=0)\nParameter description: tensor(Tensor) - Tensor to fill with received data.\nConstraints:\ndtype:Null\nstructure:tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.div(input,other,out=None)\nParameter description: out(Tensor, optional) - the output tensor.\nConstraints:\ndtype:Null\nstructure:tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.nn.TransformerDecoder(decoder_layer,num_layers,norm=None)\nParameter description: num_layers - the number of sub-decoder-layers in the decoder (required).\nConstraints:\ndtype:int\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:[0,inf)\n\n###\n\nSignature: torch.nn.quantized.functional.interpolate(input,size=None,scale_factor=None,mode=nearest,align_corners=None)\nParameter description: align_corners(bool, optional) - Geometrically, we consider the pixels of the input and output as squares rather than points. If set to `True`, the input and output tensors are aligned by the center points of their corner pixels, preserving the values at the corner pixels. If set to `False`, the input and output tensors are aligned by the corner points of their corner pixels, and the interpolation uses edge value padding for out-of-boundary values, making this operation independent of input size when `scale_factor` is kept the same. This only has an effect when `mode` is `'bilinear'`. Default: `False`\nConstraints:\ndtype:bool\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.gt(input,other,out=None)\nParameter description: out(Tensor, optional) - the output tensor that must be a BoolTensor\nConstraints:\ndtype:booltensor\nstructure:tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.empty_like(input,dtype=None,layout=None,device=None,requires_grad=False,memory_format=torch.preserve_format)\nParameter description: dtype(`torch.dtype`, optional) - the desired data type of returned Tensor. Default: if `None`, defaults to the dtype of `input`.\nConstraints:\ndtype:torch.dtype\nstructure:Null\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.nn.RNN(*args,**kwargs,input_size=None,hidden_size=None,num_layers=None,nonlinearity=None,bias=None,batch_first=None,dropout=None,bidirectional=None)\nParameter description: hidden_size - The number of features in the hidden state h\nConstraints:\ndtype:int\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:[0,inf)\n\n###\n\nSignature: torch.nn.functional.avg_pool1d(input,kernel_size,stride=None,padding=0,ceil_mode=False,count_include_pad=True)\nParameter description: padding - implicit zero paddings on both sides of the input. Can be a single number or a tuple (padW,). Default: 0\nConstraints:\ndtype:bool;int\nstructure:tuple\nshape:Null\nndim:0;1\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.autograd.grad(outputs,inputs,grad_outputs=None,retain_graph=None,create_graph=False,only_inputs=True,allow_unused=False)\nParameter description: outputs(sequence of Tensor) - outputs of the differentiated function.\nConstraints:\ndtype:Null\nstructure:sequence;tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.repeat_interleave(input,repeats,dim=None)\nParameter description: dim(int, optional) - The dimension along which to repeat values.By default, use the flattened input array, and return a flat outputarray.\nConstraints:\ndtype:int\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.nn.BatchNorm2d(num_features,eps=1e-05,momentum=0.1,affine=True,track_running_stats=True)\nParameter description: track_running_stats - a boolean value that when set to `True`, this module tracks the running mean and variance, and when set to `False`, this module does not track such statistics and always uses batch statistics in both training and eval modes. Default: `True`\nConstraints:",
            "constr": {
                "dtype": [
                    "boolean",
                    "bool"
                ],
                "structure": [],
                "shape": [],
                "ndim": [
                    "0"
                ],
                "enum": [],
                "range": []
            },
            "signature": "torch.nn.BatchNorm2d(num_features,eps=1e-05,momentum=0.1,affine=True,track_running_stats=True)",
            "descp": "track_running_stats - a boolean value that when set to `True`, this module tracks the running mean and variance, and when set to `False`, this module does not track such statistics and always uses batch statistics in both training and eval modes. Default: `True`"
        },
        {
            "fname": "torch.nn.lstm.yaml",
            "arg": "input_size",
            "prompt": "Signature: torch.distributed.recv(tensor,src=None,group=<objectobject>,tag=0)\nParameter description: tensor(Tensor) - Tensor to fill with received data.\nConstraints:\ndtype:Null\nstructure:tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.div(input,other,out=None)\nParameter description: out(Tensor, optional) - the output tensor.\nConstraints:\ndtype:Null\nstructure:tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.nn.TransformerDecoder(decoder_layer,num_layers,norm=None)\nParameter description: num_layers - the number of sub-decoder-layers in the decoder (required).\nConstraints:\ndtype:int\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:[0,inf)\n\n###\n\nSignature: torch.nn.quantized.functional.interpolate(input,size=None,scale_factor=None,mode=nearest,align_corners=None)\nParameter description: align_corners(bool, optional) - Geometrically, we consider the pixels of the input and output as squares rather than points. If set to `True`, the input and output tensors are aligned by the center points of their corner pixels, preserving the values at the corner pixels. If set to `False`, the input and output tensors are aligned by the corner points of their corner pixels, and the interpolation uses edge value padding for out-of-boundary values, making this operation independent of input size when `scale_factor` is kept the same. This only has an effect when `mode` is `'bilinear'`. Default: `False`\nConstraints:\ndtype:bool\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.gt(input,other,out=None)\nParameter description: out(Tensor, optional) - the output tensor that must be a BoolTensor\nConstraints:\ndtype:booltensor\nstructure:tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.empty_like(input,dtype=None,layout=None,device=None,requires_grad=False,memory_format=torch.preserve_format)\nParameter description: dtype(`torch.dtype`, optional) - the desired data type of returned Tensor. Default: if `None`, defaults to the dtype of `input`.\nConstraints:\ndtype:torch.dtype\nstructure:Null\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.nn.RNN(*args,**kwargs,input_size=None,hidden_size=None,num_layers=None,nonlinearity=None,bias=None,batch_first=None,dropout=None,bidirectional=None)\nParameter description: hidden_size - The number of features in the hidden state h\nConstraints:\ndtype:int\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:[0,inf)\n\n###\n\nSignature: torch.nn.functional.avg_pool1d(input,kernel_size,stride=None,padding=0,ceil_mode=False,count_include_pad=True)\nParameter description: padding - implicit zero paddings on both sides of the input. Can be a single number or a tuple (padW,). Default: 0\nConstraints:\ndtype:bool;int\nstructure:tuple\nshape:Null\nndim:0;1\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.autograd.grad(outputs,inputs,grad_outputs=None,retain_graph=None,create_graph=False,only_inputs=True,allow_unused=False)\nParameter description: outputs(sequence of Tensor) - outputs of the differentiated function.\nConstraints:\ndtype:Null\nstructure:sequence;tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.repeat_interleave(input,repeats,dim=None)\nParameter description: dim(int, optional) - The dimension along which to repeat values.By default, use the flattened input array, and return a flat outputarray.\nConstraints:\ndtype:int\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.nn.LSTM(*args,**kwargs,input_size=None,hidden_size=None,num_layers=None,bias=None,batch_first=None,dropout=None,bidirectional=None)\nParameter description: input_size - The number of expected features in the input x\nConstraints:",
            "constr": {
                "dtype": [
                    "int"
                ],
                "structure": [],
                "shape": [],
                "ndim": [
                    "0"
                ],
                "enum": [],
                "range": [
                    "[0,inf)"
                ]
            },
            "signature": "torch.nn.LSTM(*args,**kwargs,input_size=None,hidden_size=None,num_layers=None,bias=None,batch_first=None,dropout=None,bidirectional=None)",
            "descp": "input_size - The number of expected features in the input x"
        },
        {
            "fname": "torch.max2.yaml",
            "arg": "out",
            "prompt": "Signature: torch.distributed.recv(tensor,src=None,group=<objectobject>,tag=0)\nParameter description: tensor(Tensor) - Tensor to fill with received data.\nConstraints:\ndtype:Null\nstructure:tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.div(input,other,out=None)\nParameter description: out(Tensor, optional) - the output tensor.\nConstraints:\ndtype:Null\nstructure:tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.nn.TransformerDecoder(decoder_layer,num_layers,norm=None)\nParameter description: num_layers - the number of sub-decoder-layers in the decoder (required).\nConstraints:\ndtype:int\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:[0,inf)\n\n###\n\nSignature: torch.nn.quantized.functional.interpolate(input,size=None,scale_factor=None,mode=nearest,align_corners=None)\nParameter description: align_corners(bool, optional) - Geometrically, we consider the pixels of the input and output as squares rather than points. If set to `True`, the input and output tensors are aligned by the center points of their corner pixels, preserving the values at the corner pixels. If set to `False`, the input and output tensors are aligned by the corner points of their corner pixels, and the interpolation uses edge value padding for out-of-boundary values, making this operation independent of input size when `scale_factor` is kept the same. This only has an effect when `mode` is `'bilinear'`. Default: `False`\nConstraints:\ndtype:bool\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.gt(input,other,out=None)\nParameter description: out(Tensor, optional) - the output tensor that must be a BoolTensor\nConstraints:\ndtype:booltensor\nstructure:tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.empty_like(input,dtype=None,layout=None,device=None,requires_grad=False,memory_format=torch.preserve_format)\nParameter description: dtype(`torch.dtype`, optional) - the desired data type of returned Tensor. Default: if `None`, defaults to the dtype of `input`.\nConstraints:\ndtype:torch.dtype\nstructure:Null\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.nn.RNN(*args,**kwargs,input_size=None,hidden_size=None,num_layers=None,nonlinearity=None,bias=None,batch_first=None,dropout=None,bidirectional=None)\nParameter description: hidden_size - The number of features in the hidden state h\nConstraints:\ndtype:int\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:[0,inf)\n\n###\n\nSignature: torch.nn.functional.avg_pool1d(input,kernel_size,stride=None,padding=0,ceil_mode=False,count_include_pad=True)\nParameter description: padding - implicit zero paddings on both sides of the input. Can be a single number or a tuple (padW,). Default: 0\nConstraints:\ndtype:bool;int\nstructure:tuple\nshape:Null\nndim:0;1\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.autograd.grad(outputs,inputs,grad_outputs=None,retain_graph=None,create_graph=False,only_inputs=True,allow_unused=False)\nParameter description: outputs(sequence of Tensor) - outputs of the differentiated function.\nConstraints:\ndtype:Null\nstructure:sequence;tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.repeat_interleave(input,repeats,dim=None)\nParameter description: dim(int, optional) - The dimension along which to repeat values.By default, use the flattened input array, and return a flat outputarray.\nConstraints:\ndtype:int\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.max(input,dim,keepdim=False,out=None)\nParameter description: out(tuple, optional) - the result tuple of two output tensors (max, max_indices)\nConstraints:",
            "constr": {
                "dtype": [],
                "structure": [
                    "tuple",
                    "tensor"
                ],
                "shape": [],
                "ndim": [],
                "enum": [],
                "range": []
            },
            "signature": "torch.max(input,dim,keepdim=False,out=None)",
            "descp": "out(tuple, optional) - the result tuple of two output tensors (max, max_indices)"
        },
        {
            "fname": "torch.atan2.yaml",
            "arg": "out",
            "prompt": "Signature: torch.distributed.recv(tensor,src=None,group=<objectobject>,tag=0)\nParameter description: tensor(Tensor) - Tensor to fill with received data.\nConstraints:\ndtype:Null\nstructure:tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.div(input,other,out=None)\nParameter description: out(Tensor, optional) - the output tensor.\nConstraints:\ndtype:Null\nstructure:tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.nn.TransformerDecoder(decoder_layer,num_layers,norm=None)\nParameter description: num_layers - the number of sub-decoder-layers in the decoder (required).\nConstraints:\ndtype:int\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:[0,inf)\n\n###\n\nSignature: torch.nn.quantized.functional.interpolate(input,size=None,scale_factor=None,mode=nearest,align_corners=None)\nParameter description: align_corners(bool, optional) - Geometrically, we consider the pixels of the input and output as squares rather than points. If set to `True`, the input and output tensors are aligned by the center points of their corner pixels, preserving the values at the corner pixels. If set to `False`, the input and output tensors are aligned by the corner points of their corner pixels, and the interpolation uses edge value padding for out-of-boundary values, making this operation independent of input size when `scale_factor` is kept the same. This only has an effect when `mode` is `'bilinear'`. Default: `False`\nConstraints:\ndtype:bool\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.gt(input,other,out=None)\nParameter description: out(Tensor, optional) - the output tensor that must be a BoolTensor\nConstraints:\ndtype:booltensor\nstructure:tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.empty_like(input,dtype=None,layout=None,device=None,requires_grad=False,memory_format=torch.preserve_format)\nParameter description: dtype(`torch.dtype`, optional) - the desired data type of returned Tensor. Default: if `None`, defaults to the dtype of `input`.\nConstraints:\ndtype:torch.dtype\nstructure:Null\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.nn.RNN(*args,**kwargs,input_size=None,hidden_size=None,num_layers=None,nonlinearity=None,bias=None,batch_first=None,dropout=None,bidirectional=None)\nParameter description: hidden_size - The number of features in the hidden state h\nConstraints:\ndtype:int\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:[0,inf)\n\n###\n\nSignature: torch.nn.functional.avg_pool1d(input,kernel_size,stride=None,padding=0,ceil_mode=False,count_include_pad=True)\nParameter description: padding - implicit zero paddings on both sides of the input. Can be a single number or a tuple (padW,). Default: 0\nConstraints:\ndtype:bool;int\nstructure:tuple\nshape:Null\nndim:0;1\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.autograd.grad(outputs,inputs,grad_outputs=None,retain_graph=None,create_graph=False,only_inputs=True,allow_unused=False)\nParameter description: outputs(sequence of Tensor) - outputs of the differentiated function.\nConstraints:\ndtype:Null\nstructure:sequence;tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.repeat_interleave(input,repeats,dim=None)\nParameter description: dim(int, optional) - The dimension along which to repeat values.By default, use the flattened input array, and return a flat outputarray.\nConstraints:\ndtype:int\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.atan2(input,other,out=None)\nParameter description: out(Tensor, optional) - the output tensor.\nConstraints:",
            "constr": {
                "dtype": [],
                "structure": [
                    "tensor"
                ],
                "shape": [],
                "ndim": [],
                "enum": [],
                "range": []
            },
            "signature": "torch.atan2(input,other,out=None)",
            "descp": "out(Tensor, optional) - the output tensor."
        },
        {
            "fname": "torch.nn.gru.yaml",
            "arg": "dropout",
            "prompt": "Signature: torch.distributed.recv(tensor,src=None,group=<objectobject>,tag=0)\nParameter description: tensor(Tensor) - Tensor to fill with received data.\nConstraints:\ndtype:Null\nstructure:tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.div(input,other,out=None)\nParameter description: out(Tensor, optional) - the output tensor.\nConstraints:\ndtype:Null\nstructure:tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.nn.TransformerDecoder(decoder_layer,num_layers,norm=None)\nParameter description: num_layers - the number of sub-decoder-layers in the decoder (required).\nConstraints:\ndtype:int\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:[0,inf)\n\n###\n\nSignature: torch.nn.quantized.functional.interpolate(input,size=None,scale_factor=None,mode=nearest,align_corners=None)\nParameter description: align_corners(bool, optional) - Geometrically, we consider the pixels of the input and output as squares rather than points. If set to `True`, the input and output tensors are aligned by the center points of their corner pixels, preserving the values at the corner pixels. If set to `False`, the input and output tensors are aligned by the corner points of their corner pixels, and the interpolation uses edge value padding for out-of-boundary values, making this operation independent of input size when `scale_factor` is kept the same. This only has an effect when `mode` is `'bilinear'`. Default: `False`\nConstraints:\ndtype:bool\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.gt(input,other,out=None)\nParameter description: out(Tensor, optional) - the output tensor that must be a BoolTensor\nConstraints:\ndtype:booltensor\nstructure:tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.empty_like(input,dtype=None,layout=None,device=None,requires_grad=False,memory_format=torch.preserve_format)\nParameter description: dtype(`torch.dtype`, optional) - the desired data type of returned Tensor. Default: if `None`, defaults to the dtype of `input`.\nConstraints:\ndtype:torch.dtype\nstructure:Null\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.nn.RNN(*args,**kwargs,input_size=None,hidden_size=None,num_layers=None,nonlinearity=None,bias=None,batch_first=None,dropout=None,bidirectional=None)\nParameter description: hidden_size - The number of features in the hidden state h\nConstraints:\ndtype:int\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:[0,inf)\n\n###\n\nSignature: torch.nn.functional.avg_pool1d(input,kernel_size,stride=None,padding=0,ceil_mode=False,count_include_pad=True)\nParameter description: padding - implicit zero paddings on both sides of the input. Can be a single number or a tuple (padW,). Default: 0\nConstraints:\ndtype:bool;int\nstructure:tuple\nshape:Null\nndim:0;1\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.autograd.grad(outputs,inputs,grad_outputs=None,retain_graph=None,create_graph=False,only_inputs=True,allow_unused=False)\nParameter description: outputs(sequence of Tensor) - outputs of the differentiated function.\nConstraints:\ndtype:Null\nstructure:sequence;tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.repeat_interleave(input,repeats,dim=None)\nParameter description: dim(int, optional) - The dimension along which to repeat values.By default, use the flattened input array, and return a flat outputarray.\nConstraints:\ndtype:int\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.nn.GRU(*args,**kwargs,input_size=None,hidden_size=None,num_layers=None,bias=None,batch_first=None,dropout=None,bidirectional=None)\nParameter description: dropout - If non-zero, introduces a Dropout layer on the outputs of each GRU layer except the last layer, with dropout probability equal to `dropout`. Default: 0\nConstraints:",
            "constr": {
                "dtype": [
                    "int"
                ],
                "structure": [],
                "shape": [],
                "ndim": [
                    "0"
                ],
                "enum": [],
                "range": []
            },
            "signature": "torch.nn.GRU(*args,**kwargs,input_size=None,hidden_size=None,num_layers=None,bias=None,batch_first=None,dropout=None,bidirectional=None)",
            "descp": "dropout - If non-zero, introduces a Dropout layer on the outputs of each GRU layer except the last layer, with dropout probability equal to `dropout`. Default: 0"
        },
        {
            "fname": "torch.nn.layernorm.yaml",
            "arg": "eps",
            "prompt": "Signature: torch.distributed.recv(tensor,src=None,group=<objectobject>,tag=0)\nParameter description: tensor(Tensor) - Tensor to fill with received data.\nConstraints:\ndtype:Null\nstructure:tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.div(input,other,out=None)\nParameter description: out(Tensor, optional) - the output tensor.\nConstraints:\ndtype:Null\nstructure:tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.nn.TransformerDecoder(decoder_layer,num_layers,norm=None)\nParameter description: num_layers - the number of sub-decoder-layers in the decoder (required).\nConstraints:\ndtype:int\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:[0,inf)\n\n###\n\nSignature: torch.nn.quantized.functional.interpolate(input,size=None,scale_factor=None,mode=nearest,align_corners=None)\nParameter description: align_corners(bool, optional) - Geometrically, we consider the pixels of the input and output as squares rather than points. If set to `True`, the input and output tensors are aligned by the center points of their corner pixels, preserving the values at the corner pixels. If set to `False`, the input and output tensors are aligned by the corner points of their corner pixels, and the interpolation uses edge value padding for out-of-boundary values, making this operation independent of input size when `scale_factor` is kept the same. This only has an effect when `mode` is `'bilinear'`. Default: `False`\nConstraints:\ndtype:bool\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.gt(input,other,out=None)\nParameter description: out(Tensor, optional) - the output tensor that must be a BoolTensor\nConstraints:\ndtype:booltensor\nstructure:tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.empty_like(input,dtype=None,layout=None,device=None,requires_grad=False,memory_format=torch.preserve_format)\nParameter description: dtype(`torch.dtype`, optional) - the desired data type of returned Tensor. Default: if `None`, defaults to the dtype of `input`.\nConstraints:\ndtype:torch.dtype\nstructure:Null\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.nn.RNN(*args,**kwargs,input_size=None,hidden_size=None,num_layers=None,nonlinearity=None,bias=None,batch_first=None,dropout=None,bidirectional=None)\nParameter description: hidden_size - The number of features in the hidden state h\nConstraints:\ndtype:int\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:[0,inf)\n\n###\n\nSignature: torch.nn.functional.avg_pool1d(input,kernel_size,stride=None,padding=0,ceil_mode=False,count_include_pad=True)\nParameter description: padding - implicit zero paddings on both sides of the input. Can be a single number or a tuple (padW,). Default: 0\nConstraints:\ndtype:bool;int\nstructure:tuple\nshape:Null\nndim:0;1\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.autograd.grad(outputs,inputs,grad_outputs=None,retain_graph=None,create_graph=False,only_inputs=True,allow_unused=False)\nParameter description: outputs(sequence of Tensor) - outputs of the differentiated function.\nConstraints:\ndtype:Null\nstructure:sequence;tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.repeat_interleave(input,repeats,dim=None)\nParameter description: dim(int, optional) - The dimension along which to repeat values.By default, use the flattened input array, and return a flat outputarray.\nConstraints:\ndtype:int\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.nn.LayerNorm(normalized_shape,eps=1e-05,elementwise_affine=True)\nParameter description: eps - a value added to the denominator for numerical stability. Default: 1e-5\nConstraints:",
            "constr": {
                "dtype": [
                    "numeric",
                    "float",
                    "int"
                ],
                "structure": [],
                "shape": [],
                "ndim": [
                    "0"
                ],
                "enum": [],
                "range": []
            },
            "signature": "torch.nn.LayerNorm(normalized_shape,eps=1e-05,elementwise_affine=True)",
            "descp": "eps - a value added to the denominator for numerical stability. Default: 1e-5"
        },
        {
            "fname": "torch.nn.softmarginloss.yaml",
            "arg": "reduce",
            "prompt": "Signature: torch.distributed.recv(tensor,src=None,group=<objectobject>,tag=0)\nParameter description: tensor(Tensor) - Tensor to fill with received data.\nConstraints:\ndtype:Null\nstructure:tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.div(input,other,out=None)\nParameter description: out(Tensor, optional) - the output tensor.\nConstraints:\ndtype:Null\nstructure:tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.nn.TransformerDecoder(decoder_layer,num_layers,norm=None)\nParameter description: num_layers - the number of sub-decoder-layers in the decoder (required).\nConstraints:\ndtype:int\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:[0,inf)\n\n###\n\nSignature: torch.nn.quantized.functional.interpolate(input,size=None,scale_factor=None,mode=nearest,align_corners=None)\nParameter description: align_corners(bool, optional) - Geometrically, we consider the pixels of the input and output as squares rather than points. If set to `True`, the input and output tensors are aligned by the center points of their corner pixels, preserving the values at the corner pixels. If set to `False`, the input and output tensors are aligned by the corner points of their corner pixels, and the interpolation uses edge value padding for out-of-boundary values, making this operation independent of input size when `scale_factor` is kept the same. This only has an effect when `mode` is `'bilinear'`. Default: `False`\nConstraints:\ndtype:bool\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.gt(input,other,out=None)\nParameter description: out(Tensor, optional) - the output tensor that must be a BoolTensor\nConstraints:\ndtype:booltensor\nstructure:tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.empty_like(input,dtype=None,layout=None,device=None,requires_grad=False,memory_format=torch.preserve_format)\nParameter description: dtype(`torch.dtype`, optional) - the desired data type of returned Tensor. Default: if `None`, defaults to the dtype of `input`.\nConstraints:\ndtype:torch.dtype\nstructure:Null\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.nn.RNN(*args,**kwargs,input_size=None,hidden_size=None,num_layers=None,nonlinearity=None,bias=None,batch_first=None,dropout=None,bidirectional=None)\nParameter description: hidden_size - The number of features in the hidden state h\nConstraints:\ndtype:int\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:[0,inf)\n\n###\n\nSignature: torch.nn.functional.avg_pool1d(input,kernel_size,stride=None,padding=0,ceil_mode=False,count_include_pad=True)\nParameter description: padding - implicit zero paddings on both sides of the input. Can be a single number or a tuple (padW,). Default: 0\nConstraints:\ndtype:bool;int\nstructure:tuple\nshape:Null\nndim:0;1\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.autograd.grad(outputs,inputs,grad_outputs=None,retain_graph=None,create_graph=False,only_inputs=True,allow_unused=False)\nParameter description: outputs(sequence of Tensor) - outputs of the differentiated function.\nConstraints:\ndtype:Null\nstructure:sequence;tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.repeat_interleave(input,repeats,dim=None)\nParameter description: dim(int, optional) - The dimension along which to repeat values.By default, use the flattened input array, and return a flat outputarray.\nConstraints:\ndtype:int\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.nn.SoftMarginLoss(size_average=None,reduce=None,reduction=mean)\nParameter description: reduce(bool, optional) - Deprecated (see `reduction`). By default, the losses are averaged or summed over observations for each minibatch depending on `size_average`. When `reduce` is `False`, returns a loss per batch element instead and ignores `size_average`. Default: `True`\nConstraints:",
            "constr": {
                "dtype": [
                    "bool"
                ],
                "structure": [],
                "shape": [],
                "ndim": [
                    "0"
                ],
                "enum": [],
                "range": []
            },
            "signature": "torch.nn.SoftMarginLoss(size_average=None,reduce=None,reduction=mean)",
            "descp": "reduce(bool, optional) - Deprecated (see `reduction`). By default, the losses are averaged or summed over observations for each minibatch depending on `size_average`. When `reduce` is `False`, returns a loss per batch element instead and ignores `size_average`. Default: `True`"
        },
        {
            "fname": "torch.cuda.set_rng_state.yaml",
            "arg": "device",
            "prompt": "Signature: torch.distributed.recv(tensor,src=None,group=<objectobject>,tag=0)\nParameter description: tensor(Tensor) - Tensor to fill with received data.\nConstraints:\ndtype:Null\nstructure:tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.div(input,other,out=None)\nParameter description: out(Tensor, optional) - the output tensor.\nConstraints:\ndtype:Null\nstructure:tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.nn.TransformerDecoder(decoder_layer,num_layers,norm=None)\nParameter description: num_layers - the number of sub-decoder-layers in the decoder (required).\nConstraints:\ndtype:int\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:[0,inf)\n\n###\n\nSignature: torch.nn.quantized.functional.interpolate(input,size=None,scale_factor=None,mode=nearest,align_corners=None)\nParameter description: align_corners(bool, optional) - Geometrically, we consider the pixels of the input and output as squares rather than points. If set to `True`, the input and output tensors are aligned by the center points of their corner pixels, preserving the values at the corner pixels. If set to `False`, the input and output tensors are aligned by the corner points of their corner pixels, and the interpolation uses edge value padding for out-of-boundary values, making this operation independent of input size when `scale_factor` is kept the same. This only has an effect when `mode` is `'bilinear'`. Default: `False`\nConstraints:\ndtype:bool\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.gt(input,other,out=None)\nParameter description: out(Tensor, optional) - the output tensor that must be a BoolTensor\nConstraints:\ndtype:booltensor\nstructure:tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.empty_like(input,dtype=None,layout=None,device=None,requires_grad=False,memory_format=torch.preserve_format)\nParameter description: dtype(`torch.dtype`, optional) - the desired data type of returned Tensor. Default: if `None`, defaults to the dtype of `input`.\nConstraints:\ndtype:torch.dtype\nstructure:Null\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.nn.RNN(*args,**kwargs,input_size=None,hidden_size=None,num_layers=None,nonlinearity=None,bias=None,batch_first=None,dropout=None,bidirectional=None)\nParameter description: hidden_size - The number of features in the hidden state h\nConstraints:\ndtype:int\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:[0,inf)\n\n###\n\nSignature: torch.nn.functional.avg_pool1d(input,kernel_size,stride=None,padding=0,ceil_mode=False,count_include_pad=True)\nParameter description: padding - implicit zero paddings on both sides of the input. Can be a single number or a tuple (padW,). Default: 0\nConstraints:\ndtype:bool;int\nstructure:tuple\nshape:Null\nndim:0;1\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.autograd.grad(outputs,inputs,grad_outputs=None,retain_graph=None,create_graph=False,only_inputs=True,allow_unused=False)\nParameter description: outputs(sequence of Tensor) - outputs of the differentiated function.\nConstraints:\ndtype:Null\nstructure:sequence;tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.repeat_interleave(input,repeats,dim=None)\nParameter description: dim(int, optional) - The dimension along which to repeat values.By default, use the flattened input array, and return a flat outputarray.\nConstraints:\ndtype:int\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.cuda.set_rng_state(new_state,device=cuda)\nParameter description: device(torch.device or int, optional) - The device to set the RNG state. Default: `'cuda'` (i.e., `torch.device('cuda')`, the current CUDA device).\nConstraints:",
            "constr": {
                "dtype": [
                    "string",
                    "int"
                ],
                "structure": [],
                "shape": [],
                "ndim": [],
                "enum": [
                    "cuda"
                ],
                "range": []
            },
            "signature": "torch.cuda.set_rng_state(new_state,device=cuda)",
            "descp": "device(torch.device or int, optional) - The device to set the RNG state. Default: `'cuda'` (i.e., `torch.device('cuda')`, the current CUDA device)."
        },
        {
            "fname": "torch.jit.load.yaml",
            "arg": "_extra_files",
            "prompt": "Signature: torch.distributed.recv(tensor,src=None,group=<objectobject>,tag=0)\nParameter description: tensor(Tensor) - Tensor to fill with received data.\nConstraints:\ndtype:Null\nstructure:tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.div(input,other,out=None)\nParameter description: out(Tensor, optional) - the output tensor.\nConstraints:\ndtype:Null\nstructure:tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.nn.TransformerDecoder(decoder_layer,num_layers,norm=None)\nParameter description: num_layers - the number of sub-decoder-layers in the decoder (required).\nConstraints:\ndtype:int\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:[0,inf)\n\n###\n\nSignature: torch.nn.quantized.functional.interpolate(input,size=None,scale_factor=None,mode=nearest,align_corners=None)\nParameter description: align_corners(bool, optional) - Geometrically, we consider the pixels of the input and output as squares rather than points. If set to `True`, the input and output tensors are aligned by the center points of their corner pixels, preserving the values at the corner pixels. If set to `False`, the input and output tensors are aligned by the corner points of their corner pixels, and the interpolation uses edge value padding for out-of-boundary values, making this operation independent of input size when `scale_factor` is kept the same. This only has an effect when `mode` is `'bilinear'`. Default: `False`\nConstraints:\ndtype:bool\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.gt(input,other,out=None)\nParameter description: out(Tensor, optional) - the output tensor that must be a BoolTensor\nConstraints:\ndtype:booltensor\nstructure:tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.empty_like(input,dtype=None,layout=None,device=None,requires_grad=False,memory_format=torch.preserve_format)\nParameter description: dtype(`torch.dtype`, optional) - the desired data type of returned Tensor. Default: if `None`, defaults to the dtype of `input`.\nConstraints:\ndtype:torch.dtype\nstructure:Null\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.nn.RNN(*args,**kwargs,input_size=None,hidden_size=None,num_layers=None,nonlinearity=None,bias=None,batch_first=None,dropout=None,bidirectional=None)\nParameter description: hidden_size - The number of features in the hidden state h\nConstraints:\ndtype:int\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:[0,inf)\n\n###\n\nSignature: torch.nn.functional.avg_pool1d(input,kernel_size,stride=None,padding=0,ceil_mode=False,count_include_pad=True)\nParameter description: padding - implicit zero paddings on both sides of the input. Can be a single number or a tuple (padW,). Default: 0\nConstraints:\ndtype:bool;int\nstructure:tuple\nshape:Null\nndim:0;1\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.autograd.grad(outputs,inputs,grad_outputs=None,retain_graph=None,create_graph=False,only_inputs=True,allow_unused=False)\nParameter description: outputs(sequence of Tensor) - outputs of the differentiated function.\nConstraints:\ndtype:Null\nstructure:sequence;tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.repeat_interleave(input,repeats,dim=None)\nParameter description: dim(int, optional) - The dimension along which to repeat values.By default, use the flattened input array, and return a flat outputarray.\nConstraints:\ndtype:int\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.jit.load(f,map_location=None,_extra_files=ExtraFilesMap{})\nParameter description: _extra_files(dictionary of filename to content) - The extra filenames given in the map would be loaded and their content would be stored in the provided map.\nConstraints:",
            "constr": {
                "dtype": [
                    "string"
                ],
                "structure": [
                    "dictionary"
                ],
                "shape": [],
                "ndim": [],
                "enum": [],
                "range": []
            },
            "signature": "torch.jit.load(f,map_location=None,_extra_files=ExtraFilesMap{})",
            "descp": "_extra_files(dictionary of filename to content) - The extra filenames given in the map would be loaded and their content would be stored in the provided map."
        },
        {
            "fname": "torch.nn.functional.adaptive_avg_pool1d.yaml",
            "arg": "output_size",
            "prompt": "Signature: torch.distributed.recv(tensor,src=None,group=<objectobject>,tag=0)\nParameter description: tensor(Tensor) - Tensor to fill with received data.\nConstraints:\ndtype:Null\nstructure:tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.div(input,other,out=None)\nParameter description: out(Tensor, optional) - the output tensor.\nConstraints:\ndtype:Null\nstructure:tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.nn.TransformerDecoder(decoder_layer,num_layers,norm=None)\nParameter description: num_layers - the number of sub-decoder-layers in the decoder (required).\nConstraints:\ndtype:int\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:[0,inf)\n\n###\n\nSignature: torch.nn.quantized.functional.interpolate(input,size=None,scale_factor=None,mode=nearest,align_corners=None)\nParameter description: align_corners(bool, optional) - Geometrically, we consider the pixels of the input and output as squares rather than points. If set to `True`, the input and output tensors are aligned by the center points of their corner pixels, preserving the values at the corner pixels. If set to `False`, the input and output tensors are aligned by the corner points of their corner pixels, and the interpolation uses edge value padding for out-of-boundary values, making this operation independent of input size when `scale_factor` is kept the same. This only has an effect when `mode` is `'bilinear'`. Default: `False`\nConstraints:\ndtype:bool\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.gt(input,other,out=None)\nParameter description: out(Tensor, optional) - the output tensor that must be a BoolTensor\nConstraints:\ndtype:booltensor\nstructure:tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.empty_like(input,dtype=None,layout=None,device=None,requires_grad=False,memory_format=torch.preserve_format)\nParameter description: dtype(`torch.dtype`, optional) - the desired data type of returned Tensor. Default: if `None`, defaults to the dtype of `input`.\nConstraints:\ndtype:torch.dtype\nstructure:Null\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.nn.RNN(*args,**kwargs,input_size=None,hidden_size=None,num_layers=None,nonlinearity=None,bias=None,batch_first=None,dropout=None,bidirectional=None)\nParameter description: hidden_size - The number of features in the hidden state h\nConstraints:\ndtype:int\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:[0,inf)\n\n###\n\nSignature: torch.nn.functional.avg_pool1d(input,kernel_size,stride=None,padding=0,ceil_mode=False,count_include_pad=True)\nParameter description: padding - implicit zero paddings on both sides of the input. Can be a single number or a tuple (padW,). Default: 0\nConstraints:\ndtype:bool;int\nstructure:tuple\nshape:Null\nndim:0;1\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.autograd.grad(outputs,inputs,grad_outputs=None,retain_graph=None,create_graph=False,only_inputs=True,allow_unused=False)\nParameter description: outputs(sequence of Tensor) - outputs of the differentiated function.\nConstraints:\ndtype:Null\nstructure:sequence;tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.repeat_interleave(input,repeats,dim=None)\nParameter description: dim(int, optional) - The dimension along which to repeat values.By default, use the flattened input array, and return a flat outputarray.\nConstraints:\ndtype:int\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.nn.functional.adaptive_avg_pool1d(input,output_size)\nParameter description: output_size - the target output size (single integer)\nConstraints:",
            "constr": {
                "dtype": [
                    "numeric"
                ],
                "structure": [],
                "shape": [],
                "ndim": [
                    "0"
                ],
                "enum": [],
                "range": [
                    "[0,inf)"
                ]
            },
            "signature": "torch.nn.functional.adaptive_avg_pool1d(input,output_size)",
            "descp": "output_size - the target output size (single integer)"
        },
        {
            "fname": "torch.cat.yaml",
            "arg": "tensors",
            "prompt": "Signature: torch.distributed.recv(tensor,src=None,group=<objectobject>,tag=0)\nParameter description: tensor(Tensor) - Tensor to fill with received data.\nConstraints:\ndtype:Null\nstructure:tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.div(input,other,out=None)\nParameter description: out(Tensor, optional) - the output tensor.\nConstraints:\ndtype:Null\nstructure:tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.nn.TransformerDecoder(decoder_layer,num_layers,norm=None)\nParameter description: num_layers - the number of sub-decoder-layers in the decoder (required).\nConstraints:\ndtype:int\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:[0,inf)\n\n###\n\nSignature: torch.nn.quantized.functional.interpolate(input,size=None,scale_factor=None,mode=nearest,align_corners=None)\nParameter description: align_corners(bool, optional) - Geometrically, we consider the pixels of the input and output as squares rather than points. If set to `True`, the input and output tensors are aligned by the center points of their corner pixels, preserving the values at the corner pixels. If set to `False`, the input and output tensors are aligned by the corner points of their corner pixels, and the interpolation uses edge value padding for out-of-boundary values, making this operation independent of input size when `scale_factor` is kept the same. This only has an effect when `mode` is `'bilinear'`. Default: `False`\nConstraints:\ndtype:bool\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.gt(input,other,out=None)\nParameter description: out(Tensor, optional) - the output tensor that must be a BoolTensor\nConstraints:\ndtype:booltensor\nstructure:tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.empty_like(input,dtype=None,layout=None,device=None,requires_grad=False,memory_format=torch.preserve_format)\nParameter description: dtype(`torch.dtype`, optional) - the desired data type of returned Tensor. Default: if `None`, defaults to the dtype of `input`.\nConstraints:\ndtype:torch.dtype\nstructure:Null\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.nn.RNN(*args,**kwargs,input_size=None,hidden_size=None,num_layers=None,nonlinearity=None,bias=None,batch_first=None,dropout=None,bidirectional=None)\nParameter description: hidden_size - The number of features in the hidden state h\nConstraints:\ndtype:int\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:[0,inf)\n\n###\n\nSignature: torch.nn.functional.avg_pool1d(input,kernel_size,stride=None,padding=0,ceil_mode=False,count_include_pad=True)\nParameter description: padding - implicit zero paddings on both sides of the input. Can be a single number or a tuple (padW,). Default: 0\nConstraints:\ndtype:bool;int\nstructure:tuple\nshape:Null\nndim:0;1\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.autograd.grad(outputs,inputs,grad_outputs=None,retain_graph=None,create_graph=False,only_inputs=True,allow_unused=False)\nParameter description: outputs(sequence of Tensor) - outputs of the differentiated function.\nConstraints:\ndtype:Null\nstructure:sequence;tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.repeat_interleave(input,repeats,dim=None)\nParameter description: dim(int, optional) - The dimension along which to repeat values.By default, use the flattened input array, and return a flat outputarray.\nConstraints:\ndtype:int\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.cat(tensors,dim=0,out=None)\nParameter description: tensors(sequence of Tensors) - any python sequence of tensors of the same type. Non-empty tensors provided must have the same shape, except in the cat dimension.\nConstraints:",
            "constr": {
                "dtype": [],
                "structure": [
                    "sequence",
                    "tensor"
                ],
                "shape": [],
                "ndim": [],
                "enum": [],
                "range": []
            },
            "signature": "torch.cat(tensors,dim=0,out=None)",
            "descp": "tensors(sequence of Tensors) - any python sequence of tensors of the same type. Non-empty tensors provided must have the same shape, except in the cat dimension."
        },
        {
            "fname": "torch.digamma.yaml",
            "arg": "input",
            "prompt": "Signature: torch.distributed.recv(tensor,src=None,group=<objectobject>,tag=0)\nParameter description: tensor(Tensor) - Tensor to fill with received data.\nConstraints:\ndtype:Null\nstructure:tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.div(input,other,out=None)\nParameter description: out(Tensor, optional) - the output tensor.\nConstraints:\ndtype:Null\nstructure:tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.nn.TransformerDecoder(decoder_layer,num_layers,norm=None)\nParameter description: num_layers - the number of sub-decoder-layers in the decoder (required).\nConstraints:\ndtype:int\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:[0,inf)\n\n###\n\nSignature: torch.nn.quantized.functional.interpolate(input,size=None,scale_factor=None,mode=nearest,align_corners=None)\nParameter description: align_corners(bool, optional) - Geometrically, we consider the pixels of the input and output as squares rather than points. If set to `True`, the input and output tensors are aligned by the center points of their corner pixels, preserving the values at the corner pixels. If set to `False`, the input and output tensors are aligned by the corner points of their corner pixels, and the interpolation uses edge value padding for out-of-boundary values, making this operation independent of input size when `scale_factor` is kept the same. This only has an effect when `mode` is `'bilinear'`. Default: `False`\nConstraints:\ndtype:bool\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.gt(input,other,out=None)\nParameter description: out(Tensor, optional) - the output tensor that must be a BoolTensor\nConstraints:\ndtype:booltensor\nstructure:tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.empty_like(input,dtype=None,layout=None,device=None,requires_grad=False,memory_format=torch.preserve_format)\nParameter description: dtype(`torch.dtype`, optional) - the desired data type of returned Tensor. Default: if `None`, defaults to the dtype of `input`.\nConstraints:\ndtype:torch.dtype\nstructure:Null\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.nn.RNN(*args,**kwargs,input_size=None,hidden_size=None,num_layers=None,nonlinearity=None,bias=None,batch_first=None,dropout=None,bidirectional=None)\nParameter description: hidden_size - The number of features in the hidden state h\nConstraints:\ndtype:int\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:[0,inf)\n\n###\n\nSignature: torch.nn.functional.avg_pool1d(input,kernel_size,stride=None,padding=0,ceil_mode=False,count_include_pad=True)\nParameter description: padding - implicit zero paddings on both sides of the input. Can be a single number or a tuple (padW,). Default: 0\nConstraints:\ndtype:bool;int\nstructure:tuple\nshape:Null\nndim:0;1\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.autograd.grad(outputs,inputs,grad_outputs=None,retain_graph=None,create_graph=False,only_inputs=True,allow_unused=False)\nParameter description: outputs(sequence of Tensor) - outputs of the differentiated function.\nConstraints:\ndtype:Null\nstructure:sequence;tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.repeat_interleave(input,repeats,dim=None)\nParameter description: dim(int, optional) - The dimension along which to repeat values.By default, use the flattened input array, and return a flat outputarray.\nConstraints:\ndtype:int\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.digamma(input,out=None)\nParameter description: input(Tensor) - the tensor to compute the digamma function on\nConstraints:",
            "constr": {
                "dtype": [],
                "structure": [
                    "tensor"
                ],
                "shape": [],
                "ndim": [],
                "enum": [],
                "range": []
            },
            "signature": "torch.digamma(input,out=None)",
            "descp": "input(Tensor) - the tensor to compute the digamma function on"
        },
        {
            "fname": "torch.svd.yaml",
            "arg": "some",
            "prompt": "Signature: torch.distributed.recv(tensor,src=None,group=<objectobject>,tag=0)\nParameter description: tensor(Tensor) - Tensor to fill with received data.\nConstraints:\ndtype:Null\nstructure:tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.div(input,other,out=None)\nParameter description: out(Tensor, optional) - the output tensor.\nConstraints:\ndtype:Null\nstructure:tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.nn.TransformerDecoder(decoder_layer,num_layers,norm=None)\nParameter description: num_layers - the number of sub-decoder-layers in the decoder (required).\nConstraints:\ndtype:int\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:[0,inf)\n\n###\n\nSignature: torch.nn.quantized.functional.interpolate(input,size=None,scale_factor=None,mode=nearest,align_corners=None)\nParameter description: align_corners(bool, optional) - Geometrically, we consider the pixels of the input and output as squares rather than points. If set to `True`, the input and output tensors are aligned by the center points of their corner pixels, preserving the values at the corner pixels. If set to `False`, the input and output tensors are aligned by the corner points of their corner pixels, and the interpolation uses edge value padding for out-of-boundary values, making this operation independent of input size when `scale_factor` is kept the same. This only has an effect when `mode` is `'bilinear'`. Default: `False`\nConstraints:\ndtype:bool\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.gt(input,other,out=None)\nParameter description: out(Tensor, optional) - the output tensor that must be a BoolTensor\nConstraints:\ndtype:booltensor\nstructure:tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.empty_like(input,dtype=None,layout=None,device=None,requires_grad=False,memory_format=torch.preserve_format)\nParameter description: dtype(`torch.dtype`, optional) - the desired data type of returned Tensor. Default: if `None`, defaults to the dtype of `input`.\nConstraints:\ndtype:torch.dtype\nstructure:Null\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.nn.RNN(*args,**kwargs,input_size=None,hidden_size=None,num_layers=None,nonlinearity=None,bias=None,batch_first=None,dropout=None,bidirectional=None)\nParameter description: hidden_size - The number of features in the hidden state h\nConstraints:\ndtype:int\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:[0,inf)\n\n###\n\nSignature: torch.nn.functional.avg_pool1d(input,kernel_size,stride=None,padding=0,ceil_mode=False,count_include_pad=True)\nParameter description: padding - implicit zero paddings on both sides of the input. Can be a single number or a tuple (padW,). Default: 0\nConstraints:\ndtype:bool;int\nstructure:tuple\nshape:Null\nndim:0;1\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.autograd.grad(outputs,inputs,grad_outputs=None,retain_graph=None,create_graph=False,only_inputs=True,allow_unused=False)\nParameter description: outputs(sequence of Tensor) - outputs of the differentiated function.\nConstraints:\ndtype:Null\nstructure:sequence;tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.repeat_interleave(input,repeats,dim=None)\nParameter description: dim(int, optional) - The dimension along which to repeat values.By default, use the flattened input array, and return a flat outputarray.\nConstraints:\ndtype:int\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.svd(input,some=True,compute_uv=True,out=None)\nParameter description: some(bool, optional) - controls the shape of returned U and V\nConstraints:",
            "constr": {
                "dtype": [
                    "bool"
                ],
                "structure": [],
                "shape": [],
                "ndim": [
                    "0"
                ],
                "enum": [],
                "range": []
            },
            "signature": "torch.svd(input,some=True,compute_uv=True,out=None)",
            "descp": "some(bool, optional) - controls the shape of returned U and V"
        },
        {
            "fname": "torch.nn.functional.kl_div.yaml",
            "arg": "reduction",
            "prompt": "Signature: torch.distributed.recv(tensor,src=None,group=<objectobject>,tag=0)\nParameter description: tensor(Tensor) - Tensor to fill with received data.\nConstraints:\ndtype:Null\nstructure:tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.div(input,other,out=None)\nParameter description: out(Tensor, optional) - the output tensor.\nConstraints:\ndtype:Null\nstructure:tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.nn.TransformerDecoder(decoder_layer,num_layers,norm=None)\nParameter description: num_layers - the number of sub-decoder-layers in the decoder (required).\nConstraints:\ndtype:int\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:[0,inf)\n\n###\n\nSignature: torch.nn.quantized.functional.interpolate(input,size=None,scale_factor=None,mode=nearest,align_corners=None)\nParameter description: align_corners(bool, optional) - Geometrically, we consider the pixels of the input and output as squares rather than points. If set to `True`, the input and output tensors are aligned by the center points of their corner pixels, preserving the values at the corner pixels. If set to `False`, the input and output tensors are aligned by the corner points of their corner pixels, and the interpolation uses edge value padding for out-of-boundary values, making this operation independent of input size when `scale_factor` is kept the same. This only has an effect when `mode` is `'bilinear'`. Default: `False`\nConstraints:\ndtype:bool\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.gt(input,other,out=None)\nParameter description: out(Tensor, optional) - the output tensor that must be a BoolTensor\nConstraints:\ndtype:booltensor\nstructure:tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.empty_like(input,dtype=None,layout=None,device=None,requires_grad=False,memory_format=torch.preserve_format)\nParameter description: dtype(`torch.dtype`, optional) - the desired data type of returned Tensor. Default: if `None`, defaults to the dtype of `input`.\nConstraints:\ndtype:torch.dtype\nstructure:Null\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.nn.RNN(*args,**kwargs,input_size=None,hidden_size=None,num_layers=None,nonlinearity=None,bias=None,batch_first=None,dropout=None,bidirectional=None)\nParameter description: hidden_size - The number of features in the hidden state h\nConstraints:\ndtype:int\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:[0,inf)\n\n###\n\nSignature: torch.nn.functional.avg_pool1d(input,kernel_size,stride=None,padding=0,ceil_mode=False,count_include_pad=True)\nParameter description: padding - implicit zero paddings on both sides of the input. Can be a single number or a tuple (padW,). Default: 0\nConstraints:\ndtype:bool;int\nstructure:tuple\nshape:Null\nndim:0;1\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.autograd.grad(outputs,inputs,grad_outputs=None,retain_graph=None,create_graph=False,only_inputs=True,allow_unused=False)\nParameter description: outputs(sequence of Tensor) - outputs of the differentiated function.\nConstraints:\ndtype:Null\nstructure:sequence;tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.repeat_interleave(input,repeats,dim=None)\nParameter description: dim(int, optional) - The dimension along which to repeat values.By default, use the flattened input array, and return a flat outputarray.\nConstraints:\ndtype:int\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.nn.functional.kl_div(input,target,size_average=None,reduce=None,reduction=mean)\nParameter description: reduction(string, optional) - Specifies the reduction to apply to the output: `'none'` | `'batchmean'` | `'sum'` | `'mean'`. `'none'`: no reduction will be applied `'batchmean'`: the sum of the output will be divided by the batchsize `'sum'`: the output will be summed `'mean'`: the output will be divided by the number of elements in the output Default: `'mean'`\nConstraints:",
            "constr": {
                "dtype": [
                    "string"
                ],
                "structure": [],
                "shape": [],
                "ndim": [],
                "enum": [
                    "sum",
                    "batchmean",
                    "mean",
                    "none"
                ],
                "range": []
            },
            "signature": "torch.nn.functional.kl_div(input,target,size_average=None,reduce=None,reduction=mean)",
            "descp": "reduction(string, optional) - Specifies the reduction to apply to the output: `'none'` | `'batchmean'` | `'sum'` | `'mean'`. `'none'`: no reduction will be applied `'batchmean'`: the sum of the output will be divided by the batchsize `'sum'`: the output will be summed `'mean'`: the output will be divided by the number of elements in the output Default: `'mean'`"
        },
        {
            "fname": "torch.log.yaml",
            "arg": "out",
            "prompt": "Signature: torch.distributed.recv(tensor,src=None,group=<objectobject>,tag=0)\nParameter description: tensor(Tensor) - Tensor to fill with received data.\nConstraints:\ndtype:Null\nstructure:tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.div(input,other,out=None)\nParameter description: out(Tensor, optional) - the output tensor.\nConstraints:\ndtype:Null\nstructure:tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.nn.TransformerDecoder(decoder_layer,num_layers,norm=None)\nParameter description: num_layers - the number of sub-decoder-layers in the decoder (required).\nConstraints:\ndtype:int\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:[0,inf)\n\n###\n\nSignature: torch.nn.quantized.functional.interpolate(input,size=None,scale_factor=None,mode=nearest,align_corners=None)\nParameter description: align_corners(bool, optional) - Geometrically, we consider the pixels of the input and output as squares rather than points. If set to `True`, the input and output tensors are aligned by the center points of their corner pixels, preserving the values at the corner pixels. If set to `False`, the input and output tensors are aligned by the corner points of their corner pixels, and the interpolation uses edge value padding for out-of-boundary values, making this operation independent of input size when `scale_factor` is kept the same. This only has an effect when `mode` is `'bilinear'`. Default: `False`\nConstraints:\ndtype:bool\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.gt(input,other,out=None)\nParameter description: out(Tensor, optional) - the output tensor that must be a BoolTensor\nConstraints:\ndtype:booltensor\nstructure:tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.empty_like(input,dtype=None,layout=None,device=None,requires_grad=False,memory_format=torch.preserve_format)\nParameter description: dtype(`torch.dtype`, optional) - the desired data type of returned Tensor. Default: if `None`, defaults to the dtype of `input`.\nConstraints:\ndtype:torch.dtype\nstructure:Null\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.nn.RNN(*args,**kwargs,input_size=None,hidden_size=None,num_layers=None,nonlinearity=None,bias=None,batch_first=None,dropout=None,bidirectional=None)\nParameter description: hidden_size - The number of features in the hidden state h\nConstraints:\ndtype:int\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:[0,inf)\n\n###\n\nSignature: torch.nn.functional.avg_pool1d(input,kernel_size,stride=None,padding=0,ceil_mode=False,count_include_pad=True)\nParameter description: padding - implicit zero paddings on both sides of the input. Can be a single number or a tuple (padW,). Default: 0\nConstraints:\ndtype:bool;int\nstructure:tuple\nshape:Null\nndim:0;1\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.autograd.grad(outputs,inputs,grad_outputs=None,retain_graph=None,create_graph=False,only_inputs=True,allow_unused=False)\nParameter description: outputs(sequence of Tensor) - outputs of the differentiated function.\nConstraints:\ndtype:Null\nstructure:sequence;tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.repeat_interleave(input,repeats,dim=None)\nParameter description: dim(int, optional) - The dimension along which to repeat values.By default, use the flattened input array, and return a flat outputarray.\nConstraints:\ndtype:int\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.log(input,out=None)\nParameter description: out(Tensor, optional) - the output tensor.\nConstraints:",
            "constr": {
                "dtype": [],
                "structure": [
                    "tensor"
                ],
                "shape": [],
                "ndim": [],
                "enum": [],
                "range": []
            },
            "signature": "torch.log(input,out=None)",
            "descp": "out(Tensor, optional) - the output tensor."
        },
        {
            "fname": "torch.nn.utils.rnn.pack_padded_sequence.yaml",
            "arg": "enforce_sorted",
            "prompt": "Signature: torch.distributed.recv(tensor,src=None,group=<objectobject>,tag=0)\nParameter description: tensor(Tensor) - Tensor to fill with received data.\nConstraints:\ndtype:Null\nstructure:tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.div(input,other,out=None)\nParameter description: out(Tensor, optional) - the output tensor.\nConstraints:\ndtype:Null\nstructure:tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.nn.TransformerDecoder(decoder_layer,num_layers,norm=None)\nParameter description: num_layers - the number of sub-decoder-layers in the decoder (required).\nConstraints:\ndtype:int\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:[0,inf)\n\n###\n\nSignature: torch.nn.quantized.functional.interpolate(input,size=None,scale_factor=None,mode=nearest,align_corners=None)\nParameter description: align_corners(bool, optional) - Geometrically, we consider the pixels of the input and output as squares rather than points. If set to `True`, the input and output tensors are aligned by the center points of their corner pixels, preserving the values at the corner pixels. If set to `False`, the input and output tensors are aligned by the corner points of their corner pixels, and the interpolation uses edge value padding for out-of-boundary values, making this operation independent of input size when `scale_factor` is kept the same. This only has an effect when `mode` is `'bilinear'`. Default: `False`\nConstraints:\ndtype:bool\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.gt(input,other,out=None)\nParameter description: out(Tensor, optional) - the output tensor that must be a BoolTensor\nConstraints:\ndtype:booltensor\nstructure:tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.empty_like(input,dtype=None,layout=None,device=None,requires_grad=False,memory_format=torch.preserve_format)\nParameter description: dtype(`torch.dtype`, optional) - the desired data type of returned Tensor. Default: if `None`, defaults to the dtype of `input`.\nConstraints:\ndtype:torch.dtype\nstructure:Null\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.nn.RNN(*args,**kwargs,input_size=None,hidden_size=None,num_layers=None,nonlinearity=None,bias=None,batch_first=None,dropout=None,bidirectional=None)\nParameter description: hidden_size - The number of features in the hidden state h\nConstraints:\ndtype:int\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:[0,inf)\n\n###\n\nSignature: torch.nn.functional.avg_pool1d(input,kernel_size,stride=None,padding=0,ceil_mode=False,count_include_pad=True)\nParameter description: padding - implicit zero paddings on both sides of the input. Can be a single number or a tuple (padW,). Default: 0\nConstraints:\ndtype:bool;int\nstructure:tuple\nshape:Null\nndim:0;1\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.autograd.grad(outputs,inputs,grad_outputs=None,retain_graph=None,create_graph=False,only_inputs=True,allow_unused=False)\nParameter description: outputs(sequence of Tensor) - outputs of the differentiated function.\nConstraints:\ndtype:Null\nstructure:sequence;tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.repeat_interleave(input,repeats,dim=None)\nParameter description: dim(int, optional) - The dimension along which to repeat values.By default, use the flattened input array, and return a flat outputarray.\nConstraints:\ndtype:int\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.nn.utils.rnn.pack_padded_sequence(input,lengths,batch_first=False,enforce_sorted=True)\nParameter description: enforce_sorted(bool, optional) - if `True`, the input is expected to contain sequences sorted by length in a decreasing order. If `False`, the input will get sorted unconditionally. Default: `True`.\nConstraints:",
            "constr": {
                "dtype": [
                    "bool"
                ],
                "structure": [],
                "shape": [],
                "ndim": [
                    "0"
                ],
                "enum": [],
                "range": []
            },
            "signature": "torch.nn.utils.rnn.pack_padded_sequence(input,lengths,batch_first=False,enforce_sorted=True)",
            "descp": "enforce_sorted(bool, optional) - if `True`, the input is expected to contain sequences sorted by length in a decreasing order. If `False`, the input will get sorted unconditionally. Default: `True`."
        },
        {
            "fname": "torch.nn.batchnorm1d.yaml",
            "arg": "affine",
            "prompt": "Signature: torch.distributed.recv(tensor,src=None,group=<objectobject>,tag=0)\nParameter description: tensor(Tensor) - Tensor to fill with received data.\nConstraints:\ndtype:Null\nstructure:tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.div(input,other,out=None)\nParameter description: out(Tensor, optional) - the output tensor.\nConstraints:\ndtype:Null\nstructure:tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.nn.TransformerDecoder(decoder_layer,num_layers,norm=None)\nParameter description: num_layers - the number of sub-decoder-layers in the decoder (required).\nConstraints:\ndtype:int\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:[0,inf)\n\n###\n\nSignature: torch.nn.quantized.functional.interpolate(input,size=None,scale_factor=None,mode=nearest,align_corners=None)\nParameter description: align_corners(bool, optional) - Geometrically, we consider the pixels of the input and output as squares rather than points. If set to `True`, the input and output tensors are aligned by the center points of their corner pixels, preserving the values at the corner pixels. If set to `False`, the input and output tensors are aligned by the corner points of their corner pixels, and the interpolation uses edge value padding for out-of-boundary values, making this operation independent of input size when `scale_factor` is kept the same. This only has an effect when `mode` is `'bilinear'`. Default: `False`\nConstraints:\ndtype:bool\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.gt(input,other,out=None)\nParameter description: out(Tensor, optional) - the output tensor that must be a BoolTensor\nConstraints:\ndtype:booltensor\nstructure:tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.empty_like(input,dtype=None,layout=None,device=None,requires_grad=False,memory_format=torch.preserve_format)\nParameter description: dtype(`torch.dtype`, optional) - the desired data type of returned Tensor. Default: if `None`, defaults to the dtype of `input`.\nConstraints:\ndtype:torch.dtype\nstructure:Null\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.nn.RNN(*args,**kwargs,input_size=None,hidden_size=None,num_layers=None,nonlinearity=None,bias=None,batch_first=None,dropout=None,bidirectional=None)\nParameter description: hidden_size - The number of features in the hidden state h\nConstraints:\ndtype:int\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:[0,inf)\n\n###\n\nSignature: torch.nn.functional.avg_pool1d(input,kernel_size,stride=None,padding=0,ceil_mode=False,count_include_pad=True)\nParameter description: padding - implicit zero paddings on both sides of the input. Can be a single number or a tuple (padW,). Default: 0\nConstraints:\ndtype:bool;int\nstructure:tuple\nshape:Null\nndim:0;1\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.autograd.grad(outputs,inputs,grad_outputs=None,retain_graph=None,create_graph=False,only_inputs=True,allow_unused=False)\nParameter description: outputs(sequence of Tensor) - outputs of the differentiated function.\nConstraints:\ndtype:Null\nstructure:sequence;tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.repeat_interleave(input,repeats,dim=None)\nParameter description: dim(int, optional) - The dimension along which to repeat values.By default, use the flattened input array, and return a flat outputarray.\nConstraints:\ndtype:int\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.nn.BatchNorm1d(num_features,eps=1e-05,momentum=0.1,affine=True,track_running_stats=True)\nParameter description: affine - a boolean value that when set to `True`, this module has learnable affine parameters. Default: `True`\nConstraints:",
            "constr": {
                "dtype": [
                    "boolean",
                    "bool"
                ],
                "structure": [],
                "shape": [],
                "ndim": [
                    "0"
                ],
                "enum": [],
                "range": []
            },
            "signature": "torch.nn.BatchNorm1d(num_features,eps=1e-05,momentum=0.1,affine=True,track_running_stats=True)",
            "descp": "affine - a boolean value that when set to `True`, this module has learnable affine parameters. Default: `True`"
        },
        {
            "fname": "torch.logical_not.yaml",
            "arg": "out",
            "prompt": "Signature: torch.distributed.recv(tensor,src=None,group=<objectobject>,tag=0)\nParameter description: tensor(Tensor) - Tensor to fill with received data.\nConstraints:\ndtype:Null\nstructure:tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.div(input,other,out=None)\nParameter description: out(Tensor, optional) - the output tensor.\nConstraints:\ndtype:Null\nstructure:tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.nn.TransformerDecoder(decoder_layer,num_layers,norm=None)\nParameter description: num_layers - the number of sub-decoder-layers in the decoder (required).\nConstraints:\ndtype:int\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:[0,inf)\n\n###\n\nSignature: torch.nn.quantized.functional.interpolate(input,size=None,scale_factor=None,mode=nearest,align_corners=None)\nParameter description: align_corners(bool, optional) - Geometrically, we consider the pixels of the input and output as squares rather than points. If set to `True`, the input and output tensors are aligned by the center points of their corner pixels, preserving the values at the corner pixels. If set to `False`, the input and output tensors are aligned by the corner points of their corner pixels, and the interpolation uses edge value padding for out-of-boundary values, making this operation independent of input size when `scale_factor` is kept the same. This only has an effect when `mode` is `'bilinear'`. Default: `False`\nConstraints:\ndtype:bool\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.gt(input,other,out=None)\nParameter description: out(Tensor, optional) - the output tensor that must be a BoolTensor\nConstraints:\ndtype:booltensor\nstructure:tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.empty_like(input,dtype=None,layout=None,device=None,requires_grad=False,memory_format=torch.preserve_format)\nParameter description: dtype(`torch.dtype`, optional) - the desired data type of returned Tensor. Default: if `None`, defaults to the dtype of `input`.\nConstraints:\ndtype:torch.dtype\nstructure:Null\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.nn.RNN(*args,**kwargs,input_size=None,hidden_size=None,num_layers=None,nonlinearity=None,bias=None,batch_first=None,dropout=None,bidirectional=None)\nParameter description: hidden_size - The number of features in the hidden state h\nConstraints:\ndtype:int\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:[0,inf)\n\n###\n\nSignature: torch.nn.functional.avg_pool1d(input,kernel_size,stride=None,padding=0,ceil_mode=False,count_include_pad=True)\nParameter description: padding - implicit zero paddings on both sides of the input. Can be a single number or a tuple (padW,). Default: 0\nConstraints:\ndtype:bool;int\nstructure:tuple\nshape:Null\nndim:0;1\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.autograd.grad(outputs,inputs,grad_outputs=None,retain_graph=None,create_graph=False,only_inputs=True,allow_unused=False)\nParameter description: outputs(sequence of Tensor) - outputs of the differentiated function.\nConstraints:\ndtype:Null\nstructure:sequence;tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.repeat_interleave(input,repeats,dim=None)\nParameter description: dim(int, optional) - The dimension along which to repeat values.By default, use the flattened input array, and return a flat outputarray.\nConstraints:\ndtype:int\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.logical_not(input,out=None)\nParameter description: out(Tensor, optional) - the output tensor.\nConstraints:",
            "constr": {
                "dtype": [],
                "structure": [
                    "tensor"
                ],
                "shape": [],
                "ndim": [],
                "enum": [],
                "range": []
            },
            "signature": "torch.logical_not(input,out=None)",
            "descp": "out(Tensor, optional) - the output tensor."
        },
        {
            "fname": "torch.utils.checkpoint.checkpoint_sequential.yaml",
            "arg": "functions",
            "prompt": "Signature: torch.distributed.recv(tensor,src=None,group=<objectobject>,tag=0)\nParameter description: tensor(Tensor) - Tensor to fill with received data.\nConstraints:\ndtype:Null\nstructure:tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.div(input,other,out=None)\nParameter description: out(Tensor, optional) - the output tensor.\nConstraints:\ndtype:Null\nstructure:tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.nn.TransformerDecoder(decoder_layer,num_layers,norm=None)\nParameter description: num_layers - the number of sub-decoder-layers in the decoder (required).\nConstraints:\ndtype:int\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:[0,inf)\n\n###\n\nSignature: torch.nn.quantized.functional.interpolate(input,size=None,scale_factor=None,mode=nearest,align_corners=None)\nParameter description: align_corners(bool, optional) - Geometrically, we consider the pixels of the input and output as squares rather than points. If set to `True`, the input and output tensors are aligned by the center points of their corner pixels, preserving the values at the corner pixels. If set to `False`, the input and output tensors are aligned by the corner points of their corner pixels, and the interpolation uses edge value padding for out-of-boundary values, making this operation independent of input size when `scale_factor` is kept the same. This only has an effect when `mode` is `'bilinear'`. Default: `False`\nConstraints:\ndtype:bool\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.gt(input,other,out=None)\nParameter description: out(Tensor, optional) - the output tensor that must be a BoolTensor\nConstraints:\ndtype:booltensor\nstructure:tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.empty_like(input,dtype=None,layout=None,device=None,requires_grad=False,memory_format=torch.preserve_format)\nParameter description: dtype(`torch.dtype`, optional) - the desired data type of returned Tensor. Default: if `None`, defaults to the dtype of `input`.\nConstraints:\ndtype:torch.dtype\nstructure:Null\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.nn.RNN(*args,**kwargs,input_size=None,hidden_size=None,num_layers=None,nonlinearity=None,bias=None,batch_first=None,dropout=None,bidirectional=None)\nParameter description: hidden_size - The number of features in the hidden state h\nConstraints:\ndtype:int\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:[0,inf)\n\n###\n\nSignature: torch.nn.functional.avg_pool1d(input,kernel_size,stride=None,padding=0,ceil_mode=False,count_include_pad=True)\nParameter description: padding - implicit zero paddings on both sides of the input. Can be a single number or a tuple (padW,). Default: 0\nConstraints:\ndtype:bool;int\nstructure:tuple\nshape:Null\nndim:0;1\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.autograd.grad(outputs,inputs,grad_outputs=None,retain_graph=None,create_graph=False,only_inputs=True,allow_unused=False)\nParameter description: outputs(sequence of Tensor) - outputs of the differentiated function.\nConstraints:\ndtype:Null\nstructure:sequence;tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.repeat_interleave(input,repeats,dim=None)\nParameter description: dim(int, optional) - The dimension along which to repeat values.By default, use the flattened input array, and return a flat outputarray.\nConstraints:\ndtype:int\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.utils.checkpoint.checkpoint_sequential(functions,segments,input,**kwargs,preserve_rng_state=None)\nParameter description: functions - A `torch.nn.Sequential` or the list of modules or functions (comprising the model) to run sequentially.\nConstraints:",
            "constr": {
                "dtype": [],
                "structure": [
                    "list"
                ],
                "shape": [],
                "ndim": [],
                "enum": [],
                "range": []
            },
            "signature": "torch.utils.checkpoint.checkpoint_sequential(functions,segments,input,**kwargs,preserve_rng_state=None)",
            "descp": "functions - A `torch.nn.Sequential` or the list of modules or functions (comprising the model) to run sequentially."
        },
        {
            "fname": "torch.logical_and.yaml",
            "arg": "input",
            "prompt": "Signature: torch.distributed.recv(tensor,src=None,group=<objectobject>,tag=0)\nParameter description: tensor(Tensor) - Tensor to fill with received data.\nConstraints:\ndtype:Null\nstructure:tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.div(input,other,out=None)\nParameter description: out(Tensor, optional) - the output tensor.\nConstraints:\ndtype:Null\nstructure:tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.nn.TransformerDecoder(decoder_layer,num_layers,norm=None)\nParameter description: num_layers - the number of sub-decoder-layers in the decoder (required).\nConstraints:\ndtype:int\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:[0,inf)\n\n###\n\nSignature: torch.nn.quantized.functional.interpolate(input,size=None,scale_factor=None,mode=nearest,align_corners=None)\nParameter description: align_corners(bool, optional) - Geometrically, we consider the pixels of the input and output as squares rather than points. If set to `True`, the input and output tensors are aligned by the center points of their corner pixels, preserving the values at the corner pixels. If set to `False`, the input and output tensors are aligned by the corner points of their corner pixels, and the interpolation uses edge value padding for out-of-boundary values, making this operation independent of input size when `scale_factor` is kept the same. This only has an effect when `mode` is `'bilinear'`. Default: `False`\nConstraints:\ndtype:bool\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.gt(input,other,out=None)\nParameter description: out(Tensor, optional) - the output tensor that must be a BoolTensor\nConstraints:\ndtype:booltensor\nstructure:tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.empty_like(input,dtype=None,layout=None,device=None,requires_grad=False,memory_format=torch.preserve_format)\nParameter description: dtype(`torch.dtype`, optional) - the desired data type of returned Tensor. Default: if `None`, defaults to the dtype of `input`.\nConstraints:\ndtype:torch.dtype\nstructure:Null\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.nn.RNN(*args,**kwargs,input_size=None,hidden_size=None,num_layers=None,nonlinearity=None,bias=None,batch_first=None,dropout=None,bidirectional=None)\nParameter description: hidden_size - The number of features in the hidden state h\nConstraints:\ndtype:int\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:[0,inf)\n\n###\n\nSignature: torch.nn.functional.avg_pool1d(input,kernel_size,stride=None,padding=0,ceil_mode=False,count_include_pad=True)\nParameter description: padding - implicit zero paddings on both sides of the input. Can be a single number or a tuple (padW,). Default: 0\nConstraints:\ndtype:bool;int\nstructure:tuple\nshape:Null\nndim:0;1\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.autograd.grad(outputs,inputs,grad_outputs=None,retain_graph=None,create_graph=False,only_inputs=True,allow_unused=False)\nParameter description: outputs(sequence of Tensor) - outputs of the differentiated function.\nConstraints:\ndtype:Null\nstructure:sequence;tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.repeat_interleave(input,repeats,dim=None)\nParameter description: dim(int, optional) - The dimension along which to repeat values.By default, use the flattened input array, and return a flat outputarray.\nConstraints:\ndtype:int\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.logical_and(input,other,out=None)\nParameter description: input(Tensor) - the input tensor.\nConstraints:",
            "constr": {
                "dtype": [],
                "structure": [
                    "tensor"
                ],
                "shape": [],
                "ndim": [],
                "enum": [],
                "range": []
            },
            "signature": "torch.logical_and(input,other,out=None)",
            "descp": "input(Tensor) - the input tensor."
        },
        {
            "fname": "torch.mm.yaml",
            "arg": "out",
            "prompt": "Signature: torch.distributed.recv(tensor,src=None,group=<objectobject>,tag=0)\nParameter description: tensor(Tensor) - Tensor to fill with received data.\nConstraints:\ndtype:Null\nstructure:tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.div(input,other,out=None)\nParameter description: out(Tensor, optional) - the output tensor.\nConstraints:\ndtype:Null\nstructure:tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.nn.TransformerDecoder(decoder_layer,num_layers,norm=None)\nParameter description: num_layers - the number of sub-decoder-layers in the decoder (required).\nConstraints:\ndtype:int\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:[0,inf)\n\n###\n\nSignature: torch.nn.quantized.functional.interpolate(input,size=None,scale_factor=None,mode=nearest,align_corners=None)\nParameter description: align_corners(bool, optional) - Geometrically, we consider the pixels of the input and output as squares rather than points. If set to `True`, the input and output tensors are aligned by the center points of their corner pixels, preserving the values at the corner pixels. If set to `False`, the input and output tensors are aligned by the corner points of their corner pixels, and the interpolation uses edge value padding for out-of-boundary values, making this operation independent of input size when `scale_factor` is kept the same. This only has an effect when `mode` is `'bilinear'`. Default: `False`\nConstraints:\ndtype:bool\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.gt(input,other,out=None)\nParameter description: out(Tensor, optional) - the output tensor that must be a BoolTensor\nConstraints:\ndtype:booltensor\nstructure:tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.empty_like(input,dtype=None,layout=None,device=None,requires_grad=False,memory_format=torch.preserve_format)\nParameter description: dtype(`torch.dtype`, optional) - the desired data type of returned Tensor. Default: if `None`, defaults to the dtype of `input`.\nConstraints:\ndtype:torch.dtype\nstructure:Null\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.nn.RNN(*args,**kwargs,input_size=None,hidden_size=None,num_layers=None,nonlinearity=None,bias=None,batch_first=None,dropout=None,bidirectional=None)\nParameter description: hidden_size - The number of features in the hidden state h\nConstraints:\ndtype:int\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:[0,inf)\n\n###\n\nSignature: torch.nn.functional.avg_pool1d(input,kernel_size,stride=None,padding=0,ceil_mode=False,count_include_pad=True)\nParameter description: padding - implicit zero paddings on both sides of the input. Can be a single number or a tuple (padW,). Default: 0\nConstraints:\ndtype:bool;int\nstructure:tuple\nshape:Null\nndim:0;1\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.autograd.grad(outputs,inputs,grad_outputs=None,retain_graph=None,create_graph=False,only_inputs=True,allow_unused=False)\nParameter description: outputs(sequence of Tensor) - outputs of the differentiated function.\nConstraints:\ndtype:Null\nstructure:sequence;tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.repeat_interleave(input,repeats,dim=None)\nParameter description: dim(int, optional) - The dimension along which to repeat values.By default, use the flattened input array, and return a flat outputarray.\nConstraints:\ndtype:int\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.mm(input,mat2,out=None)\nParameter description: out(Tensor, optional) - the output tensor.\nConstraints:",
            "constr": {
                "dtype": [],
                "structure": [
                    "tensor"
                ],
                "shape": [],
                "ndim": [],
                "enum": [],
                "range": []
            },
            "signature": "torch.mm(input,mat2,out=None)",
            "descp": "out(Tensor, optional) - the output tensor."
        },
        {
            "fname": "torch.orgqr.yaml",
            "arg": "input2",
            "prompt": "Signature: torch.distributed.recv(tensor,src=None,group=<objectobject>,tag=0)\nParameter description: tensor(Tensor) - Tensor to fill with received data.\nConstraints:\ndtype:Null\nstructure:tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.div(input,other,out=None)\nParameter description: out(Tensor, optional) - the output tensor.\nConstraints:\ndtype:Null\nstructure:tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.nn.TransformerDecoder(decoder_layer,num_layers,norm=None)\nParameter description: num_layers - the number of sub-decoder-layers in the decoder (required).\nConstraints:\ndtype:int\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:[0,inf)\n\n###\n\nSignature: torch.nn.quantized.functional.interpolate(input,size=None,scale_factor=None,mode=nearest,align_corners=None)\nParameter description: align_corners(bool, optional) - Geometrically, we consider the pixels of the input and output as squares rather than points. If set to `True`, the input and output tensors are aligned by the center points of their corner pixels, preserving the values at the corner pixels. If set to `False`, the input and output tensors are aligned by the corner points of their corner pixels, and the interpolation uses edge value padding for out-of-boundary values, making this operation independent of input size when `scale_factor` is kept the same. This only has an effect when `mode` is `'bilinear'`. Default: `False`\nConstraints:\ndtype:bool\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.gt(input,other,out=None)\nParameter description: out(Tensor, optional) - the output tensor that must be a BoolTensor\nConstraints:\ndtype:booltensor\nstructure:tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.empty_like(input,dtype=None,layout=None,device=None,requires_grad=False,memory_format=torch.preserve_format)\nParameter description: dtype(`torch.dtype`, optional) - the desired data type of returned Tensor. Default: if `None`, defaults to the dtype of `input`.\nConstraints:\ndtype:torch.dtype\nstructure:Null\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.nn.RNN(*args,**kwargs,input_size=None,hidden_size=None,num_layers=None,nonlinearity=None,bias=None,batch_first=None,dropout=None,bidirectional=None)\nParameter description: hidden_size - The number of features in the hidden state h\nConstraints:\ndtype:int\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:[0,inf)\n\n###\n\nSignature: torch.nn.functional.avg_pool1d(input,kernel_size,stride=None,padding=0,ceil_mode=False,count_include_pad=True)\nParameter description: padding - implicit zero paddings on both sides of the input. Can be a single number or a tuple (padW,). Default: 0\nConstraints:\ndtype:bool;int\nstructure:tuple\nshape:Null\nndim:0;1\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.autograd.grad(outputs,inputs,grad_outputs=None,retain_graph=None,create_graph=False,only_inputs=True,allow_unused=False)\nParameter description: outputs(sequence of Tensor) - outputs of the differentiated function.\nConstraints:\ndtype:Null\nstructure:sequence;tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.repeat_interleave(input,repeats,dim=None)\nParameter description: dim(int, optional) - The dimension along which to repeat values.By default, use the flattened input array, and return a flat outputarray.\nConstraints:\ndtype:int\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.orgqr(input,input2)\nParameter description: input2(Tensor) - the tau from `torch.geqrf()`.\nConstraints:",
            "constr": {
                "dtype": [],
                "structure": [
                    "tensor"
                ],
                "shape": [],
                "ndim": [],
                "enum": [],
                "range": []
            },
            "signature": "torch.orgqr(input,input2)",
            "descp": "input2(Tensor) - the tau from `torch.geqrf()`."
        },
        {
            "fname": "torch.nn.transformerdecoderlayer.yaml",
            "arg": "nhead",
            "prompt": "Signature: torch.distributed.recv(tensor,src=None,group=<objectobject>,tag=0)\nParameter description: tensor(Tensor) - Tensor to fill with received data.\nConstraints:\ndtype:Null\nstructure:tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.div(input,other,out=None)\nParameter description: out(Tensor, optional) - the output tensor.\nConstraints:\ndtype:Null\nstructure:tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.nn.TransformerDecoder(decoder_layer,num_layers,norm=None)\nParameter description: num_layers - the number of sub-decoder-layers in the decoder (required).\nConstraints:\ndtype:int\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:[0,inf)\n\n###\n\nSignature: torch.nn.quantized.functional.interpolate(input,size=None,scale_factor=None,mode=nearest,align_corners=None)\nParameter description: align_corners(bool, optional) - Geometrically, we consider the pixels of the input and output as squares rather than points. If set to `True`, the input and output tensors are aligned by the center points of their corner pixels, preserving the values at the corner pixels. If set to `False`, the input and output tensors are aligned by the corner points of their corner pixels, and the interpolation uses edge value padding for out-of-boundary values, making this operation independent of input size when `scale_factor` is kept the same. This only has an effect when `mode` is `'bilinear'`. Default: `False`\nConstraints:\ndtype:bool\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.gt(input,other,out=None)\nParameter description: out(Tensor, optional) - the output tensor that must be a BoolTensor\nConstraints:\ndtype:booltensor\nstructure:tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.empty_like(input,dtype=None,layout=None,device=None,requires_grad=False,memory_format=torch.preserve_format)\nParameter description: dtype(`torch.dtype`, optional) - the desired data type of returned Tensor. Default: if `None`, defaults to the dtype of `input`.\nConstraints:\ndtype:torch.dtype\nstructure:Null\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.nn.RNN(*args,**kwargs,input_size=None,hidden_size=None,num_layers=None,nonlinearity=None,bias=None,batch_first=None,dropout=None,bidirectional=None)\nParameter description: hidden_size - The number of features in the hidden state h\nConstraints:\ndtype:int\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:[0,inf)\n\n###\n\nSignature: torch.nn.functional.avg_pool1d(input,kernel_size,stride=None,padding=0,ceil_mode=False,count_include_pad=True)\nParameter description: padding - implicit zero paddings on both sides of the input. Can be a single number or a tuple (padW,). Default: 0\nConstraints:\ndtype:bool;int\nstructure:tuple\nshape:Null\nndim:0;1\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.autograd.grad(outputs,inputs,grad_outputs=None,retain_graph=None,create_graph=False,only_inputs=True,allow_unused=False)\nParameter description: outputs(sequence of Tensor) - outputs of the differentiated function.\nConstraints:\ndtype:Null\nstructure:sequence;tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.repeat_interleave(input,repeats,dim=None)\nParameter description: dim(int, optional) - The dimension along which to repeat values.By default, use the flattened input array, and return a flat outputarray.\nConstraints:\ndtype:int\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.nn.TransformerDecoderLayer(d_model,nhead,dim_feedforward=2048,dropout=0.1,activation=relu)\nParameter description: nhead - the number of heads in the multiheadattention models (required).\nConstraints:",
            "constr": {
                "dtype": [
                    "int"
                ],
                "structure": [],
                "shape": [],
                "ndim": [
                    "0"
                ],
                "enum": [],
                "range": [
                    "[0,inf)"
                ]
            },
            "signature": "torch.nn.TransformerDecoderLayer(d_model,nhead,dim_feedforward=2048,dropout=0.1,activation=relu)",
            "descp": "nhead - the number of heads in the multiheadattention models (required)."
        },
        {
            "fname": "torch.histc.yaml",
            "arg": "min",
            "prompt": "Signature: torch.distributed.recv(tensor,src=None,group=<objectobject>,tag=0)\nParameter description: tensor(Tensor) - Tensor to fill with received data.\nConstraints:\ndtype:Null\nstructure:tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.div(input,other,out=None)\nParameter description: out(Tensor, optional) - the output tensor.\nConstraints:\ndtype:Null\nstructure:tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.nn.TransformerDecoder(decoder_layer,num_layers,norm=None)\nParameter description: num_layers - the number of sub-decoder-layers in the decoder (required).\nConstraints:\ndtype:int\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:[0,inf)\n\n###\n\nSignature: torch.nn.quantized.functional.interpolate(input,size=None,scale_factor=None,mode=nearest,align_corners=None)\nParameter description: align_corners(bool, optional) - Geometrically, we consider the pixels of the input and output as squares rather than points. If set to `True`, the input and output tensors are aligned by the center points of their corner pixels, preserving the values at the corner pixels. If set to `False`, the input and output tensors are aligned by the corner points of their corner pixels, and the interpolation uses edge value padding for out-of-boundary values, making this operation independent of input size when `scale_factor` is kept the same. This only has an effect when `mode` is `'bilinear'`. Default: `False`\nConstraints:\ndtype:bool\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.gt(input,other,out=None)\nParameter description: out(Tensor, optional) - the output tensor that must be a BoolTensor\nConstraints:\ndtype:booltensor\nstructure:tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.empty_like(input,dtype=None,layout=None,device=None,requires_grad=False,memory_format=torch.preserve_format)\nParameter description: dtype(`torch.dtype`, optional) - the desired data type of returned Tensor. Default: if `None`, defaults to the dtype of `input`.\nConstraints:\ndtype:torch.dtype\nstructure:Null\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.nn.RNN(*args,**kwargs,input_size=None,hidden_size=None,num_layers=None,nonlinearity=None,bias=None,batch_first=None,dropout=None,bidirectional=None)\nParameter description: hidden_size - The number of features in the hidden state h\nConstraints:\ndtype:int\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:[0,inf)\n\n###\n\nSignature: torch.nn.functional.avg_pool1d(input,kernel_size,stride=None,padding=0,ceil_mode=False,count_include_pad=True)\nParameter description: padding - implicit zero paddings on both sides of the input. Can be a single number or a tuple (padW,). Default: 0\nConstraints:\ndtype:bool;int\nstructure:tuple\nshape:Null\nndim:0;1\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.autograd.grad(outputs,inputs,grad_outputs=None,retain_graph=None,create_graph=False,only_inputs=True,allow_unused=False)\nParameter description: outputs(sequence of Tensor) - outputs of the differentiated function.\nConstraints:\ndtype:Null\nstructure:sequence;tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.repeat_interleave(input,repeats,dim=None)\nParameter description: dim(int, optional) - The dimension along which to repeat values.By default, use the flattened input array, and return a flat outputarray.\nConstraints:\ndtype:int\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.histc(input,bins=100,min=0,max=0,out=None)\nParameter description: min(int) - lower end of the range (inclusive)\nConstraints:",
            "constr": {
                "dtype": [
                    "int"
                ],
                "structure": [],
                "shape": [],
                "ndim": [
                    "0"
                ],
                "enum": [],
                "range": []
            },
            "signature": "torch.histc(input,bins=100,min=0,max=0,out=None)",
            "descp": "min(int) - lower end of the range (inclusive)"
        },
        {
            "fname": "torch.cartesian_prod.yaml",
            "arg": "*tensors",
            "prompt": "Signature: torch.distributed.recv(tensor,src=None,group=<objectobject>,tag=0)\nParameter description: tensor(Tensor) - Tensor to fill with received data.\nConstraints:\ndtype:Null\nstructure:tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.div(input,other,out=None)\nParameter description: out(Tensor, optional) - the output tensor.\nConstraints:\ndtype:Null\nstructure:tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.nn.TransformerDecoder(decoder_layer,num_layers,norm=None)\nParameter description: num_layers - the number of sub-decoder-layers in the decoder (required).\nConstraints:\ndtype:int\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:[0,inf)\n\n###\n\nSignature: torch.nn.quantized.functional.interpolate(input,size=None,scale_factor=None,mode=nearest,align_corners=None)\nParameter description: align_corners(bool, optional) - Geometrically, we consider the pixels of the input and output as squares rather than points. If set to `True`, the input and output tensors are aligned by the center points of their corner pixels, preserving the values at the corner pixels. If set to `False`, the input and output tensors are aligned by the corner points of their corner pixels, and the interpolation uses edge value padding for out-of-boundary values, making this operation independent of input size when `scale_factor` is kept the same. This only has an effect when `mode` is `'bilinear'`. Default: `False`\nConstraints:\ndtype:bool\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.gt(input,other,out=None)\nParameter description: out(Tensor, optional) - the output tensor that must be a BoolTensor\nConstraints:\ndtype:booltensor\nstructure:tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.empty_like(input,dtype=None,layout=None,device=None,requires_grad=False,memory_format=torch.preserve_format)\nParameter description: dtype(`torch.dtype`, optional) - the desired data type of returned Tensor. Default: if `None`, defaults to the dtype of `input`.\nConstraints:\ndtype:torch.dtype\nstructure:Null\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.nn.RNN(*args,**kwargs,input_size=None,hidden_size=None,num_layers=None,nonlinearity=None,bias=None,batch_first=None,dropout=None,bidirectional=None)\nParameter description: hidden_size - The number of features in the hidden state h\nConstraints:\ndtype:int\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:[0,inf)\n\n###\n\nSignature: torch.nn.functional.avg_pool1d(input,kernel_size,stride=None,padding=0,ceil_mode=False,count_include_pad=True)\nParameter description: padding - implicit zero paddings on both sides of the input. Can be a single number or a tuple (padW,). Default: 0\nConstraints:\ndtype:bool;int\nstructure:tuple\nshape:Null\nndim:0;1\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.autograd.grad(outputs,inputs,grad_outputs=None,retain_graph=None,create_graph=False,only_inputs=True,allow_unused=False)\nParameter description: outputs(sequence of Tensor) - outputs of the differentiated function.\nConstraints:\ndtype:Null\nstructure:sequence;tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.repeat_interleave(input,repeats,dim=None)\nParameter description: dim(int, optional) - The dimension along which to repeat values.By default, use the flattened input array, and return a flat outputarray.\nConstraints:\ndtype:int\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.cartesian_prod(*tensors)\nParameter description: *tensors - any number of 1 dimensional tensors.\nConstraints:",
            "constr": {
                "dtype": [
                    "int"
                ],
                "structure": [
                    "tensor"
                ],
                "shape": [],
                "ndim": [],
                "enum": [],
                "range": [
                    "[0,inf)"
                ]
            },
            "signature": "torch.cartesian_prod(*tensors)",
            "descp": "*tensors - any number of 1 dimensional tensors."
        },
        {
            "fname": "torch.nn.utils.spectral_norm.yaml",
            "arg": "dim",
            "prompt": "Signature: torch.distributed.recv(tensor,src=None,group=<objectobject>,tag=0)\nParameter description: tensor(Tensor) - Tensor to fill with received data.\nConstraints:\ndtype:Null\nstructure:tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.div(input,other,out=None)\nParameter description: out(Tensor, optional) - the output tensor.\nConstraints:\ndtype:Null\nstructure:tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.nn.TransformerDecoder(decoder_layer,num_layers,norm=None)\nParameter description: num_layers - the number of sub-decoder-layers in the decoder (required).\nConstraints:\ndtype:int\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:[0,inf)\n\n###\n\nSignature: torch.nn.quantized.functional.interpolate(input,size=None,scale_factor=None,mode=nearest,align_corners=None)\nParameter description: align_corners(bool, optional) - Geometrically, we consider the pixels of the input and output as squares rather than points. If set to `True`, the input and output tensors are aligned by the center points of their corner pixels, preserving the values at the corner pixels. If set to `False`, the input and output tensors are aligned by the corner points of their corner pixels, and the interpolation uses edge value padding for out-of-boundary values, making this operation independent of input size when `scale_factor` is kept the same. This only has an effect when `mode` is `'bilinear'`. Default: `False`\nConstraints:\ndtype:bool\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.gt(input,other,out=None)\nParameter description: out(Tensor, optional) - the output tensor that must be a BoolTensor\nConstraints:\ndtype:booltensor\nstructure:tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.empty_like(input,dtype=None,layout=None,device=None,requires_grad=False,memory_format=torch.preserve_format)\nParameter description: dtype(`torch.dtype`, optional) - the desired data type of returned Tensor. Default: if `None`, defaults to the dtype of `input`.\nConstraints:\ndtype:torch.dtype\nstructure:Null\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.nn.RNN(*args,**kwargs,input_size=None,hidden_size=None,num_layers=None,nonlinearity=None,bias=None,batch_first=None,dropout=None,bidirectional=None)\nParameter description: hidden_size - The number of features in the hidden state h\nConstraints:\ndtype:int\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:[0,inf)\n\n###\n\nSignature: torch.nn.functional.avg_pool1d(input,kernel_size,stride=None,padding=0,ceil_mode=False,count_include_pad=True)\nParameter description: padding - implicit zero paddings on both sides of the input. Can be a single number or a tuple (padW,). Default: 0\nConstraints:\ndtype:bool;int\nstructure:tuple\nshape:Null\nndim:0;1\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.autograd.grad(outputs,inputs,grad_outputs=None,retain_graph=None,create_graph=False,only_inputs=True,allow_unused=False)\nParameter description: outputs(sequence of Tensor) - outputs of the differentiated function.\nConstraints:\ndtype:Null\nstructure:sequence;tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.repeat_interleave(input,repeats,dim=None)\nParameter description: dim(int, optional) - The dimension along which to repeat values.By default, use the flattened input array, and return a flat outputarray.\nConstraints:\ndtype:int\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.nn.utils.spectral_norm(module,name=weight,n_power_iterations=1,eps=1e-12,dim=None)\nParameter description: dim(int, optional) - dimension corresponding to number of outputs, the default is `0`, except for modules that are instances of ConvTranspose{1,2,3}d, when it is `1`\nConstraints:",
            "constr": {
                "dtype": [
                    "int"
                ],
                "structure": [],
                "shape": [],
                "ndim": [
                    "0"
                ],
                "enum": [],
                "range": []
            },
            "signature": "torch.nn.utils.spectral_norm(module,name=weight,n_power_iterations=1,eps=1e-12,dim=None)",
            "descp": "dim(int, optional) - dimension corresponding to number of outputs, the default is `0`, except for modules that are instances of ConvTranspose{1,2,3}d, when it is `1`"
        },
        {
            "fname": "torch.autograd.functional.hessian.yaml",
            "arg": "create_graph",
            "prompt": "Signature: torch.distributed.recv(tensor,src=None,group=<objectobject>,tag=0)\nParameter description: tensor(Tensor) - Tensor to fill with received data.\nConstraints:\ndtype:Null\nstructure:tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.div(input,other,out=None)\nParameter description: out(Tensor, optional) - the output tensor.\nConstraints:\ndtype:Null\nstructure:tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.nn.TransformerDecoder(decoder_layer,num_layers,norm=None)\nParameter description: num_layers - the number of sub-decoder-layers in the decoder (required).\nConstraints:\ndtype:int\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:[0,inf)\n\n###\n\nSignature: torch.nn.quantized.functional.interpolate(input,size=None,scale_factor=None,mode=nearest,align_corners=None)\nParameter description: align_corners(bool, optional) - Geometrically, we consider the pixels of the input and output as squares rather than points. If set to `True`, the input and output tensors are aligned by the center points of their corner pixels, preserving the values at the corner pixels. If set to `False`, the input and output tensors are aligned by the corner points of their corner pixels, and the interpolation uses edge value padding for out-of-boundary values, making this operation independent of input size when `scale_factor` is kept the same. This only has an effect when `mode` is `'bilinear'`. Default: `False`\nConstraints:\ndtype:bool\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.gt(input,other,out=None)\nParameter description: out(Tensor, optional) - the output tensor that must be a BoolTensor\nConstraints:\ndtype:booltensor\nstructure:tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.empty_like(input,dtype=None,layout=None,device=None,requires_grad=False,memory_format=torch.preserve_format)\nParameter description: dtype(`torch.dtype`, optional) - the desired data type of returned Tensor. Default: if `None`, defaults to the dtype of `input`.\nConstraints:\ndtype:torch.dtype\nstructure:Null\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.nn.RNN(*args,**kwargs,input_size=None,hidden_size=None,num_layers=None,nonlinearity=None,bias=None,batch_first=None,dropout=None,bidirectional=None)\nParameter description: hidden_size - The number of features in the hidden state h\nConstraints:\ndtype:int\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:[0,inf)\n\n###\n\nSignature: torch.nn.functional.avg_pool1d(input,kernel_size,stride=None,padding=0,ceil_mode=False,count_include_pad=True)\nParameter description: padding - implicit zero paddings on both sides of the input. Can be a single number or a tuple (padW,). Default: 0\nConstraints:\ndtype:bool;int\nstructure:tuple\nshape:Null\nndim:0;1\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.autograd.grad(outputs,inputs,grad_outputs=None,retain_graph=None,create_graph=False,only_inputs=True,allow_unused=False)\nParameter description: outputs(sequence of Tensor) - outputs of the differentiated function.\nConstraints:\ndtype:Null\nstructure:sequence;tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.repeat_interleave(input,repeats,dim=None)\nParameter description: dim(int, optional) - The dimension along which to repeat values.By default, use the flattened input array, and return a flat outputarray.\nConstraints:\ndtype:int\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.autograd.functional.hessian(func,inputs,create_graph=False,strict=False)\nParameter description: create_graph(bool, optional) - If `True`, the Hessian will be computed in a differentiable manner. Note that when `strict` is `False`, the result can not require gradients or be disconnected from the inputs. Defaults to `False`.\nConstraints:",
            "constr": {
                "dtype": [
                    "bool"
                ],
                "structure": [],
                "shape": [],
                "ndim": [
                    "0"
                ],
                "enum": [],
                "range": []
            },
            "signature": "torch.autograd.functional.hessian(func,inputs,create_graph=False,strict=False)",
            "descp": "create_graph(bool, optional) - If `True`, the Hessian will be computed in a differentiable manner. Note that when `strict` is `False`, the result can not require gradients or be disconnected from the inputs. Defaults to `False`."
        },
        {
            "fname": "torch.nn.functional.gumbel_softmax.yaml",
            "arg": "dim",
            "prompt": "Signature: torch.distributed.recv(tensor,src=None,group=<objectobject>,tag=0)\nParameter description: tensor(Tensor) - Tensor to fill with received data.\nConstraints:\ndtype:Null\nstructure:tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.div(input,other,out=None)\nParameter description: out(Tensor, optional) - the output tensor.\nConstraints:\ndtype:Null\nstructure:tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.nn.TransformerDecoder(decoder_layer,num_layers,norm=None)\nParameter description: num_layers - the number of sub-decoder-layers in the decoder (required).\nConstraints:\ndtype:int\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:[0,inf)\n\n###\n\nSignature: torch.nn.quantized.functional.interpolate(input,size=None,scale_factor=None,mode=nearest,align_corners=None)\nParameter description: align_corners(bool, optional) - Geometrically, we consider the pixels of the input and output as squares rather than points. If set to `True`, the input and output tensors are aligned by the center points of their corner pixels, preserving the values at the corner pixels. If set to `False`, the input and output tensors are aligned by the corner points of their corner pixels, and the interpolation uses edge value padding for out-of-boundary values, making this operation independent of input size when `scale_factor` is kept the same. This only has an effect when `mode` is `'bilinear'`. Default: `False`\nConstraints:\ndtype:bool\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.gt(input,other,out=None)\nParameter description: out(Tensor, optional) - the output tensor that must be a BoolTensor\nConstraints:\ndtype:booltensor\nstructure:tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.empty_like(input,dtype=None,layout=None,device=None,requires_grad=False,memory_format=torch.preserve_format)\nParameter description: dtype(`torch.dtype`, optional) - the desired data type of returned Tensor. Default: if `None`, defaults to the dtype of `input`.\nConstraints:\ndtype:torch.dtype\nstructure:Null\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.nn.RNN(*args,**kwargs,input_size=None,hidden_size=None,num_layers=None,nonlinearity=None,bias=None,batch_first=None,dropout=None,bidirectional=None)\nParameter description: hidden_size - The number of features in the hidden state h\nConstraints:\ndtype:int\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:[0,inf)\n\n###\n\nSignature: torch.nn.functional.avg_pool1d(input,kernel_size,stride=None,padding=0,ceil_mode=False,count_include_pad=True)\nParameter description: padding - implicit zero paddings on both sides of the input. Can be a single number or a tuple (padW,). Default: 0\nConstraints:\ndtype:bool;int\nstructure:tuple\nshape:Null\nndim:0;1\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.autograd.grad(outputs,inputs,grad_outputs=None,retain_graph=None,create_graph=False,only_inputs=True,allow_unused=False)\nParameter description: outputs(sequence of Tensor) - outputs of the differentiated function.\nConstraints:\ndtype:Null\nstructure:sequence;tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.repeat_interleave(input,repeats,dim=None)\nParameter description: dim(int, optional) - The dimension along which to repeat values.By default, use the flattened input array, and return a flat outputarray.\nConstraints:\ndtype:int\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.nn.functional.gumbel_softmax(logits,tau=1,hard=False,eps=1e-10,dim=-1)\nParameter description: dim(int) - A dimension along which softmax will be computed. Default: -1.\nConstraints:",
            "constr": {
                "dtype": [
                    "bool",
                    "int"
                ],
                "structure": [],
                "shape": [],
                "ndim": [
                    "0"
                ],
                "enum": [],
                "range": []
            },
            "signature": "torch.nn.functional.gumbel_softmax(logits,tau=1,hard=False,eps=1e-10,dim=-1)",
            "descp": "dim(int) - A dimension along which softmax will be computed. Default: -1."
        },
        {
            "fname": "torch.pinverse.yaml",
            "arg": "input",
            "prompt": "Signature: torch.distributed.recv(tensor,src=None,group=<objectobject>,tag=0)\nParameter description: tensor(Tensor) - Tensor to fill with received data.\nConstraints:\ndtype:Null\nstructure:tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.div(input,other,out=None)\nParameter description: out(Tensor, optional) - the output tensor.\nConstraints:\ndtype:Null\nstructure:tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.nn.TransformerDecoder(decoder_layer,num_layers,norm=None)\nParameter description: num_layers - the number of sub-decoder-layers in the decoder (required).\nConstraints:\ndtype:int\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:[0,inf)\n\n###\n\nSignature: torch.nn.quantized.functional.interpolate(input,size=None,scale_factor=None,mode=nearest,align_corners=None)\nParameter description: align_corners(bool, optional) - Geometrically, we consider the pixels of the input and output as squares rather than points. If set to `True`, the input and output tensors are aligned by the center points of their corner pixels, preserving the values at the corner pixels. If set to `False`, the input and output tensors are aligned by the corner points of their corner pixels, and the interpolation uses edge value padding for out-of-boundary values, making this operation independent of input size when `scale_factor` is kept the same. This only has an effect when `mode` is `'bilinear'`. Default: `False`\nConstraints:\ndtype:bool\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.gt(input,other,out=None)\nParameter description: out(Tensor, optional) - the output tensor that must be a BoolTensor\nConstraints:\ndtype:booltensor\nstructure:tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.empty_like(input,dtype=None,layout=None,device=None,requires_grad=False,memory_format=torch.preserve_format)\nParameter description: dtype(`torch.dtype`, optional) - the desired data type of returned Tensor. Default: if `None`, defaults to the dtype of `input`.\nConstraints:\ndtype:torch.dtype\nstructure:Null\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.nn.RNN(*args,**kwargs,input_size=None,hidden_size=None,num_layers=None,nonlinearity=None,bias=None,batch_first=None,dropout=None,bidirectional=None)\nParameter description: hidden_size - The number of features in the hidden state h\nConstraints:\ndtype:int\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:[0,inf)\n\n###\n\nSignature: torch.nn.functional.avg_pool1d(input,kernel_size,stride=None,padding=0,ceil_mode=False,count_include_pad=True)\nParameter description: padding - implicit zero paddings on both sides of the input. Can be a single number or a tuple (padW,). Default: 0\nConstraints:\ndtype:bool;int\nstructure:tuple\nshape:Null\nndim:0;1\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.autograd.grad(outputs,inputs,grad_outputs=None,retain_graph=None,create_graph=False,only_inputs=True,allow_unused=False)\nParameter description: outputs(sequence of Tensor) - outputs of the differentiated function.\nConstraints:\ndtype:Null\nstructure:sequence;tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.repeat_interleave(input,repeats,dim=None)\nParameter description: dim(int, optional) - The dimension along which to repeat values.By default, use the flattened input array, and return a flat outputarray.\nConstraints:\ndtype:int\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.pinverse(input,rcond=1e-15)\nParameter description: input(Tensor) - The input tensor of size (*, m, n)  where *  is zero or more batch dimensions\nConstraints:",
            "constr": {
                "dtype": [],
                "structure": [
                    "tensor"
                ],
                "shape": [
                    "(*, m, n)"
                ],
                "ndim": [],
                "enum": [],
                "range": []
            },
            "signature": "torch.pinverse(input,rcond=1e-15)",
            "descp": "input(Tensor) - The input tensor of size (*, m, n)  where *  is zero or more batch dimensions"
        },
        {
            "fname": "torch.nn.maxunpool2d.yaml",
            "arg": "padding",
            "prompt": "Signature: torch.distributed.recv(tensor,src=None,group=<objectobject>,tag=0)\nParameter description: tensor(Tensor) - Tensor to fill with received data.\nConstraints:\ndtype:Null\nstructure:tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.div(input,other,out=None)\nParameter description: out(Tensor, optional) - the output tensor.\nConstraints:\ndtype:Null\nstructure:tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.nn.TransformerDecoder(decoder_layer,num_layers,norm=None)\nParameter description: num_layers - the number of sub-decoder-layers in the decoder (required).\nConstraints:\ndtype:int\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:[0,inf)\n\n###\n\nSignature: torch.nn.quantized.functional.interpolate(input,size=None,scale_factor=None,mode=nearest,align_corners=None)\nParameter description: align_corners(bool, optional) - Geometrically, we consider the pixels of the input and output as squares rather than points. If set to `True`, the input and output tensors are aligned by the center points of their corner pixels, preserving the values at the corner pixels. If set to `False`, the input and output tensors are aligned by the corner points of their corner pixels, and the interpolation uses edge value padding for out-of-boundary values, making this operation independent of input size when `scale_factor` is kept the same. This only has an effect when `mode` is `'bilinear'`. Default: `False`\nConstraints:\ndtype:bool\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.gt(input,other,out=None)\nParameter description: out(Tensor, optional) - the output tensor that must be a BoolTensor\nConstraints:\ndtype:booltensor\nstructure:tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.empty_like(input,dtype=None,layout=None,device=None,requires_grad=False,memory_format=torch.preserve_format)\nParameter description: dtype(`torch.dtype`, optional) - the desired data type of returned Tensor. Default: if `None`, defaults to the dtype of `input`.\nConstraints:\ndtype:torch.dtype\nstructure:Null\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.nn.RNN(*args,**kwargs,input_size=None,hidden_size=None,num_layers=None,nonlinearity=None,bias=None,batch_first=None,dropout=None,bidirectional=None)\nParameter description: hidden_size - The number of features in the hidden state h\nConstraints:\ndtype:int\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:[0,inf)\n\n###\n\nSignature: torch.nn.functional.avg_pool1d(input,kernel_size,stride=None,padding=0,ceil_mode=False,count_include_pad=True)\nParameter description: padding - implicit zero paddings on both sides of the input. Can be a single number or a tuple (padW,). Default: 0\nConstraints:\ndtype:bool;int\nstructure:tuple\nshape:Null\nndim:0;1\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.autograd.grad(outputs,inputs,grad_outputs=None,retain_graph=None,create_graph=False,only_inputs=True,allow_unused=False)\nParameter description: outputs(sequence of Tensor) - outputs of the differentiated function.\nConstraints:\ndtype:Null\nstructure:sequence;tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.repeat_interleave(input,repeats,dim=None)\nParameter description: dim(int, optional) - The dimension along which to repeat values.By default, use the flattened input array, and return a flat outputarray.\nConstraints:\ndtype:int\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.nn.MaxUnpool2d(kernel_size,stride=None,padding=0)\nParameter description: padding(int or tuple) - Padding that was added to the input\nConstraints:",
            "constr": {
                "dtype": [
                    "int"
                ],
                "structure": [
                    "tuple"
                ],
                "shape": [],
                "ndim": [
                    "0"
                ],
                "enum": [],
                "range": []
            },
            "signature": "torch.nn.MaxUnpool2d(kernel_size,stride=None,padding=0)",
            "descp": "padding(int or tuple) - Padding that was added to the input"
        },
        {
            "fname": "torch.nn.functional.conv_transpose2d.yaml",
            "arg": "input",
            "prompt": "Signature: torch.distributed.recv(tensor,src=None,group=<objectobject>,tag=0)\nParameter description: tensor(Tensor) - Tensor to fill with received data.\nConstraints:\ndtype:Null\nstructure:tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.div(input,other,out=None)\nParameter description: out(Tensor, optional) - the output tensor.\nConstraints:\ndtype:Null\nstructure:tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.nn.TransformerDecoder(decoder_layer,num_layers,norm=None)\nParameter description: num_layers - the number of sub-decoder-layers in the decoder (required).\nConstraints:\ndtype:int\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:[0,inf)\n\n###\n\nSignature: torch.nn.quantized.functional.interpolate(input,size=None,scale_factor=None,mode=nearest,align_corners=None)\nParameter description: align_corners(bool, optional) - Geometrically, we consider the pixels of the input and output as squares rather than points. If set to `True`, the input and output tensors are aligned by the center points of their corner pixels, preserving the values at the corner pixels. If set to `False`, the input and output tensors are aligned by the corner points of their corner pixels, and the interpolation uses edge value padding for out-of-boundary values, making this operation independent of input size when `scale_factor` is kept the same. This only has an effect when `mode` is `'bilinear'`. Default: `False`\nConstraints:\ndtype:bool\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.gt(input,other,out=None)\nParameter description: out(Tensor, optional) - the output tensor that must be a BoolTensor\nConstraints:\ndtype:booltensor\nstructure:tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.empty_like(input,dtype=None,layout=None,device=None,requires_grad=False,memory_format=torch.preserve_format)\nParameter description: dtype(`torch.dtype`, optional) - the desired data type of returned Tensor. Default: if `None`, defaults to the dtype of `input`.\nConstraints:\ndtype:torch.dtype\nstructure:Null\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.nn.RNN(*args,**kwargs,input_size=None,hidden_size=None,num_layers=None,nonlinearity=None,bias=None,batch_first=None,dropout=None,bidirectional=None)\nParameter description: hidden_size - The number of features in the hidden state h\nConstraints:\ndtype:int\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:[0,inf)\n\n###\n\nSignature: torch.nn.functional.avg_pool1d(input,kernel_size,stride=None,padding=0,ceil_mode=False,count_include_pad=True)\nParameter description: padding - implicit zero paddings on both sides of the input. Can be a single number or a tuple (padW,). Default: 0\nConstraints:\ndtype:bool;int\nstructure:tuple\nshape:Null\nndim:0;1\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.autograd.grad(outputs,inputs,grad_outputs=None,retain_graph=None,create_graph=False,only_inputs=True,allow_unused=False)\nParameter description: outputs(sequence of Tensor) - outputs of the differentiated function.\nConstraints:\ndtype:Null\nstructure:sequence;tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.repeat_interleave(input,repeats,dim=None)\nParameter description: dim(int, optional) - The dimension along which to repeat values.By default, use the flattened input array, and return a flat outputarray.\nConstraints:\ndtype:int\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.nn.functional.conv_transpose2d(input,weight,bias=None,stride=1,padding=0,output_padding=0,groups=1,dilation=1)\nParameter description: input - input tensor of shape (minibatch , in _channels , iH , iW) \nConstraints:",
            "constr": {
                "dtype": [],
                "structure": [
                    "tensor"
                ],
                "shape": [
                    "(minibatch , in _channels , iH , iW)"
                ],
                "ndim": [],
                "enum": [],
                "range": []
            },
            "signature": "torch.nn.functional.conv_transpose2d(input,weight,bias=None,stride=1,padding=0,output_padding=0,groups=1,dilation=1)",
            "descp": "input - input tensor of shape (minibatch , in _channels , iH , iW) "
        },
        {
            "fname": "torch.hamming_window.yaml",
            "arg": "periodic",
            "prompt": "Signature: torch.distributed.recv(tensor,src=None,group=<objectobject>,tag=0)\nParameter description: tensor(Tensor) - Tensor to fill with received data.\nConstraints:\ndtype:Null\nstructure:tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.div(input,other,out=None)\nParameter description: out(Tensor, optional) - the output tensor.\nConstraints:\ndtype:Null\nstructure:tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.nn.TransformerDecoder(decoder_layer,num_layers,norm=None)\nParameter description: num_layers - the number of sub-decoder-layers in the decoder (required).\nConstraints:\ndtype:int\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:[0,inf)\n\n###\n\nSignature: torch.nn.quantized.functional.interpolate(input,size=None,scale_factor=None,mode=nearest,align_corners=None)\nParameter description: align_corners(bool, optional) - Geometrically, we consider the pixels of the input and output as squares rather than points. If set to `True`, the input and output tensors are aligned by the center points of their corner pixels, preserving the values at the corner pixels. If set to `False`, the input and output tensors are aligned by the corner points of their corner pixels, and the interpolation uses edge value padding for out-of-boundary values, making this operation independent of input size when `scale_factor` is kept the same. This only has an effect when `mode` is `'bilinear'`. Default: `False`\nConstraints:\ndtype:bool\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.gt(input,other,out=None)\nParameter description: out(Tensor, optional) - the output tensor that must be a BoolTensor\nConstraints:\ndtype:booltensor\nstructure:tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.empty_like(input,dtype=None,layout=None,device=None,requires_grad=False,memory_format=torch.preserve_format)\nParameter description: dtype(`torch.dtype`, optional) - the desired data type of returned Tensor. Default: if `None`, defaults to the dtype of `input`.\nConstraints:\ndtype:torch.dtype\nstructure:Null\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.nn.RNN(*args,**kwargs,input_size=None,hidden_size=None,num_layers=None,nonlinearity=None,bias=None,batch_first=None,dropout=None,bidirectional=None)\nParameter description: hidden_size - The number of features in the hidden state h\nConstraints:\ndtype:int\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:[0,inf)\n\n###\n\nSignature: torch.nn.functional.avg_pool1d(input,kernel_size,stride=None,padding=0,ceil_mode=False,count_include_pad=True)\nParameter description: padding - implicit zero paddings on both sides of the input. Can be a single number or a tuple (padW,). Default: 0\nConstraints:\ndtype:bool;int\nstructure:tuple\nshape:Null\nndim:0;1\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.autograd.grad(outputs,inputs,grad_outputs=None,retain_graph=None,create_graph=False,only_inputs=True,allow_unused=False)\nParameter description: outputs(sequence of Tensor) - outputs of the differentiated function.\nConstraints:\ndtype:Null\nstructure:sequence;tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.repeat_interleave(input,repeats,dim=None)\nParameter description: dim(int, optional) - The dimension along which to repeat values.By default, use the flattened input array, and return a flat outputarray.\nConstraints:\ndtype:int\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.hamming_window(window_length,periodic=True,alpha=0.54,beta=0.46,dtype=None,layout=torch.strided,device=None,requires_grad=False)\nParameter description: periodic(bool, optional) - If True, returns a window to be used as periodic function. If False, return a symmetric window.\nConstraints:",
            "constr": {
                "dtype": [
                    "bool"
                ],
                "structure": [],
                "shape": [],
                "ndim": [
                    "0"
                ],
                "enum": [],
                "range": []
            },
            "signature": "torch.hamming_window(window_length,periodic=True,alpha=0.54,beta=0.46,dtype=None,layout=torch.strided,device=None,requires_grad=False)",
            "descp": "periodic(bool, optional) - If True, returns a window to be used as periodic function. If False, return a symmetric window."
        },
        {
            "fname": "torch.mv.yaml",
            "arg": "out",
            "prompt": "Signature: torch.distributed.recv(tensor,src=None,group=<objectobject>,tag=0)\nParameter description: tensor(Tensor) - Tensor to fill with received data.\nConstraints:\ndtype:Null\nstructure:tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.div(input,other,out=None)\nParameter description: out(Tensor, optional) - the output tensor.\nConstraints:\ndtype:Null\nstructure:tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.nn.TransformerDecoder(decoder_layer,num_layers,norm=None)\nParameter description: num_layers - the number of sub-decoder-layers in the decoder (required).\nConstraints:\ndtype:int\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:[0,inf)\n\n###\n\nSignature: torch.nn.quantized.functional.interpolate(input,size=None,scale_factor=None,mode=nearest,align_corners=None)\nParameter description: align_corners(bool, optional) - Geometrically, we consider the pixels of the input and output as squares rather than points. If set to `True`, the input and output tensors are aligned by the center points of their corner pixels, preserving the values at the corner pixels. If set to `False`, the input and output tensors are aligned by the corner points of their corner pixels, and the interpolation uses edge value padding for out-of-boundary values, making this operation independent of input size when `scale_factor` is kept the same. This only has an effect when `mode` is `'bilinear'`. Default: `False`\nConstraints:\ndtype:bool\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.gt(input,other,out=None)\nParameter description: out(Tensor, optional) - the output tensor that must be a BoolTensor\nConstraints:\ndtype:booltensor\nstructure:tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.empty_like(input,dtype=None,layout=None,device=None,requires_grad=False,memory_format=torch.preserve_format)\nParameter description: dtype(`torch.dtype`, optional) - the desired data type of returned Tensor. Default: if `None`, defaults to the dtype of `input`.\nConstraints:\ndtype:torch.dtype\nstructure:Null\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.nn.RNN(*args,**kwargs,input_size=None,hidden_size=None,num_layers=None,nonlinearity=None,bias=None,batch_first=None,dropout=None,bidirectional=None)\nParameter description: hidden_size - The number of features in the hidden state h\nConstraints:\ndtype:int\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:[0,inf)\n\n###\n\nSignature: torch.nn.functional.avg_pool1d(input,kernel_size,stride=None,padding=0,ceil_mode=False,count_include_pad=True)\nParameter description: padding - implicit zero paddings on both sides of the input. Can be a single number or a tuple (padW,). Default: 0\nConstraints:\ndtype:bool;int\nstructure:tuple\nshape:Null\nndim:0;1\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.autograd.grad(outputs,inputs,grad_outputs=None,retain_graph=None,create_graph=False,only_inputs=True,allow_unused=False)\nParameter description: outputs(sequence of Tensor) - outputs of the differentiated function.\nConstraints:\ndtype:Null\nstructure:sequence;tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.repeat_interleave(input,repeats,dim=None)\nParameter description: dim(int, optional) - The dimension along which to repeat values.By default, use the flattened input array, and return a flat outputarray.\nConstraints:\ndtype:int\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.mv(input,vec,out=None)\nParameter description: out(Tensor, optional) - the output tensor.\nConstraints:",
            "constr": {
                "dtype": [],
                "structure": [
                    "tensor"
                ],
                "shape": [],
                "ndim": [],
                "enum": [],
                "range": []
            },
            "signature": "torch.mv(input,vec,out=None)",
            "descp": "out(Tensor, optional) - the output tensor."
        },
        {
            "fname": "torch.nn.embedding.yaml",
            "arg": "sparse",
            "prompt": "Signature: torch.distributed.recv(tensor,src=None,group=<objectobject>,tag=0)\nParameter description: tensor(Tensor) - Tensor to fill with received data.\nConstraints:\ndtype:Null\nstructure:tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.div(input,other,out=None)\nParameter description: out(Tensor, optional) - the output tensor.\nConstraints:\ndtype:Null\nstructure:tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.nn.TransformerDecoder(decoder_layer,num_layers,norm=None)\nParameter description: num_layers - the number of sub-decoder-layers in the decoder (required).\nConstraints:\ndtype:int\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:[0,inf)\n\n###\n\nSignature: torch.nn.quantized.functional.interpolate(input,size=None,scale_factor=None,mode=nearest,align_corners=None)\nParameter description: align_corners(bool, optional) - Geometrically, we consider the pixels of the input and output as squares rather than points. If set to `True`, the input and output tensors are aligned by the center points of their corner pixels, preserving the values at the corner pixels. If set to `False`, the input and output tensors are aligned by the corner points of their corner pixels, and the interpolation uses edge value padding for out-of-boundary values, making this operation independent of input size when `scale_factor` is kept the same. This only has an effect when `mode` is `'bilinear'`. Default: `False`\nConstraints:\ndtype:bool\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.gt(input,other,out=None)\nParameter description: out(Tensor, optional) - the output tensor that must be a BoolTensor\nConstraints:\ndtype:booltensor\nstructure:tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.empty_like(input,dtype=None,layout=None,device=None,requires_grad=False,memory_format=torch.preserve_format)\nParameter description: dtype(`torch.dtype`, optional) - the desired data type of returned Tensor. Default: if `None`, defaults to the dtype of `input`.\nConstraints:\ndtype:torch.dtype\nstructure:Null\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.nn.RNN(*args,**kwargs,input_size=None,hidden_size=None,num_layers=None,nonlinearity=None,bias=None,batch_first=None,dropout=None,bidirectional=None)\nParameter description: hidden_size - The number of features in the hidden state h\nConstraints:\ndtype:int\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:[0,inf)\n\n###\n\nSignature: torch.nn.functional.avg_pool1d(input,kernel_size,stride=None,padding=0,ceil_mode=False,count_include_pad=True)\nParameter description: padding - implicit zero paddings on both sides of the input. Can be a single number or a tuple (padW,). Default: 0\nConstraints:\ndtype:bool;int\nstructure:tuple\nshape:Null\nndim:0;1\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.autograd.grad(outputs,inputs,grad_outputs=None,retain_graph=None,create_graph=False,only_inputs=True,allow_unused=False)\nParameter description: outputs(sequence of Tensor) - outputs of the differentiated function.\nConstraints:\ndtype:Null\nstructure:sequence;tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.repeat_interleave(input,repeats,dim=None)\nParameter description: dim(int, optional) - The dimension along which to repeat values.By default, use the flattened input array, and return a flat outputarray.\nConstraints:\ndtype:int\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.nn.Embedding(num_embeddings,embedding_dim,padding_idx=None,max_norm=None,norm_type=2.0,scale_grad_by_freq=False,sparse=False,_weight=None)\nParameter description: sparse(bool, optional) - If `True`, gradient w.r.t. `weight` matrix will be a sparse tensor. See Notes for more details regarding sparse gradients.\nConstraints:",
            "constr": {
                "dtype": [
                    "bool"
                ],
                "structure": [],
                "shape": [],
                "ndim": [
                    "0"
                ],
                "enum": [],
                "range": []
            },
            "signature": "torch.nn.Embedding(num_embeddings,embedding_dim,padding_idx=None,max_norm=None,norm_type=2.0,scale_grad_by_freq=False,sparse=False,_weight=None)",
            "descp": "sparse(bool, optional) - If `True`, gradient w.r.t. `weight` matrix will be a sparse tensor. See Notes for more details regarding sparse gradients."
        },
        {
            "fname": "torch.reciprocal.yaml",
            "arg": "out",
            "prompt": "Signature: torch.distributed.recv(tensor,src=None,group=<objectobject>,tag=0)\nParameter description: tensor(Tensor) - Tensor to fill with received data.\nConstraints:\ndtype:Null\nstructure:tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.div(input,other,out=None)\nParameter description: out(Tensor, optional) - the output tensor.\nConstraints:\ndtype:Null\nstructure:tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.nn.TransformerDecoder(decoder_layer,num_layers,norm=None)\nParameter description: num_layers - the number of sub-decoder-layers in the decoder (required).\nConstraints:\ndtype:int\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:[0,inf)\n\n###\n\nSignature: torch.nn.quantized.functional.interpolate(input,size=None,scale_factor=None,mode=nearest,align_corners=None)\nParameter description: align_corners(bool, optional) - Geometrically, we consider the pixels of the input and output as squares rather than points. If set to `True`, the input and output tensors are aligned by the center points of their corner pixels, preserving the values at the corner pixels. If set to `False`, the input and output tensors are aligned by the corner points of their corner pixels, and the interpolation uses edge value padding for out-of-boundary values, making this operation independent of input size when `scale_factor` is kept the same. This only has an effect when `mode` is `'bilinear'`. Default: `False`\nConstraints:\ndtype:bool\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.gt(input,other,out=None)\nParameter description: out(Tensor, optional) - the output tensor that must be a BoolTensor\nConstraints:\ndtype:booltensor\nstructure:tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.empty_like(input,dtype=None,layout=None,device=None,requires_grad=False,memory_format=torch.preserve_format)\nParameter description: dtype(`torch.dtype`, optional) - the desired data type of returned Tensor. Default: if `None`, defaults to the dtype of `input`.\nConstraints:\ndtype:torch.dtype\nstructure:Null\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.nn.RNN(*args,**kwargs,input_size=None,hidden_size=None,num_layers=None,nonlinearity=None,bias=None,batch_first=None,dropout=None,bidirectional=None)\nParameter description: hidden_size - The number of features in the hidden state h\nConstraints:\ndtype:int\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:[0,inf)\n\n###\n\nSignature: torch.nn.functional.avg_pool1d(input,kernel_size,stride=None,padding=0,ceil_mode=False,count_include_pad=True)\nParameter description: padding - implicit zero paddings on both sides of the input. Can be a single number or a tuple (padW,). Default: 0\nConstraints:\ndtype:bool;int\nstructure:tuple\nshape:Null\nndim:0;1\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.autograd.grad(outputs,inputs,grad_outputs=None,retain_graph=None,create_graph=False,only_inputs=True,allow_unused=False)\nParameter description: outputs(sequence of Tensor) - outputs of the differentiated function.\nConstraints:\ndtype:Null\nstructure:sequence;tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.repeat_interleave(input,repeats,dim=None)\nParameter description: dim(int, optional) - The dimension along which to repeat values.By default, use the flattened input array, and return a flat outputarray.\nConstraints:\ndtype:int\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.reciprocal(input,out=None)\nParameter description: out(Tensor, optional) - the output tensor.\nConstraints:",
            "constr": {
                "dtype": [],
                "structure": [
                    "tensor"
                ],
                "shape": [],
                "ndim": [],
                "enum": [],
                "range": []
            },
            "signature": "torch.reciprocal(input,out=None)",
            "descp": "out(Tensor, optional) - the output tensor."
        },
        {
            "fname": "torch.diagflat.yaml",
            "arg": "input",
            "prompt": "Signature: torch.distributed.recv(tensor,src=None,group=<objectobject>,tag=0)\nParameter description: tensor(Tensor) - Tensor to fill with received data.\nConstraints:\ndtype:Null\nstructure:tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.div(input,other,out=None)\nParameter description: out(Tensor, optional) - the output tensor.\nConstraints:\ndtype:Null\nstructure:tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.nn.TransformerDecoder(decoder_layer,num_layers,norm=None)\nParameter description: num_layers - the number of sub-decoder-layers in the decoder (required).\nConstraints:\ndtype:int\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:[0,inf)\n\n###\n\nSignature: torch.nn.quantized.functional.interpolate(input,size=None,scale_factor=None,mode=nearest,align_corners=None)\nParameter description: align_corners(bool, optional) - Geometrically, we consider the pixels of the input and output as squares rather than points. If set to `True`, the input and output tensors are aligned by the center points of their corner pixels, preserving the values at the corner pixels. If set to `False`, the input and output tensors are aligned by the corner points of their corner pixels, and the interpolation uses edge value padding for out-of-boundary values, making this operation independent of input size when `scale_factor` is kept the same. This only has an effect when `mode` is `'bilinear'`. Default: `False`\nConstraints:\ndtype:bool\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.gt(input,other,out=None)\nParameter description: out(Tensor, optional) - the output tensor that must be a BoolTensor\nConstraints:\ndtype:booltensor\nstructure:tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.empty_like(input,dtype=None,layout=None,device=None,requires_grad=False,memory_format=torch.preserve_format)\nParameter description: dtype(`torch.dtype`, optional) - the desired data type of returned Tensor. Default: if `None`, defaults to the dtype of `input`.\nConstraints:\ndtype:torch.dtype\nstructure:Null\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.nn.RNN(*args,**kwargs,input_size=None,hidden_size=None,num_layers=None,nonlinearity=None,bias=None,batch_first=None,dropout=None,bidirectional=None)\nParameter description: hidden_size - The number of features in the hidden state h\nConstraints:\ndtype:int\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:[0,inf)\n\n###\n\nSignature: torch.nn.functional.avg_pool1d(input,kernel_size,stride=None,padding=0,ceil_mode=False,count_include_pad=True)\nParameter description: padding - implicit zero paddings on both sides of the input. Can be a single number or a tuple (padW,). Default: 0\nConstraints:\ndtype:bool;int\nstructure:tuple\nshape:Null\nndim:0;1\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.autograd.grad(outputs,inputs,grad_outputs=None,retain_graph=None,create_graph=False,only_inputs=True,allow_unused=False)\nParameter description: outputs(sequence of Tensor) - outputs of the differentiated function.\nConstraints:\ndtype:Null\nstructure:sequence;tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.repeat_interleave(input,repeats,dim=None)\nParameter description: dim(int, optional) - The dimension along which to repeat values.By default, use the flattened input array, and return a flat outputarray.\nConstraints:\ndtype:int\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.diagflat(input,offset=0)\nParameter description: input(Tensor) - the input tensor.\nConstraints:",
            "constr": {
                "dtype": [],
                "structure": [
                    "tensor"
                ],
                "shape": [],
                "ndim": [],
                "enum": [],
                "range": []
            },
            "signature": "torch.diagflat(input,offset=0)",
            "descp": "input(Tensor) - the input tensor."
        },
        {
            "fname": "torch.nn.mseloss.yaml",
            "arg": "reduce",
            "prompt": "Signature: torch.distributed.recv(tensor,src=None,group=<objectobject>,tag=0)\nParameter description: tensor(Tensor) - Tensor to fill with received data.\nConstraints:\ndtype:Null\nstructure:tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.div(input,other,out=None)\nParameter description: out(Tensor, optional) - the output tensor.\nConstraints:\ndtype:Null\nstructure:tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.nn.TransformerDecoder(decoder_layer,num_layers,norm=None)\nParameter description: num_layers - the number of sub-decoder-layers in the decoder (required).\nConstraints:\ndtype:int\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:[0,inf)\n\n###\n\nSignature: torch.nn.quantized.functional.interpolate(input,size=None,scale_factor=None,mode=nearest,align_corners=None)\nParameter description: align_corners(bool, optional) - Geometrically, we consider the pixels of the input and output as squares rather than points. If set to `True`, the input and output tensors are aligned by the center points of their corner pixels, preserving the values at the corner pixels. If set to `False`, the input and output tensors are aligned by the corner points of their corner pixels, and the interpolation uses edge value padding for out-of-boundary values, making this operation independent of input size when `scale_factor` is kept the same. This only has an effect when `mode` is `'bilinear'`. Default: `False`\nConstraints:\ndtype:bool\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.gt(input,other,out=None)\nParameter description: out(Tensor, optional) - the output tensor that must be a BoolTensor\nConstraints:\ndtype:booltensor\nstructure:tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.empty_like(input,dtype=None,layout=None,device=None,requires_grad=False,memory_format=torch.preserve_format)\nParameter description: dtype(`torch.dtype`, optional) - the desired data type of returned Tensor. Default: if `None`, defaults to the dtype of `input`.\nConstraints:\ndtype:torch.dtype\nstructure:Null\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.nn.RNN(*args,**kwargs,input_size=None,hidden_size=None,num_layers=None,nonlinearity=None,bias=None,batch_first=None,dropout=None,bidirectional=None)\nParameter description: hidden_size - The number of features in the hidden state h\nConstraints:\ndtype:int\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:[0,inf)\n\n###\n\nSignature: torch.nn.functional.avg_pool1d(input,kernel_size,stride=None,padding=0,ceil_mode=False,count_include_pad=True)\nParameter description: padding - implicit zero paddings on both sides of the input. Can be a single number or a tuple (padW,). Default: 0\nConstraints:\ndtype:bool;int\nstructure:tuple\nshape:Null\nndim:0;1\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.autograd.grad(outputs,inputs,grad_outputs=None,retain_graph=None,create_graph=False,only_inputs=True,allow_unused=False)\nParameter description: outputs(sequence of Tensor) - outputs of the differentiated function.\nConstraints:\ndtype:Null\nstructure:sequence;tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.repeat_interleave(input,repeats,dim=None)\nParameter description: dim(int, optional) - The dimension along which to repeat values.By default, use the flattened input array, and return a flat outputarray.\nConstraints:\ndtype:int\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.nn.MSELoss(size_average=None,reduce=None,reduction=mean)\nParameter description: reduce(bool, optional) - Deprecated (see `reduction`). By default, the losses are averaged or summed over observations for each minibatch depending on `size_average`. When `reduce` is `False`, returns a loss per batch element instead and ignores `size_average`. Default: `True`\nConstraints:",
            "constr": {
                "dtype": [
                    "bool"
                ],
                "structure": [],
                "shape": [],
                "ndim": [
                    "0"
                ],
                "enum": [],
                "range": []
            },
            "signature": "torch.nn.MSELoss(size_average=None,reduce=None,reduction=mean)",
            "descp": "reduce(bool, optional) - Deprecated (see `reduction`). By default, the losses are averaged or summed over observations for each minibatch depending on `size_average`. When `reduce` is `False`, returns a loss per batch element instead and ignores `size_average`. Default: `True`"
        },
        {
            "fname": "torch.svd.yaml",
            "arg": "compute_uv",
            "prompt": "Signature: torch.distributed.recv(tensor,src=None,group=<objectobject>,tag=0)\nParameter description: tensor(Tensor) - Tensor to fill with received data.\nConstraints:\ndtype:Null\nstructure:tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.div(input,other,out=None)\nParameter description: out(Tensor, optional) - the output tensor.\nConstraints:\ndtype:Null\nstructure:tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.nn.TransformerDecoder(decoder_layer,num_layers,norm=None)\nParameter description: num_layers - the number of sub-decoder-layers in the decoder (required).\nConstraints:\ndtype:int\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:[0,inf)\n\n###\n\nSignature: torch.nn.quantized.functional.interpolate(input,size=None,scale_factor=None,mode=nearest,align_corners=None)\nParameter description: align_corners(bool, optional) - Geometrically, we consider the pixels of the input and output as squares rather than points. If set to `True`, the input and output tensors are aligned by the center points of their corner pixels, preserving the values at the corner pixels. If set to `False`, the input and output tensors are aligned by the corner points of their corner pixels, and the interpolation uses edge value padding for out-of-boundary values, making this operation independent of input size when `scale_factor` is kept the same. This only has an effect when `mode` is `'bilinear'`. Default: `False`\nConstraints:\ndtype:bool\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.gt(input,other,out=None)\nParameter description: out(Tensor, optional) - the output tensor that must be a BoolTensor\nConstraints:\ndtype:booltensor\nstructure:tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.empty_like(input,dtype=None,layout=None,device=None,requires_grad=False,memory_format=torch.preserve_format)\nParameter description: dtype(`torch.dtype`, optional) - the desired data type of returned Tensor. Default: if `None`, defaults to the dtype of `input`.\nConstraints:\ndtype:torch.dtype\nstructure:Null\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.nn.RNN(*args,**kwargs,input_size=None,hidden_size=None,num_layers=None,nonlinearity=None,bias=None,batch_first=None,dropout=None,bidirectional=None)\nParameter description: hidden_size - The number of features in the hidden state h\nConstraints:\ndtype:int\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:[0,inf)\n\n###\n\nSignature: torch.nn.functional.avg_pool1d(input,kernel_size,stride=None,padding=0,ceil_mode=False,count_include_pad=True)\nParameter description: padding - implicit zero paddings on both sides of the input. Can be a single number or a tuple (padW,). Default: 0\nConstraints:\ndtype:bool;int\nstructure:tuple\nshape:Null\nndim:0;1\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.autograd.grad(outputs,inputs,grad_outputs=None,retain_graph=None,create_graph=False,only_inputs=True,allow_unused=False)\nParameter description: outputs(sequence of Tensor) - outputs of the differentiated function.\nConstraints:\ndtype:Null\nstructure:sequence;tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.repeat_interleave(input,repeats,dim=None)\nParameter description: dim(int, optional) - The dimension along which to repeat values.By default, use the flattened input array, and return a flat outputarray.\nConstraints:\ndtype:int\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.svd(input,some=True,compute_uv=True,out=None)\nParameter description: compute_uv(bool, optional) - option whether to compute U and V or not\nConstraints:",
            "constr": {
                "dtype": [
                    "bool"
                ],
                "structure": [],
                "shape": [],
                "ndim": [
                    "0"
                ],
                "enum": [],
                "range": []
            },
            "signature": "torch.svd(input,some=True,compute_uv=True,out=None)",
            "descp": "compute_uv(bool, optional) - option whether to compute U and V or not"
        },
        {
            "fname": "torch.argsort.yaml",
            "arg": "descending",
            "prompt": "Signature: torch.distributed.recv(tensor,src=None,group=<objectobject>,tag=0)\nParameter description: tensor(Tensor) - Tensor to fill with received data.\nConstraints:\ndtype:Null\nstructure:tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.div(input,other,out=None)\nParameter description: out(Tensor, optional) - the output tensor.\nConstraints:\ndtype:Null\nstructure:tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.nn.TransformerDecoder(decoder_layer,num_layers,norm=None)\nParameter description: num_layers - the number of sub-decoder-layers in the decoder (required).\nConstraints:\ndtype:int\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:[0,inf)\n\n###\n\nSignature: torch.nn.quantized.functional.interpolate(input,size=None,scale_factor=None,mode=nearest,align_corners=None)\nParameter description: align_corners(bool, optional) - Geometrically, we consider the pixels of the input and output as squares rather than points. If set to `True`, the input and output tensors are aligned by the center points of their corner pixels, preserving the values at the corner pixels. If set to `False`, the input and output tensors are aligned by the corner points of their corner pixels, and the interpolation uses edge value padding for out-of-boundary values, making this operation independent of input size when `scale_factor` is kept the same. This only has an effect when `mode` is `'bilinear'`. Default: `False`\nConstraints:\ndtype:bool\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.gt(input,other,out=None)\nParameter description: out(Tensor, optional) - the output tensor that must be a BoolTensor\nConstraints:\ndtype:booltensor\nstructure:tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.empty_like(input,dtype=None,layout=None,device=None,requires_grad=False,memory_format=torch.preserve_format)\nParameter description: dtype(`torch.dtype`, optional) - the desired data type of returned Tensor. Default: if `None`, defaults to the dtype of `input`.\nConstraints:\ndtype:torch.dtype\nstructure:Null\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.nn.RNN(*args,**kwargs,input_size=None,hidden_size=None,num_layers=None,nonlinearity=None,bias=None,batch_first=None,dropout=None,bidirectional=None)\nParameter description: hidden_size - The number of features in the hidden state h\nConstraints:\ndtype:int\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:[0,inf)\n\n###\n\nSignature: torch.nn.functional.avg_pool1d(input,kernel_size,stride=None,padding=0,ceil_mode=False,count_include_pad=True)\nParameter description: padding - implicit zero paddings on both sides of the input. Can be a single number or a tuple (padW,). Default: 0\nConstraints:\ndtype:bool;int\nstructure:tuple\nshape:Null\nndim:0;1\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.autograd.grad(outputs,inputs,grad_outputs=None,retain_graph=None,create_graph=False,only_inputs=True,allow_unused=False)\nParameter description: outputs(sequence of Tensor) - outputs of the differentiated function.\nConstraints:\ndtype:Null\nstructure:sequence;tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.repeat_interleave(input,repeats,dim=None)\nParameter description: dim(int, optional) - The dimension along which to repeat values.By default, use the flattened input array, and return a flat outputarray.\nConstraints:\ndtype:int\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.argsort(input,dim=-1,descending=False)\nParameter description: descending(bool, optional) - controls the sorting order (ascending or descending)\nConstraints:",
            "constr": {
                "dtype": [
                    "bool"
                ],
                "structure": [],
                "shape": [],
                "ndim": [
                    "0"
                ],
                "enum": [],
                "range": []
            },
            "signature": "torch.argsort(input,dim=-1,descending=False)",
            "descp": "descending(bool, optional) - controls the sorting order (ascending or descending)"
        },
        {
            "fname": "torch.nn.maxunpool1d.yaml",
            "arg": "stride",
            "prompt": "Signature: torch.distributed.recv(tensor,src=None,group=<objectobject>,tag=0)\nParameter description: tensor(Tensor) - Tensor to fill with received data.\nConstraints:\ndtype:Null\nstructure:tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.div(input,other,out=None)\nParameter description: out(Tensor, optional) - the output tensor.\nConstraints:\ndtype:Null\nstructure:tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.nn.TransformerDecoder(decoder_layer,num_layers,norm=None)\nParameter description: num_layers - the number of sub-decoder-layers in the decoder (required).\nConstraints:\ndtype:int\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:[0,inf)\n\n###\n\nSignature: torch.nn.quantized.functional.interpolate(input,size=None,scale_factor=None,mode=nearest,align_corners=None)\nParameter description: align_corners(bool, optional) - Geometrically, we consider the pixels of the input and output as squares rather than points. If set to `True`, the input and output tensors are aligned by the center points of their corner pixels, preserving the values at the corner pixels. If set to `False`, the input and output tensors are aligned by the corner points of their corner pixels, and the interpolation uses edge value padding for out-of-boundary values, making this operation independent of input size when `scale_factor` is kept the same. This only has an effect when `mode` is `'bilinear'`. Default: `False`\nConstraints:\ndtype:bool\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.gt(input,other,out=None)\nParameter description: out(Tensor, optional) - the output tensor that must be a BoolTensor\nConstraints:\ndtype:booltensor\nstructure:tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.empty_like(input,dtype=None,layout=None,device=None,requires_grad=False,memory_format=torch.preserve_format)\nParameter description: dtype(`torch.dtype`, optional) - the desired data type of returned Tensor. Default: if `None`, defaults to the dtype of `input`.\nConstraints:\ndtype:torch.dtype\nstructure:Null\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.nn.RNN(*args,**kwargs,input_size=None,hidden_size=None,num_layers=None,nonlinearity=None,bias=None,batch_first=None,dropout=None,bidirectional=None)\nParameter description: hidden_size - The number of features in the hidden state h\nConstraints:\ndtype:int\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:[0,inf)\n\n###\n\nSignature: torch.nn.functional.avg_pool1d(input,kernel_size,stride=None,padding=0,ceil_mode=False,count_include_pad=True)\nParameter description: padding - implicit zero paddings on both sides of the input. Can be a single number or a tuple (padW,). Default: 0\nConstraints:\ndtype:bool;int\nstructure:tuple\nshape:Null\nndim:0;1\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.autograd.grad(outputs,inputs,grad_outputs=None,retain_graph=None,create_graph=False,only_inputs=True,allow_unused=False)\nParameter description: outputs(sequence of Tensor) - outputs of the differentiated function.\nConstraints:\ndtype:Null\nstructure:sequence;tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.repeat_interleave(input,repeats,dim=None)\nParameter description: dim(int, optional) - The dimension along which to repeat values.By default, use the flattened input array, and return a flat outputarray.\nConstraints:\ndtype:int\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.nn.MaxUnpool1d(kernel_size,stride=None,padding=0)\nParameter description: stride(int or tuple) - Stride of the max pooling window. It is set to `kernel_size` by default.\nConstraints:",
            "constr": {
                "dtype": [
                    "int"
                ],
                "structure": [
                    "tuple"
                ],
                "shape": [],
                "ndim": [],
                "enum": [],
                "range": [
                    "[0,inf)"
                ]
            },
            "signature": "torch.nn.MaxUnpool1d(kernel_size,stride=None,padding=0)",
            "descp": "stride(int or tuple) - Stride of the max pooling window. It is set to `kernel_size` by default."
        },
        {
            "fname": "torch.nn.functional.log_softmax.yaml",
            "arg": "input",
            "prompt": "Signature: torch.distributed.recv(tensor,src=None,group=<objectobject>,tag=0)\nParameter description: tensor(Tensor) - Tensor to fill with received data.\nConstraints:\ndtype:Null\nstructure:tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.div(input,other,out=None)\nParameter description: out(Tensor, optional) - the output tensor.\nConstraints:\ndtype:Null\nstructure:tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.nn.TransformerDecoder(decoder_layer,num_layers,norm=None)\nParameter description: num_layers - the number of sub-decoder-layers in the decoder (required).\nConstraints:\ndtype:int\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:[0,inf)\n\n###\n\nSignature: torch.nn.quantized.functional.interpolate(input,size=None,scale_factor=None,mode=nearest,align_corners=None)\nParameter description: align_corners(bool, optional) - Geometrically, we consider the pixels of the input and output as squares rather than points. If set to `True`, the input and output tensors are aligned by the center points of their corner pixels, preserving the values at the corner pixels. If set to `False`, the input and output tensors are aligned by the corner points of their corner pixels, and the interpolation uses edge value padding for out-of-boundary values, making this operation independent of input size when `scale_factor` is kept the same. This only has an effect when `mode` is `'bilinear'`. Default: `False`\nConstraints:\ndtype:bool\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.gt(input,other,out=None)\nParameter description: out(Tensor, optional) - the output tensor that must be a BoolTensor\nConstraints:\ndtype:booltensor\nstructure:tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.empty_like(input,dtype=None,layout=None,device=None,requires_grad=False,memory_format=torch.preserve_format)\nParameter description: dtype(`torch.dtype`, optional) - the desired data type of returned Tensor. Default: if `None`, defaults to the dtype of `input`.\nConstraints:\ndtype:torch.dtype\nstructure:Null\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.nn.RNN(*args,**kwargs,input_size=None,hidden_size=None,num_layers=None,nonlinearity=None,bias=None,batch_first=None,dropout=None,bidirectional=None)\nParameter description: hidden_size - The number of features in the hidden state h\nConstraints:\ndtype:int\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:[0,inf)\n\n###\n\nSignature: torch.nn.functional.avg_pool1d(input,kernel_size,stride=None,padding=0,ceil_mode=False,count_include_pad=True)\nParameter description: padding - implicit zero paddings on both sides of the input. Can be a single number or a tuple (padW,). Default: 0\nConstraints:\ndtype:bool;int\nstructure:tuple\nshape:Null\nndim:0;1\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.autograd.grad(outputs,inputs,grad_outputs=None,retain_graph=None,create_graph=False,only_inputs=True,allow_unused=False)\nParameter description: outputs(sequence of Tensor) - outputs of the differentiated function.\nConstraints:\ndtype:Null\nstructure:sequence;tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.repeat_interleave(input,repeats,dim=None)\nParameter description: dim(int, optional) - The dimension along which to repeat values.By default, use the flattened input array, and return a flat outputarray.\nConstraints:\ndtype:int\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.nn.functional.log_softmax(input,dim=None,_stacklevel=3,dtype=None)\nParameter description: input(Tensor) - input\nConstraints:",
            "constr": {
                "dtype": [],
                "structure": [
                    "tensor"
                ],
                "shape": [],
                "ndim": [],
                "enum": [],
                "range": []
            },
            "signature": "torch.nn.functional.log_softmax(input,dim=None,_stacklevel=3,dtype=None)",
            "descp": "input(Tensor) - input"
        },
        {
            "fname": "torch.quantization.swap_module.yaml",
            "arg": "mapping",
            "prompt": "Signature: torch.distributed.recv(tensor,src=None,group=<objectobject>,tag=0)\nParameter description: tensor(Tensor) - Tensor to fill with received data.\nConstraints:\ndtype:Null\nstructure:tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.div(input,other,out=None)\nParameter description: out(Tensor, optional) - the output tensor.\nConstraints:\ndtype:Null\nstructure:tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.nn.TransformerDecoder(decoder_layer,num_layers,norm=None)\nParameter description: num_layers - the number of sub-decoder-layers in the decoder (required).\nConstraints:\ndtype:int\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:[0,inf)\n\n###\n\nSignature: torch.nn.quantized.functional.interpolate(input,size=None,scale_factor=None,mode=nearest,align_corners=None)\nParameter description: align_corners(bool, optional) - Geometrically, we consider the pixels of the input and output as squares rather than points. If set to `True`, the input and output tensors are aligned by the center points of their corner pixels, preserving the values at the corner pixels. If set to `False`, the input and output tensors are aligned by the corner points of their corner pixels, and the interpolation uses edge value padding for out-of-boundary values, making this operation independent of input size when `scale_factor` is kept the same. This only has an effect when `mode` is `'bilinear'`. Default: `False`\nConstraints:\ndtype:bool\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.gt(input,other,out=None)\nParameter description: out(Tensor, optional) - the output tensor that must be a BoolTensor\nConstraints:\ndtype:booltensor\nstructure:tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.empty_like(input,dtype=None,layout=None,device=None,requires_grad=False,memory_format=torch.preserve_format)\nParameter description: dtype(`torch.dtype`, optional) - the desired data type of returned Tensor. Default: if `None`, defaults to the dtype of `input`.\nConstraints:\ndtype:torch.dtype\nstructure:Null\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.nn.RNN(*args,**kwargs,input_size=None,hidden_size=None,num_layers=None,nonlinearity=None,bias=None,batch_first=None,dropout=None,bidirectional=None)\nParameter description: hidden_size - The number of features in the hidden state h\nConstraints:\ndtype:int\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:[0,inf)\n\n###\n\nSignature: torch.nn.functional.avg_pool1d(input,kernel_size,stride=None,padding=0,ceil_mode=False,count_include_pad=True)\nParameter description: padding - implicit zero paddings on both sides of the input. Can be a single number or a tuple (padW,). Default: 0\nConstraints:\ndtype:bool;int\nstructure:tuple\nshape:Null\nndim:0;1\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.autograd.grad(outputs,inputs,grad_outputs=None,retain_graph=None,create_graph=False,only_inputs=True,allow_unused=False)\nParameter description: outputs(sequence of Tensor) - outputs of the differentiated function.\nConstraints:\ndtype:Null\nstructure:sequence;tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.repeat_interleave(input,repeats,dim=None)\nParameter description: dim(int, optional) - The dimension along which to repeat values.By default, use the flattened input array, and return a flat outputarray.\nConstraints:\ndtype:int\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.quantization.swap_module(mod,mapping)\nParameter description: mapping - a dictionary that maps from nn module to nnq module\nConstraints:",
            "constr": {
                "dtype": [],
                "structure": [
                    "dictionary"
                ],
                "shape": [],
                "ndim": [],
                "enum": [],
                "range": []
            },
            "signature": "torch.quantization.swap_module(mod,mapping)",
            "descp": "mapping - a dictionary that maps from nn module to nnq module"
        },
        {
            "fname": "torch.chain_matmul.yaml",
            "arg": "*matrices",
            "prompt": "Signature: torch.distributed.recv(tensor,src=None,group=<objectobject>,tag=0)\nParameter description: tensor(Tensor) - Tensor to fill with received data.\nConstraints:\ndtype:Null\nstructure:tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.div(input,other,out=None)\nParameter description: out(Tensor, optional) - the output tensor.\nConstraints:\ndtype:Null\nstructure:tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.nn.TransformerDecoder(decoder_layer,num_layers,norm=None)\nParameter description: num_layers - the number of sub-decoder-layers in the decoder (required).\nConstraints:\ndtype:int\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:[0,inf)\n\n###\n\nSignature: torch.nn.quantized.functional.interpolate(input,size=None,scale_factor=None,mode=nearest,align_corners=None)\nParameter description: align_corners(bool, optional) - Geometrically, we consider the pixels of the input and output as squares rather than points. If set to `True`, the input and output tensors are aligned by the center points of their corner pixels, preserving the values at the corner pixels. If set to `False`, the input and output tensors are aligned by the corner points of their corner pixels, and the interpolation uses edge value padding for out-of-boundary values, making this operation independent of input size when `scale_factor` is kept the same. This only has an effect when `mode` is `'bilinear'`. Default: `False`\nConstraints:\ndtype:bool\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.gt(input,other,out=None)\nParameter description: out(Tensor, optional) - the output tensor that must be a BoolTensor\nConstraints:\ndtype:booltensor\nstructure:tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.empty_like(input,dtype=None,layout=None,device=None,requires_grad=False,memory_format=torch.preserve_format)\nParameter description: dtype(`torch.dtype`, optional) - the desired data type of returned Tensor. Default: if `None`, defaults to the dtype of `input`.\nConstraints:\ndtype:torch.dtype\nstructure:Null\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.nn.RNN(*args,**kwargs,input_size=None,hidden_size=None,num_layers=None,nonlinearity=None,bias=None,batch_first=None,dropout=None,bidirectional=None)\nParameter description: hidden_size - The number of features in the hidden state h\nConstraints:\ndtype:int\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:[0,inf)\n\n###\n\nSignature: torch.nn.functional.avg_pool1d(input,kernel_size,stride=None,padding=0,ceil_mode=False,count_include_pad=True)\nParameter description: padding - implicit zero paddings on both sides of the input. Can be a single number or a tuple (padW,). Default: 0\nConstraints:\ndtype:bool;int\nstructure:tuple\nshape:Null\nndim:0;1\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.autograd.grad(outputs,inputs,grad_outputs=None,retain_graph=None,create_graph=False,only_inputs=True,allow_unused=False)\nParameter description: outputs(sequence of Tensor) - outputs of the differentiated function.\nConstraints:\ndtype:Null\nstructure:sequence;tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.repeat_interleave(input,repeats,dim=None)\nParameter description: dim(int, optional) - The dimension along which to repeat values.By default, use the flattened input array, and return a flat outputarray.\nConstraints:\ndtype:int\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.chain_matmul(*matrices)\nParameter description: *matrices(Tensors...) - a sequence of 2 or more 2-D tensors whose product is to be determined.\nConstraints:",
            "constr": {
                "dtype": [],
                "structure": [
                    "sequence",
                    "tensor"
                ],
                "shape": [],
                "ndim": [
                    "1"
                ],
                "enum": [],
                "range": []
            },
            "signature": "torch.chain_matmul(*matrices)",
            "descp": "*matrices(Tensors...) - a sequence of 2 or more 2-D tensors whose product is to be determined."
        },
        {
            "fname": "torch.nn.avgpool1d.yaml",
            "arg": "kernel_size",
            "prompt": "Signature: torch.distributed.recv(tensor,src=None,group=<objectobject>,tag=0)\nParameter description: tensor(Tensor) - Tensor to fill with received data.\nConstraints:\ndtype:Null\nstructure:tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.div(input,other,out=None)\nParameter description: out(Tensor, optional) - the output tensor.\nConstraints:\ndtype:Null\nstructure:tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.nn.TransformerDecoder(decoder_layer,num_layers,norm=None)\nParameter description: num_layers - the number of sub-decoder-layers in the decoder (required).\nConstraints:\ndtype:int\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:[0,inf)\n\n###\n\nSignature: torch.nn.quantized.functional.interpolate(input,size=None,scale_factor=None,mode=nearest,align_corners=None)\nParameter description: align_corners(bool, optional) - Geometrically, we consider the pixels of the input and output as squares rather than points. If set to `True`, the input and output tensors are aligned by the center points of their corner pixels, preserving the values at the corner pixels. If set to `False`, the input and output tensors are aligned by the corner points of their corner pixels, and the interpolation uses edge value padding for out-of-boundary values, making this operation independent of input size when `scale_factor` is kept the same. This only has an effect when `mode` is `'bilinear'`. Default: `False`\nConstraints:\ndtype:bool\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.gt(input,other,out=None)\nParameter description: out(Tensor, optional) - the output tensor that must be a BoolTensor\nConstraints:\ndtype:booltensor\nstructure:tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.empty_like(input,dtype=None,layout=None,device=None,requires_grad=False,memory_format=torch.preserve_format)\nParameter description: dtype(`torch.dtype`, optional) - the desired data type of returned Tensor. Default: if `None`, defaults to the dtype of `input`.\nConstraints:\ndtype:torch.dtype\nstructure:Null\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.nn.RNN(*args,**kwargs,input_size=None,hidden_size=None,num_layers=None,nonlinearity=None,bias=None,batch_first=None,dropout=None,bidirectional=None)\nParameter description: hidden_size - The number of features in the hidden state h\nConstraints:\ndtype:int\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:[0,inf)\n\n###\n\nSignature: torch.nn.functional.avg_pool1d(input,kernel_size,stride=None,padding=0,ceil_mode=False,count_include_pad=True)\nParameter description: padding - implicit zero paddings on both sides of the input. Can be a single number or a tuple (padW,). Default: 0\nConstraints:\ndtype:bool;int\nstructure:tuple\nshape:Null\nndim:0;1\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.autograd.grad(outputs,inputs,grad_outputs=None,retain_graph=None,create_graph=False,only_inputs=True,allow_unused=False)\nParameter description: outputs(sequence of Tensor) - outputs of the differentiated function.\nConstraints:\ndtype:Null\nstructure:sequence;tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.repeat_interleave(input,repeats,dim=None)\nParameter description: dim(int, optional) - The dimension along which to repeat values.By default, use the flattened input array, and return a flat outputarray.\nConstraints:\ndtype:int\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.nn.AvgPool1d(kernel_size,stride=None,padding=0,ceil_mode=False,count_include_pad=True)\nParameter description: kernel_size - the size of the window\nConstraints:",
            "constr": {
                "dtype": [
                    "int"
                ],
                "structure": [],
                "shape": [],
                "ndim": [],
                "enum": [],
                "range": [
                    "[0,inf)"
                ]
            },
            "signature": "torch.nn.AvgPool1d(kernel_size,stride=None,padding=0,ceil_mode=False,count_include_pad=True)",
            "descp": "kernel_size - the size of the window"
        },
        {
            "fname": "torch.distributed.send.yaml",
            "arg": "dst",
            "prompt": "Signature: torch.distributed.recv(tensor,src=None,group=<objectobject>,tag=0)\nParameter description: tensor(Tensor) - Tensor to fill with received data.\nConstraints:\ndtype:Null\nstructure:tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.div(input,other,out=None)\nParameter description: out(Tensor, optional) - the output tensor.\nConstraints:\ndtype:Null\nstructure:tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.nn.TransformerDecoder(decoder_layer,num_layers,norm=None)\nParameter description: num_layers - the number of sub-decoder-layers in the decoder (required).\nConstraints:\ndtype:int\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:[0,inf)\n\n###\n\nSignature: torch.nn.quantized.functional.interpolate(input,size=None,scale_factor=None,mode=nearest,align_corners=None)\nParameter description: align_corners(bool, optional) - Geometrically, we consider the pixels of the input and output as squares rather than points. If set to `True`, the input and output tensors are aligned by the center points of their corner pixels, preserving the values at the corner pixels. If set to `False`, the input and output tensors are aligned by the corner points of their corner pixels, and the interpolation uses edge value padding for out-of-boundary values, making this operation independent of input size when `scale_factor` is kept the same. This only has an effect when `mode` is `'bilinear'`. Default: `False`\nConstraints:\ndtype:bool\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.gt(input,other,out=None)\nParameter description: out(Tensor, optional) - the output tensor that must be a BoolTensor\nConstraints:\ndtype:booltensor\nstructure:tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.empty_like(input,dtype=None,layout=None,device=None,requires_grad=False,memory_format=torch.preserve_format)\nParameter description: dtype(`torch.dtype`, optional) - the desired data type of returned Tensor. Default: if `None`, defaults to the dtype of `input`.\nConstraints:\ndtype:torch.dtype\nstructure:Null\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.nn.RNN(*args,**kwargs,input_size=None,hidden_size=None,num_layers=None,nonlinearity=None,bias=None,batch_first=None,dropout=None,bidirectional=None)\nParameter description: hidden_size - The number of features in the hidden state h\nConstraints:\ndtype:int\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:[0,inf)\n\n###\n\nSignature: torch.nn.functional.avg_pool1d(input,kernel_size,stride=None,padding=0,ceil_mode=False,count_include_pad=True)\nParameter description: padding - implicit zero paddings on both sides of the input. Can be a single number or a tuple (padW,). Default: 0\nConstraints:\ndtype:bool;int\nstructure:tuple\nshape:Null\nndim:0;1\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.autograd.grad(outputs,inputs,grad_outputs=None,retain_graph=None,create_graph=False,only_inputs=True,allow_unused=False)\nParameter description: outputs(sequence of Tensor) - outputs of the differentiated function.\nConstraints:\ndtype:Null\nstructure:sequence;tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.repeat_interleave(input,repeats,dim=None)\nParameter description: dim(int, optional) - The dimension along which to repeat values.By default, use the flattened input array, and return a flat outputarray.\nConstraints:\ndtype:int\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.distributed.send(tensor,dst,group=<objectobject>,tag=0)\nParameter description: dst(int) - Destination rank.\nConstraints:",
            "constr": {
                "dtype": [
                    "int"
                ],
                "structure": [],
                "shape": [],
                "ndim": [
                    "0"
                ],
                "enum": [],
                "range": []
            },
            "signature": "torch.distributed.send(tensor,dst,group=<objectobject>,tag=0)",
            "descp": "dst(int) - Destination rank."
        },
        {
            "fname": "torch.utils.checkpoint.checkpoint.yaml",
            "arg": "preserve_rng_state",
            "prompt": "Signature: torch.distributed.recv(tensor,src=None,group=<objectobject>,tag=0)\nParameter description: tensor(Tensor) - Tensor to fill with received data.\nConstraints:\ndtype:Null\nstructure:tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.div(input,other,out=None)\nParameter description: out(Tensor, optional) - the output tensor.\nConstraints:\ndtype:Null\nstructure:tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.nn.TransformerDecoder(decoder_layer,num_layers,norm=None)\nParameter description: num_layers - the number of sub-decoder-layers in the decoder (required).\nConstraints:\ndtype:int\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:[0,inf)\n\n###\n\nSignature: torch.nn.quantized.functional.interpolate(input,size=None,scale_factor=None,mode=nearest,align_corners=None)\nParameter description: align_corners(bool, optional) - Geometrically, we consider the pixels of the input and output as squares rather than points. If set to `True`, the input and output tensors are aligned by the center points of their corner pixels, preserving the values at the corner pixels. If set to `False`, the input and output tensors are aligned by the corner points of their corner pixels, and the interpolation uses edge value padding for out-of-boundary values, making this operation independent of input size when `scale_factor` is kept the same. This only has an effect when `mode` is `'bilinear'`. Default: `False`\nConstraints:\ndtype:bool\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.gt(input,other,out=None)\nParameter description: out(Tensor, optional) - the output tensor that must be a BoolTensor\nConstraints:\ndtype:booltensor\nstructure:tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.empty_like(input,dtype=None,layout=None,device=None,requires_grad=False,memory_format=torch.preserve_format)\nParameter description: dtype(`torch.dtype`, optional) - the desired data type of returned Tensor. Default: if `None`, defaults to the dtype of `input`.\nConstraints:\ndtype:torch.dtype\nstructure:Null\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.nn.RNN(*args,**kwargs,input_size=None,hidden_size=None,num_layers=None,nonlinearity=None,bias=None,batch_first=None,dropout=None,bidirectional=None)\nParameter description: hidden_size - The number of features in the hidden state h\nConstraints:\ndtype:int\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:[0,inf)\n\n###\n\nSignature: torch.nn.functional.avg_pool1d(input,kernel_size,stride=None,padding=0,ceil_mode=False,count_include_pad=True)\nParameter description: padding - implicit zero paddings on both sides of the input. Can be a single number or a tuple (padW,). Default: 0\nConstraints:\ndtype:bool;int\nstructure:tuple\nshape:Null\nndim:0;1\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.autograd.grad(outputs,inputs,grad_outputs=None,retain_graph=None,create_graph=False,only_inputs=True,allow_unused=False)\nParameter description: outputs(sequence of Tensor) - outputs of the differentiated function.\nConstraints:\ndtype:Null\nstructure:sequence;tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.repeat_interleave(input,repeats,dim=None)\nParameter description: dim(int, optional) - The dimension along which to repeat values.By default, use the flattened input array, and return a flat outputarray.\nConstraints:\ndtype:int\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.utils.checkpoint.checkpoint(function,*args,**kwargs,preserve_rng_state=None)\nParameter description: preserve_rng_state(bool, optional, default=True) - Omit stashing and restoring the RNG state during each checkpoint.\nConstraints:",
            "constr": {
                "dtype": [
                    "bool"
                ],
                "structure": [],
                "shape": [],
                "ndim": [],
                "enum": [],
                "range": []
            },
            "signature": "torch.utils.checkpoint.checkpoint(function,*args,**kwargs,preserve_rng_state=None)",
            "descp": "preserve_rng_state(bool, optional, default=True) - Omit stashing and restoring the RNG state during each checkpoint."
        },
        {
            "fname": "torch.renorm.yaml",
            "arg": "input",
            "prompt": "Signature: torch.distributed.recv(tensor,src=None,group=<objectobject>,tag=0)\nParameter description: tensor(Tensor) - Tensor to fill with received data.\nConstraints:\ndtype:Null\nstructure:tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.div(input,other,out=None)\nParameter description: out(Tensor, optional) - the output tensor.\nConstraints:\ndtype:Null\nstructure:tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.nn.TransformerDecoder(decoder_layer,num_layers,norm=None)\nParameter description: num_layers - the number of sub-decoder-layers in the decoder (required).\nConstraints:\ndtype:int\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:[0,inf)\n\n###\n\nSignature: torch.nn.quantized.functional.interpolate(input,size=None,scale_factor=None,mode=nearest,align_corners=None)\nParameter description: align_corners(bool, optional) - Geometrically, we consider the pixels of the input and output as squares rather than points. If set to `True`, the input and output tensors are aligned by the center points of their corner pixels, preserving the values at the corner pixels. If set to `False`, the input and output tensors are aligned by the corner points of their corner pixels, and the interpolation uses edge value padding for out-of-boundary values, making this operation independent of input size when `scale_factor` is kept the same. This only has an effect when `mode` is `'bilinear'`. Default: `False`\nConstraints:\ndtype:bool\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.gt(input,other,out=None)\nParameter description: out(Tensor, optional) - the output tensor that must be a BoolTensor\nConstraints:\ndtype:booltensor\nstructure:tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.empty_like(input,dtype=None,layout=None,device=None,requires_grad=False,memory_format=torch.preserve_format)\nParameter description: dtype(`torch.dtype`, optional) - the desired data type of returned Tensor. Default: if `None`, defaults to the dtype of `input`.\nConstraints:\ndtype:torch.dtype\nstructure:Null\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.nn.RNN(*args,**kwargs,input_size=None,hidden_size=None,num_layers=None,nonlinearity=None,bias=None,batch_first=None,dropout=None,bidirectional=None)\nParameter description: hidden_size - The number of features in the hidden state h\nConstraints:\ndtype:int\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:[0,inf)\n\n###\n\nSignature: torch.nn.functional.avg_pool1d(input,kernel_size,stride=None,padding=0,ceil_mode=False,count_include_pad=True)\nParameter description: padding - implicit zero paddings on both sides of the input. Can be a single number or a tuple (padW,). Default: 0\nConstraints:\ndtype:bool;int\nstructure:tuple\nshape:Null\nndim:0;1\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.autograd.grad(outputs,inputs,grad_outputs=None,retain_graph=None,create_graph=False,only_inputs=True,allow_unused=False)\nParameter description: outputs(sequence of Tensor) - outputs of the differentiated function.\nConstraints:\ndtype:Null\nstructure:sequence;tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.repeat_interleave(input,repeats,dim=None)\nParameter description: dim(int, optional) - The dimension along which to repeat values.By default, use the flattened input array, and return a flat outputarray.\nConstraints:\ndtype:int\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.renorm(input,p,dim,maxnorm,out=None)\nParameter description: input(Tensor) - the input tensor.\nConstraints:",
            "constr": {
                "dtype": [],
                "structure": [
                    "tensor"
                ],
                "shape": [],
                "ndim": [],
                "enum": [],
                "range": []
            },
            "signature": "torch.renorm(input,p,dim,maxnorm,out=None)",
            "descp": "input(Tensor) - the input tensor."
        },
        {
            "fname": "torch.nn.localresponsenorm.yaml",
            "arg": "alpha",
            "prompt": "Signature: torch.distributed.recv(tensor,src=None,group=<objectobject>,tag=0)\nParameter description: tensor(Tensor) - Tensor to fill with received data.\nConstraints:\ndtype:Null\nstructure:tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.div(input,other,out=None)\nParameter description: out(Tensor, optional) - the output tensor.\nConstraints:\ndtype:Null\nstructure:tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.nn.TransformerDecoder(decoder_layer,num_layers,norm=None)\nParameter description: num_layers - the number of sub-decoder-layers in the decoder (required).\nConstraints:\ndtype:int\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:[0,inf)\n\n###\n\nSignature: torch.nn.quantized.functional.interpolate(input,size=None,scale_factor=None,mode=nearest,align_corners=None)\nParameter description: align_corners(bool, optional) - Geometrically, we consider the pixels of the input and output as squares rather than points. If set to `True`, the input and output tensors are aligned by the center points of their corner pixels, preserving the values at the corner pixels. If set to `False`, the input and output tensors are aligned by the corner points of their corner pixels, and the interpolation uses edge value padding for out-of-boundary values, making this operation independent of input size when `scale_factor` is kept the same. This only has an effect when `mode` is `'bilinear'`. Default: `False`\nConstraints:\ndtype:bool\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.gt(input,other,out=None)\nParameter description: out(Tensor, optional) - the output tensor that must be a BoolTensor\nConstraints:\ndtype:booltensor\nstructure:tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.empty_like(input,dtype=None,layout=None,device=None,requires_grad=False,memory_format=torch.preserve_format)\nParameter description: dtype(`torch.dtype`, optional) - the desired data type of returned Tensor. Default: if `None`, defaults to the dtype of `input`.\nConstraints:\ndtype:torch.dtype\nstructure:Null\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.nn.RNN(*args,**kwargs,input_size=None,hidden_size=None,num_layers=None,nonlinearity=None,bias=None,batch_first=None,dropout=None,bidirectional=None)\nParameter description: hidden_size - The number of features in the hidden state h\nConstraints:\ndtype:int\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:[0,inf)\n\n###\n\nSignature: torch.nn.functional.avg_pool1d(input,kernel_size,stride=None,padding=0,ceil_mode=False,count_include_pad=True)\nParameter description: padding - implicit zero paddings on both sides of the input. Can be a single number or a tuple (padW,). Default: 0\nConstraints:\ndtype:bool;int\nstructure:tuple\nshape:Null\nndim:0;1\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.autograd.grad(outputs,inputs,grad_outputs=None,retain_graph=None,create_graph=False,only_inputs=True,allow_unused=False)\nParameter description: outputs(sequence of Tensor) - outputs of the differentiated function.\nConstraints:\ndtype:Null\nstructure:sequence;tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.repeat_interleave(input,repeats,dim=None)\nParameter description: dim(int, optional) - The dimension along which to repeat values.By default, use the flattened input array, and return a flat outputarray.\nConstraints:\ndtype:int\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.nn.LocalResponseNorm(size,alpha=0.0001,beta=0.75,k=1.0)\nParameter description: alpha - multiplicative factor. Default: 0.0001\nConstraints:",
            "constr": {
                "dtype": [
                    "numeric",
                    "float",
                    "bool"
                ],
                "structure": [],
                "shape": [],
                "ndim": [
                    "0"
                ],
                "enum": [],
                "range": []
            },
            "signature": "torch.nn.LocalResponseNorm(size,alpha=0.0001,beta=0.75,k=1.0)",
            "descp": "alpha - multiplicative factor. Default: 0.0001"
        },
        {
            "fname": "torch.nn.maxpool3d.yaml",
            "arg": "return_indices",
            "prompt": "Signature: torch.distributed.recv(tensor,src=None,group=<objectobject>,tag=0)\nParameter description: tensor(Tensor) - Tensor to fill with received data.\nConstraints:\ndtype:Null\nstructure:tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.div(input,other,out=None)\nParameter description: out(Tensor, optional) - the output tensor.\nConstraints:\ndtype:Null\nstructure:tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.nn.TransformerDecoder(decoder_layer,num_layers,norm=None)\nParameter description: num_layers - the number of sub-decoder-layers in the decoder (required).\nConstraints:\ndtype:int\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:[0,inf)\n\n###\n\nSignature: torch.nn.quantized.functional.interpolate(input,size=None,scale_factor=None,mode=nearest,align_corners=None)\nParameter description: align_corners(bool, optional) - Geometrically, we consider the pixels of the input and output as squares rather than points. If set to `True`, the input and output tensors are aligned by the center points of their corner pixels, preserving the values at the corner pixels. If set to `False`, the input and output tensors are aligned by the corner points of their corner pixels, and the interpolation uses edge value padding for out-of-boundary values, making this operation independent of input size when `scale_factor` is kept the same. This only has an effect when `mode` is `'bilinear'`. Default: `False`\nConstraints:\ndtype:bool\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.gt(input,other,out=None)\nParameter description: out(Tensor, optional) - the output tensor that must be a BoolTensor\nConstraints:\ndtype:booltensor\nstructure:tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.empty_like(input,dtype=None,layout=None,device=None,requires_grad=False,memory_format=torch.preserve_format)\nParameter description: dtype(`torch.dtype`, optional) - the desired data type of returned Tensor. Default: if `None`, defaults to the dtype of `input`.\nConstraints:\ndtype:torch.dtype\nstructure:Null\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.nn.RNN(*args,**kwargs,input_size=None,hidden_size=None,num_layers=None,nonlinearity=None,bias=None,batch_first=None,dropout=None,bidirectional=None)\nParameter description: hidden_size - The number of features in the hidden state h\nConstraints:\ndtype:int\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:[0,inf)\n\n###\n\nSignature: torch.nn.functional.avg_pool1d(input,kernel_size,stride=None,padding=0,ceil_mode=False,count_include_pad=True)\nParameter description: padding - implicit zero paddings on both sides of the input. Can be a single number or a tuple (padW,). Default: 0\nConstraints:\ndtype:bool;int\nstructure:tuple\nshape:Null\nndim:0;1\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.autograd.grad(outputs,inputs,grad_outputs=None,retain_graph=None,create_graph=False,only_inputs=True,allow_unused=False)\nParameter description: outputs(sequence of Tensor) - outputs of the differentiated function.\nConstraints:\ndtype:Null\nstructure:sequence;tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.repeat_interleave(input,repeats,dim=None)\nParameter description: dim(int, optional) - The dimension along which to repeat values.By default, use the flattened input array, and return a flat outputarray.\nConstraints:\ndtype:int\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.nn.MaxPool3d(kernel_size,stride=None,padding=0,dilation=1,return_indices=False,ceil_mode=False)\nParameter description: return_indices - if `True`, will return the max indices along with the outputs. Useful for `torch.nn.MaxUnpool3d` later\nConstraints:",
            "constr": {
                "dtype": [
                    "bool"
                ],
                "structure": [],
                "shape": [],
                "ndim": [
                    "0"
                ],
                "enum": [],
                "range": []
            },
            "signature": "torch.nn.MaxPool3d(kernel_size,stride=None,padding=0,dilation=1,return_indices=False,ceil_mode=False)",
            "descp": "return_indices - if `True`, will return the max indices along with the outputs. Useful for `torch.nn.MaxUnpool3d` later"
        },
        {
            "fname": "torch.abs.yaml",
            "arg": "input",
            "prompt": "Signature: torch.distributed.recv(tensor,src=None,group=<objectobject>,tag=0)\nParameter description: tensor(Tensor) - Tensor to fill with received data.\nConstraints:\ndtype:Null\nstructure:tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.div(input,other,out=None)\nParameter description: out(Tensor, optional) - the output tensor.\nConstraints:\ndtype:Null\nstructure:tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.nn.TransformerDecoder(decoder_layer,num_layers,norm=None)\nParameter description: num_layers - the number of sub-decoder-layers in the decoder (required).\nConstraints:\ndtype:int\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:[0,inf)\n\n###\n\nSignature: torch.nn.quantized.functional.interpolate(input,size=None,scale_factor=None,mode=nearest,align_corners=None)\nParameter description: align_corners(bool, optional) - Geometrically, we consider the pixels of the input and output as squares rather than points. If set to `True`, the input and output tensors are aligned by the center points of their corner pixels, preserving the values at the corner pixels. If set to `False`, the input and output tensors are aligned by the corner points of their corner pixels, and the interpolation uses edge value padding for out-of-boundary values, making this operation independent of input size when `scale_factor` is kept the same. This only has an effect when `mode` is `'bilinear'`. Default: `False`\nConstraints:\ndtype:bool\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.gt(input,other,out=None)\nParameter description: out(Tensor, optional) - the output tensor that must be a BoolTensor\nConstraints:\ndtype:booltensor\nstructure:tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.empty_like(input,dtype=None,layout=None,device=None,requires_grad=False,memory_format=torch.preserve_format)\nParameter description: dtype(`torch.dtype`, optional) - the desired data type of returned Tensor. Default: if `None`, defaults to the dtype of `input`.\nConstraints:\ndtype:torch.dtype\nstructure:Null\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.nn.RNN(*args,**kwargs,input_size=None,hidden_size=None,num_layers=None,nonlinearity=None,bias=None,batch_first=None,dropout=None,bidirectional=None)\nParameter description: hidden_size - The number of features in the hidden state h\nConstraints:\ndtype:int\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:[0,inf)\n\n###\n\nSignature: torch.nn.functional.avg_pool1d(input,kernel_size,stride=None,padding=0,ceil_mode=False,count_include_pad=True)\nParameter description: padding - implicit zero paddings on both sides of the input. Can be a single number or a tuple (padW,). Default: 0\nConstraints:\ndtype:bool;int\nstructure:tuple\nshape:Null\nndim:0;1\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.autograd.grad(outputs,inputs,grad_outputs=None,retain_graph=None,create_graph=False,only_inputs=True,allow_unused=False)\nParameter description: outputs(sequence of Tensor) - outputs of the differentiated function.\nConstraints:\ndtype:Null\nstructure:sequence;tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.repeat_interleave(input,repeats,dim=None)\nParameter description: dim(int, optional) - The dimension along which to repeat values.By default, use the flattened input array, and return a flat outputarray.\nConstraints:\ndtype:int\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.abs(input,out=None)\nParameter description: input(Tensor) - the input tensor.\nConstraints:",
            "constr": {
                "dtype": [],
                "structure": [
                    "tensor"
                ],
                "shape": [],
                "ndim": [],
                "enum": [],
                "range": []
            },
            "signature": "torch.abs(input,out=None)",
            "descp": "input(Tensor) - the input tensor."
        },
        {
            "fname": "torch.logical_or.yaml",
            "arg": "out",
            "prompt": "Signature: torch.distributed.recv(tensor,src=None,group=<objectobject>,tag=0)\nParameter description: tensor(Tensor) - Tensor to fill with received data.\nConstraints:\ndtype:Null\nstructure:tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.div(input,other,out=None)\nParameter description: out(Tensor, optional) - the output tensor.\nConstraints:\ndtype:Null\nstructure:tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.nn.TransformerDecoder(decoder_layer,num_layers,norm=None)\nParameter description: num_layers - the number of sub-decoder-layers in the decoder (required).\nConstraints:\ndtype:int\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:[0,inf)\n\n###\n\nSignature: torch.nn.quantized.functional.interpolate(input,size=None,scale_factor=None,mode=nearest,align_corners=None)\nParameter description: align_corners(bool, optional) - Geometrically, we consider the pixels of the input and output as squares rather than points. If set to `True`, the input and output tensors are aligned by the center points of their corner pixels, preserving the values at the corner pixels. If set to `False`, the input and output tensors are aligned by the corner points of their corner pixels, and the interpolation uses edge value padding for out-of-boundary values, making this operation independent of input size when `scale_factor` is kept the same. This only has an effect when `mode` is `'bilinear'`. Default: `False`\nConstraints:\ndtype:bool\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.gt(input,other,out=None)\nParameter description: out(Tensor, optional) - the output tensor that must be a BoolTensor\nConstraints:\ndtype:booltensor\nstructure:tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.empty_like(input,dtype=None,layout=None,device=None,requires_grad=False,memory_format=torch.preserve_format)\nParameter description: dtype(`torch.dtype`, optional) - the desired data type of returned Tensor. Default: if `None`, defaults to the dtype of `input`.\nConstraints:\ndtype:torch.dtype\nstructure:Null\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.nn.RNN(*args,**kwargs,input_size=None,hidden_size=None,num_layers=None,nonlinearity=None,bias=None,batch_first=None,dropout=None,bidirectional=None)\nParameter description: hidden_size - The number of features in the hidden state h\nConstraints:\ndtype:int\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:[0,inf)\n\n###\n\nSignature: torch.nn.functional.avg_pool1d(input,kernel_size,stride=None,padding=0,ceil_mode=False,count_include_pad=True)\nParameter description: padding - implicit zero paddings on both sides of the input. Can be a single number or a tuple (padW,). Default: 0\nConstraints:\ndtype:bool;int\nstructure:tuple\nshape:Null\nndim:0;1\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.autograd.grad(outputs,inputs,grad_outputs=None,retain_graph=None,create_graph=False,only_inputs=True,allow_unused=False)\nParameter description: outputs(sequence of Tensor) - outputs of the differentiated function.\nConstraints:\ndtype:Null\nstructure:sequence;tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.repeat_interleave(input,repeats,dim=None)\nParameter description: dim(int, optional) - The dimension along which to repeat values.By default, use the flattened input array, and return a flat outputarray.\nConstraints:\ndtype:int\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.logical_or(input,other,out=None)\nParameter description: out(Tensor, optional) - the output tensor.\nConstraints:",
            "constr": {
                "dtype": [],
                "structure": [
                    "tensor"
                ],
                "shape": [],
                "ndim": [],
                "enum": [],
                "range": []
            },
            "signature": "torch.logical_or(input,other,out=None)",
            "descp": "out(Tensor, optional) - the output tensor."
        },
        {
            "fname": "torch.nn.upsample.yaml",
            "arg": "size",
            "prompt": "Signature: torch.distributed.recv(tensor,src=None,group=<objectobject>,tag=0)\nParameter description: tensor(Tensor) - Tensor to fill with received data.\nConstraints:\ndtype:Null\nstructure:tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.div(input,other,out=None)\nParameter description: out(Tensor, optional) - the output tensor.\nConstraints:\ndtype:Null\nstructure:tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.nn.TransformerDecoder(decoder_layer,num_layers,norm=None)\nParameter description: num_layers - the number of sub-decoder-layers in the decoder (required).\nConstraints:\ndtype:int\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:[0,inf)\n\n###\n\nSignature: torch.nn.quantized.functional.interpolate(input,size=None,scale_factor=None,mode=nearest,align_corners=None)\nParameter description: align_corners(bool, optional) - Geometrically, we consider the pixels of the input and output as squares rather than points. If set to `True`, the input and output tensors are aligned by the center points of their corner pixels, preserving the values at the corner pixels. If set to `False`, the input and output tensors are aligned by the corner points of their corner pixels, and the interpolation uses edge value padding for out-of-boundary values, making this operation independent of input size when `scale_factor` is kept the same. This only has an effect when `mode` is `'bilinear'`. Default: `False`\nConstraints:\ndtype:bool\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.gt(input,other,out=None)\nParameter description: out(Tensor, optional) - the output tensor that must be a BoolTensor\nConstraints:\ndtype:booltensor\nstructure:tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.empty_like(input,dtype=None,layout=None,device=None,requires_grad=False,memory_format=torch.preserve_format)\nParameter description: dtype(`torch.dtype`, optional) - the desired data type of returned Tensor. Default: if `None`, defaults to the dtype of `input`.\nConstraints:\ndtype:torch.dtype\nstructure:Null\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.nn.RNN(*args,**kwargs,input_size=None,hidden_size=None,num_layers=None,nonlinearity=None,bias=None,batch_first=None,dropout=None,bidirectional=None)\nParameter description: hidden_size - The number of features in the hidden state h\nConstraints:\ndtype:int\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:[0,inf)\n\n###\n\nSignature: torch.nn.functional.avg_pool1d(input,kernel_size,stride=None,padding=0,ceil_mode=False,count_include_pad=True)\nParameter description: padding - implicit zero paddings on both sides of the input. Can be a single number or a tuple (padW,). Default: 0\nConstraints:\ndtype:bool;int\nstructure:tuple\nshape:Null\nndim:0;1\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.autograd.grad(outputs,inputs,grad_outputs=None,retain_graph=None,create_graph=False,only_inputs=True,allow_unused=False)\nParameter description: outputs(sequence of Tensor) - outputs of the differentiated function.\nConstraints:\ndtype:Null\nstructure:sequence;tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.repeat_interleave(input,repeats,dim=None)\nParameter description: dim(int, optional) - The dimension along which to repeat values.By default, use the flattened input array, and return a flat outputarray.\nConstraints:\ndtype:int\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.nn.Upsample(size=None,scale_factor=None,mode=nearest,align_corners=None)\nParameter description: size(int or Tuple[int] or Tuple[int, int] or Tuple[int, int, int], optional) - output spatial sizes\nConstraints:",
            "constr": {
                "dtype": [
                    "int"
                ],
                "structure": [
                    "tuple"
                ],
                "shape": [],
                "ndim": [
                    "0",
                    "1"
                ],
                "enum": [],
                "range": [
                    "[0,inf)"
                ]
            },
            "signature": "torch.nn.Upsample(size=None,scale_factor=None,mode=nearest,align_corners=None)",
            "descp": "size(int or Tuple[int] or Tuple[int, int] or Tuple[int, int, int], optional) - output spatial sizes"
        },
        {
            "fname": "torch.cuda.comm.broadcast_coalesced.yaml",
            "arg": "tensors",
            "prompt": "Signature: torch.distributed.recv(tensor,src=None,group=<objectobject>,tag=0)\nParameter description: tensor(Tensor) - Tensor to fill with received data.\nConstraints:\ndtype:Null\nstructure:tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.div(input,other,out=None)\nParameter description: out(Tensor, optional) - the output tensor.\nConstraints:\ndtype:Null\nstructure:tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.nn.TransformerDecoder(decoder_layer,num_layers,norm=None)\nParameter description: num_layers - the number of sub-decoder-layers in the decoder (required).\nConstraints:\ndtype:int\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:[0,inf)\n\n###\n\nSignature: torch.nn.quantized.functional.interpolate(input,size=None,scale_factor=None,mode=nearest,align_corners=None)\nParameter description: align_corners(bool, optional) - Geometrically, we consider the pixels of the input and output as squares rather than points. If set to `True`, the input and output tensors are aligned by the center points of their corner pixels, preserving the values at the corner pixels. If set to `False`, the input and output tensors are aligned by the corner points of their corner pixels, and the interpolation uses edge value padding for out-of-boundary values, making this operation independent of input size when `scale_factor` is kept the same. This only has an effect when `mode` is `'bilinear'`. Default: `False`\nConstraints:\ndtype:bool\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.gt(input,other,out=None)\nParameter description: out(Tensor, optional) - the output tensor that must be a BoolTensor\nConstraints:\ndtype:booltensor\nstructure:tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.empty_like(input,dtype=None,layout=None,device=None,requires_grad=False,memory_format=torch.preserve_format)\nParameter description: dtype(`torch.dtype`, optional) - the desired data type of returned Tensor. Default: if `None`, defaults to the dtype of `input`.\nConstraints:\ndtype:torch.dtype\nstructure:Null\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.nn.RNN(*args,**kwargs,input_size=None,hidden_size=None,num_layers=None,nonlinearity=None,bias=None,batch_first=None,dropout=None,bidirectional=None)\nParameter description: hidden_size - The number of features in the hidden state h\nConstraints:\ndtype:int\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:[0,inf)\n\n###\n\nSignature: torch.nn.functional.avg_pool1d(input,kernel_size,stride=None,padding=0,ceil_mode=False,count_include_pad=True)\nParameter description: padding - implicit zero paddings on both sides of the input. Can be a single number or a tuple (padW,). Default: 0\nConstraints:\ndtype:bool;int\nstructure:tuple\nshape:Null\nndim:0;1\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.autograd.grad(outputs,inputs,grad_outputs=None,retain_graph=None,create_graph=False,only_inputs=True,allow_unused=False)\nParameter description: outputs(sequence of Tensor) - outputs of the differentiated function.\nConstraints:\ndtype:Null\nstructure:sequence;tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.repeat_interleave(input,repeats,dim=None)\nParameter description: dim(int, optional) - The dimension along which to repeat values.By default, use the flattened input array, and return a flat outputarray.\nConstraints:\ndtype:int\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.cuda.comm.broadcast_coalesced(tensors,devices,buffer_size=10485760)\nParameter description: tensors(sequence) - tensors to broadcast.\nConstraints:",
            "constr": {
                "dtype": [],
                "structure": [
                    "sequence",
                    "tensor"
                ],
                "shape": [],
                "ndim": [],
                "enum": [],
                "range": []
            },
            "signature": "torch.cuda.comm.broadcast_coalesced(tensors,devices,buffer_size=10485760)",
            "descp": "tensors(sequence) - tensors to broadcast."
        },
        {
            "fname": "torch.topk.yaml",
            "arg": "dim",
            "prompt": "Signature: torch.distributed.recv(tensor,src=None,group=<objectobject>,tag=0)\nParameter description: tensor(Tensor) - Tensor to fill with received data.\nConstraints:\ndtype:Null\nstructure:tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.div(input,other,out=None)\nParameter description: out(Tensor, optional) - the output tensor.\nConstraints:\ndtype:Null\nstructure:tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.nn.TransformerDecoder(decoder_layer,num_layers,norm=None)\nParameter description: num_layers - the number of sub-decoder-layers in the decoder (required).\nConstraints:\ndtype:int\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:[0,inf)\n\n###\n\nSignature: torch.nn.quantized.functional.interpolate(input,size=None,scale_factor=None,mode=nearest,align_corners=None)\nParameter description: align_corners(bool, optional) - Geometrically, we consider the pixels of the input and output as squares rather than points. If set to `True`, the input and output tensors are aligned by the center points of their corner pixels, preserving the values at the corner pixels. If set to `False`, the input and output tensors are aligned by the corner points of their corner pixels, and the interpolation uses edge value padding for out-of-boundary values, making this operation independent of input size when `scale_factor` is kept the same. This only has an effect when `mode` is `'bilinear'`. Default: `False`\nConstraints:\ndtype:bool\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.gt(input,other,out=None)\nParameter description: out(Tensor, optional) - the output tensor that must be a BoolTensor\nConstraints:\ndtype:booltensor\nstructure:tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.empty_like(input,dtype=None,layout=None,device=None,requires_grad=False,memory_format=torch.preserve_format)\nParameter description: dtype(`torch.dtype`, optional) - the desired data type of returned Tensor. Default: if `None`, defaults to the dtype of `input`.\nConstraints:\ndtype:torch.dtype\nstructure:Null\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.nn.RNN(*args,**kwargs,input_size=None,hidden_size=None,num_layers=None,nonlinearity=None,bias=None,batch_first=None,dropout=None,bidirectional=None)\nParameter description: hidden_size - The number of features in the hidden state h\nConstraints:\ndtype:int\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:[0,inf)\n\n###\n\nSignature: torch.nn.functional.avg_pool1d(input,kernel_size,stride=None,padding=0,ceil_mode=False,count_include_pad=True)\nParameter description: padding - implicit zero paddings on both sides of the input. Can be a single number or a tuple (padW,). Default: 0\nConstraints:\ndtype:bool;int\nstructure:tuple\nshape:Null\nndim:0;1\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.autograd.grad(outputs,inputs,grad_outputs=None,retain_graph=None,create_graph=False,only_inputs=True,allow_unused=False)\nParameter description: outputs(sequence of Tensor) - outputs of the differentiated function.\nConstraints:\ndtype:Null\nstructure:sequence;tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.repeat_interleave(input,repeats,dim=None)\nParameter description: dim(int, optional) - The dimension along which to repeat values.By default, use the flattened input array, and return a flat outputarray.\nConstraints:\ndtype:int\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.topk(input,k,dim=None,largest=True,sorted=True,out=None)\nParameter description: dim(int, optional) - the dimension to sort along\nConstraints:",
            "constr": {
                "dtype": [
                    "int"
                ],
                "structure": [],
                "shape": [],
                "ndim": [
                    "0"
                ],
                "enum": [],
                "range": []
            },
            "signature": "torch.topk(input,k,dim=None,largest=True,sorted=True,out=None)",
            "descp": "dim(int, optional) - the dimension to sort along"
        },
        {
            "fname": "torch.quantization.convert.yaml",
            "arg": "mapping",
            "prompt": "Signature: torch.distributed.recv(tensor,src=None,group=<objectobject>,tag=0)\nParameter description: tensor(Tensor) - Tensor to fill with received data.\nConstraints:\ndtype:Null\nstructure:tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.div(input,other,out=None)\nParameter description: out(Tensor, optional) - the output tensor.\nConstraints:\ndtype:Null\nstructure:tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.nn.TransformerDecoder(decoder_layer,num_layers,norm=None)\nParameter description: num_layers - the number of sub-decoder-layers in the decoder (required).\nConstraints:\ndtype:int\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:[0,inf)\n\n###\n\nSignature: torch.nn.quantized.functional.interpolate(input,size=None,scale_factor=None,mode=nearest,align_corners=None)\nParameter description: align_corners(bool, optional) - Geometrically, we consider the pixels of the input and output as squares rather than points. If set to `True`, the input and output tensors are aligned by the center points of their corner pixels, preserving the values at the corner pixels. If set to `False`, the input and output tensors are aligned by the corner points of their corner pixels, and the interpolation uses edge value padding for out-of-boundary values, making this operation independent of input size when `scale_factor` is kept the same. This only has an effect when `mode` is `'bilinear'`. Default: `False`\nConstraints:\ndtype:bool\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.gt(input,other,out=None)\nParameter description: out(Tensor, optional) - the output tensor that must be a BoolTensor\nConstraints:\ndtype:booltensor\nstructure:tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.empty_like(input,dtype=None,layout=None,device=None,requires_grad=False,memory_format=torch.preserve_format)\nParameter description: dtype(`torch.dtype`, optional) - the desired data type of returned Tensor. Default: if `None`, defaults to the dtype of `input`.\nConstraints:\ndtype:torch.dtype\nstructure:Null\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.nn.RNN(*args,**kwargs,input_size=None,hidden_size=None,num_layers=None,nonlinearity=None,bias=None,batch_first=None,dropout=None,bidirectional=None)\nParameter description: hidden_size - The number of features in the hidden state h\nConstraints:\ndtype:int\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:[0,inf)\n\n###\n\nSignature: torch.nn.functional.avg_pool1d(input,kernel_size,stride=None,padding=0,ceil_mode=False,count_include_pad=True)\nParameter description: padding - implicit zero paddings on both sides of the input. Can be a single number or a tuple (padW,). Default: 0\nConstraints:\ndtype:bool;int\nstructure:tuple\nshape:Null\nndim:0;1\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.autograd.grad(outputs,inputs,grad_outputs=None,retain_graph=None,create_graph=False,only_inputs=True,allow_unused=False)\nParameter description: outputs(sequence of Tensor) - outputs of the differentiated function.\nConstraints:\ndtype:Null\nstructure:sequence;tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.repeat_interleave(input,repeats,dim=None)\nParameter description: dim(int, optional) - The dimension along which to repeat values.By default, use the flattened input array, and return a flat outputarray.\nConstraints:\ndtype:int\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.quantization.convert(module,mapping=None,inplace=False)\nParameter description: mapping - a dictionary that maps from float module type to quantized module type, can be overwrritten to allow swapping user defined Modules\nConstraints:",
            "constr": {
                "dtype": [],
                "structure": [
                    "dictionary"
                ],
                "shape": [],
                "ndim": [],
                "enum": [],
                "range": []
            },
            "signature": "torch.quantization.convert(module,mapping=None,inplace=False)",
            "descp": "mapping - a dictionary that maps from float module type to quantized module type, can be overwrritten to allow swapping user defined Modules"
        },
        {
            "fname": "torch.logical_and.yaml",
            "arg": "out",
            "prompt": "Signature: torch.distributed.recv(tensor,src=None,group=<objectobject>,tag=0)\nParameter description: tensor(Tensor) - Tensor to fill with received data.\nConstraints:\ndtype:Null\nstructure:tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.div(input,other,out=None)\nParameter description: out(Tensor, optional) - the output tensor.\nConstraints:\ndtype:Null\nstructure:tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.nn.TransformerDecoder(decoder_layer,num_layers,norm=None)\nParameter description: num_layers - the number of sub-decoder-layers in the decoder (required).\nConstraints:\ndtype:int\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:[0,inf)\n\n###\n\nSignature: torch.nn.quantized.functional.interpolate(input,size=None,scale_factor=None,mode=nearest,align_corners=None)\nParameter description: align_corners(bool, optional) - Geometrically, we consider the pixels of the input and output as squares rather than points. If set to `True`, the input and output tensors are aligned by the center points of their corner pixels, preserving the values at the corner pixels. If set to `False`, the input and output tensors are aligned by the corner points of their corner pixels, and the interpolation uses edge value padding for out-of-boundary values, making this operation independent of input size when `scale_factor` is kept the same. This only has an effect when `mode` is `'bilinear'`. Default: `False`\nConstraints:\ndtype:bool\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.gt(input,other,out=None)\nParameter description: out(Tensor, optional) - the output tensor that must be a BoolTensor\nConstraints:\ndtype:booltensor\nstructure:tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.empty_like(input,dtype=None,layout=None,device=None,requires_grad=False,memory_format=torch.preserve_format)\nParameter description: dtype(`torch.dtype`, optional) - the desired data type of returned Tensor. Default: if `None`, defaults to the dtype of `input`.\nConstraints:\ndtype:torch.dtype\nstructure:Null\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.nn.RNN(*args,**kwargs,input_size=None,hidden_size=None,num_layers=None,nonlinearity=None,bias=None,batch_first=None,dropout=None,bidirectional=None)\nParameter description: hidden_size - The number of features in the hidden state h\nConstraints:\ndtype:int\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:[0,inf)\n\n###\n\nSignature: torch.nn.functional.avg_pool1d(input,kernel_size,stride=None,padding=0,ceil_mode=False,count_include_pad=True)\nParameter description: padding - implicit zero paddings on both sides of the input. Can be a single number or a tuple (padW,). Default: 0\nConstraints:\ndtype:bool;int\nstructure:tuple\nshape:Null\nndim:0;1\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.autograd.grad(outputs,inputs,grad_outputs=None,retain_graph=None,create_graph=False,only_inputs=True,allow_unused=False)\nParameter description: outputs(sequence of Tensor) - outputs of the differentiated function.\nConstraints:\ndtype:Null\nstructure:sequence;tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.repeat_interleave(input,repeats,dim=None)\nParameter description: dim(int, optional) - The dimension along which to repeat values.By default, use the flattened input array, and return a flat outputarray.\nConstraints:\ndtype:int\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.logical_and(input,other,out=None)\nParameter description: out(Tensor, optional) - the output tensor.\nConstraints:",
            "constr": {
                "dtype": [],
                "structure": [
                    "tensor"
                ],
                "shape": [],
                "ndim": [],
                "enum": [],
                "range": []
            },
            "signature": "torch.logical_and(input,other,out=None)",
            "descp": "out(Tensor, optional) - the output tensor."
        },
        {
            "fname": "torch.hub.load_state_dict_from_url.yaml",
            "arg": "progress",
            "prompt": "Signature: torch.distributed.recv(tensor,src=None,group=<objectobject>,tag=0)\nParameter description: tensor(Tensor) - Tensor to fill with received data.\nConstraints:\ndtype:Null\nstructure:tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.div(input,other,out=None)\nParameter description: out(Tensor, optional) - the output tensor.\nConstraints:\ndtype:Null\nstructure:tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.nn.TransformerDecoder(decoder_layer,num_layers,norm=None)\nParameter description: num_layers - the number of sub-decoder-layers in the decoder (required).\nConstraints:\ndtype:int\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:[0,inf)\n\n###\n\nSignature: torch.nn.quantized.functional.interpolate(input,size=None,scale_factor=None,mode=nearest,align_corners=None)\nParameter description: align_corners(bool, optional) - Geometrically, we consider the pixels of the input and output as squares rather than points. If set to `True`, the input and output tensors are aligned by the center points of their corner pixels, preserving the values at the corner pixels. If set to `False`, the input and output tensors are aligned by the corner points of their corner pixels, and the interpolation uses edge value padding for out-of-boundary values, making this operation independent of input size when `scale_factor` is kept the same. This only has an effect when `mode` is `'bilinear'`. Default: `False`\nConstraints:\ndtype:bool\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.gt(input,other,out=None)\nParameter description: out(Tensor, optional) - the output tensor that must be a BoolTensor\nConstraints:\ndtype:booltensor\nstructure:tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.empty_like(input,dtype=None,layout=None,device=None,requires_grad=False,memory_format=torch.preserve_format)\nParameter description: dtype(`torch.dtype`, optional) - the desired data type of returned Tensor. Default: if `None`, defaults to the dtype of `input`.\nConstraints:\ndtype:torch.dtype\nstructure:Null\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.nn.RNN(*args,**kwargs,input_size=None,hidden_size=None,num_layers=None,nonlinearity=None,bias=None,batch_first=None,dropout=None,bidirectional=None)\nParameter description: hidden_size - The number of features in the hidden state h\nConstraints:\ndtype:int\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:[0,inf)\n\n###\n\nSignature: torch.nn.functional.avg_pool1d(input,kernel_size,stride=None,padding=0,ceil_mode=False,count_include_pad=True)\nParameter description: padding - implicit zero paddings on both sides of the input. Can be a single number or a tuple (padW,). Default: 0\nConstraints:\ndtype:bool;int\nstructure:tuple\nshape:Null\nndim:0;1\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.autograd.grad(outputs,inputs,grad_outputs=None,retain_graph=None,create_graph=False,only_inputs=True,allow_unused=False)\nParameter description: outputs(sequence of Tensor) - outputs of the differentiated function.\nConstraints:\ndtype:Null\nstructure:sequence;tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.repeat_interleave(input,repeats,dim=None)\nParameter description: dim(int, optional) - The dimension along which to repeat values.By default, use the flattened input array, and return a flat outputarray.\nConstraints:\ndtype:int\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.hub.load_state_dict_from_url(url,model_dir=None,map_location=None,progress=True,check_hash=False)\nParameter description: progress(bool, optional) - whether or not to display a progress bar to stderr. Default: True\nConstraints:",
            "constr": {
                "dtype": [
                    "bool"
                ],
                "structure": [],
                "shape": [],
                "ndim": [
                    "0"
                ],
                "enum": [],
                "range": []
            },
            "signature": "torch.hub.load_state_dict_from_url(url,model_dir=None,map_location=None,progress=True,check_hash=False)",
            "descp": "progress(bool, optional) - whether or not to display a progress bar to stderr. Default: True"
        },
        {
            "fname": "torch.eq.yaml",
            "arg": "other",
            "prompt": "Signature: torch.distributed.recv(tensor,src=None,group=<objectobject>,tag=0)\nParameter description: tensor(Tensor) - Tensor to fill with received data.\nConstraints:\ndtype:Null\nstructure:tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.div(input,other,out=None)\nParameter description: out(Tensor, optional) - the output tensor.\nConstraints:\ndtype:Null\nstructure:tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.nn.TransformerDecoder(decoder_layer,num_layers,norm=None)\nParameter description: num_layers - the number of sub-decoder-layers in the decoder (required).\nConstraints:\ndtype:int\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:[0,inf)\n\n###\n\nSignature: torch.nn.quantized.functional.interpolate(input,size=None,scale_factor=None,mode=nearest,align_corners=None)\nParameter description: align_corners(bool, optional) - Geometrically, we consider the pixels of the input and output as squares rather than points. If set to `True`, the input and output tensors are aligned by the center points of their corner pixels, preserving the values at the corner pixels. If set to `False`, the input and output tensors are aligned by the corner points of their corner pixels, and the interpolation uses edge value padding for out-of-boundary values, making this operation independent of input size when `scale_factor` is kept the same. This only has an effect when `mode` is `'bilinear'`. Default: `False`\nConstraints:\ndtype:bool\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.gt(input,other,out=None)\nParameter description: out(Tensor, optional) - the output tensor that must be a BoolTensor\nConstraints:\ndtype:booltensor\nstructure:tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.empty_like(input,dtype=None,layout=None,device=None,requires_grad=False,memory_format=torch.preserve_format)\nParameter description: dtype(`torch.dtype`, optional) - the desired data type of returned Tensor. Default: if `None`, defaults to the dtype of `input`.\nConstraints:\ndtype:torch.dtype\nstructure:Null\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.nn.RNN(*args,**kwargs,input_size=None,hidden_size=None,num_layers=None,nonlinearity=None,bias=None,batch_first=None,dropout=None,bidirectional=None)\nParameter description: hidden_size - The number of features in the hidden state h\nConstraints:\ndtype:int\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:[0,inf)\n\n###\n\nSignature: torch.nn.functional.avg_pool1d(input,kernel_size,stride=None,padding=0,ceil_mode=False,count_include_pad=True)\nParameter description: padding - implicit zero paddings on both sides of the input. Can be a single number or a tuple (padW,). Default: 0\nConstraints:\ndtype:bool;int\nstructure:tuple\nshape:Null\nndim:0;1\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.autograd.grad(outputs,inputs,grad_outputs=None,retain_graph=None,create_graph=False,only_inputs=True,allow_unused=False)\nParameter description: outputs(sequence of Tensor) - outputs of the differentiated function.\nConstraints:\ndtype:Null\nstructure:sequence;tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.repeat_interleave(input,repeats,dim=None)\nParameter description: dim(int, optional) - The dimension along which to repeat values.By default, use the flattened input array, and return a flat outputarray.\nConstraints:\ndtype:int\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.eq(input,other,out=None)\nParameter description: other(Tensor or float) - the tensor or value to compare\nConstraints:",
            "constr": {
                "dtype": [
                    "float"
                ],
                "structure": [
                    "tensor"
                ],
                "shape": [],
                "ndim": [],
                "enum": [],
                "range": []
            },
            "signature": "torch.eq(input,other,out=None)",
            "descp": "other(Tensor or float) - the tensor or value to compare"
        },
        {
            "fname": "torch.nn.bceloss.yaml",
            "arg": "reduce",
            "prompt": "Signature: torch.distributed.recv(tensor,src=None,group=<objectobject>,tag=0)\nParameter description: tensor(Tensor) - Tensor to fill with received data.\nConstraints:\ndtype:Null\nstructure:tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.div(input,other,out=None)\nParameter description: out(Tensor, optional) - the output tensor.\nConstraints:\ndtype:Null\nstructure:tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.nn.TransformerDecoder(decoder_layer,num_layers,norm=None)\nParameter description: num_layers - the number of sub-decoder-layers in the decoder (required).\nConstraints:\ndtype:int\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:[0,inf)\n\n###\n\nSignature: torch.nn.quantized.functional.interpolate(input,size=None,scale_factor=None,mode=nearest,align_corners=None)\nParameter description: align_corners(bool, optional) - Geometrically, we consider the pixels of the input and output as squares rather than points. If set to `True`, the input and output tensors are aligned by the center points of their corner pixels, preserving the values at the corner pixels. If set to `False`, the input and output tensors are aligned by the corner points of their corner pixels, and the interpolation uses edge value padding for out-of-boundary values, making this operation independent of input size when `scale_factor` is kept the same. This only has an effect when `mode` is `'bilinear'`. Default: `False`\nConstraints:\ndtype:bool\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.gt(input,other,out=None)\nParameter description: out(Tensor, optional) - the output tensor that must be a BoolTensor\nConstraints:\ndtype:booltensor\nstructure:tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.empty_like(input,dtype=None,layout=None,device=None,requires_grad=False,memory_format=torch.preserve_format)\nParameter description: dtype(`torch.dtype`, optional) - the desired data type of returned Tensor. Default: if `None`, defaults to the dtype of `input`.\nConstraints:\ndtype:torch.dtype\nstructure:Null\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.nn.RNN(*args,**kwargs,input_size=None,hidden_size=None,num_layers=None,nonlinearity=None,bias=None,batch_first=None,dropout=None,bidirectional=None)\nParameter description: hidden_size - The number of features in the hidden state h\nConstraints:\ndtype:int\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:[0,inf)\n\n###\n\nSignature: torch.nn.functional.avg_pool1d(input,kernel_size,stride=None,padding=0,ceil_mode=False,count_include_pad=True)\nParameter description: padding - implicit zero paddings on both sides of the input. Can be a single number or a tuple (padW,). Default: 0\nConstraints:\ndtype:bool;int\nstructure:tuple\nshape:Null\nndim:0;1\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.autograd.grad(outputs,inputs,grad_outputs=None,retain_graph=None,create_graph=False,only_inputs=True,allow_unused=False)\nParameter description: outputs(sequence of Tensor) - outputs of the differentiated function.\nConstraints:\ndtype:Null\nstructure:sequence;tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.repeat_interleave(input,repeats,dim=None)\nParameter description: dim(int, optional) - The dimension along which to repeat values.By default, use the flattened input array, and return a flat outputarray.\nConstraints:\ndtype:int\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.nn.BCELoss(weight=None,size_average=None,reduce=None,reduction=mean)\nParameter description: reduce(bool, optional) - Deprecated (see `reduction`). By default, the losses are averaged or summed over observations for each minibatch depending on `size_average`. When `reduce` is `False`, returns a loss per batch element instead and ignores `size_average`. Default: `True`\nConstraints:",
            "constr": {
                "dtype": [
                    "bool"
                ],
                "structure": [],
                "shape": [],
                "ndim": [
                    "0"
                ],
                "enum": [],
                "range": []
            },
            "signature": "torch.nn.BCELoss(weight=None,size_average=None,reduce=None,reduction=mean)",
            "descp": "reduce(bool, optional) - Deprecated (see `reduction`). By default, the losses are averaged or summed over observations for each minibatch depending on `size_average`. When `reduce` is `False`, returns a loss per batch element instead and ignores `size_average`. Default: `True`"
        },
        {
            "fname": "torch.mode.yaml",
            "arg": "input",
            "prompt": "Signature: torch.distributed.recv(tensor,src=None,group=<objectobject>,tag=0)\nParameter description: tensor(Tensor) - Tensor to fill with received data.\nConstraints:\ndtype:Null\nstructure:tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.div(input,other,out=None)\nParameter description: out(Tensor, optional) - the output tensor.\nConstraints:\ndtype:Null\nstructure:tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.nn.TransformerDecoder(decoder_layer,num_layers,norm=None)\nParameter description: num_layers - the number of sub-decoder-layers in the decoder (required).\nConstraints:\ndtype:int\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:[0,inf)\n\n###\n\nSignature: torch.nn.quantized.functional.interpolate(input,size=None,scale_factor=None,mode=nearest,align_corners=None)\nParameter description: align_corners(bool, optional) - Geometrically, we consider the pixels of the input and output as squares rather than points. If set to `True`, the input and output tensors are aligned by the center points of their corner pixels, preserving the values at the corner pixels. If set to `False`, the input and output tensors are aligned by the corner points of their corner pixels, and the interpolation uses edge value padding for out-of-boundary values, making this operation independent of input size when `scale_factor` is kept the same. This only has an effect when `mode` is `'bilinear'`. Default: `False`\nConstraints:\ndtype:bool\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.gt(input,other,out=None)\nParameter description: out(Tensor, optional) - the output tensor that must be a BoolTensor\nConstraints:\ndtype:booltensor\nstructure:tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.empty_like(input,dtype=None,layout=None,device=None,requires_grad=False,memory_format=torch.preserve_format)\nParameter description: dtype(`torch.dtype`, optional) - the desired data type of returned Tensor. Default: if `None`, defaults to the dtype of `input`.\nConstraints:\ndtype:torch.dtype\nstructure:Null\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.nn.RNN(*args,**kwargs,input_size=None,hidden_size=None,num_layers=None,nonlinearity=None,bias=None,batch_first=None,dropout=None,bidirectional=None)\nParameter description: hidden_size - The number of features in the hidden state h\nConstraints:\ndtype:int\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:[0,inf)\n\n###\n\nSignature: torch.nn.functional.avg_pool1d(input,kernel_size,stride=None,padding=0,ceil_mode=False,count_include_pad=True)\nParameter description: padding - implicit zero paddings on both sides of the input. Can be a single number or a tuple (padW,). Default: 0\nConstraints:\ndtype:bool;int\nstructure:tuple\nshape:Null\nndim:0;1\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.autograd.grad(outputs,inputs,grad_outputs=None,retain_graph=None,create_graph=False,only_inputs=True,allow_unused=False)\nParameter description: outputs(sequence of Tensor) - outputs of the differentiated function.\nConstraints:\ndtype:Null\nstructure:sequence;tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.repeat_interleave(input,repeats,dim=None)\nParameter description: dim(int, optional) - The dimension along which to repeat values.By default, use the flattened input array, and return a flat outputarray.\nConstraints:\ndtype:int\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.mode(input,dim=-1,keepdim=False,out=None)\nParameter description: input(Tensor) - the input tensor.\nConstraints:",
            "constr": {
                "dtype": [],
                "structure": [
                    "tensor"
                ],
                "shape": [],
                "ndim": [],
                "enum": [],
                "range": []
            },
            "signature": "torch.mode(input,dim=-1,keepdim=False,out=None)",
            "descp": "input(Tensor) - the input tensor."
        },
        {
            "fname": "torch.matmul.yaml",
            "arg": "out",
            "prompt": "Signature: torch.distributed.recv(tensor,src=None,group=<objectobject>,tag=0)\nParameter description: tensor(Tensor) - Tensor to fill with received data.\nConstraints:\ndtype:Null\nstructure:tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.div(input,other,out=None)\nParameter description: out(Tensor, optional) - the output tensor.\nConstraints:\ndtype:Null\nstructure:tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.nn.TransformerDecoder(decoder_layer,num_layers,norm=None)\nParameter description: num_layers - the number of sub-decoder-layers in the decoder (required).\nConstraints:\ndtype:int\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:[0,inf)\n\n###\n\nSignature: torch.nn.quantized.functional.interpolate(input,size=None,scale_factor=None,mode=nearest,align_corners=None)\nParameter description: align_corners(bool, optional) - Geometrically, we consider the pixels of the input and output as squares rather than points. If set to `True`, the input and output tensors are aligned by the center points of their corner pixels, preserving the values at the corner pixels. If set to `False`, the input and output tensors are aligned by the corner points of their corner pixels, and the interpolation uses edge value padding for out-of-boundary values, making this operation independent of input size when `scale_factor` is kept the same. This only has an effect when `mode` is `'bilinear'`. Default: `False`\nConstraints:\ndtype:bool\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.gt(input,other,out=None)\nParameter description: out(Tensor, optional) - the output tensor that must be a BoolTensor\nConstraints:\ndtype:booltensor\nstructure:tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.empty_like(input,dtype=None,layout=None,device=None,requires_grad=False,memory_format=torch.preserve_format)\nParameter description: dtype(`torch.dtype`, optional) - the desired data type of returned Tensor. Default: if `None`, defaults to the dtype of `input`.\nConstraints:\ndtype:torch.dtype\nstructure:Null\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.nn.RNN(*args,**kwargs,input_size=None,hidden_size=None,num_layers=None,nonlinearity=None,bias=None,batch_first=None,dropout=None,bidirectional=None)\nParameter description: hidden_size - The number of features in the hidden state h\nConstraints:\ndtype:int\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:[0,inf)\n\n###\n\nSignature: torch.nn.functional.avg_pool1d(input,kernel_size,stride=None,padding=0,ceil_mode=False,count_include_pad=True)\nParameter description: padding - implicit zero paddings on both sides of the input. Can be a single number or a tuple (padW,). Default: 0\nConstraints:\ndtype:bool;int\nstructure:tuple\nshape:Null\nndim:0;1\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.autograd.grad(outputs,inputs,grad_outputs=None,retain_graph=None,create_graph=False,only_inputs=True,allow_unused=False)\nParameter description: outputs(sequence of Tensor) - outputs of the differentiated function.\nConstraints:\ndtype:Null\nstructure:sequence;tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.repeat_interleave(input,repeats,dim=None)\nParameter description: dim(int, optional) - The dimension along which to repeat values.By default, use the flattened input array, and return a flat outputarray.\nConstraints:\ndtype:int\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.matmul(input,other,out=None)\nParameter description: out(Tensor, optional) - the output tensor.\nConstraints:",
            "constr": {
                "dtype": [],
                "structure": [
                    "tensor"
                ],
                "shape": [],
                "ndim": [],
                "enum": [],
                "range": []
            },
            "signature": "torch.matmul(input,other,out=None)",
            "descp": "out(Tensor, optional) - the output tensor."
        },
        {
            "fname": "torch.nn.adaptivemaxpool1d.yaml",
            "arg": "output_size",
            "prompt": "Signature: torch.distributed.recv(tensor,src=None,group=<objectobject>,tag=0)\nParameter description: tensor(Tensor) - Tensor to fill with received data.\nConstraints:\ndtype:Null\nstructure:tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.div(input,other,out=None)\nParameter description: out(Tensor, optional) - the output tensor.\nConstraints:\ndtype:Null\nstructure:tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.nn.TransformerDecoder(decoder_layer,num_layers,norm=None)\nParameter description: num_layers - the number of sub-decoder-layers in the decoder (required).\nConstraints:\ndtype:int\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:[0,inf)\n\n###\n\nSignature: torch.nn.quantized.functional.interpolate(input,size=None,scale_factor=None,mode=nearest,align_corners=None)\nParameter description: align_corners(bool, optional) - Geometrically, we consider the pixels of the input and output as squares rather than points. If set to `True`, the input and output tensors are aligned by the center points of their corner pixels, preserving the values at the corner pixels. If set to `False`, the input and output tensors are aligned by the corner points of their corner pixels, and the interpolation uses edge value padding for out-of-boundary values, making this operation independent of input size when `scale_factor` is kept the same. This only has an effect when `mode` is `'bilinear'`. Default: `False`\nConstraints:\ndtype:bool\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.gt(input,other,out=None)\nParameter description: out(Tensor, optional) - the output tensor that must be a BoolTensor\nConstraints:\ndtype:booltensor\nstructure:tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.empty_like(input,dtype=None,layout=None,device=None,requires_grad=False,memory_format=torch.preserve_format)\nParameter description: dtype(`torch.dtype`, optional) - the desired data type of returned Tensor. Default: if `None`, defaults to the dtype of `input`.\nConstraints:\ndtype:torch.dtype\nstructure:Null\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.nn.RNN(*args,**kwargs,input_size=None,hidden_size=None,num_layers=None,nonlinearity=None,bias=None,batch_first=None,dropout=None,bidirectional=None)\nParameter description: hidden_size - The number of features in the hidden state h\nConstraints:\ndtype:int\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:[0,inf)\n\n###\n\nSignature: torch.nn.functional.avg_pool1d(input,kernel_size,stride=None,padding=0,ceil_mode=False,count_include_pad=True)\nParameter description: padding - implicit zero paddings on both sides of the input. Can be a single number or a tuple (padW,). Default: 0\nConstraints:\ndtype:bool;int\nstructure:tuple\nshape:Null\nndim:0;1\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.autograd.grad(outputs,inputs,grad_outputs=None,retain_graph=None,create_graph=False,only_inputs=True,allow_unused=False)\nParameter description: outputs(sequence of Tensor) - outputs of the differentiated function.\nConstraints:\ndtype:Null\nstructure:sequence;tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.repeat_interleave(input,repeats,dim=None)\nParameter description: dim(int, optional) - The dimension along which to repeat values.By default, use the flattened input array, and return a flat outputarray.\nConstraints:\ndtype:int\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.nn.AdaptiveMaxPool1d(output_size,return_indices=False)\nParameter description: output_size - the target output size H\nConstraints:",
            "constr": {
                "dtype": [],
                "structure": [],
                "shape": [
                    "[H]"
                ],
                "ndim": [],
                "enum": [],
                "range": [
                    "[0,inf)"
                ]
            },
            "signature": "torch.nn.AdaptiveMaxPool1d(output_size,return_indices=False)",
            "descp": "output_size - the target output size H"
        },
        {
            "fname": "torch.unique.yaml",
            "arg": "sorted",
            "prompt": "Signature: torch.distributed.recv(tensor,src=None,group=<objectobject>,tag=0)\nParameter description: tensor(Tensor) - Tensor to fill with received data.\nConstraints:\ndtype:Null\nstructure:tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.div(input,other,out=None)\nParameter description: out(Tensor, optional) - the output tensor.\nConstraints:\ndtype:Null\nstructure:tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.nn.TransformerDecoder(decoder_layer,num_layers,norm=None)\nParameter description: num_layers - the number of sub-decoder-layers in the decoder (required).\nConstraints:\ndtype:int\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:[0,inf)\n\n###\n\nSignature: torch.nn.quantized.functional.interpolate(input,size=None,scale_factor=None,mode=nearest,align_corners=None)\nParameter description: align_corners(bool, optional) - Geometrically, we consider the pixels of the input and output as squares rather than points. If set to `True`, the input and output tensors are aligned by the center points of their corner pixels, preserving the values at the corner pixels. If set to `False`, the input and output tensors are aligned by the corner points of their corner pixels, and the interpolation uses edge value padding for out-of-boundary values, making this operation independent of input size when `scale_factor` is kept the same. This only has an effect when `mode` is `'bilinear'`. Default: `False`\nConstraints:\ndtype:bool\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.gt(input,other,out=None)\nParameter description: out(Tensor, optional) - the output tensor that must be a BoolTensor\nConstraints:\ndtype:booltensor\nstructure:tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.empty_like(input,dtype=None,layout=None,device=None,requires_grad=False,memory_format=torch.preserve_format)\nParameter description: dtype(`torch.dtype`, optional) - the desired data type of returned Tensor. Default: if `None`, defaults to the dtype of `input`.\nConstraints:\ndtype:torch.dtype\nstructure:Null\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.nn.RNN(*args,**kwargs,input_size=None,hidden_size=None,num_layers=None,nonlinearity=None,bias=None,batch_first=None,dropout=None,bidirectional=None)\nParameter description: hidden_size - The number of features in the hidden state h\nConstraints:\ndtype:int\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:[0,inf)\n\n###\n\nSignature: torch.nn.functional.avg_pool1d(input,kernel_size,stride=None,padding=0,ceil_mode=False,count_include_pad=True)\nParameter description: padding - implicit zero paddings on both sides of the input. Can be a single number or a tuple (padW,). Default: 0\nConstraints:\ndtype:bool;int\nstructure:tuple\nshape:Null\nndim:0;1\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.autograd.grad(outputs,inputs,grad_outputs=None,retain_graph=None,create_graph=False,only_inputs=True,allow_unused=False)\nParameter description: outputs(sequence of Tensor) - outputs of the differentiated function.\nConstraints:\ndtype:Null\nstructure:sequence;tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.repeat_interleave(input,repeats,dim=None)\nParameter description: dim(int, optional) - The dimension along which to repeat values.By default, use the flattened input array, and return a flat outputarray.\nConstraints:\ndtype:int\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.unique(input,sorted=True,return_inverse=False,return_counts=False,dim=None)\nParameter description: sorted(bool) - Whether to sort the unique elements in ascending order before returning as output.\nConstraints:",
            "constr": {
                "dtype": [
                    "bool"
                ],
                "structure": [],
                "shape": [],
                "ndim": [
                    "0"
                ],
                "enum": [],
                "range": []
            },
            "signature": "torch.unique(input,sorted=True,return_inverse=False,return_counts=False,dim=None)",
            "descp": "sorted(bool) - Whether to sort the unique elements in ascending order before returning as output."
        },
        {
            "fname": "torch.renorm.yaml",
            "arg": "dim",
            "prompt": "Signature: torch.distributed.recv(tensor,src=None,group=<objectobject>,tag=0)\nParameter description: tensor(Tensor) - Tensor to fill with received data.\nConstraints:\ndtype:Null\nstructure:tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.div(input,other,out=None)\nParameter description: out(Tensor, optional) - the output tensor.\nConstraints:\ndtype:Null\nstructure:tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.nn.TransformerDecoder(decoder_layer,num_layers,norm=None)\nParameter description: num_layers - the number of sub-decoder-layers in the decoder (required).\nConstraints:\ndtype:int\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:[0,inf)\n\n###\n\nSignature: torch.nn.quantized.functional.interpolate(input,size=None,scale_factor=None,mode=nearest,align_corners=None)\nParameter description: align_corners(bool, optional) - Geometrically, we consider the pixels of the input and output as squares rather than points. If set to `True`, the input and output tensors are aligned by the center points of their corner pixels, preserving the values at the corner pixels. If set to `False`, the input and output tensors are aligned by the corner points of their corner pixels, and the interpolation uses edge value padding for out-of-boundary values, making this operation independent of input size when `scale_factor` is kept the same. This only has an effect when `mode` is `'bilinear'`. Default: `False`\nConstraints:\ndtype:bool\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.gt(input,other,out=None)\nParameter description: out(Tensor, optional) - the output tensor that must be a BoolTensor\nConstraints:\ndtype:booltensor\nstructure:tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.empty_like(input,dtype=None,layout=None,device=None,requires_grad=False,memory_format=torch.preserve_format)\nParameter description: dtype(`torch.dtype`, optional) - the desired data type of returned Tensor. Default: if `None`, defaults to the dtype of `input`.\nConstraints:\ndtype:torch.dtype\nstructure:Null\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.nn.RNN(*args,**kwargs,input_size=None,hidden_size=None,num_layers=None,nonlinearity=None,bias=None,batch_first=None,dropout=None,bidirectional=None)\nParameter description: hidden_size - The number of features in the hidden state h\nConstraints:\ndtype:int\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:[0,inf)\n\n###\n\nSignature: torch.nn.functional.avg_pool1d(input,kernel_size,stride=None,padding=0,ceil_mode=False,count_include_pad=True)\nParameter description: padding - implicit zero paddings on both sides of the input. Can be a single number or a tuple (padW,). Default: 0\nConstraints:\ndtype:bool;int\nstructure:tuple\nshape:Null\nndim:0;1\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.autograd.grad(outputs,inputs,grad_outputs=None,retain_graph=None,create_graph=False,only_inputs=True,allow_unused=False)\nParameter description: outputs(sequence of Tensor) - outputs of the differentiated function.\nConstraints:\ndtype:Null\nstructure:sequence;tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.repeat_interleave(input,repeats,dim=None)\nParameter description: dim(int, optional) - The dimension along which to repeat values.By default, use the flattened input array, and return a flat outputarray.\nConstraints:\ndtype:int\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.renorm(input,p,dim,maxnorm,out=None)\nParameter description: dim(int) - the dimension to slice over to get the sub-tensors\nConstraints:",
            "constr": {
                "dtype": [
                    "int"
                ],
                "structure": [],
                "shape": [],
                "ndim": [
                    "0"
                ],
                "enum": [],
                "range": []
            },
            "signature": "torch.renorm(input,p,dim,maxnorm,out=None)",
            "descp": "dim(int) - the dimension to slice over to get the sub-tensors"
        },
        {
            "fname": "torch.lobpcg.yaml",
            "arg": "method",
            "prompt": "Signature: torch.distributed.recv(tensor,src=None,group=<objectobject>,tag=0)\nParameter description: tensor(Tensor) - Tensor to fill with received data.\nConstraints:\ndtype:Null\nstructure:tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.div(input,other,out=None)\nParameter description: out(Tensor, optional) - the output tensor.\nConstraints:\ndtype:Null\nstructure:tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.nn.TransformerDecoder(decoder_layer,num_layers,norm=None)\nParameter description: num_layers - the number of sub-decoder-layers in the decoder (required).\nConstraints:\ndtype:int\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:[0,inf)\n\n###\n\nSignature: torch.nn.quantized.functional.interpolate(input,size=None,scale_factor=None,mode=nearest,align_corners=None)\nParameter description: align_corners(bool, optional) - Geometrically, we consider the pixels of the input and output as squares rather than points. If set to `True`, the input and output tensors are aligned by the center points of their corner pixels, preserving the values at the corner pixels. If set to `False`, the input and output tensors are aligned by the corner points of their corner pixels, and the interpolation uses edge value padding for out-of-boundary values, making this operation independent of input size when `scale_factor` is kept the same. This only has an effect when `mode` is `'bilinear'`. Default: `False`\nConstraints:\ndtype:bool\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.gt(input,other,out=None)\nParameter description: out(Tensor, optional) - the output tensor that must be a BoolTensor\nConstraints:\ndtype:booltensor\nstructure:tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.empty_like(input,dtype=None,layout=None,device=None,requires_grad=False,memory_format=torch.preserve_format)\nParameter description: dtype(`torch.dtype`, optional) - the desired data type of returned Tensor. Default: if `None`, defaults to the dtype of `input`.\nConstraints:\ndtype:torch.dtype\nstructure:Null\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.nn.RNN(*args,**kwargs,input_size=None,hidden_size=None,num_layers=None,nonlinearity=None,bias=None,batch_first=None,dropout=None,bidirectional=None)\nParameter description: hidden_size - The number of features in the hidden state h\nConstraints:\ndtype:int\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:[0,inf)\n\n###\n\nSignature: torch.nn.functional.avg_pool1d(input,kernel_size,stride=None,padding=0,ceil_mode=False,count_include_pad=True)\nParameter description: padding - implicit zero paddings on both sides of the input. Can be a single number or a tuple (padW,). Default: 0\nConstraints:\ndtype:bool;int\nstructure:tuple\nshape:Null\nndim:0;1\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.autograd.grad(outputs,inputs,grad_outputs=None,retain_graph=None,create_graph=False,only_inputs=True,allow_unused=False)\nParameter description: outputs(sequence of Tensor) - outputs of the differentiated function.\nConstraints:\ndtype:Null\nstructure:sequence;tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.repeat_interleave(input,repeats,dim=None)\nParameter description: dim(int, optional) - The dimension along which to repeat values.By default, use the flattened input array, and return a flat outputarray.\nConstraints:\ndtype:int\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.lobpcg(A,k=None,B=None,X=None,n=None,iK=None,niter=None,tol=None,largest=None,method=None,tracker=None,ortho_iparams=None,ortho_fparams=None,ortho_bparams=None)\nParameter description: method(str, optional) - select LOBPCG method. See the description of the function above. Default is \"ortho\".\nConstraints:",
            "constr": {
                "dtype": [
                    "str"
                ],
                "structure": [],
                "shape": [],
                "ndim": [],
                "enum": [],
                "range": []
            },
            "signature": "torch.lobpcg(A,k=None,B=None,X=None,n=None,iK=None,niter=None,tol=None,largest=None,method=None,tracker=None,ortho_iparams=None,ortho_fparams=None,ortho_bparams=None)",
            "descp": "method(str, optional) - select LOBPCG method. See the description of the function above. Default is \"ortho\"."
        },
        {
            "fname": "torch.tensordot.yaml",
            "arg": "a",
            "prompt": "Signature: torch.distributed.recv(tensor,src=None,group=<objectobject>,tag=0)\nParameter description: tensor(Tensor) - Tensor to fill with received data.\nConstraints:\ndtype:Null\nstructure:tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.div(input,other,out=None)\nParameter description: out(Tensor, optional) - the output tensor.\nConstraints:\ndtype:Null\nstructure:tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.nn.TransformerDecoder(decoder_layer,num_layers,norm=None)\nParameter description: num_layers - the number of sub-decoder-layers in the decoder (required).\nConstraints:\ndtype:int\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:[0,inf)\n\n###\n\nSignature: torch.nn.quantized.functional.interpolate(input,size=None,scale_factor=None,mode=nearest,align_corners=None)\nParameter description: align_corners(bool, optional) - Geometrically, we consider the pixels of the input and output as squares rather than points. If set to `True`, the input and output tensors are aligned by the center points of their corner pixels, preserving the values at the corner pixels. If set to `False`, the input and output tensors are aligned by the corner points of their corner pixels, and the interpolation uses edge value padding for out-of-boundary values, making this operation independent of input size when `scale_factor` is kept the same. This only has an effect when `mode` is `'bilinear'`. Default: `False`\nConstraints:\ndtype:bool\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.gt(input,other,out=None)\nParameter description: out(Tensor, optional) - the output tensor that must be a BoolTensor\nConstraints:\ndtype:booltensor\nstructure:tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.empty_like(input,dtype=None,layout=None,device=None,requires_grad=False,memory_format=torch.preserve_format)\nParameter description: dtype(`torch.dtype`, optional) - the desired data type of returned Tensor. Default: if `None`, defaults to the dtype of `input`.\nConstraints:\ndtype:torch.dtype\nstructure:Null\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.nn.RNN(*args,**kwargs,input_size=None,hidden_size=None,num_layers=None,nonlinearity=None,bias=None,batch_first=None,dropout=None,bidirectional=None)\nParameter description: hidden_size - The number of features in the hidden state h\nConstraints:\ndtype:int\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:[0,inf)\n\n###\n\nSignature: torch.nn.functional.avg_pool1d(input,kernel_size,stride=None,padding=0,ceil_mode=False,count_include_pad=True)\nParameter description: padding - implicit zero paddings on both sides of the input. Can be a single number or a tuple (padW,). Default: 0\nConstraints:\ndtype:bool;int\nstructure:tuple\nshape:Null\nndim:0;1\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.autograd.grad(outputs,inputs,grad_outputs=None,retain_graph=None,create_graph=False,only_inputs=True,allow_unused=False)\nParameter description: outputs(sequence of Tensor) - outputs of the differentiated function.\nConstraints:\ndtype:Null\nstructure:sequence;tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.repeat_interleave(input,repeats,dim=None)\nParameter description: dim(int, optional) - The dimension along which to repeat values.By default, use the flattened input array, and return a flat outputarray.\nConstraints:\ndtype:int\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.tensordot(a,b,dims=2)\nParameter description: a(Tensor) - Left tensor to contract\nConstraints:",
            "constr": {
                "dtype": [],
                "structure": [
                    "tensor"
                ],
                "shape": [],
                "ndim": [],
                "enum": [],
                "range": []
            },
            "signature": "torch.tensordot(a,b,dims=2)",
            "descp": "a(Tensor) - Left tensor to contract"
        },
        {
            "fname": "torch.index_select.yaml",
            "arg": "index",
            "prompt": "Signature: torch.distributed.recv(tensor,src=None,group=<objectobject>,tag=0)\nParameter description: tensor(Tensor) - Tensor to fill with received data.\nConstraints:\ndtype:Null\nstructure:tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.div(input,other,out=None)\nParameter description: out(Tensor, optional) - the output tensor.\nConstraints:\ndtype:Null\nstructure:tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.nn.TransformerDecoder(decoder_layer,num_layers,norm=None)\nParameter description: num_layers - the number of sub-decoder-layers in the decoder (required).\nConstraints:\ndtype:int\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:[0,inf)\n\n###\n\nSignature: torch.nn.quantized.functional.interpolate(input,size=None,scale_factor=None,mode=nearest,align_corners=None)\nParameter description: align_corners(bool, optional) - Geometrically, we consider the pixels of the input and output as squares rather than points. If set to `True`, the input and output tensors are aligned by the center points of their corner pixels, preserving the values at the corner pixels. If set to `False`, the input and output tensors are aligned by the corner points of their corner pixels, and the interpolation uses edge value padding for out-of-boundary values, making this operation independent of input size when `scale_factor` is kept the same. This only has an effect when `mode` is `'bilinear'`. Default: `False`\nConstraints:\ndtype:bool\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.gt(input,other,out=None)\nParameter description: out(Tensor, optional) - the output tensor that must be a BoolTensor\nConstraints:\ndtype:booltensor\nstructure:tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.empty_like(input,dtype=None,layout=None,device=None,requires_grad=False,memory_format=torch.preserve_format)\nParameter description: dtype(`torch.dtype`, optional) - the desired data type of returned Tensor. Default: if `None`, defaults to the dtype of `input`.\nConstraints:\ndtype:torch.dtype\nstructure:Null\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.nn.RNN(*args,**kwargs,input_size=None,hidden_size=None,num_layers=None,nonlinearity=None,bias=None,batch_first=None,dropout=None,bidirectional=None)\nParameter description: hidden_size - The number of features in the hidden state h\nConstraints:\ndtype:int\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:[0,inf)\n\n###\n\nSignature: torch.nn.functional.avg_pool1d(input,kernel_size,stride=None,padding=0,ceil_mode=False,count_include_pad=True)\nParameter description: padding - implicit zero paddings on both sides of the input. Can be a single number or a tuple (padW,). Default: 0\nConstraints:\ndtype:bool;int\nstructure:tuple\nshape:Null\nndim:0;1\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.autograd.grad(outputs,inputs,grad_outputs=None,retain_graph=None,create_graph=False,only_inputs=True,allow_unused=False)\nParameter description: outputs(sequence of Tensor) - outputs of the differentiated function.\nConstraints:\ndtype:Null\nstructure:sequence;tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.repeat_interleave(input,repeats,dim=None)\nParameter description: dim(int, optional) - The dimension along which to repeat values.By default, use the flattened input array, and return a flat outputarray.\nConstraints:\ndtype:int\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.index_select(input,dim,index,out=None)\nParameter description: index(LongTensor) - the 1-D tensor containing the indices to index\nConstraints:",
            "constr": {
                "dtype": [
                    "int",
                    "longtensor"
                ],
                "structure": [
                    "tensor"
                ],
                "shape": [],
                "ndim": [
                    "1"
                ],
                "enum": [],
                "range": []
            },
            "signature": "torch.index_select(input,dim,index,out=None)",
            "descp": "index(LongTensor) - the 1-D tensor containing the indices to index"
        },
        {
            "fname": "torch.autograd.functional.jacobian.yaml",
            "arg": "strict",
            "prompt": "Signature: torch.distributed.recv(tensor,src=None,group=<objectobject>,tag=0)\nParameter description: tensor(Tensor) - Tensor to fill with received data.\nConstraints:\ndtype:Null\nstructure:tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.div(input,other,out=None)\nParameter description: out(Tensor, optional) - the output tensor.\nConstraints:\ndtype:Null\nstructure:tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.nn.TransformerDecoder(decoder_layer,num_layers,norm=None)\nParameter description: num_layers - the number of sub-decoder-layers in the decoder (required).\nConstraints:\ndtype:int\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:[0,inf)\n\n###\n\nSignature: torch.nn.quantized.functional.interpolate(input,size=None,scale_factor=None,mode=nearest,align_corners=None)\nParameter description: align_corners(bool, optional) - Geometrically, we consider the pixels of the input and output as squares rather than points. If set to `True`, the input and output tensors are aligned by the center points of their corner pixels, preserving the values at the corner pixels. If set to `False`, the input and output tensors are aligned by the corner points of their corner pixels, and the interpolation uses edge value padding for out-of-boundary values, making this operation independent of input size when `scale_factor` is kept the same. This only has an effect when `mode` is `'bilinear'`. Default: `False`\nConstraints:\ndtype:bool\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.gt(input,other,out=None)\nParameter description: out(Tensor, optional) - the output tensor that must be a BoolTensor\nConstraints:\ndtype:booltensor\nstructure:tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.empty_like(input,dtype=None,layout=None,device=None,requires_grad=False,memory_format=torch.preserve_format)\nParameter description: dtype(`torch.dtype`, optional) - the desired data type of returned Tensor. Default: if `None`, defaults to the dtype of `input`.\nConstraints:\ndtype:torch.dtype\nstructure:Null\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.nn.RNN(*args,**kwargs,input_size=None,hidden_size=None,num_layers=None,nonlinearity=None,bias=None,batch_first=None,dropout=None,bidirectional=None)\nParameter description: hidden_size - The number of features in the hidden state h\nConstraints:\ndtype:int\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:[0,inf)\n\n###\n\nSignature: torch.nn.functional.avg_pool1d(input,kernel_size,stride=None,padding=0,ceil_mode=False,count_include_pad=True)\nParameter description: padding - implicit zero paddings on both sides of the input. Can be a single number or a tuple (padW,). Default: 0\nConstraints:\ndtype:bool;int\nstructure:tuple\nshape:Null\nndim:0;1\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.autograd.grad(outputs,inputs,grad_outputs=None,retain_graph=None,create_graph=False,only_inputs=True,allow_unused=False)\nParameter description: outputs(sequence of Tensor) - outputs of the differentiated function.\nConstraints:\ndtype:Null\nstructure:sequence;tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.repeat_interleave(input,repeats,dim=None)\nParameter description: dim(int, optional) - The dimension along which to repeat values.By default, use the flattened input array, and return a flat outputarray.\nConstraints:\ndtype:int\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.autograd.functional.jacobian(func,inputs,create_graph=False,strict=False)\nParameter description: strict(bool, optional) - If `True`, an error will be raised when we detect that there exists an input such that all the outputs are independent of it. If `False`, we return a Tensor of zeros as the jacobian for said inputs, which is the expected mathematical value. Defaults to `False`.\nConstraints:",
            "constr": {
                "dtype": [
                    "bool"
                ],
                "structure": [],
                "shape": [],
                "ndim": [
                    "0"
                ],
                "enum": [],
                "range": []
            },
            "signature": "torch.autograd.functional.jacobian(func,inputs,create_graph=False,strict=False)",
            "descp": "strict(bool, optional) - If `True`, an error will be raised when we detect that there exists an input such that all the outputs are independent of it. If `False`, we return a Tensor of zeros as the jacobian for said inputs, which is the expected mathematical value. Defaults to `False`."
        },
        {
            "fname": "torch.logspace.yaml",
            "arg": "steps",
            "prompt": "Signature: torch.distributed.recv(tensor,src=None,group=<objectobject>,tag=0)\nParameter description: tensor(Tensor) - Tensor to fill with received data.\nConstraints:\ndtype:Null\nstructure:tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.div(input,other,out=None)\nParameter description: out(Tensor, optional) - the output tensor.\nConstraints:\ndtype:Null\nstructure:tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.nn.TransformerDecoder(decoder_layer,num_layers,norm=None)\nParameter description: num_layers - the number of sub-decoder-layers in the decoder (required).\nConstraints:\ndtype:int\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:[0,inf)\n\n###\n\nSignature: torch.nn.quantized.functional.interpolate(input,size=None,scale_factor=None,mode=nearest,align_corners=None)\nParameter description: align_corners(bool, optional) - Geometrically, we consider the pixels of the input and output as squares rather than points. If set to `True`, the input and output tensors are aligned by the center points of their corner pixels, preserving the values at the corner pixels. If set to `False`, the input and output tensors are aligned by the corner points of their corner pixels, and the interpolation uses edge value padding for out-of-boundary values, making this operation independent of input size when `scale_factor` is kept the same. This only has an effect when `mode` is `'bilinear'`. Default: `False`\nConstraints:\ndtype:bool\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.gt(input,other,out=None)\nParameter description: out(Tensor, optional) - the output tensor that must be a BoolTensor\nConstraints:\ndtype:booltensor\nstructure:tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.empty_like(input,dtype=None,layout=None,device=None,requires_grad=False,memory_format=torch.preserve_format)\nParameter description: dtype(`torch.dtype`, optional) - the desired data type of returned Tensor. Default: if `None`, defaults to the dtype of `input`.\nConstraints:\ndtype:torch.dtype\nstructure:Null\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.nn.RNN(*args,**kwargs,input_size=None,hidden_size=None,num_layers=None,nonlinearity=None,bias=None,batch_first=None,dropout=None,bidirectional=None)\nParameter description: hidden_size - The number of features in the hidden state h\nConstraints:\ndtype:int\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:[0,inf)\n\n###\n\nSignature: torch.nn.functional.avg_pool1d(input,kernel_size,stride=None,padding=0,ceil_mode=False,count_include_pad=True)\nParameter description: padding - implicit zero paddings on both sides of the input. Can be a single number or a tuple (padW,). Default: 0\nConstraints:\ndtype:bool;int\nstructure:tuple\nshape:Null\nndim:0;1\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.autograd.grad(outputs,inputs,grad_outputs=None,retain_graph=None,create_graph=False,only_inputs=True,allow_unused=False)\nParameter description: outputs(sequence of Tensor) - outputs of the differentiated function.\nConstraints:\ndtype:Null\nstructure:sequence;tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.repeat_interleave(input,repeats,dim=None)\nParameter description: dim(int, optional) - The dimension along which to repeat values.By default, use the flattened input array, and return a flat outputarray.\nConstraints:\ndtype:int\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.logspace(start,end,steps=100,base=10.0,out=None,dtype=None,layout=torch.strided,device=None,requires_grad=False)\nParameter description: steps(int) - number of points to sample between `start` and `end`. Default: `100`.\nConstraints:",
            "constr": {
                "dtype": [
                    "int"
                ],
                "structure": [],
                "shape": [],
                "ndim": [
                    "0"
                ],
                "enum": [],
                "range": [
                    "[0,inf)"
                ]
            },
            "signature": "torch.logspace(start,end,steps=100,base=10.0,out=None,dtype=None,layout=torch.strided,device=None,requires_grad=False)",
            "descp": "steps(int) - number of points to sample between `start` and `end`. Default: `100`."
        },
        {
            "fname": "torch.hub.list.yaml",
            "arg": "force_reload",
            "prompt": "Signature: torch.distributed.recv(tensor,src=None,group=<objectobject>,tag=0)\nParameter description: tensor(Tensor) - Tensor to fill with received data.\nConstraints:\ndtype:Null\nstructure:tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.div(input,other,out=None)\nParameter description: out(Tensor, optional) - the output tensor.\nConstraints:\ndtype:Null\nstructure:tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.nn.TransformerDecoder(decoder_layer,num_layers,norm=None)\nParameter description: num_layers - the number of sub-decoder-layers in the decoder (required).\nConstraints:\ndtype:int\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:[0,inf)\n\n###\n\nSignature: torch.nn.quantized.functional.interpolate(input,size=None,scale_factor=None,mode=nearest,align_corners=None)\nParameter description: align_corners(bool, optional) - Geometrically, we consider the pixels of the input and output as squares rather than points. If set to `True`, the input and output tensors are aligned by the center points of their corner pixels, preserving the values at the corner pixels. If set to `False`, the input and output tensors are aligned by the corner points of their corner pixels, and the interpolation uses edge value padding for out-of-boundary values, making this operation independent of input size when `scale_factor` is kept the same. This only has an effect when `mode` is `'bilinear'`. Default: `False`\nConstraints:\ndtype:bool\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.gt(input,other,out=None)\nParameter description: out(Tensor, optional) - the output tensor that must be a BoolTensor\nConstraints:\ndtype:booltensor\nstructure:tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.empty_like(input,dtype=None,layout=None,device=None,requires_grad=False,memory_format=torch.preserve_format)\nParameter description: dtype(`torch.dtype`, optional) - the desired data type of returned Tensor. Default: if `None`, defaults to the dtype of `input`.\nConstraints:\ndtype:torch.dtype\nstructure:Null\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.nn.RNN(*args,**kwargs,input_size=None,hidden_size=None,num_layers=None,nonlinearity=None,bias=None,batch_first=None,dropout=None,bidirectional=None)\nParameter description: hidden_size - The number of features in the hidden state h\nConstraints:\ndtype:int\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:[0,inf)\n\n###\n\nSignature: torch.nn.functional.avg_pool1d(input,kernel_size,stride=None,padding=0,ceil_mode=False,count_include_pad=True)\nParameter description: padding - implicit zero paddings on both sides of the input. Can be a single number or a tuple (padW,). Default: 0\nConstraints:\ndtype:bool;int\nstructure:tuple\nshape:Null\nndim:0;1\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.autograd.grad(outputs,inputs,grad_outputs=None,retain_graph=None,create_graph=False,only_inputs=True,allow_unused=False)\nParameter description: outputs(sequence of Tensor) - outputs of the differentiated function.\nConstraints:\ndtype:Null\nstructure:sequence;tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.repeat_interleave(input,repeats,dim=None)\nParameter description: dim(int, optional) - The dimension along which to repeat values.By default, use the flattened input array, and return a flat outputarray.\nConstraints:\ndtype:int\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.hub.list(github,force_reload=False)\nParameter description: force_reload(bool, optional) - whether to discard the existing cache and force a fresh download. Default is False.\nConstraints:",
            "constr": {
                "dtype": [
                    "bool"
                ],
                "structure": [],
                "shape": [],
                "ndim": [
                    "0"
                ],
                "enum": [],
                "range": []
            },
            "signature": "torch.hub.list(github,force_reload=False)",
            "descp": "force_reload(bool, optional) - whether to discard the existing cache and force a fresh download. Default is False."
        },
        {
            "fname": "torch.logsumexp.yaml",
            "arg": "out",
            "prompt": "Signature: torch.distributed.recv(tensor,src=None,group=<objectobject>,tag=0)\nParameter description: tensor(Tensor) - Tensor to fill with received data.\nConstraints:\ndtype:Null\nstructure:tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.div(input,other,out=None)\nParameter description: out(Tensor, optional) - the output tensor.\nConstraints:\ndtype:Null\nstructure:tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.nn.TransformerDecoder(decoder_layer,num_layers,norm=None)\nParameter description: num_layers - the number of sub-decoder-layers in the decoder (required).\nConstraints:\ndtype:int\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:[0,inf)\n\n###\n\nSignature: torch.nn.quantized.functional.interpolate(input,size=None,scale_factor=None,mode=nearest,align_corners=None)\nParameter description: align_corners(bool, optional) - Geometrically, we consider the pixels of the input and output as squares rather than points. If set to `True`, the input and output tensors are aligned by the center points of their corner pixels, preserving the values at the corner pixels. If set to `False`, the input and output tensors are aligned by the corner points of their corner pixels, and the interpolation uses edge value padding for out-of-boundary values, making this operation independent of input size when `scale_factor` is kept the same. This only has an effect when `mode` is `'bilinear'`. Default: `False`\nConstraints:\ndtype:bool\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.gt(input,other,out=None)\nParameter description: out(Tensor, optional) - the output tensor that must be a BoolTensor\nConstraints:\ndtype:booltensor\nstructure:tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.empty_like(input,dtype=None,layout=None,device=None,requires_grad=False,memory_format=torch.preserve_format)\nParameter description: dtype(`torch.dtype`, optional) - the desired data type of returned Tensor. Default: if `None`, defaults to the dtype of `input`.\nConstraints:\ndtype:torch.dtype\nstructure:Null\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.nn.RNN(*args,**kwargs,input_size=None,hidden_size=None,num_layers=None,nonlinearity=None,bias=None,batch_first=None,dropout=None,bidirectional=None)\nParameter description: hidden_size - The number of features in the hidden state h\nConstraints:\ndtype:int\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:[0,inf)\n\n###\n\nSignature: torch.nn.functional.avg_pool1d(input,kernel_size,stride=None,padding=0,ceil_mode=False,count_include_pad=True)\nParameter description: padding - implicit zero paddings on both sides of the input. Can be a single number or a tuple (padW,). Default: 0\nConstraints:\ndtype:bool;int\nstructure:tuple\nshape:Null\nndim:0;1\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.autograd.grad(outputs,inputs,grad_outputs=None,retain_graph=None,create_graph=False,only_inputs=True,allow_unused=False)\nParameter description: outputs(sequence of Tensor) - outputs of the differentiated function.\nConstraints:\ndtype:Null\nstructure:sequence;tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.repeat_interleave(input,repeats,dim=None)\nParameter description: dim(int, optional) - The dimension along which to repeat values.By default, use the flattened input array, and return a flat outputarray.\nConstraints:\ndtype:int\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.logsumexp(input,dim,keepdim=False,out=None)\nParameter description: out(Tensor, optional) - the output tensor.\nConstraints:",
            "constr": {
                "dtype": [],
                "structure": [
                    "tensor"
                ],
                "shape": [],
                "ndim": [],
                "enum": [],
                "range": []
            },
            "signature": "torch.logsumexp(input,dim,keepdim=False,out=None)",
            "descp": "out(Tensor, optional) - the output tensor."
        },
        {
            "fname": "torch.nn.adaptivemaxpool2d.yaml",
            "arg": "return_indices",
            "prompt": "Signature: torch.distributed.recv(tensor,src=None,group=<objectobject>,tag=0)\nParameter description: tensor(Tensor) - Tensor to fill with received data.\nConstraints:\ndtype:Null\nstructure:tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.div(input,other,out=None)\nParameter description: out(Tensor, optional) - the output tensor.\nConstraints:\ndtype:Null\nstructure:tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.nn.TransformerDecoder(decoder_layer,num_layers,norm=None)\nParameter description: num_layers - the number of sub-decoder-layers in the decoder (required).\nConstraints:\ndtype:int\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:[0,inf)\n\n###\n\nSignature: torch.nn.quantized.functional.interpolate(input,size=None,scale_factor=None,mode=nearest,align_corners=None)\nParameter description: align_corners(bool, optional) - Geometrically, we consider the pixels of the input and output as squares rather than points. If set to `True`, the input and output tensors are aligned by the center points of their corner pixels, preserving the values at the corner pixels. If set to `False`, the input and output tensors are aligned by the corner points of their corner pixels, and the interpolation uses edge value padding for out-of-boundary values, making this operation independent of input size when `scale_factor` is kept the same. This only has an effect when `mode` is `'bilinear'`. Default: `False`\nConstraints:\ndtype:bool\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.gt(input,other,out=None)\nParameter description: out(Tensor, optional) - the output tensor that must be a BoolTensor\nConstraints:\ndtype:booltensor\nstructure:tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.empty_like(input,dtype=None,layout=None,device=None,requires_grad=False,memory_format=torch.preserve_format)\nParameter description: dtype(`torch.dtype`, optional) - the desired data type of returned Tensor. Default: if `None`, defaults to the dtype of `input`.\nConstraints:\ndtype:torch.dtype\nstructure:Null\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.nn.RNN(*args,**kwargs,input_size=None,hidden_size=None,num_layers=None,nonlinearity=None,bias=None,batch_first=None,dropout=None,bidirectional=None)\nParameter description: hidden_size - The number of features in the hidden state h\nConstraints:\ndtype:int\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:[0,inf)\n\n###\n\nSignature: torch.nn.functional.avg_pool1d(input,kernel_size,stride=None,padding=0,ceil_mode=False,count_include_pad=True)\nParameter description: padding - implicit zero paddings on both sides of the input. Can be a single number or a tuple (padW,). Default: 0\nConstraints:\ndtype:bool;int\nstructure:tuple\nshape:Null\nndim:0;1\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.autograd.grad(outputs,inputs,grad_outputs=None,retain_graph=None,create_graph=False,only_inputs=True,allow_unused=False)\nParameter description: outputs(sequence of Tensor) - outputs of the differentiated function.\nConstraints:\ndtype:Null\nstructure:sequence;tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.repeat_interleave(input,repeats,dim=None)\nParameter description: dim(int, optional) - The dimension along which to repeat values.By default, use the flattened input array, and return a flat outputarray.\nConstraints:\ndtype:int\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.nn.AdaptiveMaxPool2d(output_size,return_indices=False)\nParameter description: return_indices - if `True`, will return the indices along with the outputs. Useful to pass to nn.MaxUnpool2d. Default: `False`\nConstraints:",
            "constr": {
                "dtype": [
                    "bool"
                ],
                "structure": [],
                "shape": [],
                "ndim": [
                    "0"
                ],
                "enum": [],
                "range": []
            },
            "signature": "torch.nn.AdaptiveMaxPool2d(output_size,return_indices=False)",
            "descp": "return_indices - if `True`, will return the indices along with the outputs. Useful to pass to nn.MaxUnpool2d. Default: `False`"
        },
        {
            "fname": "torch.pinverse.yaml",
            "arg": "rcond",
            "prompt": "Signature: torch.distributed.recv(tensor,src=None,group=<objectobject>,tag=0)\nParameter description: tensor(Tensor) - Tensor to fill with received data.\nConstraints:\ndtype:Null\nstructure:tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.div(input,other,out=None)\nParameter description: out(Tensor, optional) - the output tensor.\nConstraints:\ndtype:Null\nstructure:tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.nn.TransformerDecoder(decoder_layer,num_layers,norm=None)\nParameter description: num_layers - the number of sub-decoder-layers in the decoder (required).\nConstraints:\ndtype:int\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:[0,inf)\n\n###\n\nSignature: torch.nn.quantized.functional.interpolate(input,size=None,scale_factor=None,mode=nearest,align_corners=None)\nParameter description: align_corners(bool, optional) - Geometrically, we consider the pixels of the input and output as squares rather than points. If set to `True`, the input and output tensors are aligned by the center points of their corner pixels, preserving the values at the corner pixels. If set to `False`, the input and output tensors are aligned by the corner points of their corner pixels, and the interpolation uses edge value padding for out-of-boundary values, making this operation independent of input size when `scale_factor` is kept the same. This only has an effect when `mode` is `'bilinear'`. Default: `False`\nConstraints:\ndtype:bool\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.gt(input,other,out=None)\nParameter description: out(Tensor, optional) - the output tensor that must be a BoolTensor\nConstraints:\ndtype:booltensor\nstructure:tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.empty_like(input,dtype=None,layout=None,device=None,requires_grad=False,memory_format=torch.preserve_format)\nParameter description: dtype(`torch.dtype`, optional) - the desired data type of returned Tensor. Default: if `None`, defaults to the dtype of `input`.\nConstraints:\ndtype:torch.dtype\nstructure:Null\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.nn.RNN(*args,**kwargs,input_size=None,hidden_size=None,num_layers=None,nonlinearity=None,bias=None,batch_first=None,dropout=None,bidirectional=None)\nParameter description: hidden_size - The number of features in the hidden state h\nConstraints:\ndtype:int\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:[0,inf)\n\n###\n\nSignature: torch.nn.functional.avg_pool1d(input,kernel_size,stride=None,padding=0,ceil_mode=False,count_include_pad=True)\nParameter description: padding - implicit zero paddings on both sides of the input. Can be a single number or a tuple (padW,). Default: 0\nConstraints:\ndtype:bool;int\nstructure:tuple\nshape:Null\nndim:0;1\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.autograd.grad(outputs,inputs,grad_outputs=None,retain_graph=None,create_graph=False,only_inputs=True,allow_unused=False)\nParameter description: outputs(sequence of Tensor) - outputs of the differentiated function.\nConstraints:\ndtype:Null\nstructure:sequence;tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.repeat_interleave(input,repeats,dim=None)\nParameter description: dim(int, optional) - The dimension along which to repeat values.By default, use the flattened input array, and return a flat outputarray.\nConstraints:\ndtype:int\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.pinverse(input,rcond=1e-15)\nParameter description: rcond(float) - A floating point value to determine the cutoff for small singular values. Default: 1e-15\nConstraints:",
            "constr": {
                "dtype": [
                    "float",
                    "int"
                ],
                "structure": [],
                "shape": [],
                "ndim": [
                    "0"
                ],
                "enum": [],
                "range": []
            },
            "signature": "torch.pinverse(input,rcond=1e-15)",
            "descp": "rcond(float) - A floating point value to determine the cutoff for small singular values. Default: 1e-15"
        },
        {
            "fname": "torch.nn.utils.remove_spectral_norm.yaml",
            "arg": "name",
            "prompt": "Signature: torch.distributed.recv(tensor,src=None,group=<objectobject>,tag=0)\nParameter description: tensor(Tensor) - Tensor to fill with received data.\nConstraints:\ndtype:Null\nstructure:tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.div(input,other,out=None)\nParameter description: out(Tensor, optional) - the output tensor.\nConstraints:\ndtype:Null\nstructure:tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.nn.TransformerDecoder(decoder_layer,num_layers,norm=None)\nParameter description: num_layers - the number of sub-decoder-layers in the decoder (required).\nConstraints:\ndtype:int\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:[0,inf)\n\n###\n\nSignature: torch.nn.quantized.functional.interpolate(input,size=None,scale_factor=None,mode=nearest,align_corners=None)\nParameter description: align_corners(bool, optional) - Geometrically, we consider the pixels of the input and output as squares rather than points. If set to `True`, the input and output tensors are aligned by the center points of their corner pixels, preserving the values at the corner pixels. If set to `False`, the input and output tensors are aligned by the corner points of their corner pixels, and the interpolation uses edge value padding for out-of-boundary values, making this operation independent of input size when `scale_factor` is kept the same. This only has an effect when `mode` is `'bilinear'`. Default: `False`\nConstraints:\ndtype:bool\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.gt(input,other,out=None)\nParameter description: out(Tensor, optional) - the output tensor that must be a BoolTensor\nConstraints:\ndtype:booltensor\nstructure:tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.empty_like(input,dtype=None,layout=None,device=None,requires_grad=False,memory_format=torch.preserve_format)\nParameter description: dtype(`torch.dtype`, optional) - the desired data type of returned Tensor. Default: if `None`, defaults to the dtype of `input`.\nConstraints:\ndtype:torch.dtype\nstructure:Null\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.nn.RNN(*args,**kwargs,input_size=None,hidden_size=None,num_layers=None,nonlinearity=None,bias=None,batch_first=None,dropout=None,bidirectional=None)\nParameter description: hidden_size - The number of features in the hidden state h\nConstraints:\ndtype:int\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:[0,inf)\n\n###\n\nSignature: torch.nn.functional.avg_pool1d(input,kernel_size,stride=None,padding=0,ceil_mode=False,count_include_pad=True)\nParameter description: padding - implicit zero paddings on both sides of the input. Can be a single number or a tuple (padW,). Default: 0\nConstraints:\ndtype:bool;int\nstructure:tuple\nshape:Null\nndim:0;1\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.autograd.grad(outputs,inputs,grad_outputs=None,retain_graph=None,create_graph=False,only_inputs=True,allow_unused=False)\nParameter description: outputs(sequence of Tensor) - outputs of the differentiated function.\nConstraints:\ndtype:Null\nstructure:sequence;tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.repeat_interleave(input,repeats,dim=None)\nParameter description: dim(int, optional) - The dimension along which to repeat values.By default, use the flattened input array, and return a flat outputarray.\nConstraints:\ndtype:int\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.nn.utils.remove_spectral_norm(module,name=weight)\nParameter description: name(str, optional) - name of weight parameter\nConstraints:",
            "constr": {
                "dtype": [
                    "string",
                    "str"
                ],
                "structure": [],
                "shape": [],
                "ndim": [
                    "0"
                ],
                "enum": [],
                "range": []
            },
            "signature": "torch.nn.utils.remove_spectral_norm(module,name=weight)",
            "descp": "name(str, optional) - name of weight parameter"
        },
        {
            "fname": "torch.floor_divide.yaml",
            "arg": "other",
            "prompt": "Signature: torch.distributed.recv(tensor,src=None,group=<objectobject>,tag=0)\nParameter description: tensor(Tensor) - Tensor to fill with received data.\nConstraints:\ndtype:Null\nstructure:tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.div(input,other,out=None)\nParameter description: out(Tensor, optional) - the output tensor.\nConstraints:\ndtype:Null\nstructure:tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.nn.TransformerDecoder(decoder_layer,num_layers,norm=None)\nParameter description: num_layers - the number of sub-decoder-layers in the decoder (required).\nConstraints:\ndtype:int\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:[0,inf)\n\n###\n\nSignature: torch.nn.quantized.functional.interpolate(input,size=None,scale_factor=None,mode=nearest,align_corners=None)\nParameter description: align_corners(bool, optional) - Geometrically, we consider the pixels of the input and output as squares rather than points. If set to `True`, the input and output tensors are aligned by the center points of their corner pixels, preserving the values at the corner pixels. If set to `False`, the input and output tensors are aligned by the corner points of their corner pixels, and the interpolation uses edge value padding for out-of-boundary values, making this operation independent of input size when `scale_factor` is kept the same. This only has an effect when `mode` is `'bilinear'`. Default: `False`\nConstraints:\ndtype:bool\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.gt(input,other,out=None)\nParameter description: out(Tensor, optional) - the output tensor that must be a BoolTensor\nConstraints:\ndtype:booltensor\nstructure:tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.empty_like(input,dtype=None,layout=None,device=None,requires_grad=False,memory_format=torch.preserve_format)\nParameter description: dtype(`torch.dtype`, optional) - the desired data type of returned Tensor. Default: if `None`, defaults to the dtype of `input`.\nConstraints:\ndtype:torch.dtype\nstructure:Null\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.nn.RNN(*args,**kwargs,input_size=None,hidden_size=None,num_layers=None,nonlinearity=None,bias=None,batch_first=None,dropout=None,bidirectional=None)\nParameter description: hidden_size - The number of features in the hidden state h\nConstraints:\ndtype:int\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:[0,inf)\n\n###\n\nSignature: torch.nn.functional.avg_pool1d(input,kernel_size,stride=None,padding=0,ceil_mode=False,count_include_pad=True)\nParameter description: padding - implicit zero paddings on both sides of the input. Can be a single number or a tuple (padW,). Default: 0\nConstraints:\ndtype:bool;int\nstructure:tuple\nshape:Null\nndim:0;1\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.autograd.grad(outputs,inputs,grad_outputs=None,retain_graph=None,create_graph=False,only_inputs=True,allow_unused=False)\nParameter description: outputs(sequence of Tensor) - outputs of the differentiated function.\nConstraints:\ndtype:Null\nstructure:sequence;tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.repeat_interleave(input,repeats,dim=None)\nParameter description: dim(int, optional) - The dimension along which to repeat values.By default, use the flattened input array, and return a flat outputarray.\nConstraints:\ndtype:int\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.floor_divide(input,other,out=None)\nParameter description: other(Tensor or Scalar) - the denominator\nConstraints:",
            "constr": {
                "dtype": [],
                "structure": [
                    "tensor"
                ],
                "shape": [],
                "ndim": [
                    "0"
                ],
                "enum": [],
                "range": []
            },
            "signature": "torch.floor_divide(input,other,out=None)",
            "descp": "other(Tensor or Scalar) - the denominator"
        },
        {
            "fname": "torch.addmm.yaml",
            "arg": "alpha",
            "prompt": "Signature: torch.distributed.recv(tensor,src=None,group=<objectobject>,tag=0)\nParameter description: tensor(Tensor) - Tensor to fill with received data.\nConstraints:\ndtype:Null\nstructure:tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.div(input,other,out=None)\nParameter description: out(Tensor, optional) - the output tensor.\nConstraints:\ndtype:Null\nstructure:tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.nn.TransformerDecoder(decoder_layer,num_layers,norm=None)\nParameter description: num_layers - the number of sub-decoder-layers in the decoder (required).\nConstraints:\ndtype:int\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:[0,inf)\n\n###\n\nSignature: torch.nn.quantized.functional.interpolate(input,size=None,scale_factor=None,mode=nearest,align_corners=None)\nParameter description: align_corners(bool, optional) - Geometrically, we consider the pixels of the input and output as squares rather than points. If set to `True`, the input and output tensors are aligned by the center points of their corner pixels, preserving the values at the corner pixels. If set to `False`, the input and output tensors are aligned by the corner points of their corner pixels, and the interpolation uses edge value padding for out-of-boundary values, making this operation independent of input size when `scale_factor` is kept the same. This only has an effect when `mode` is `'bilinear'`. Default: `False`\nConstraints:\ndtype:bool\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.gt(input,other,out=None)\nParameter description: out(Tensor, optional) - the output tensor that must be a BoolTensor\nConstraints:\ndtype:booltensor\nstructure:tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.empty_like(input,dtype=None,layout=None,device=None,requires_grad=False,memory_format=torch.preserve_format)\nParameter description: dtype(`torch.dtype`, optional) - the desired data type of returned Tensor. Default: if `None`, defaults to the dtype of `input`.\nConstraints:\ndtype:torch.dtype\nstructure:Null\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.nn.RNN(*args,**kwargs,input_size=None,hidden_size=None,num_layers=None,nonlinearity=None,bias=None,batch_first=None,dropout=None,bidirectional=None)\nParameter description: hidden_size - The number of features in the hidden state h\nConstraints:\ndtype:int\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:[0,inf)\n\n###\n\nSignature: torch.nn.functional.avg_pool1d(input,kernel_size,stride=None,padding=0,ceil_mode=False,count_include_pad=True)\nParameter description: padding - implicit zero paddings on both sides of the input. Can be a single number or a tuple (padW,). Default: 0\nConstraints:\ndtype:bool;int\nstructure:tuple\nshape:Null\nndim:0;1\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.autograd.grad(outputs,inputs,grad_outputs=None,retain_graph=None,create_graph=False,only_inputs=True,allow_unused=False)\nParameter description: outputs(sequence of Tensor) - outputs of the differentiated function.\nConstraints:\ndtype:Null\nstructure:sequence;tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.repeat_interleave(input,repeats,dim=None)\nParameter description: dim(int, optional) - The dimension along which to repeat values.By default, use the flattened input array, and return a flat outputarray.\nConstraints:\ndtype:int\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.addmm(input,mat1,mat2,beta=1,alpha=1,out=None)\nParameter description: alpha(Number, optional) - multiplier for mat1 @ mat2  ( alpha )\nConstraints:",
            "constr": {
                "dtype": [
                    "numeric",
                    "int"
                ],
                "structure": [],
                "shape": [],
                "ndim": [
                    "0"
                ],
                "enum": [],
                "range": []
            },
            "signature": "torch.addmm(input,mat1,mat2,beta=1,alpha=1,out=None)",
            "descp": "alpha(Number, optional) - multiplier for mat1 @ mat2  ( alpha )"
        },
        {
            "fname": "torch.std2.yaml",
            "arg": "unbiased",
            "prompt": "Signature: torch.distributed.recv(tensor,src=None,group=<objectobject>,tag=0)\nParameter description: tensor(Tensor) - Tensor to fill with received data.\nConstraints:\ndtype:Null\nstructure:tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.div(input,other,out=None)\nParameter description: out(Tensor, optional) - the output tensor.\nConstraints:\ndtype:Null\nstructure:tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.nn.TransformerDecoder(decoder_layer,num_layers,norm=None)\nParameter description: num_layers - the number of sub-decoder-layers in the decoder (required).\nConstraints:\ndtype:int\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:[0,inf)\n\n###\n\nSignature: torch.nn.quantized.functional.interpolate(input,size=None,scale_factor=None,mode=nearest,align_corners=None)\nParameter description: align_corners(bool, optional) - Geometrically, we consider the pixels of the input and output as squares rather than points. If set to `True`, the input and output tensors are aligned by the center points of their corner pixels, preserving the values at the corner pixels. If set to `False`, the input and output tensors are aligned by the corner points of their corner pixels, and the interpolation uses edge value padding for out-of-boundary values, making this operation independent of input size when `scale_factor` is kept the same. This only has an effect when `mode` is `'bilinear'`. Default: `False`\nConstraints:\ndtype:bool\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.gt(input,other,out=None)\nParameter description: out(Tensor, optional) - the output tensor that must be a BoolTensor\nConstraints:\ndtype:booltensor\nstructure:tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.empty_like(input,dtype=None,layout=None,device=None,requires_grad=False,memory_format=torch.preserve_format)\nParameter description: dtype(`torch.dtype`, optional) - the desired data type of returned Tensor. Default: if `None`, defaults to the dtype of `input`.\nConstraints:\ndtype:torch.dtype\nstructure:Null\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.nn.RNN(*args,**kwargs,input_size=None,hidden_size=None,num_layers=None,nonlinearity=None,bias=None,batch_first=None,dropout=None,bidirectional=None)\nParameter description: hidden_size - The number of features in the hidden state h\nConstraints:\ndtype:int\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:[0,inf)\n\n###\n\nSignature: torch.nn.functional.avg_pool1d(input,kernel_size,stride=None,padding=0,ceil_mode=False,count_include_pad=True)\nParameter description: padding - implicit zero paddings on both sides of the input. Can be a single number or a tuple (padW,). Default: 0\nConstraints:\ndtype:bool;int\nstructure:tuple\nshape:Null\nndim:0;1\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.autograd.grad(outputs,inputs,grad_outputs=None,retain_graph=None,create_graph=False,only_inputs=True,allow_unused=False)\nParameter description: outputs(sequence of Tensor) - outputs of the differentiated function.\nConstraints:\ndtype:Null\nstructure:sequence;tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.repeat_interleave(input,repeats,dim=None)\nParameter description: dim(int, optional) - The dimension along which to repeat values.By default, use the flattened input array, and return a flat outputarray.\nConstraints:\ndtype:int\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.std(input,dim,unbiased=True,keepdim=False,out=None)\nParameter description: unbiased(bool) - whether to use the unbiased estimation or not\nConstraints:",
            "constr": {
                "dtype": [
                    "bool"
                ],
                "structure": [],
                "shape": [],
                "ndim": [
                    "0"
                ],
                "enum": [],
                "range": []
            },
            "signature": "torch.std(input,dim,unbiased=True,keepdim=False,out=None)",
            "descp": "unbiased(bool) - whether to use the unbiased estimation or not"
        },
        {
            "fname": "torch.addcdiv.yaml",
            "arg": "input",
            "prompt": "Signature: torch.distributed.recv(tensor,src=None,group=<objectobject>,tag=0)\nParameter description: tensor(Tensor) - Tensor to fill with received data.\nConstraints:\ndtype:Null\nstructure:tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.div(input,other,out=None)\nParameter description: out(Tensor, optional) - the output tensor.\nConstraints:\ndtype:Null\nstructure:tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.nn.TransformerDecoder(decoder_layer,num_layers,norm=None)\nParameter description: num_layers - the number of sub-decoder-layers in the decoder (required).\nConstraints:\ndtype:int\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:[0,inf)\n\n###\n\nSignature: torch.nn.quantized.functional.interpolate(input,size=None,scale_factor=None,mode=nearest,align_corners=None)\nParameter description: align_corners(bool, optional) - Geometrically, we consider the pixels of the input and output as squares rather than points. If set to `True`, the input and output tensors are aligned by the center points of their corner pixels, preserving the values at the corner pixels. If set to `False`, the input and output tensors are aligned by the corner points of their corner pixels, and the interpolation uses edge value padding for out-of-boundary values, making this operation independent of input size when `scale_factor` is kept the same. This only has an effect when `mode` is `'bilinear'`. Default: `False`\nConstraints:\ndtype:bool\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.gt(input,other,out=None)\nParameter description: out(Tensor, optional) - the output tensor that must be a BoolTensor\nConstraints:\ndtype:booltensor\nstructure:tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.empty_like(input,dtype=None,layout=None,device=None,requires_grad=False,memory_format=torch.preserve_format)\nParameter description: dtype(`torch.dtype`, optional) - the desired data type of returned Tensor. Default: if `None`, defaults to the dtype of `input`.\nConstraints:\ndtype:torch.dtype\nstructure:Null\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.nn.RNN(*args,**kwargs,input_size=None,hidden_size=None,num_layers=None,nonlinearity=None,bias=None,batch_first=None,dropout=None,bidirectional=None)\nParameter description: hidden_size - The number of features in the hidden state h\nConstraints:\ndtype:int\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:[0,inf)\n\n###\n\nSignature: torch.nn.functional.avg_pool1d(input,kernel_size,stride=None,padding=0,ceil_mode=False,count_include_pad=True)\nParameter description: padding - implicit zero paddings on both sides of the input. Can be a single number or a tuple (padW,). Default: 0\nConstraints:\ndtype:bool;int\nstructure:tuple\nshape:Null\nndim:0;1\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.autograd.grad(outputs,inputs,grad_outputs=None,retain_graph=None,create_graph=False,only_inputs=True,allow_unused=False)\nParameter description: outputs(sequence of Tensor) - outputs of the differentiated function.\nConstraints:\ndtype:Null\nstructure:sequence;tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.repeat_interleave(input,repeats,dim=None)\nParameter description: dim(int, optional) - The dimension along which to repeat values.By default, use the flattened input array, and return a flat outputarray.\nConstraints:\ndtype:int\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.addcdiv(input,tensor1,tensor2,value=1,out=None)\nParameter description: input(Tensor) - the tensor to be added\nConstraints:",
            "constr": {
                "dtype": [],
                "structure": [
                    "tensor"
                ],
                "shape": [],
                "ndim": [],
                "enum": [],
                "range": []
            },
            "signature": "torch.addcdiv(input,tensor1,tensor2,value=1,out=None)",
            "descp": "input(Tensor) - the tensor to be added"
        },
        {
            "fname": "torch.nn.functional.kl_div.yaml",
            "arg": "target",
            "prompt": "Signature: torch.distributed.recv(tensor,src=None,group=<objectobject>,tag=0)\nParameter description: tensor(Tensor) - Tensor to fill with received data.\nConstraints:\ndtype:Null\nstructure:tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.div(input,other,out=None)\nParameter description: out(Tensor, optional) - the output tensor.\nConstraints:\ndtype:Null\nstructure:tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.nn.TransformerDecoder(decoder_layer,num_layers,norm=None)\nParameter description: num_layers - the number of sub-decoder-layers in the decoder (required).\nConstraints:\ndtype:int\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:[0,inf)\n\n###\n\nSignature: torch.nn.quantized.functional.interpolate(input,size=None,scale_factor=None,mode=nearest,align_corners=None)\nParameter description: align_corners(bool, optional) - Geometrically, we consider the pixels of the input and output as squares rather than points. If set to `True`, the input and output tensors are aligned by the center points of their corner pixels, preserving the values at the corner pixels. If set to `False`, the input and output tensors are aligned by the corner points of their corner pixels, and the interpolation uses edge value padding for out-of-boundary values, making this operation independent of input size when `scale_factor` is kept the same. This only has an effect when `mode` is `'bilinear'`. Default: `False`\nConstraints:\ndtype:bool\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.gt(input,other,out=None)\nParameter description: out(Tensor, optional) - the output tensor that must be a BoolTensor\nConstraints:\ndtype:booltensor\nstructure:tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.empty_like(input,dtype=None,layout=None,device=None,requires_grad=False,memory_format=torch.preserve_format)\nParameter description: dtype(`torch.dtype`, optional) - the desired data type of returned Tensor. Default: if `None`, defaults to the dtype of `input`.\nConstraints:\ndtype:torch.dtype\nstructure:Null\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.nn.RNN(*args,**kwargs,input_size=None,hidden_size=None,num_layers=None,nonlinearity=None,bias=None,batch_first=None,dropout=None,bidirectional=None)\nParameter description: hidden_size - The number of features in the hidden state h\nConstraints:\ndtype:int\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:[0,inf)\n\n###\n\nSignature: torch.nn.functional.avg_pool1d(input,kernel_size,stride=None,padding=0,ceil_mode=False,count_include_pad=True)\nParameter description: padding - implicit zero paddings on both sides of the input. Can be a single number or a tuple (padW,). Default: 0\nConstraints:\ndtype:bool;int\nstructure:tuple\nshape:Null\nndim:0;1\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.autograd.grad(outputs,inputs,grad_outputs=None,retain_graph=None,create_graph=False,only_inputs=True,allow_unused=False)\nParameter description: outputs(sequence of Tensor) - outputs of the differentiated function.\nConstraints:\ndtype:Null\nstructure:sequence;tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.repeat_interleave(input,repeats,dim=None)\nParameter description: dim(int, optional) - The dimension along which to repeat values.By default, use the flattened input array, and return a flat outputarray.\nConstraints:\ndtype:int\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.nn.functional.kl_div(input,target,size_average=None,reduce=None,reduction=mean)\nParameter description: target - Tensor of the same shape as input\nConstraints:",
            "constr": {
                "dtype": [],
                "structure": [
                    "tensor"
                ],
                "shape": [
                    "&input"
                ],
                "ndim": [],
                "enum": [],
                "range": []
            },
            "signature": "torch.nn.functional.kl_div(input,target,size_average=None,reduce=None,reduction=mean)",
            "descp": "target - Tensor of the same shape as input"
        },
        {
            "fname": "torch.nn.init.uniform_.yaml",
            "arg": "a",
            "prompt": "Signature: torch.distributed.recv(tensor,src=None,group=<objectobject>,tag=0)\nParameter description: tensor(Tensor) - Tensor to fill with received data.\nConstraints:\ndtype:Null\nstructure:tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.div(input,other,out=None)\nParameter description: out(Tensor, optional) - the output tensor.\nConstraints:\ndtype:Null\nstructure:tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.nn.TransformerDecoder(decoder_layer,num_layers,norm=None)\nParameter description: num_layers - the number of sub-decoder-layers in the decoder (required).\nConstraints:\ndtype:int\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:[0,inf)\n\n###\n\nSignature: torch.nn.quantized.functional.interpolate(input,size=None,scale_factor=None,mode=nearest,align_corners=None)\nParameter description: align_corners(bool, optional) - Geometrically, we consider the pixels of the input and output as squares rather than points. If set to `True`, the input and output tensors are aligned by the center points of their corner pixels, preserving the values at the corner pixels. If set to `False`, the input and output tensors are aligned by the corner points of their corner pixels, and the interpolation uses edge value padding for out-of-boundary values, making this operation independent of input size when `scale_factor` is kept the same. This only has an effect when `mode` is `'bilinear'`. Default: `False`\nConstraints:\ndtype:bool\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.gt(input,other,out=None)\nParameter description: out(Tensor, optional) - the output tensor that must be a BoolTensor\nConstraints:\ndtype:booltensor\nstructure:tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.empty_like(input,dtype=None,layout=None,device=None,requires_grad=False,memory_format=torch.preserve_format)\nParameter description: dtype(`torch.dtype`, optional) - the desired data type of returned Tensor. Default: if `None`, defaults to the dtype of `input`.\nConstraints:\ndtype:torch.dtype\nstructure:Null\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.nn.RNN(*args,**kwargs,input_size=None,hidden_size=None,num_layers=None,nonlinearity=None,bias=None,batch_first=None,dropout=None,bidirectional=None)\nParameter description: hidden_size - The number of features in the hidden state h\nConstraints:\ndtype:int\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:[0,inf)\n\n###\n\nSignature: torch.nn.functional.avg_pool1d(input,kernel_size,stride=None,padding=0,ceil_mode=False,count_include_pad=True)\nParameter description: padding - implicit zero paddings on both sides of the input. Can be a single number or a tuple (padW,). Default: 0\nConstraints:\ndtype:bool;int\nstructure:tuple\nshape:Null\nndim:0;1\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.autograd.grad(outputs,inputs,grad_outputs=None,retain_graph=None,create_graph=False,only_inputs=True,allow_unused=False)\nParameter description: outputs(sequence of Tensor) - outputs of the differentiated function.\nConstraints:\ndtype:Null\nstructure:sequence;tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.repeat_interleave(input,repeats,dim=None)\nParameter description: dim(int, optional) - The dimension along which to repeat values.By default, use the flattened input array, and return a flat outputarray.\nConstraints:\ndtype:int\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.nn.init.uniform_(tensor,a=0.0,b=1.0)\nParameter description: a - the lower bound of the uniform distribution\nConstraints:",
            "constr": {
                "dtype": [
                    "numeric",
                    "float"
                ],
                "structure": [],
                "shape": [],
                "ndim": [
                    "0"
                ],
                "enum": [],
                "range": []
            },
            "signature": "torch.nn.init.uniform_(tensor,a=0.0,b=1.0)",
            "descp": "a - the lower bound of the uniform distribution"
        },
        {
            "fname": "torch.gather.yaml",
            "arg": "index",
            "prompt": "Signature: torch.distributed.recv(tensor,src=None,group=<objectobject>,tag=0)\nParameter description: tensor(Tensor) - Tensor to fill with received data.\nConstraints:\ndtype:Null\nstructure:tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.div(input,other,out=None)\nParameter description: out(Tensor, optional) - the output tensor.\nConstraints:\ndtype:Null\nstructure:tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.nn.TransformerDecoder(decoder_layer,num_layers,norm=None)\nParameter description: num_layers - the number of sub-decoder-layers in the decoder (required).\nConstraints:\ndtype:int\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:[0,inf)\n\n###\n\nSignature: torch.nn.quantized.functional.interpolate(input,size=None,scale_factor=None,mode=nearest,align_corners=None)\nParameter description: align_corners(bool, optional) - Geometrically, we consider the pixels of the input and output as squares rather than points. If set to `True`, the input and output tensors are aligned by the center points of their corner pixels, preserving the values at the corner pixels. If set to `False`, the input and output tensors are aligned by the corner points of their corner pixels, and the interpolation uses edge value padding for out-of-boundary values, making this operation independent of input size when `scale_factor` is kept the same. This only has an effect when `mode` is `'bilinear'`. Default: `False`\nConstraints:\ndtype:bool\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.gt(input,other,out=None)\nParameter description: out(Tensor, optional) - the output tensor that must be a BoolTensor\nConstraints:\ndtype:booltensor\nstructure:tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.empty_like(input,dtype=None,layout=None,device=None,requires_grad=False,memory_format=torch.preserve_format)\nParameter description: dtype(`torch.dtype`, optional) - the desired data type of returned Tensor. Default: if `None`, defaults to the dtype of `input`.\nConstraints:\ndtype:torch.dtype\nstructure:Null\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.nn.RNN(*args,**kwargs,input_size=None,hidden_size=None,num_layers=None,nonlinearity=None,bias=None,batch_first=None,dropout=None,bidirectional=None)\nParameter description: hidden_size - The number of features in the hidden state h\nConstraints:\ndtype:int\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:[0,inf)\n\n###\n\nSignature: torch.nn.functional.avg_pool1d(input,kernel_size,stride=None,padding=0,ceil_mode=False,count_include_pad=True)\nParameter description: padding - implicit zero paddings on both sides of the input. Can be a single number or a tuple (padW,). Default: 0\nConstraints:\ndtype:bool;int\nstructure:tuple\nshape:Null\nndim:0;1\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.autograd.grad(outputs,inputs,grad_outputs=None,retain_graph=None,create_graph=False,only_inputs=True,allow_unused=False)\nParameter description: outputs(sequence of Tensor) - outputs of the differentiated function.\nConstraints:\ndtype:Null\nstructure:sequence;tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.repeat_interleave(input,repeats,dim=None)\nParameter description: dim(int, optional) - The dimension along which to repeat values.By default, use the flattened input array, and return a flat outputarray.\nConstraints:\ndtype:int\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.gather(input,dim,index,out=None,sparse_grad=False)\nParameter description: index(LongTensor) - the indices of elements to gather\nConstraints:",
            "constr": {
                "dtype": [
                    "int",
                    "longtensor"
                ],
                "structure": [],
                "shape": [],
                "ndim": [
                    "1"
                ],
                "enum": [],
                "range": []
            },
            "signature": "torch.gather(input,dim,index,out=None,sparse_grad=False)",
            "descp": "index(LongTensor) - the indices of elements to gather"
        },
        {
            "fname": "torch.addmv.yaml",
            "arg": "vec",
            "prompt": "Signature: torch.distributed.recv(tensor,src=None,group=<objectobject>,tag=0)\nParameter description: tensor(Tensor) - Tensor to fill with received data.\nConstraints:\ndtype:Null\nstructure:tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.div(input,other,out=None)\nParameter description: out(Tensor, optional) - the output tensor.\nConstraints:\ndtype:Null\nstructure:tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.nn.TransformerDecoder(decoder_layer,num_layers,norm=None)\nParameter description: num_layers - the number of sub-decoder-layers in the decoder (required).\nConstraints:\ndtype:int\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:[0,inf)\n\n###\n\nSignature: torch.nn.quantized.functional.interpolate(input,size=None,scale_factor=None,mode=nearest,align_corners=None)\nParameter description: align_corners(bool, optional) - Geometrically, we consider the pixels of the input and output as squares rather than points. If set to `True`, the input and output tensors are aligned by the center points of their corner pixels, preserving the values at the corner pixels. If set to `False`, the input and output tensors are aligned by the corner points of their corner pixels, and the interpolation uses edge value padding for out-of-boundary values, making this operation independent of input size when `scale_factor` is kept the same. This only has an effect when `mode` is `'bilinear'`. Default: `False`\nConstraints:\ndtype:bool\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.gt(input,other,out=None)\nParameter description: out(Tensor, optional) - the output tensor that must be a BoolTensor\nConstraints:\ndtype:booltensor\nstructure:tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.empty_like(input,dtype=None,layout=None,device=None,requires_grad=False,memory_format=torch.preserve_format)\nParameter description: dtype(`torch.dtype`, optional) - the desired data type of returned Tensor. Default: if `None`, defaults to the dtype of `input`.\nConstraints:\ndtype:torch.dtype\nstructure:Null\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.nn.RNN(*args,**kwargs,input_size=None,hidden_size=None,num_layers=None,nonlinearity=None,bias=None,batch_first=None,dropout=None,bidirectional=None)\nParameter description: hidden_size - The number of features in the hidden state h\nConstraints:\ndtype:int\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:[0,inf)\n\n###\n\nSignature: torch.nn.functional.avg_pool1d(input,kernel_size,stride=None,padding=0,ceil_mode=False,count_include_pad=True)\nParameter description: padding - implicit zero paddings on both sides of the input. Can be a single number or a tuple (padW,). Default: 0\nConstraints:\ndtype:bool;int\nstructure:tuple\nshape:Null\nndim:0;1\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.autograd.grad(outputs,inputs,grad_outputs=None,retain_graph=None,create_graph=False,only_inputs=True,allow_unused=False)\nParameter description: outputs(sequence of Tensor) - outputs of the differentiated function.\nConstraints:\ndtype:Null\nstructure:sequence;tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.repeat_interleave(input,repeats,dim=None)\nParameter description: dim(int, optional) - The dimension along which to repeat values.By default, use the flattened input array, and return a flat outputarray.\nConstraints:\ndtype:int\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.addmv(input,mat,vec,beta=1,alpha=1,out=None)\nParameter description: vec(Tensor) - vector to be multiplied\nConstraints:",
            "constr": {
                "dtype": [],
                "structure": [
                    "vector",
                    "tensor"
                ],
                "shape": [],
                "ndim": [
                    "1"
                ],
                "enum": [],
                "range": []
            },
            "signature": "torch.addmv(input,mat,vec,beta=1,alpha=1,out=None)",
            "descp": "vec(Tensor) - vector to be multiplied"
        },
        {
            "fname": "torch.matrix_rank.yaml",
            "arg": "symmetric",
            "prompt": "Signature: torch.distributed.recv(tensor,src=None,group=<objectobject>,tag=0)\nParameter description: tensor(Tensor) - Tensor to fill with received data.\nConstraints:\ndtype:Null\nstructure:tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.div(input,other,out=None)\nParameter description: out(Tensor, optional) - the output tensor.\nConstraints:\ndtype:Null\nstructure:tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.nn.TransformerDecoder(decoder_layer,num_layers,norm=None)\nParameter description: num_layers - the number of sub-decoder-layers in the decoder (required).\nConstraints:\ndtype:int\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:[0,inf)\n\n###\n\nSignature: torch.nn.quantized.functional.interpolate(input,size=None,scale_factor=None,mode=nearest,align_corners=None)\nParameter description: align_corners(bool, optional) - Geometrically, we consider the pixels of the input and output as squares rather than points. If set to `True`, the input and output tensors are aligned by the center points of their corner pixels, preserving the values at the corner pixels. If set to `False`, the input and output tensors are aligned by the corner points of their corner pixels, and the interpolation uses edge value padding for out-of-boundary values, making this operation independent of input size when `scale_factor` is kept the same. This only has an effect when `mode` is `'bilinear'`. Default: `False`\nConstraints:\ndtype:bool\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.gt(input,other,out=None)\nParameter description: out(Tensor, optional) - the output tensor that must be a BoolTensor\nConstraints:\ndtype:booltensor\nstructure:tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.empty_like(input,dtype=None,layout=None,device=None,requires_grad=False,memory_format=torch.preserve_format)\nParameter description: dtype(`torch.dtype`, optional) - the desired data type of returned Tensor. Default: if `None`, defaults to the dtype of `input`.\nConstraints:\ndtype:torch.dtype\nstructure:Null\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.nn.RNN(*args,**kwargs,input_size=None,hidden_size=None,num_layers=None,nonlinearity=None,bias=None,batch_first=None,dropout=None,bidirectional=None)\nParameter description: hidden_size - The number of features in the hidden state h\nConstraints:\ndtype:int\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:[0,inf)\n\n###\n\nSignature: torch.nn.functional.avg_pool1d(input,kernel_size,stride=None,padding=0,ceil_mode=False,count_include_pad=True)\nParameter description: padding - implicit zero paddings on both sides of the input. Can be a single number or a tuple (padW,). Default: 0\nConstraints:\ndtype:bool;int\nstructure:tuple\nshape:Null\nndim:0;1\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.autograd.grad(outputs,inputs,grad_outputs=None,retain_graph=None,create_graph=False,only_inputs=True,allow_unused=False)\nParameter description: outputs(sequence of Tensor) - outputs of the differentiated function.\nConstraints:\ndtype:Null\nstructure:sequence;tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.repeat_interleave(input,repeats,dim=None)\nParameter description: dim(int, optional) - The dimension along which to repeat values.By default, use the flattened input array, and return a flat outputarray.\nConstraints:\ndtype:int\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.matrix_rank(input,tol=None,symmetric=False)\nParameter description: symmetric(bool, optional) - indicates whether `input` is symmetric. Default: `False`\nConstraints:",
            "constr": {
                "dtype": [
                    "bool"
                ],
                "structure": [],
                "shape": [],
                "ndim": [
                    "0"
                ],
                "enum": [],
                "range": []
            },
            "signature": "torch.matrix_rank(input,tol=None,symmetric=False)",
            "descp": "symmetric(bool, optional) - indicates whether `input` is symmetric. Default: `False`"
        },
        {
            "fname": "torch.mm.yaml",
            "arg": "input",
            "prompt": "Signature: torch.distributed.recv(tensor,src=None,group=<objectobject>,tag=0)\nParameter description: tensor(Tensor) - Tensor to fill with received data.\nConstraints:\ndtype:Null\nstructure:tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.div(input,other,out=None)\nParameter description: out(Tensor, optional) - the output tensor.\nConstraints:\ndtype:Null\nstructure:tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.nn.TransformerDecoder(decoder_layer,num_layers,norm=None)\nParameter description: num_layers - the number of sub-decoder-layers in the decoder (required).\nConstraints:\ndtype:int\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:[0,inf)\n\n###\n\nSignature: torch.nn.quantized.functional.interpolate(input,size=None,scale_factor=None,mode=nearest,align_corners=None)\nParameter description: align_corners(bool, optional) - Geometrically, we consider the pixels of the input and output as squares rather than points. If set to `True`, the input and output tensors are aligned by the center points of their corner pixels, preserving the values at the corner pixels. If set to `False`, the input and output tensors are aligned by the corner points of their corner pixels, and the interpolation uses edge value padding for out-of-boundary values, making this operation independent of input size when `scale_factor` is kept the same. This only has an effect when `mode` is `'bilinear'`. Default: `False`\nConstraints:\ndtype:bool\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.gt(input,other,out=None)\nParameter description: out(Tensor, optional) - the output tensor that must be a BoolTensor\nConstraints:\ndtype:booltensor\nstructure:tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.empty_like(input,dtype=None,layout=None,device=None,requires_grad=False,memory_format=torch.preserve_format)\nParameter description: dtype(`torch.dtype`, optional) - the desired data type of returned Tensor. Default: if `None`, defaults to the dtype of `input`.\nConstraints:\ndtype:torch.dtype\nstructure:Null\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.nn.RNN(*args,**kwargs,input_size=None,hidden_size=None,num_layers=None,nonlinearity=None,bias=None,batch_first=None,dropout=None,bidirectional=None)\nParameter description: hidden_size - The number of features in the hidden state h\nConstraints:\ndtype:int\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:[0,inf)\n\n###\n\nSignature: torch.nn.functional.avg_pool1d(input,kernel_size,stride=None,padding=0,ceil_mode=False,count_include_pad=True)\nParameter description: padding - implicit zero paddings on both sides of the input. Can be a single number or a tuple (padW,). Default: 0\nConstraints:\ndtype:bool;int\nstructure:tuple\nshape:Null\nndim:0;1\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.autograd.grad(outputs,inputs,grad_outputs=None,retain_graph=None,create_graph=False,only_inputs=True,allow_unused=False)\nParameter description: outputs(sequence of Tensor) - outputs of the differentiated function.\nConstraints:\ndtype:Null\nstructure:sequence;tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.repeat_interleave(input,repeats,dim=None)\nParameter description: dim(int, optional) - The dimension along which to repeat values.By default, use the flattened input array, and return a flat outputarray.\nConstraints:\ndtype:int\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.mm(input,mat2,out=None)\nParameter description: input(Tensor) - the first matrix to be multiplied\nConstraints:",
            "constr": {
                "dtype": [
                    "numeric"
                ],
                "structure": [
                    "tensor"
                ],
                "shape": [],
                "ndim": [],
                "enum": [],
                "range": []
            },
            "signature": "torch.mm(input,mat2,out=None)",
            "descp": "input(Tensor) - the first matrix to be multiplied"
        },
        {
            "fname": "torch.quantization.quantize.yaml",
            "arg": "inplace",
            "prompt": "Signature: torch.distributed.recv(tensor,src=None,group=<objectobject>,tag=0)\nParameter description: tensor(Tensor) - Tensor to fill with received data.\nConstraints:\ndtype:Null\nstructure:tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.div(input,other,out=None)\nParameter description: out(Tensor, optional) - the output tensor.\nConstraints:\ndtype:Null\nstructure:tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.nn.TransformerDecoder(decoder_layer,num_layers,norm=None)\nParameter description: num_layers - the number of sub-decoder-layers in the decoder (required).\nConstraints:\ndtype:int\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:[0,inf)\n\n###\n\nSignature: torch.nn.quantized.functional.interpolate(input,size=None,scale_factor=None,mode=nearest,align_corners=None)\nParameter description: align_corners(bool, optional) - Geometrically, we consider the pixels of the input and output as squares rather than points. If set to `True`, the input and output tensors are aligned by the center points of their corner pixels, preserving the values at the corner pixels. If set to `False`, the input and output tensors are aligned by the corner points of their corner pixels, and the interpolation uses edge value padding for out-of-boundary values, making this operation independent of input size when `scale_factor` is kept the same. This only has an effect when `mode` is `'bilinear'`. Default: `False`\nConstraints:\ndtype:bool\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.gt(input,other,out=None)\nParameter description: out(Tensor, optional) - the output tensor that must be a BoolTensor\nConstraints:\ndtype:booltensor\nstructure:tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.empty_like(input,dtype=None,layout=None,device=None,requires_grad=False,memory_format=torch.preserve_format)\nParameter description: dtype(`torch.dtype`, optional) - the desired data type of returned Tensor. Default: if `None`, defaults to the dtype of `input`.\nConstraints:\ndtype:torch.dtype\nstructure:Null\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.nn.RNN(*args,**kwargs,input_size=None,hidden_size=None,num_layers=None,nonlinearity=None,bias=None,batch_first=None,dropout=None,bidirectional=None)\nParameter description: hidden_size - The number of features in the hidden state h\nConstraints:\ndtype:int\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:[0,inf)\n\n###\n\nSignature: torch.nn.functional.avg_pool1d(input,kernel_size,stride=None,padding=0,ceil_mode=False,count_include_pad=True)\nParameter description: padding - implicit zero paddings on both sides of the input. Can be a single number or a tuple (padW,). Default: 0\nConstraints:\ndtype:bool;int\nstructure:tuple\nshape:Null\nndim:0;1\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.autograd.grad(outputs,inputs,grad_outputs=None,retain_graph=None,create_graph=False,only_inputs=True,allow_unused=False)\nParameter description: outputs(sequence of Tensor) - outputs of the differentiated function.\nConstraints:\ndtype:Null\nstructure:sequence;tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.repeat_interleave(input,repeats,dim=None)\nParameter description: dim(int, optional) - The dimension along which to repeat values.By default, use the flattened input array, and return a flat outputarray.\nConstraints:\ndtype:int\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.quantization.quantize(model,run_fn,run_args,mapping=None,inplace=False)\nParameter description: inplace - carry out model transformations in-place, the original module is mutated\nConstraints:",
            "constr": {
                "dtype": [
                    "bool"
                ],
                "structure": [],
                "shape": [],
                "ndim": [
                    "0"
                ],
                "enum": [],
                "range": []
            },
            "signature": "torch.quantization.quantize(model,run_fn,run_args,mapping=None,inplace=False)",
            "descp": "inplace - carry out model transformations in-place, the original module is mutated"
        },
        {
            "fname": "torch.clamp.yaml",
            "arg": "out",
            "prompt": "Signature: torch.distributed.recv(tensor,src=None,group=<objectobject>,tag=0)\nParameter description: tensor(Tensor) - Tensor to fill with received data.\nConstraints:\ndtype:Null\nstructure:tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.div(input,other,out=None)\nParameter description: out(Tensor, optional) - the output tensor.\nConstraints:\ndtype:Null\nstructure:tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.nn.TransformerDecoder(decoder_layer,num_layers,norm=None)\nParameter description: num_layers - the number of sub-decoder-layers in the decoder (required).\nConstraints:\ndtype:int\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:[0,inf)\n\n###\n\nSignature: torch.nn.quantized.functional.interpolate(input,size=None,scale_factor=None,mode=nearest,align_corners=None)\nParameter description: align_corners(bool, optional) - Geometrically, we consider the pixels of the input and output as squares rather than points. If set to `True`, the input and output tensors are aligned by the center points of their corner pixels, preserving the values at the corner pixels. If set to `False`, the input and output tensors are aligned by the corner points of their corner pixels, and the interpolation uses edge value padding for out-of-boundary values, making this operation independent of input size when `scale_factor` is kept the same. This only has an effect when `mode` is `'bilinear'`. Default: `False`\nConstraints:\ndtype:bool\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.gt(input,other,out=None)\nParameter description: out(Tensor, optional) - the output tensor that must be a BoolTensor\nConstraints:\ndtype:booltensor\nstructure:tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.empty_like(input,dtype=None,layout=None,device=None,requires_grad=False,memory_format=torch.preserve_format)\nParameter description: dtype(`torch.dtype`, optional) - the desired data type of returned Tensor. Default: if `None`, defaults to the dtype of `input`.\nConstraints:\ndtype:torch.dtype\nstructure:Null\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.nn.RNN(*args,**kwargs,input_size=None,hidden_size=None,num_layers=None,nonlinearity=None,bias=None,batch_first=None,dropout=None,bidirectional=None)\nParameter description: hidden_size - The number of features in the hidden state h\nConstraints:\ndtype:int\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:[0,inf)\n\n###\n\nSignature: torch.nn.functional.avg_pool1d(input,kernel_size,stride=None,padding=0,ceil_mode=False,count_include_pad=True)\nParameter description: padding - implicit zero paddings on both sides of the input. Can be a single number or a tuple (padW,). Default: 0\nConstraints:\ndtype:bool;int\nstructure:tuple\nshape:Null\nndim:0;1\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.autograd.grad(outputs,inputs,grad_outputs=None,retain_graph=None,create_graph=False,only_inputs=True,allow_unused=False)\nParameter description: outputs(sequence of Tensor) - outputs of the differentiated function.\nConstraints:\ndtype:Null\nstructure:sequence;tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.repeat_interleave(input,repeats,dim=None)\nParameter description: dim(int, optional) - The dimension along which to repeat values.By default, use the flattened input array, and return a flat outputarray.\nConstraints:\ndtype:int\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.clamp(input,min,max,out=None)\nParameter description: out(Tensor, optional) - the output tensor.\nConstraints:",
            "constr": {
                "dtype": [],
                "structure": [
                    "tensor"
                ],
                "shape": [],
                "ndim": [],
                "enum": [],
                "range": []
            },
            "signature": "torch.clamp(input,min,max,out=None)",
            "descp": "out(Tensor, optional) - the output tensor."
        },
        {
            "fname": "torch.nn.init.ones_.yaml",
            "arg": "tensor",
            "prompt": "Signature: torch.distributed.recv(tensor,src=None,group=<objectobject>,tag=0)\nParameter description: tensor(Tensor) - Tensor to fill with received data.\nConstraints:\ndtype:Null\nstructure:tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.div(input,other,out=None)\nParameter description: out(Tensor, optional) - the output tensor.\nConstraints:\ndtype:Null\nstructure:tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.nn.TransformerDecoder(decoder_layer,num_layers,norm=None)\nParameter description: num_layers - the number of sub-decoder-layers in the decoder (required).\nConstraints:\ndtype:int\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:[0,inf)\n\n###\n\nSignature: torch.nn.quantized.functional.interpolate(input,size=None,scale_factor=None,mode=nearest,align_corners=None)\nParameter description: align_corners(bool, optional) - Geometrically, we consider the pixels of the input and output as squares rather than points. If set to `True`, the input and output tensors are aligned by the center points of their corner pixels, preserving the values at the corner pixels. If set to `False`, the input and output tensors are aligned by the corner points of their corner pixels, and the interpolation uses edge value padding for out-of-boundary values, making this operation independent of input size when `scale_factor` is kept the same. This only has an effect when `mode` is `'bilinear'`. Default: `False`\nConstraints:\ndtype:bool\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.gt(input,other,out=None)\nParameter description: out(Tensor, optional) - the output tensor that must be a BoolTensor\nConstraints:\ndtype:booltensor\nstructure:tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.empty_like(input,dtype=None,layout=None,device=None,requires_grad=False,memory_format=torch.preserve_format)\nParameter description: dtype(`torch.dtype`, optional) - the desired data type of returned Tensor. Default: if `None`, defaults to the dtype of `input`.\nConstraints:\ndtype:torch.dtype\nstructure:Null\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.nn.RNN(*args,**kwargs,input_size=None,hidden_size=None,num_layers=None,nonlinearity=None,bias=None,batch_first=None,dropout=None,bidirectional=None)\nParameter description: hidden_size - The number of features in the hidden state h\nConstraints:\ndtype:int\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:[0,inf)\n\n###\n\nSignature: torch.nn.functional.avg_pool1d(input,kernel_size,stride=None,padding=0,ceil_mode=False,count_include_pad=True)\nParameter description: padding - implicit zero paddings on both sides of the input. Can be a single number or a tuple (padW,). Default: 0\nConstraints:\ndtype:bool;int\nstructure:tuple\nshape:Null\nndim:0;1\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.autograd.grad(outputs,inputs,grad_outputs=None,retain_graph=None,create_graph=False,only_inputs=True,allow_unused=False)\nParameter description: outputs(sequence of Tensor) - outputs of the differentiated function.\nConstraints:\ndtype:Null\nstructure:sequence;tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.repeat_interleave(input,repeats,dim=None)\nParameter description: dim(int, optional) - The dimension along which to repeat values.By default, use the flattened input array, and return a flat outputarray.\nConstraints:\ndtype:int\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.nn.init.ones_(tensor)\nParameter description: tensor - an n-dimensional torch.Tensor\nConstraints:",
            "constr": {
                "dtype": [],
                "structure": [
                    "torch.tensor"
                ],
                "shape": [],
                "ndim": [],
                "enum": [],
                "range": []
            },
            "signature": "torch.nn.init.ones_(tensor)",
            "descp": "tensor - an n-dimensional torch.Tensor"
        },
        {
            "fname": "torch.round.yaml",
            "arg": "out",
            "prompt": "Signature: torch.distributed.recv(tensor,src=None,group=<objectobject>,tag=0)\nParameter description: tensor(Tensor) - Tensor to fill with received data.\nConstraints:\ndtype:Null\nstructure:tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.div(input,other,out=None)\nParameter description: out(Tensor, optional) - the output tensor.\nConstraints:\ndtype:Null\nstructure:tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.nn.TransformerDecoder(decoder_layer,num_layers,norm=None)\nParameter description: num_layers - the number of sub-decoder-layers in the decoder (required).\nConstraints:\ndtype:int\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:[0,inf)\n\n###\n\nSignature: torch.nn.quantized.functional.interpolate(input,size=None,scale_factor=None,mode=nearest,align_corners=None)\nParameter description: align_corners(bool, optional) - Geometrically, we consider the pixels of the input and output as squares rather than points. If set to `True`, the input and output tensors are aligned by the center points of their corner pixels, preserving the values at the corner pixels. If set to `False`, the input and output tensors are aligned by the corner points of their corner pixels, and the interpolation uses edge value padding for out-of-boundary values, making this operation independent of input size when `scale_factor` is kept the same. This only has an effect when `mode` is `'bilinear'`. Default: `False`\nConstraints:\ndtype:bool\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.gt(input,other,out=None)\nParameter description: out(Tensor, optional) - the output tensor that must be a BoolTensor\nConstraints:\ndtype:booltensor\nstructure:tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.empty_like(input,dtype=None,layout=None,device=None,requires_grad=False,memory_format=torch.preserve_format)\nParameter description: dtype(`torch.dtype`, optional) - the desired data type of returned Tensor. Default: if `None`, defaults to the dtype of `input`.\nConstraints:\ndtype:torch.dtype\nstructure:Null\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.nn.RNN(*args,**kwargs,input_size=None,hidden_size=None,num_layers=None,nonlinearity=None,bias=None,batch_first=None,dropout=None,bidirectional=None)\nParameter description: hidden_size - The number of features in the hidden state h\nConstraints:\ndtype:int\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:[0,inf)\n\n###\n\nSignature: torch.nn.functional.avg_pool1d(input,kernel_size,stride=None,padding=0,ceil_mode=False,count_include_pad=True)\nParameter description: padding - implicit zero paddings on both sides of the input. Can be a single number or a tuple (padW,). Default: 0\nConstraints:\ndtype:bool;int\nstructure:tuple\nshape:Null\nndim:0;1\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.autograd.grad(outputs,inputs,grad_outputs=None,retain_graph=None,create_graph=False,only_inputs=True,allow_unused=False)\nParameter description: outputs(sequence of Tensor) - outputs of the differentiated function.\nConstraints:\ndtype:Null\nstructure:sequence;tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.repeat_interleave(input,repeats,dim=None)\nParameter description: dim(int, optional) - The dimension along which to repeat values.By default, use the flattened input array, and return a flat outputarray.\nConstraints:\ndtype:int\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.round(input,out=None)\nParameter description: out(Tensor, optional) - the output tensor.\nConstraints:",
            "constr": {
                "dtype": [],
                "structure": [
                    "tensor"
                ],
                "shape": [],
                "ndim": [],
                "enum": [],
                "range": []
            },
            "signature": "torch.round(input,out=None)",
            "descp": "out(Tensor, optional) - the output tensor."
        },
        {
            "fname": "torch.renorm.yaml",
            "arg": "p",
            "prompt": "Signature: torch.distributed.recv(tensor,src=None,group=<objectobject>,tag=0)\nParameter description: tensor(Tensor) - Tensor to fill with received data.\nConstraints:\ndtype:Null\nstructure:tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.div(input,other,out=None)\nParameter description: out(Tensor, optional) - the output tensor.\nConstraints:\ndtype:Null\nstructure:tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.nn.TransformerDecoder(decoder_layer,num_layers,norm=None)\nParameter description: num_layers - the number of sub-decoder-layers in the decoder (required).\nConstraints:\ndtype:int\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:[0,inf)\n\n###\n\nSignature: torch.nn.quantized.functional.interpolate(input,size=None,scale_factor=None,mode=nearest,align_corners=None)\nParameter description: align_corners(bool, optional) - Geometrically, we consider the pixels of the input and output as squares rather than points. If set to `True`, the input and output tensors are aligned by the center points of their corner pixels, preserving the values at the corner pixels. If set to `False`, the input and output tensors are aligned by the corner points of their corner pixels, and the interpolation uses edge value padding for out-of-boundary values, making this operation independent of input size when `scale_factor` is kept the same. This only has an effect when `mode` is `'bilinear'`. Default: `False`\nConstraints:\ndtype:bool\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.gt(input,other,out=None)\nParameter description: out(Tensor, optional) - the output tensor that must be a BoolTensor\nConstraints:\ndtype:booltensor\nstructure:tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.empty_like(input,dtype=None,layout=None,device=None,requires_grad=False,memory_format=torch.preserve_format)\nParameter description: dtype(`torch.dtype`, optional) - the desired data type of returned Tensor. Default: if `None`, defaults to the dtype of `input`.\nConstraints:\ndtype:torch.dtype\nstructure:Null\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.nn.RNN(*args,**kwargs,input_size=None,hidden_size=None,num_layers=None,nonlinearity=None,bias=None,batch_first=None,dropout=None,bidirectional=None)\nParameter description: hidden_size - The number of features in the hidden state h\nConstraints:\ndtype:int\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:[0,inf)\n\n###\n\nSignature: torch.nn.functional.avg_pool1d(input,kernel_size,stride=None,padding=0,ceil_mode=False,count_include_pad=True)\nParameter description: padding - implicit zero paddings on both sides of the input. Can be a single number or a tuple (padW,). Default: 0\nConstraints:\ndtype:bool;int\nstructure:tuple\nshape:Null\nndim:0;1\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.autograd.grad(outputs,inputs,grad_outputs=None,retain_graph=None,create_graph=False,only_inputs=True,allow_unused=False)\nParameter description: outputs(sequence of Tensor) - outputs of the differentiated function.\nConstraints:\ndtype:Null\nstructure:sequence;tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.repeat_interleave(input,repeats,dim=None)\nParameter description: dim(int, optional) - The dimension along which to repeat values.By default, use the flattened input array, and return a flat outputarray.\nConstraints:\ndtype:int\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.renorm(input,p,dim,maxnorm,out=None)\nParameter description: p(float) - the power for the norm computation\nConstraints:",
            "constr": {
                "dtype": [
                    "float",
                    "int"
                ],
                "structure": [],
                "shape": [],
                "ndim": [],
                "enum": [],
                "range": []
            },
            "signature": "torch.renorm(input,p,dim,maxnorm,out=None)",
            "descp": "p(float) - the power for the norm computation"
        },
        {
            "fname": "torch.nn.ctcloss.yaml",
            "arg": "zero_infinity",
            "prompt": "Signature: torch.distributed.recv(tensor,src=None,group=<objectobject>,tag=0)\nParameter description: tensor(Tensor) - Tensor to fill with received data.\nConstraints:\ndtype:Null\nstructure:tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.div(input,other,out=None)\nParameter description: out(Tensor, optional) - the output tensor.\nConstraints:\ndtype:Null\nstructure:tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.nn.TransformerDecoder(decoder_layer,num_layers,norm=None)\nParameter description: num_layers - the number of sub-decoder-layers in the decoder (required).\nConstraints:\ndtype:int\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:[0,inf)\n\n###\n\nSignature: torch.nn.quantized.functional.interpolate(input,size=None,scale_factor=None,mode=nearest,align_corners=None)\nParameter description: align_corners(bool, optional) - Geometrically, we consider the pixels of the input and output as squares rather than points. If set to `True`, the input and output tensors are aligned by the center points of their corner pixels, preserving the values at the corner pixels. If set to `False`, the input and output tensors are aligned by the corner points of their corner pixels, and the interpolation uses edge value padding for out-of-boundary values, making this operation independent of input size when `scale_factor` is kept the same. This only has an effect when `mode` is `'bilinear'`. Default: `False`\nConstraints:\ndtype:bool\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.gt(input,other,out=None)\nParameter description: out(Tensor, optional) - the output tensor that must be a BoolTensor\nConstraints:\ndtype:booltensor\nstructure:tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.empty_like(input,dtype=None,layout=None,device=None,requires_grad=False,memory_format=torch.preserve_format)\nParameter description: dtype(`torch.dtype`, optional) - the desired data type of returned Tensor. Default: if `None`, defaults to the dtype of `input`.\nConstraints:\ndtype:torch.dtype\nstructure:Null\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.nn.RNN(*args,**kwargs,input_size=None,hidden_size=None,num_layers=None,nonlinearity=None,bias=None,batch_first=None,dropout=None,bidirectional=None)\nParameter description: hidden_size - The number of features in the hidden state h\nConstraints:\ndtype:int\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:[0,inf)\n\n###\n\nSignature: torch.nn.functional.avg_pool1d(input,kernel_size,stride=None,padding=0,ceil_mode=False,count_include_pad=True)\nParameter description: padding - implicit zero paddings on both sides of the input. Can be a single number or a tuple (padW,). Default: 0\nConstraints:\ndtype:bool;int\nstructure:tuple\nshape:Null\nndim:0;1\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.autograd.grad(outputs,inputs,grad_outputs=None,retain_graph=None,create_graph=False,only_inputs=True,allow_unused=False)\nParameter description: outputs(sequence of Tensor) - outputs of the differentiated function.\nConstraints:\ndtype:Null\nstructure:sequence;tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.repeat_interleave(input,repeats,dim=None)\nParameter description: dim(int, optional) - The dimension along which to repeat values.By default, use the flattened input array, and return a flat outputarray.\nConstraints:\ndtype:int\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.nn.CTCLoss(blank=0,reduction=mean,zero_infinity=False)\nParameter description: zero_infinity(bool, optional) - Whether to zero infinite losses and the associated gradients. Default: `False` Infinite losses mainly occur when the inputs are too short to be aligned to the targets.\nConstraints:",
            "constr": {
                "dtype": [
                    "bool"
                ],
                "structure": [],
                "shape": [],
                "ndim": [
                    "0"
                ],
                "enum": [],
                "range": []
            },
            "signature": "torch.nn.CTCLoss(blank=0,reduction=mean,zero_infinity=False)",
            "descp": "zero_infinity(bool, optional) - Whether to zero infinite losses and the associated gradients. Default: `False` Infinite losses mainly occur when the inputs are too short to be aligned to the targets."
        },
        {
            "fname": "torch.pca_lowrank.yaml",
            "arg": "niter",
            "prompt": "Signature: torch.distributed.recv(tensor,src=None,group=<objectobject>,tag=0)\nParameter description: tensor(Tensor) - Tensor to fill with received data.\nConstraints:\ndtype:Null\nstructure:tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.div(input,other,out=None)\nParameter description: out(Tensor, optional) - the output tensor.\nConstraints:\ndtype:Null\nstructure:tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.nn.TransformerDecoder(decoder_layer,num_layers,norm=None)\nParameter description: num_layers - the number of sub-decoder-layers in the decoder (required).\nConstraints:\ndtype:int\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:[0,inf)\n\n###\n\nSignature: torch.nn.quantized.functional.interpolate(input,size=None,scale_factor=None,mode=nearest,align_corners=None)\nParameter description: align_corners(bool, optional) - Geometrically, we consider the pixels of the input and output as squares rather than points. If set to `True`, the input and output tensors are aligned by the center points of their corner pixels, preserving the values at the corner pixels. If set to `False`, the input and output tensors are aligned by the corner points of their corner pixels, and the interpolation uses edge value padding for out-of-boundary values, making this operation independent of input size when `scale_factor` is kept the same. This only has an effect when `mode` is `'bilinear'`. Default: `False`\nConstraints:\ndtype:bool\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.gt(input,other,out=None)\nParameter description: out(Tensor, optional) - the output tensor that must be a BoolTensor\nConstraints:\ndtype:booltensor\nstructure:tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.empty_like(input,dtype=None,layout=None,device=None,requires_grad=False,memory_format=torch.preserve_format)\nParameter description: dtype(`torch.dtype`, optional) - the desired data type of returned Tensor. Default: if `None`, defaults to the dtype of `input`.\nConstraints:\ndtype:torch.dtype\nstructure:Null\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.nn.RNN(*args,**kwargs,input_size=None,hidden_size=None,num_layers=None,nonlinearity=None,bias=None,batch_first=None,dropout=None,bidirectional=None)\nParameter description: hidden_size - The number of features in the hidden state h\nConstraints:\ndtype:int\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:[0,inf)\n\n###\n\nSignature: torch.nn.functional.avg_pool1d(input,kernel_size,stride=None,padding=0,ceil_mode=False,count_include_pad=True)\nParameter description: padding - implicit zero paddings on both sides of the input. Can be a single number or a tuple (padW,). Default: 0\nConstraints:\ndtype:bool;int\nstructure:tuple\nshape:Null\nndim:0;1\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.autograd.grad(outputs,inputs,grad_outputs=None,retain_graph=None,create_graph=False,only_inputs=True,allow_unused=False)\nParameter description: outputs(sequence of Tensor) - outputs of the differentiated function.\nConstraints:\ndtype:Null\nstructure:sequence;tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.repeat_interleave(input,repeats,dim=None)\nParameter description: dim(int, optional) - The dimension along which to repeat values.By default, use the flattened input array, and return a flat outputarray.\nConstraints:\ndtype:int\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.pca_lowrank(A,q=None,center=True,niter=2)\nParameter description: niter(int, optional) - the number of subspace iterations to conduct; niter must be a nonnegative integer, and defaults to 2.\nConstraints:",
            "constr": {
                "dtype": [
                    "int"
                ],
                "structure": [],
                "shape": [],
                "ndim": [
                    "0"
                ],
                "enum": [],
                "range": [
                    "[0,inf)"
                ]
            },
            "signature": "torch.pca_lowrank(A,q=None,center=True,niter=2)",
            "descp": "niter(int, optional) - the number of subspace iterations to conduct; niter must be a nonnegative integer, and defaults to 2."
        },
        {
            "fname": "torch.nn.functional.nll_loss.yaml",
            "arg": "reduce",
            "prompt": "Signature: torch.distributed.recv(tensor,src=None,group=<objectobject>,tag=0)\nParameter description: tensor(Tensor) - Tensor to fill with received data.\nConstraints:\ndtype:Null\nstructure:tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.div(input,other,out=None)\nParameter description: out(Tensor, optional) - the output tensor.\nConstraints:\ndtype:Null\nstructure:tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.nn.TransformerDecoder(decoder_layer,num_layers,norm=None)\nParameter description: num_layers - the number of sub-decoder-layers in the decoder (required).\nConstraints:\ndtype:int\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:[0,inf)\n\n###\n\nSignature: torch.nn.quantized.functional.interpolate(input,size=None,scale_factor=None,mode=nearest,align_corners=None)\nParameter description: align_corners(bool, optional) - Geometrically, we consider the pixels of the input and output as squares rather than points. If set to `True`, the input and output tensors are aligned by the center points of their corner pixels, preserving the values at the corner pixels. If set to `False`, the input and output tensors are aligned by the corner points of their corner pixels, and the interpolation uses edge value padding for out-of-boundary values, making this operation independent of input size when `scale_factor` is kept the same. This only has an effect when `mode` is `'bilinear'`. Default: `False`\nConstraints:\ndtype:bool\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.gt(input,other,out=None)\nParameter description: out(Tensor, optional) - the output tensor that must be a BoolTensor\nConstraints:\ndtype:booltensor\nstructure:tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.empty_like(input,dtype=None,layout=None,device=None,requires_grad=False,memory_format=torch.preserve_format)\nParameter description: dtype(`torch.dtype`, optional) - the desired data type of returned Tensor. Default: if `None`, defaults to the dtype of `input`.\nConstraints:\ndtype:torch.dtype\nstructure:Null\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.nn.RNN(*args,**kwargs,input_size=None,hidden_size=None,num_layers=None,nonlinearity=None,bias=None,batch_first=None,dropout=None,bidirectional=None)\nParameter description: hidden_size - The number of features in the hidden state h\nConstraints:\ndtype:int\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:[0,inf)\n\n###\n\nSignature: torch.nn.functional.avg_pool1d(input,kernel_size,stride=None,padding=0,ceil_mode=False,count_include_pad=True)\nParameter description: padding - implicit zero paddings on both sides of the input. Can be a single number or a tuple (padW,). Default: 0\nConstraints:\ndtype:bool;int\nstructure:tuple\nshape:Null\nndim:0;1\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.autograd.grad(outputs,inputs,grad_outputs=None,retain_graph=None,create_graph=False,only_inputs=True,allow_unused=False)\nParameter description: outputs(sequence of Tensor) - outputs of the differentiated function.\nConstraints:\ndtype:Null\nstructure:sequence;tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.repeat_interleave(input,repeats,dim=None)\nParameter description: dim(int, optional) - The dimension along which to repeat values.By default, use the flattened input array, and return a flat outputarray.\nConstraints:\ndtype:int\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.nn.functional.nll_loss(input,target,weight=None,size_average=None,ignore_index=-100,reduce=None,reduction=mean)\nParameter description: reduce(bool, optional) - Deprecated (see `reduction`). By default, the losses are averaged or summed over observations for each minibatch depending on `size_average`. When `reduce` is `False`, returns a loss per batch element instead and ignores `size_average`. Default: `True`\nConstraints:",
            "constr": {
                "dtype": [
                    "bool"
                ],
                "structure": [],
                "shape": [],
                "ndim": [
                    "0"
                ],
                "enum": [],
                "range": []
            },
            "signature": "torch.nn.functional.nll_loss(input,target,weight=None,size_average=None,ignore_index=-100,reduce=None,reduction=mean)",
            "descp": "reduce(bool, optional) - Deprecated (see `reduction`). By default, the losses are averaged or summed over observations for each minibatch depending on `size_average`. When `reduce` is `False`, returns a loss per batch element instead and ignores `size_average`. Default: `True`"
        },
        {
            "fname": "torch.nn.init.zeros_.yaml",
            "arg": "tensor",
            "prompt": "Signature: torch.distributed.recv(tensor,src=None,group=<objectobject>,tag=0)\nParameter description: tensor(Tensor) - Tensor to fill with received data.\nConstraints:\ndtype:Null\nstructure:tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.div(input,other,out=None)\nParameter description: out(Tensor, optional) - the output tensor.\nConstraints:\ndtype:Null\nstructure:tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.nn.TransformerDecoder(decoder_layer,num_layers,norm=None)\nParameter description: num_layers - the number of sub-decoder-layers in the decoder (required).\nConstraints:\ndtype:int\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:[0,inf)\n\n###\n\nSignature: torch.nn.quantized.functional.interpolate(input,size=None,scale_factor=None,mode=nearest,align_corners=None)\nParameter description: align_corners(bool, optional) - Geometrically, we consider the pixels of the input and output as squares rather than points. If set to `True`, the input and output tensors are aligned by the center points of their corner pixels, preserving the values at the corner pixels. If set to `False`, the input and output tensors are aligned by the corner points of their corner pixels, and the interpolation uses edge value padding for out-of-boundary values, making this operation independent of input size when `scale_factor` is kept the same. This only has an effect when `mode` is `'bilinear'`. Default: `False`\nConstraints:\ndtype:bool\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.gt(input,other,out=None)\nParameter description: out(Tensor, optional) - the output tensor that must be a BoolTensor\nConstraints:\ndtype:booltensor\nstructure:tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.empty_like(input,dtype=None,layout=None,device=None,requires_grad=False,memory_format=torch.preserve_format)\nParameter description: dtype(`torch.dtype`, optional) - the desired data type of returned Tensor. Default: if `None`, defaults to the dtype of `input`.\nConstraints:\ndtype:torch.dtype\nstructure:Null\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.nn.RNN(*args,**kwargs,input_size=None,hidden_size=None,num_layers=None,nonlinearity=None,bias=None,batch_first=None,dropout=None,bidirectional=None)\nParameter description: hidden_size - The number of features in the hidden state h\nConstraints:\ndtype:int\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:[0,inf)\n\n###\n\nSignature: torch.nn.functional.avg_pool1d(input,kernel_size,stride=None,padding=0,ceil_mode=False,count_include_pad=True)\nParameter description: padding - implicit zero paddings on both sides of the input. Can be a single number or a tuple (padW,). Default: 0\nConstraints:\ndtype:bool;int\nstructure:tuple\nshape:Null\nndim:0;1\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.autograd.grad(outputs,inputs,grad_outputs=None,retain_graph=None,create_graph=False,only_inputs=True,allow_unused=False)\nParameter description: outputs(sequence of Tensor) - outputs of the differentiated function.\nConstraints:\ndtype:Null\nstructure:sequence;tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.repeat_interleave(input,repeats,dim=None)\nParameter description: dim(int, optional) - The dimension along which to repeat values.By default, use the flattened input array, and return a flat outputarray.\nConstraints:\ndtype:int\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.nn.init.zeros_(tensor)\nParameter description: tensor - an n-dimensional torch.Tensor\nConstraints:",
            "constr": {
                "dtype": [],
                "structure": [
                    "torch.tensor"
                ],
                "shape": [],
                "ndim": [],
                "enum": [],
                "range": []
            },
            "signature": "torch.nn.init.zeros_(tensor)",
            "descp": "tensor - an n-dimensional torch.Tensor"
        },
        {
            "fname": "torch.nn.bilinear.yaml",
            "arg": "in2_features",
            "prompt": "Signature: torch.distributed.recv(tensor,src=None,group=<objectobject>,tag=0)\nParameter description: tensor(Tensor) - Tensor to fill with received data.\nConstraints:\ndtype:Null\nstructure:tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.div(input,other,out=None)\nParameter description: out(Tensor, optional) - the output tensor.\nConstraints:\ndtype:Null\nstructure:tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.nn.TransformerDecoder(decoder_layer,num_layers,norm=None)\nParameter description: num_layers - the number of sub-decoder-layers in the decoder (required).\nConstraints:\ndtype:int\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:[0,inf)\n\n###\n\nSignature: torch.nn.quantized.functional.interpolate(input,size=None,scale_factor=None,mode=nearest,align_corners=None)\nParameter description: align_corners(bool, optional) - Geometrically, we consider the pixels of the input and output as squares rather than points. If set to `True`, the input and output tensors are aligned by the center points of their corner pixels, preserving the values at the corner pixels. If set to `False`, the input and output tensors are aligned by the corner points of their corner pixels, and the interpolation uses edge value padding for out-of-boundary values, making this operation independent of input size when `scale_factor` is kept the same. This only has an effect when `mode` is `'bilinear'`. Default: `False`\nConstraints:\ndtype:bool\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.gt(input,other,out=None)\nParameter description: out(Tensor, optional) - the output tensor that must be a BoolTensor\nConstraints:\ndtype:booltensor\nstructure:tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.empty_like(input,dtype=None,layout=None,device=None,requires_grad=False,memory_format=torch.preserve_format)\nParameter description: dtype(`torch.dtype`, optional) - the desired data type of returned Tensor. Default: if `None`, defaults to the dtype of `input`.\nConstraints:\ndtype:torch.dtype\nstructure:Null\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.nn.RNN(*args,**kwargs,input_size=None,hidden_size=None,num_layers=None,nonlinearity=None,bias=None,batch_first=None,dropout=None,bidirectional=None)\nParameter description: hidden_size - The number of features in the hidden state h\nConstraints:\ndtype:int\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:[0,inf)\n\n###\n\nSignature: torch.nn.functional.avg_pool1d(input,kernel_size,stride=None,padding=0,ceil_mode=False,count_include_pad=True)\nParameter description: padding - implicit zero paddings on both sides of the input. Can be a single number or a tuple (padW,). Default: 0\nConstraints:\ndtype:bool;int\nstructure:tuple\nshape:Null\nndim:0;1\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.autograd.grad(outputs,inputs,grad_outputs=None,retain_graph=None,create_graph=False,only_inputs=True,allow_unused=False)\nParameter description: outputs(sequence of Tensor) - outputs of the differentiated function.\nConstraints:\ndtype:Null\nstructure:sequence;tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.repeat_interleave(input,repeats,dim=None)\nParameter description: dim(int, optional) - The dimension along which to repeat values.By default, use the flattened input array, and return a flat outputarray.\nConstraints:\ndtype:int\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.nn.Bilinear(in1_features,in2_features,out_features,bias=True)\nParameter description: in2_features - size of each second input sample\nConstraints:",
            "constr": {
                "dtype": [
                    "int"
                ],
                "structure": [],
                "shape": [],
                "ndim": [],
                "enum": [],
                "range": [
                    "[0,inf)"
                ]
            },
            "signature": "torch.nn.Bilinear(in1_features,in2_features,out_features,bias=True)",
            "descp": "in2_features - size of each second input sample"
        },
        {
            "fname": "torch.allclose.yaml",
            "arg": "equal_nan",
            "prompt": "Signature: torch.distributed.recv(tensor,src=None,group=<objectobject>,tag=0)\nParameter description: tensor(Tensor) - Tensor to fill with received data.\nConstraints:\ndtype:Null\nstructure:tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.div(input,other,out=None)\nParameter description: out(Tensor, optional) - the output tensor.\nConstraints:\ndtype:Null\nstructure:tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.nn.TransformerDecoder(decoder_layer,num_layers,norm=None)\nParameter description: num_layers - the number of sub-decoder-layers in the decoder (required).\nConstraints:\ndtype:int\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:[0,inf)\n\n###\n\nSignature: torch.nn.quantized.functional.interpolate(input,size=None,scale_factor=None,mode=nearest,align_corners=None)\nParameter description: align_corners(bool, optional) - Geometrically, we consider the pixels of the input and output as squares rather than points. If set to `True`, the input and output tensors are aligned by the center points of their corner pixels, preserving the values at the corner pixels. If set to `False`, the input and output tensors are aligned by the corner points of their corner pixels, and the interpolation uses edge value padding for out-of-boundary values, making this operation independent of input size when `scale_factor` is kept the same. This only has an effect when `mode` is `'bilinear'`. Default: `False`\nConstraints:\ndtype:bool\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.gt(input,other,out=None)\nParameter description: out(Tensor, optional) - the output tensor that must be a BoolTensor\nConstraints:\ndtype:booltensor\nstructure:tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.empty_like(input,dtype=None,layout=None,device=None,requires_grad=False,memory_format=torch.preserve_format)\nParameter description: dtype(`torch.dtype`, optional) - the desired data type of returned Tensor. Default: if `None`, defaults to the dtype of `input`.\nConstraints:\ndtype:torch.dtype\nstructure:Null\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.nn.RNN(*args,**kwargs,input_size=None,hidden_size=None,num_layers=None,nonlinearity=None,bias=None,batch_first=None,dropout=None,bidirectional=None)\nParameter description: hidden_size - The number of features in the hidden state h\nConstraints:\ndtype:int\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:[0,inf)\n\n###\n\nSignature: torch.nn.functional.avg_pool1d(input,kernel_size,stride=None,padding=0,ceil_mode=False,count_include_pad=True)\nParameter description: padding - implicit zero paddings on both sides of the input. Can be a single number or a tuple (padW,). Default: 0\nConstraints:\ndtype:bool;int\nstructure:tuple\nshape:Null\nndim:0;1\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.autograd.grad(outputs,inputs,grad_outputs=None,retain_graph=None,create_graph=False,only_inputs=True,allow_unused=False)\nParameter description: outputs(sequence of Tensor) - outputs of the differentiated function.\nConstraints:\ndtype:Null\nstructure:sequence;tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.repeat_interleave(input,repeats,dim=None)\nParameter description: dim(int, optional) - The dimension along which to repeat values.By default, use the flattened input array, and return a flat outputarray.\nConstraints:\ndtype:int\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.allclose(input,other,rtol=1e-05,atol=1e-08,equal_nan=False)\nParameter description: equal_nan(bool, optional) - if `True`, then two `NaN` s will be compared as equal. Default: `False`\nConstraints:",
            "constr": {
                "dtype": [
                    "bool"
                ],
                "structure": [],
                "shape": [],
                "ndim": [
                    "0"
                ],
                "enum": [],
                "range": []
            },
            "signature": "torch.allclose(input,other,rtol=1e-05,atol=1e-08,equal_nan=False)",
            "descp": "equal_nan(bool, optional) - if `True`, then two `NaN` s will be compared as equal. Default: `False`"
        },
        {
            "fname": "torch.nn.maxpool1d.yaml",
            "arg": "dilation",
            "prompt": "Signature: torch.distributed.recv(tensor,src=None,group=<objectobject>,tag=0)\nParameter description: tensor(Tensor) - Tensor to fill with received data.\nConstraints:\ndtype:Null\nstructure:tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.div(input,other,out=None)\nParameter description: out(Tensor, optional) - the output tensor.\nConstraints:\ndtype:Null\nstructure:tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.nn.TransformerDecoder(decoder_layer,num_layers,norm=None)\nParameter description: num_layers - the number of sub-decoder-layers in the decoder (required).\nConstraints:\ndtype:int\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:[0,inf)\n\n###\n\nSignature: torch.nn.quantized.functional.interpolate(input,size=None,scale_factor=None,mode=nearest,align_corners=None)\nParameter description: align_corners(bool, optional) - Geometrically, we consider the pixels of the input and output as squares rather than points. If set to `True`, the input and output tensors are aligned by the center points of their corner pixels, preserving the values at the corner pixels. If set to `False`, the input and output tensors are aligned by the corner points of their corner pixels, and the interpolation uses edge value padding for out-of-boundary values, making this operation independent of input size when `scale_factor` is kept the same. This only has an effect when `mode` is `'bilinear'`. Default: `False`\nConstraints:\ndtype:bool\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.gt(input,other,out=None)\nParameter description: out(Tensor, optional) - the output tensor that must be a BoolTensor\nConstraints:\ndtype:booltensor\nstructure:tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.empty_like(input,dtype=None,layout=None,device=None,requires_grad=False,memory_format=torch.preserve_format)\nParameter description: dtype(`torch.dtype`, optional) - the desired data type of returned Tensor. Default: if `None`, defaults to the dtype of `input`.\nConstraints:\ndtype:torch.dtype\nstructure:Null\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.nn.RNN(*args,**kwargs,input_size=None,hidden_size=None,num_layers=None,nonlinearity=None,bias=None,batch_first=None,dropout=None,bidirectional=None)\nParameter description: hidden_size - The number of features in the hidden state h\nConstraints:\ndtype:int\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:[0,inf)\n\n###\n\nSignature: torch.nn.functional.avg_pool1d(input,kernel_size,stride=None,padding=0,ceil_mode=False,count_include_pad=True)\nParameter description: padding - implicit zero paddings on both sides of the input. Can be a single number or a tuple (padW,). Default: 0\nConstraints:\ndtype:bool;int\nstructure:tuple\nshape:Null\nndim:0;1\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.autograd.grad(outputs,inputs,grad_outputs=None,retain_graph=None,create_graph=False,only_inputs=True,allow_unused=False)\nParameter description: outputs(sequence of Tensor) - outputs of the differentiated function.\nConstraints:\ndtype:Null\nstructure:sequence;tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.repeat_interleave(input,repeats,dim=None)\nParameter description: dim(int, optional) - The dimension along which to repeat values.By default, use the flattened input array, and return a flat outputarray.\nConstraints:\ndtype:int\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.nn.MaxPool1d(kernel_size,stride=None,padding=0,dilation=1,return_indices=False,ceil_mode=False)\nParameter description: dilation - a parameter that controls the stride of elements in the window\nConstraints:",
            "constr": {
                "dtype": [
                    "int"
                ],
                "structure": [],
                "shape": [],
                "ndim": [
                    "0"
                ],
                "enum": [],
                "range": []
            },
            "signature": "torch.nn.MaxPool1d(kernel_size,stride=None,padding=0,dilation=1,return_indices=False,ceil_mode=False)",
            "descp": "dilation - a parameter that controls the stride of elements in the window"
        },
        {
            "fname": "torch.autograd.gradgradcheck.yaml",
            "arg": "grad_outputs",
            "prompt": "Signature: torch.distributed.recv(tensor,src=None,group=<objectobject>,tag=0)\nParameter description: tensor(Tensor) - Tensor to fill with received data.\nConstraints:\ndtype:Null\nstructure:tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.div(input,other,out=None)\nParameter description: out(Tensor, optional) - the output tensor.\nConstraints:\ndtype:Null\nstructure:tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.nn.TransformerDecoder(decoder_layer,num_layers,norm=None)\nParameter description: num_layers - the number of sub-decoder-layers in the decoder (required).\nConstraints:\ndtype:int\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:[0,inf)\n\n###\n\nSignature: torch.nn.quantized.functional.interpolate(input,size=None,scale_factor=None,mode=nearest,align_corners=None)\nParameter description: align_corners(bool, optional) - Geometrically, we consider the pixels of the input and output as squares rather than points. If set to `True`, the input and output tensors are aligned by the center points of their corner pixels, preserving the values at the corner pixels. If set to `False`, the input and output tensors are aligned by the corner points of their corner pixels, and the interpolation uses edge value padding for out-of-boundary values, making this operation independent of input size when `scale_factor` is kept the same. This only has an effect when `mode` is `'bilinear'`. Default: `False`\nConstraints:\ndtype:bool\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.gt(input,other,out=None)\nParameter description: out(Tensor, optional) - the output tensor that must be a BoolTensor\nConstraints:\ndtype:booltensor\nstructure:tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.empty_like(input,dtype=None,layout=None,device=None,requires_grad=False,memory_format=torch.preserve_format)\nParameter description: dtype(`torch.dtype`, optional) - the desired data type of returned Tensor. Default: if `None`, defaults to the dtype of `input`.\nConstraints:\ndtype:torch.dtype\nstructure:Null\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.nn.RNN(*args,**kwargs,input_size=None,hidden_size=None,num_layers=None,nonlinearity=None,bias=None,batch_first=None,dropout=None,bidirectional=None)\nParameter description: hidden_size - The number of features in the hidden state h\nConstraints:\ndtype:int\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:[0,inf)\n\n###\n\nSignature: torch.nn.functional.avg_pool1d(input,kernel_size,stride=None,padding=0,ceil_mode=False,count_include_pad=True)\nParameter description: padding - implicit zero paddings on both sides of the input. Can be a single number or a tuple (padW,). Default: 0\nConstraints:\ndtype:bool;int\nstructure:tuple\nshape:Null\nndim:0;1\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.autograd.grad(outputs,inputs,grad_outputs=None,retain_graph=None,create_graph=False,only_inputs=True,allow_unused=False)\nParameter description: outputs(sequence of Tensor) - outputs of the differentiated function.\nConstraints:\ndtype:Null\nstructure:sequence;tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.repeat_interleave(input,repeats,dim=None)\nParameter description: dim(int, optional) - The dimension along which to repeat values.By default, use the flattened input array, and return a flat outputarray.\nConstraints:\ndtype:int\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.autograd.gradgradcheck(func,inputs,grad_outputs=None,eps=1e-06,atol=1e-05,rtol=0.001,gen_non_contig_grad_outputs=False,raise_exception=True,nondet_tol=0.0)\nParameter description: grad_outputs(tuple of Tensor or Tensor, optional) - The gradients with respect to the function's outputs.\nConstraints:",
            "constr": {
                "dtype": [
                    "numeric"
                ],
                "structure": [
                    "tuple",
                    "tensor"
                ],
                "shape": [],
                "ndim": [],
                "enum": [],
                "range": []
            },
            "signature": "torch.autograd.gradgradcheck(func,inputs,grad_outputs=None,eps=1e-06,atol=1e-05,rtol=0.001,gen_non_contig_grad_outputs=False,raise_exception=True,nondet_tol=0.0)",
            "descp": "grad_outputs(tuple of Tensor or Tensor, optional) - The gradients with respect to the function's outputs."
        },
        {
            "fname": "torch.distributed.isend.yaml",
            "arg": "tensor",
            "prompt": "Signature: torch.distributed.recv(tensor,src=None,group=<objectobject>,tag=0)\nParameter description: tensor(Tensor) - Tensor to fill with received data.\nConstraints:\ndtype:Null\nstructure:tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.div(input,other,out=None)\nParameter description: out(Tensor, optional) - the output tensor.\nConstraints:\ndtype:Null\nstructure:tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.nn.TransformerDecoder(decoder_layer,num_layers,norm=None)\nParameter description: num_layers - the number of sub-decoder-layers in the decoder (required).\nConstraints:\ndtype:int\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:[0,inf)\n\n###\n\nSignature: torch.nn.quantized.functional.interpolate(input,size=None,scale_factor=None,mode=nearest,align_corners=None)\nParameter description: align_corners(bool, optional) - Geometrically, we consider the pixels of the input and output as squares rather than points. If set to `True`, the input and output tensors are aligned by the center points of their corner pixels, preserving the values at the corner pixels. If set to `False`, the input and output tensors are aligned by the corner points of their corner pixels, and the interpolation uses edge value padding for out-of-boundary values, making this operation independent of input size when `scale_factor` is kept the same. This only has an effect when `mode` is `'bilinear'`. Default: `False`\nConstraints:\ndtype:bool\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.gt(input,other,out=None)\nParameter description: out(Tensor, optional) - the output tensor that must be a BoolTensor\nConstraints:\ndtype:booltensor\nstructure:tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.empty_like(input,dtype=None,layout=None,device=None,requires_grad=False,memory_format=torch.preserve_format)\nParameter description: dtype(`torch.dtype`, optional) - the desired data type of returned Tensor. Default: if `None`, defaults to the dtype of `input`.\nConstraints:\ndtype:torch.dtype\nstructure:Null\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.nn.RNN(*args,**kwargs,input_size=None,hidden_size=None,num_layers=None,nonlinearity=None,bias=None,batch_first=None,dropout=None,bidirectional=None)\nParameter description: hidden_size - The number of features in the hidden state h\nConstraints:\ndtype:int\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:[0,inf)\n\n###\n\nSignature: torch.nn.functional.avg_pool1d(input,kernel_size,stride=None,padding=0,ceil_mode=False,count_include_pad=True)\nParameter description: padding - implicit zero paddings on both sides of the input. Can be a single number or a tuple (padW,). Default: 0\nConstraints:\ndtype:bool;int\nstructure:tuple\nshape:Null\nndim:0;1\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.autograd.grad(outputs,inputs,grad_outputs=None,retain_graph=None,create_graph=False,only_inputs=True,allow_unused=False)\nParameter description: outputs(sequence of Tensor) - outputs of the differentiated function.\nConstraints:\ndtype:Null\nstructure:sequence;tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.repeat_interleave(input,repeats,dim=None)\nParameter description: dim(int, optional) - The dimension along which to repeat values.By default, use the flattened input array, and return a flat outputarray.\nConstraints:\ndtype:int\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.distributed.isend(tensor,dst,group=<objectobject>,tag=0)\nParameter description: tensor(Tensor) - Tensor to send.\nConstraints:",
            "constr": {
                "dtype": [],
                "structure": [
                    "tensor"
                ],
                "shape": [],
                "ndim": [],
                "enum": [],
                "range": []
            },
            "signature": "torch.distributed.isend(tensor,dst,group=<objectobject>,tag=0)",
            "descp": "tensor(Tensor) - Tensor to send."
        },
        {
            "fname": "torch.nn.quantized.functional.conv2d.yaml",
            "arg": "dilation",
            "prompt": "Signature: torch.distributed.recv(tensor,src=None,group=<objectobject>,tag=0)\nParameter description: tensor(Tensor) - Tensor to fill with received data.\nConstraints:\ndtype:Null\nstructure:tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.div(input,other,out=None)\nParameter description: out(Tensor, optional) - the output tensor.\nConstraints:\ndtype:Null\nstructure:tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.nn.TransformerDecoder(decoder_layer,num_layers,norm=None)\nParameter description: num_layers - the number of sub-decoder-layers in the decoder (required).\nConstraints:\ndtype:int\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:[0,inf)\n\n###\n\nSignature: torch.nn.quantized.functional.interpolate(input,size=None,scale_factor=None,mode=nearest,align_corners=None)\nParameter description: align_corners(bool, optional) - Geometrically, we consider the pixels of the input and output as squares rather than points. If set to `True`, the input and output tensors are aligned by the center points of their corner pixels, preserving the values at the corner pixels. If set to `False`, the input and output tensors are aligned by the corner points of their corner pixels, and the interpolation uses edge value padding for out-of-boundary values, making this operation independent of input size when `scale_factor` is kept the same. This only has an effect when `mode` is `'bilinear'`. Default: `False`\nConstraints:\ndtype:bool\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.gt(input,other,out=None)\nParameter description: out(Tensor, optional) - the output tensor that must be a BoolTensor\nConstraints:\ndtype:booltensor\nstructure:tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.empty_like(input,dtype=None,layout=None,device=None,requires_grad=False,memory_format=torch.preserve_format)\nParameter description: dtype(`torch.dtype`, optional) - the desired data type of returned Tensor. Default: if `None`, defaults to the dtype of `input`.\nConstraints:\ndtype:torch.dtype\nstructure:Null\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.nn.RNN(*args,**kwargs,input_size=None,hidden_size=None,num_layers=None,nonlinearity=None,bias=None,batch_first=None,dropout=None,bidirectional=None)\nParameter description: hidden_size - The number of features in the hidden state h\nConstraints:\ndtype:int\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:[0,inf)\n\n###\n\nSignature: torch.nn.functional.avg_pool1d(input,kernel_size,stride=None,padding=0,ceil_mode=False,count_include_pad=True)\nParameter description: padding - implicit zero paddings on both sides of the input. Can be a single number or a tuple (padW,). Default: 0\nConstraints:\ndtype:bool;int\nstructure:tuple\nshape:Null\nndim:0;1\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.autograd.grad(outputs,inputs,grad_outputs=None,retain_graph=None,create_graph=False,only_inputs=True,allow_unused=False)\nParameter description: outputs(sequence of Tensor) - outputs of the differentiated function.\nConstraints:\ndtype:Null\nstructure:sequence;tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.repeat_interleave(input,repeats,dim=None)\nParameter description: dim(int, optional) - The dimension along which to repeat values.By default, use the flattened input array, and return a flat outputarray.\nConstraints:\ndtype:int\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.nn.quantized.functional.conv2d(input,weight,bias,stride=1,padding=0,dilation=1,groups=1,padding_mode=zeros,scale=1.0,zero_point=0,dtype=torch.quint8)\nParameter description: dilation - the spacing between kernel elements. Can be a single number or a tuple (dH, dW). Default: 1\nConstraints:",
            "constr": {
                "dtype": [
                    "numeric",
                    "int"
                ],
                "structure": [
                    "tuple"
                ],
                "shape": [],
                "ndim": [
                    "0",
                    "1"
                ],
                "enum": [],
                "range": []
            },
            "signature": "torch.nn.quantized.functional.conv2d(input,weight,bias,stride=1,padding=0,dilation=1,groups=1,padding_mode=zeros,scale=1.0,zero_point=0,dtype=torch.quint8)",
            "descp": "dilation - the spacing between kernel elements. Can be a single number or a tuple (dH, dW). Default: 1"
        },
        {
            "fname": "torch.quantize_per_channel.yaml",
            "arg": "axis",
            "prompt": "Signature: torch.distributed.recv(tensor,src=None,group=<objectobject>,tag=0)\nParameter description: tensor(Tensor) - Tensor to fill with received data.\nConstraints:\ndtype:Null\nstructure:tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.div(input,other,out=None)\nParameter description: out(Tensor, optional) - the output tensor.\nConstraints:\ndtype:Null\nstructure:tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.nn.TransformerDecoder(decoder_layer,num_layers,norm=None)\nParameter description: num_layers - the number of sub-decoder-layers in the decoder (required).\nConstraints:\ndtype:int\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:[0,inf)\n\n###\n\nSignature: torch.nn.quantized.functional.interpolate(input,size=None,scale_factor=None,mode=nearest,align_corners=None)\nParameter description: align_corners(bool, optional) - Geometrically, we consider the pixels of the input and output as squares rather than points. If set to `True`, the input and output tensors are aligned by the center points of their corner pixels, preserving the values at the corner pixels. If set to `False`, the input and output tensors are aligned by the corner points of their corner pixels, and the interpolation uses edge value padding for out-of-boundary values, making this operation independent of input size when `scale_factor` is kept the same. This only has an effect when `mode` is `'bilinear'`. Default: `False`\nConstraints:\ndtype:bool\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.gt(input,other,out=None)\nParameter description: out(Tensor, optional) - the output tensor that must be a BoolTensor\nConstraints:\ndtype:booltensor\nstructure:tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.empty_like(input,dtype=None,layout=None,device=None,requires_grad=False,memory_format=torch.preserve_format)\nParameter description: dtype(`torch.dtype`, optional) - the desired data type of returned Tensor. Default: if `None`, defaults to the dtype of `input`.\nConstraints:\ndtype:torch.dtype\nstructure:Null\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.nn.RNN(*args,**kwargs,input_size=None,hidden_size=None,num_layers=None,nonlinearity=None,bias=None,batch_first=None,dropout=None,bidirectional=None)\nParameter description: hidden_size - The number of features in the hidden state h\nConstraints:\ndtype:int\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:[0,inf)\n\n###\n\nSignature: torch.nn.functional.avg_pool1d(input,kernel_size,stride=None,padding=0,ceil_mode=False,count_include_pad=True)\nParameter description: padding - implicit zero paddings on both sides of the input. Can be a single number or a tuple (padW,). Default: 0\nConstraints:\ndtype:bool;int\nstructure:tuple\nshape:Null\nndim:0;1\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.autograd.grad(outputs,inputs,grad_outputs=None,retain_graph=None,create_graph=False,only_inputs=True,allow_unused=False)\nParameter description: outputs(sequence of Tensor) - outputs of the differentiated function.\nConstraints:\ndtype:Null\nstructure:sequence;tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.repeat_interleave(input,repeats,dim=None)\nParameter description: dim(int, optional) - The dimension along which to repeat values.By default, use the flattened input array, and return a flat outputarray.\nConstraints:\ndtype:int\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.quantize_per_channel(input,scales,zero_points,axis,dtype)\nParameter description: axis(int) - dimension on which apply per-channel quantization\nConstraints:",
            "constr": {
                "dtype": [
                    "int"
                ],
                "structure": [],
                "shape": [],
                "ndim": [
                    "0"
                ],
                "enum": [],
                "range": []
            },
            "signature": "torch.quantize_per_channel(input,scales,zero_points,axis,dtype)",
            "descp": "axis(int) - dimension on which apply per-channel quantization"
        },
        {
            "fname": "torch.nn.dataparallel.yaml",
            "arg": "output_device",
            "prompt": "Signature: torch.distributed.recv(tensor,src=None,group=<objectobject>,tag=0)\nParameter description: tensor(Tensor) - Tensor to fill with received data.\nConstraints:\ndtype:Null\nstructure:tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.div(input,other,out=None)\nParameter description: out(Tensor, optional) - the output tensor.\nConstraints:\ndtype:Null\nstructure:tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.nn.TransformerDecoder(decoder_layer,num_layers,norm=None)\nParameter description: num_layers - the number of sub-decoder-layers in the decoder (required).\nConstraints:\ndtype:int\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:[0,inf)\n\n###\n\nSignature: torch.nn.quantized.functional.interpolate(input,size=None,scale_factor=None,mode=nearest,align_corners=None)\nParameter description: align_corners(bool, optional) - Geometrically, we consider the pixels of the input and output as squares rather than points. If set to `True`, the input and output tensors are aligned by the center points of their corner pixels, preserving the values at the corner pixels. If set to `False`, the input and output tensors are aligned by the corner points of their corner pixels, and the interpolation uses edge value padding for out-of-boundary values, making this operation independent of input size when `scale_factor` is kept the same. This only has an effect when `mode` is `'bilinear'`. Default: `False`\nConstraints:\ndtype:bool\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.gt(input,other,out=None)\nParameter description: out(Tensor, optional) - the output tensor that must be a BoolTensor\nConstraints:\ndtype:booltensor\nstructure:tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.empty_like(input,dtype=None,layout=None,device=None,requires_grad=False,memory_format=torch.preserve_format)\nParameter description: dtype(`torch.dtype`, optional) - the desired data type of returned Tensor. Default: if `None`, defaults to the dtype of `input`.\nConstraints:\ndtype:torch.dtype\nstructure:Null\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.nn.RNN(*args,**kwargs,input_size=None,hidden_size=None,num_layers=None,nonlinearity=None,bias=None,batch_first=None,dropout=None,bidirectional=None)\nParameter description: hidden_size - The number of features in the hidden state h\nConstraints:\ndtype:int\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:[0,inf)\n\n###\n\nSignature: torch.nn.functional.avg_pool1d(input,kernel_size,stride=None,padding=0,ceil_mode=False,count_include_pad=True)\nParameter description: padding - implicit zero paddings on both sides of the input. Can be a single number or a tuple (padW,). Default: 0\nConstraints:\ndtype:bool;int\nstructure:tuple\nshape:Null\nndim:0;1\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.autograd.grad(outputs,inputs,grad_outputs=None,retain_graph=None,create_graph=False,only_inputs=True,allow_unused=False)\nParameter description: outputs(sequence of Tensor) - outputs of the differentiated function.\nConstraints:\ndtype:Null\nstructure:sequence;tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.repeat_interleave(input,repeats,dim=None)\nParameter description: dim(int, optional) - The dimension along which to repeat values.By default, use the flattened input array, and return a flat outputarray.\nConstraints:\ndtype:int\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.nn.DataParallel(module,device_ids=None,output_device=None,dim=0)\nParameter description: output_device(int or torch.device) - device location of output (default: device_ids[0])\nConstraints:",
            "constr": {
                "dtype": [
                    "int"
                ],
                "structure": [],
                "shape": [],
                "ndim": [],
                "enum": [],
                "range": []
            },
            "signature": "torch.nn.DataParallel(module,device_ids=None,output_device=None,dim=0)",
            "descp": "output_device(int or torch.device) - device location of output (default: device_ids[0])"
        },
        {
            "fname": "torch.nn.hingeembeddingloss.yaml",
            "arg": "size_average",
            "prompt": "Signature: torch.distributed.recv(tensor,src=None,group=<objectobject>,tag=0)\nParameter description: tensor(Tensor) - Tensor to fill with received data.\nConstraints:\ndtype:Null\nstructure:tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.div(input,other,out=None)\nParameter description: out(Tensor, optional) - the output tensor.\nConstraints:\ndtype:Null\nstructure:tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.nn.TransformerDecoder(decoder_layer,num_layers,norm=None)\nParameter description: num_layers - the number of sub-decoder-layers in the decoder (required).\nConstraints:\ndtype:int\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:[0,inf)\n\n###\n\nSignature: torch.nn.quantized.functional.interpolate(input,size=None,scale_factor=None,mode=nearest,align_corners=None)\nParameter description: align_corners(bool, optional) - Geometrically, we consider the pixels of the input and output as squares rather than points. If set to `True`, the input and output tensors are aligned by the center points of their corner pixels, preserving the values at the corner pixels. If set to `False`, the input and output tensors are aligned by the corner points of their corner pixels, and the interpolation uses edge value padding for out-of-boundary values, making this operation independent of input size when `scale_factor` is kept the same. This only has an effect when `mode` is `'bilinear'`. Default: `False`\nConstraints:\ndtype:bool\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.gt(input,other,out=None)\nParameter description: out(Tensor, optional) - the output tensor that must be a BoolTensor\nConstraints:\ndtype:booltensor\nstructure:tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.empty_like(input,dtype=None,layout=None,device=None,requires_grad=False,memory_format=torch.preserve_format)\nParameter description: dtype(`torch.dtype`, optional) - the desired data type of returned Tensor. Default: if `None`, defaults to the dtype of `input`.\nConstraints:\ndtype:torch.dtype\nstructure:Null\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.nn.RNN(*args,**kwargs,input_size=None,hidden_size=None,num_layers=None,nonlinearity=None,bias=None,batch_first=None,dropout=None,bidirectional=None)\nParameter description: hidden_size - The number of features in the hidden state h\nConstraints:\ndtype:int\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:[0,inf)\n\n###\n\nSignature: torch.nn.functional.avg_pool1d(input,kernel_size,stride=None,padding=0,ceil_mode=False,count_include_pad=True)\nParameter description: padding - implicit zero paddings on both sides of the input. Can be a single number or a tuple (padW,). Default: 0\nConstraints:\ndtype:bool;int\nstructure:tuple\nshape:Null\nndim:0;1\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.autograd.grad(outputs,inputs,grad_outputs=None,retain_graph=None,create_graph=False,only_inputs=True,allow_unused=False)\nParameter description: outputs(sequence of Tensor) - outputs of the differentiated function.\nConstraints:\ndtype:Null\nstructure:sequence;tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.repeat_interleave(input,repeats,dim=None)\nParameter description: dim(int, optional) - The dimension along which to repeat values.By default, use the flattened input array, and return a flat outputarray.\nConstraints:\ndtype:int\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.nn.HingeEmbeddingLoss(margin=1.0,size_average=None,reduce=None,reduction=mean)\nParameter description: size_average(bool, optional) - Deprecated (see `reduction`). By default, the losses are averaged over each loss element in the batch. Note that for some losses, there are multiple elements per sample. If the field `size_average` is set to `False`, the losses are instead summed for each minibatch. Ignored when reduce is `False`. Default: `True`\nConstraints:",
            "constr": {
                "dtype": [
                    "bool"
                ],
                "structure": [],
                "shape": [],
                "ndim": [
                    "0"
                ],
                "enum": [],
                "range": []
            },
            "signature": "torch.nn.HingeEmbeddingLoss(margin=1.0,size_average=None,reduce=None,reduction=mean)",
            "descp": "size_average(bool, optional) - Deprecated (see `reduction`). By default, the losses are averaged over each loss element in the batch. Note that for some losses, there are multiple elements per sample. If the field `size_average` is set to `False`, the losses are instead summed for each minibatch. Ignored when reduce is `False`. Default: `True`"
        },
        {
            "fname": "torch.var_mean2.yaml",
            "arg": "keepdim",
            "prompt": "Signature: torch.distributed.recv(tensor,src=None,group=<objectobject>,tag=0)\nParameter description: tensor(Tensor) - Tensor to fill with received data.\nConstraints:\ndtype:Null\nstructure:tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.div(input,other,out=None)\nParameter description: out(Tensor, optional) - the output tensor.\nConstraints:\ndtype:Null\nstructure:tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.nn.TransformerDecoder(decoder_layer,num_layers,norm=None)\nParameter description: num_layers - the number of sub-decoder-layers in the decoder (required).\nConstraints:\ndtype:int\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:[0,inf)\n\n###\n\nSignature: torch.nn.quantized.functional.interpolate(input,size=None,scale_factor=None,mode=nearest,align_corners=None)\nParameter description: align_corners(bool, optional) - Geometrically, we consider the pixels of the input and output as squares rather than points. If set to `True`, the input and output tensors are aligned by the center points of their corner pixels, preserving the values at the corner pixels. If set to `False`, the input and output tensors are aligned by the corner points of their corner pixels, and the interpolation uses edge value padding for out-of-boundary values, making this operation independent of input size when `scale_factor` is kept the same. This only has an effect when `mode` is `'bilinear'`. Default: `False`\nConstraints:\ndtype:bool\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.gt(input,other,out=None)\nParameter description: out(Tensor, optional) - the output tensor that must be a BoolTensor\nConstraints:\ndtype:booltensor\nstructure:tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.empty_like(input,dtype=None,layout=None,device=None,requires_grad=False,memory_format=torch.preserve_format)\nParameter description: dtype(`torch.dtype`, optional) - the desired data type of returned Tensor. Default: if `None`, defaults to the dtype of `input`.\nConstraints:\ndtype:torch.dtype\nstructure:Null\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.nn.RNN(*args,**kwargs,input_size=None,hidden_size=None,num_layers=None,nonlinearity=None,bias=None,batch_first=None,dropout=None,bidirectional=None)\nParameter description: hidden_size - The number of features in the hidden state h\nConstraints:\ndtype:int\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:[0,inf)\n\n###\n\nSignature: torch.nn.functional.avg_pool1d(input,kernel_size,stride=None,padding=0,ceil_mode=False,count_include_pad=True)\nParameter description: padding - implicit zero paddings on both sides of the input. Can be a single number or a tuple (padW,). Default: 0\nConstraints:\ndtype:bool;int\nstructure:tuple\nshape:Null\nndim:0;1\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.autograd.grad(outputs,inputs,grad_outputs=None,retain_graph=None,create_graph=False,only_inputs=True,allow_unused=False)\nParameter description: outputs(sequence of Tensor) - outputs of the differentiated function.\nConstraints:\ndtype:Null\nstructure:sequence;tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.repeat_interleave(input,repeats,dim=None)\nParameter description: dim(int, optional) - The dimension along which to repeat values.By default, use the flattened input array, and return a flat outputarray.\nConstraints:\ndtype:int\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.var_mean(input,dim,keepdim=False,unbiased=True)\nParameter description: keepdim(bool) - whether the output tensor has `dim` retained or not.\nConstraints:",
            "constr": {
                "dtype": [
                    "bool"
                ],
                "structure": [],
                "shape": [],
                "ndim": [
                    "0"
                ],
                "enum": [],
                "range": []
            },
            "signature": "torch.var_mean(input,dim,keepdim=False,unbiased=True)",
            "descp": "keepdim(bool) - whether the output tensor has `dim` retained or not."
        },
        {
            "fname": "torch.nn.batchnorm3d.yaml",
            "arg": "momentum",
            "prompt": "Signature: torch.distributed.recv(tensor,src=None,group=<objectobject>,tag=0)\nParameter description: tensor(Tensor) - Tensor to fill with received data.\nConstraints:\ndtype:Null\nstructure:tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.div(input,other,out=None)\nParameter description: out(Tensor, optional) - the output tensor.\nConstraints:\ndtype:Null\nstructure:tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.nn.TransformerDecoder(decoder_layer,num_layers,norm=None)\nParameter description: num_layers - the number of sub-decoder-layers in the decoder (required).\nConstraints:\ndtype:int\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:[0,inf)\n\n###\n\nSignature: torch.nn.quantized.functional.interpolate(input,size=None,scale_factor=None,mode=nearest,align_corners=None)\nParameter description: align_corners(bool, optional) - Geometrically, we consider the pixels of the input and output as squares rather than points. If set to `True`, the input and output tensors are aligned by the center points of their corner pixels, preserving the values at the corner pixels. If set to `False`, the input and output tensors are aligned by the corner points of their corner pixels, and the interpolation uses edge value padding for out-of-boundary values, making this operation independent of input size when `scale_factor` is kept the same. This only has an effect when `mode` is `'bilinear'`. Default: `False`\nConstraints:\ndtype:bool\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.gt(input,other,out=None)\nParameter description: out(Tensor, optional) - the output tensor that must be a BoolTensor\nConstraints:\ndtype:booltensor\nstructure:tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.empty_like(input,dtype=None,layout=None,device=None,requires_grad=False,memory_format=torch.preserve_format)\nParameter description: dtype(`torch.dtype`, optional) - the desired data type of returned Tensor. Default: if `None`, defaults to the dtype of `input`.\nConstraints:\ndtype:torch.dtype\nstructure:Null\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.nn.RNN(*args,**kwargs,input_size=None,hidden_size=None,num_layers=None,nonlinearity=None,bias=None,batch_first=None,dropout=None,bidirectional=None)\nParameter description: hidden_size - The number of features in the hidden state h\nConstraints:\ndtype:int\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:[0,inf)\n\n###\n\nSignature: torch.nn.functional.avg_pool1d(input,kernel_size,stride=None,padding=0,ceil_mode=False,count_include_pad=True)\nParameter description: padding - implicit zero paddings on both sides of the input. Can be a single number or a tuple (padW,). Default: 0\nConstraints:\ndtype:bool;int\nstructure:tuple\nshape:Null\nndim:0;1\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.autograd.grad(outputs,inputs,grad_outputs=None,retain_graph=None,create_graph=False,only_inputs=True,allow_unused=False)\nParameter description: outputs(sequence of Tensor) - outputs of the differentiated function.\nConstraints:\ndtype:Null\nstructure:sequence;tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.repeat_interleave(input,repeats,dim=None)\nParameter description: dim(int, optional) - The dimension along which to repeat values.By default, use the flattened input array, and return a flat outputarray.\nConstraints:\ndtype:int\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.nn.BatchNorm3d(num_features,eps=1e-05,momentum=0.1,affine=True,track_running_stats=True)\nParameter description: momentum - the value used for the running_mean and running_var computation. Can be set to `None` for cumulative moving average (i.e. simple average). Default: 0.1\nConstraints:",
            "constr": {
                "dtype": [
                    "bool",
                    "float"
                ],
                "structure": [],
                "shape": [],
                "ndim": [
                    "0"
                ],
                "enum": [],
                "range": []
            },
            "signature": "torch.nn.BatchNorm3d(num_features,eps=1e-05,momentum=0.1,affine=True,track_running_stats=True)",
            "descp": "momentum - the value used for the running_mean and running_var computation. Can be set to `None` for cumulative moving average (i.e. simple average). Default: 0.1"
        },
        {
            "fname": "torch.nn.convtranspose3d.yaml",
            "arg": "padding",
            "prompt": "Signature: torch.distributed.recv(tensor,src=None,group=<objectobject>,tag=0)\nParameter description: tensor(Tensor) - Tensor to fill with received data.\nConstraints:\ndtype:Null\nstructure:tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.div(input,other,out=None)\nParameter description: out(Tensor, optional) - the output tensor.\nConstraints:\ndtype:Null\nstructure:tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.nn.TransformerDecoder(decoder_layer,num_layers,norm=None)\nParameter description: num_layers - the number of sub-decoder-layers in the decoder (required).\nConstraints:\ndtype:int\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:[0,inf)\n\n###\n\nSignature: torch.nn.quantized.functional.interpolate(input,size=None,scale_factor=None,mode=nearest,align_corners=None)\nParameter description: align_corners(bool, optional) - Geometrically, we consider the pixels of the input and output as squares rather than points. If set to `True`, the input and output tensors are aligned by the center points of their corner pixels, preserving the values at the corner pixels. If set to `False`, the input and output tensors are aligned by the corner points of their corner pixels, and the interpolation uses edge value padding for out-of-boundary values, making this operation independent of input size when `scale_factor` is kept the same. This only has an effect when `mode` is `'bilinear'`. Default: `False`\nConstraints:\ndtype:bool\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.gt(input,other,out=None)\nParameter description: out(Tensor, optional) - the output tensor that must be a BoolTensor\nConstraints:\ndtype:booltensor\nstructure:tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.empty_like(input,dtype=None,layout=None,device=None,requires_grad=False,memory_format=torch.preserve_format)\nParameter description: dtype(`torch.dtype`, optional) - the desired data type of returned Tensor. Default: if `None`, defaults to the dtype of `input`.\nConstraints:\ndtype:torch.dtype\nstructure:Null\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.nn.RNN(*args,**kwargs,input_size=None,hidden_size=None,num_layers=None,nonlinearity=None,bias=None,batch_first=None,dropout=None,bidirectional=None)\nParameter description: hidden_size - The number of features in the hidden state h\nConstraints:\ndtype:int\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:[0,inf)\n\n###\n\nSignature: torch.nn.functional.avg_pool1d(input,kernel_size,stride=None,padding=0,ceil_mode=False,count_include_pad=True)\nParameter description: padding - implicit zero paddings on both sides of the input. Can be a single number or a tuple (padW,). Default: 0\nConstraints:\ndtype:bool;int\nstructure:tuple\nshape:Null\nndim:0;1\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.autograd.grad(outputs,inputs,grad_outputs=None,retain_graph=None,create_graph=False,only_inputs=True,allow_unused=False)\nParameter description: outputs(sequence of Tensor) - outputs of the differentiated function.\nConstraints:\ndtype:Null\nstructure:sequence;tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.repeat_interleave(input,repeats,dim=None)\nParameter description: dim(int, optional) - The dimension along which to repeat values.By default, use the flattened input array, and return a flat outputarray.\nConstraints:\ndtype:int\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.nn.ConvTranspose3d(in_channels,out_channels,kernel_size,stride=1,padding=0,output_padding=0,groups=1,bias=True,dilation=1,padding_mode=zeros)\nParameter description: padding(int or tuple, optional) - `dilation * (kernel_size - 1) - padding` zero-padding will be added to both sides of each dimension in the input. Default: 0\nConstraints:",
            "constr": {
                "dtype": [
                    "int"
                ],
                "structure": [
                    "tuple"
                ],
                "shape": [],
                "ndim": [
                    "0",
                    "1"
                ],
                "enum": [],
                "range": []
            },
            "signature": "torch.nn.ConvTranspose3d(in_channels,out_channels,kernel_size,stride=1,padding=0,output_padding=0,groups=1,bias=True,dilation=1,padding_mode=zeros)",
            "descp": "padding(int or tuple, optional) - `dilation * (kernel_size - 1) - padding` zero-padding will be added to both sides of each dimension in the input. Default: 0"
        },
        {
            "fname": "torch.linspace.yaml",
            "arg": "dtype",
            "prompt": "Signature: torch.distributed.recv(tensor,src=None,group=<objectobject>,tag=0)\nParameter description: tensor(Tensor) - Tensor to fill with received data.\nConstraints:\ndtype:Null\nstructure:tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.div(input,other,out=None)\nParameter description: out(Tensor, optional) - the output tensor.\nConstraints:\ndtype:Null\nstructure:tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.nn.TransformerDecoder(decoder_layer,num_layers,norm=None)\nParameter description: num_layers - the number of sub-decoder-layers in the decoder (required).\nConstraints:\ndtype:int\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:[0,inf)\n\n###\n\nSignature: torch.nn.quantized.functional.interpolate(input,size=None,scale_factor=None,mode=nearest,align_corners=None)\nParameter description: align_corners(bool, optional) - Geometrically, we consider the pixels of the input and output as squares rather than points. If set to `True`, the input and output tensors are aligned by the center points of their corner pixels, preserving the values at the corner pixels. If set to `False`, the input and output tensors are aligned by the corner points of their corner pixels, and the interpolation uses edge value padding for out-of-boundary values, making this operation independent of input size when `scale_factor` is kept the same. This only has an effect when `mode` is `'bilinear'`. Default: `False`\nConstraints:\ndtype:bool\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.gt(input,other,out=None)\nParameter description: out(Tensor, optional) - the output tensor that must be a BoolTensor\nConstraints:\ndtype:booltensor\nstructure:tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.empty_like(input,dtype=None,layout=None,device=None,requires_grad=False,memory_format=torch.preserve_format)\nParameter description: dtype(`torch.dtype`, optional) - the desired data type of returned Tensor. Default: if `None`, defaults to the dtype of `input`.\nConstraints:\ndtype:torch.dtype\nstructure:Null\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.nn.RNN(*args,**kwargs,input_size=None,hidden_size=None,num_layers=None,nonlinearity=None,bias=None,batch_first=None,dropout=None,bidirectional=None)\nParameter description: hidden_size - The number of features in the hidden state h\nConstraints:\ndtype:int\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:[0,inf)\n\n###\n\nSignature: torch.nn.functional.avg_pool1d(input,kernel_size,stride=None,padding=0,ceil_mode=False,count_include_pad=True)\nParameter description: padding - implicit zero paddings on both sides of the input. Can be a single number or a tuple (padW,). Default: 0\nConstraints:\ndtype:bool;int\nstructure:tuple\nshape:Null\nndim:0;1\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.autograd.grad(outputs,inputs,grad_outputs=None,retain_graph=None,create_graph=False,only_inputs=True,allow_unused=False)\nParameter description: outputs(sequence of Tensor) - outputs of the differentiated function.\nConstraints:\ndtype:Null\nstructure:sequence;tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.repeat_interleave(input,repeats,dim=None)\nParameter description: dim(int, optional) - The dimension along which to repeat values.By default, use the flattened input array, and return a flat outputarray.\nConstraints:\ndtype:int\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.linspace(start,end,steps=100,out=None,dtype=None,layout=torch.strided,device=None,requires_grad=False)\nParameter description: dtype(`torch.dtype`, optional) - the desired data type of returned tensor. Default: if `None`, uses a global default (see `torch.set_default_tensor_type()`).\nConstraints:",
            "constr": {
                "dtype": [
                    "torch.dtype"
                ],
                "structure": [],
                "shape": [],
                "ndim": [],
                "enum": [],
                "range": []
            },
            "signature": "torch.linspace(start,end,steps=100,out=None,dtype=None,layout=torch.strided,device=None,requires_grad=False)",
            "descp": "dtype(`torch.dtype`, optional) - the desired data type of returned tensor. Default: if `None`, uses a global default (see `torch.set_default_tensor_type()`)."
        },
        {
            "fname": "torch.nn.convtranspose2d.yaml",
            "arg": "output_padding",
            "prompt": "Signature: torch.distributed.recv(tensor,src=None,group=<objectobject>,tag=0)\nParameter description: tensor(Tensor) - Tensor to fill with received data.\nConstraints:\ndtype:Null\nstructure:tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.div(input,other,out=None)\nParameter description: out(Tensor, optional) - the output tensor.\nConstraints:\ndtype:Null\nstructure:tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.nn.TransformerDecoder(decoder_layer,num_layers,norm=None)\nParameter description: num_layers - the number of sub-decoder-layers in the decoder (required).\nConstraints:\ndtype:int\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:[0,inf)\n\n###\n\nSignature: torch.nn.quantized.functional.interpolate(input,size=None,scale_factor=None,mode=nearest,align_corners=None)\nParameter description: align_corners(bool, optional) - Geometrically, we consider the pixels of the input and output as squares rather than points. If set to `True`, the input and output tensors are aligned by the center points of their corner pixels, preserving the values at the corner pixels. If set to `False`, the input and output tensors are aligned by the corner points of their corner pixels, and the interpolation uses edge value padding for out-of-boundary values, making this operation independent of input size when `scale_factor` is kept the same. This only has an effect when `mode` is `'bilinear'`. Default: `False`\nConstraints:\ndtype:bool\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.gt(input,other,out=None)\nParameter description: out(Tensor, optional) - the output tensor that must be a BoolTensor\nConstraints:\ndtype:booltensor\nstructure:tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.empty_like(input,dtype=None,layout=None,device=None,requires_grad=False,memory_format=torch.preserve_format)\nParameter description: dtype(`torch.dtype`, optional) - the desired data type of returned Tensor. Default: if `None`, defaults to the dtype of `input`.\nConstraints:\ndtype:torch.dtype\nstructure:Null\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.nn.RNN(*args,**kwargs,input_size=None,hidden_size=None,num_layers=None,nonlinearity=None,bias=None,batch_first=None,dropout=None,bidirectional=None)\nParameter description: hidden_size - The number of features in the hidden state h\nConstraints:\ndtype:int\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:[0,inf)\n\n###\n\nSignature: torch.nn.functional.avg_pool1d(input,kernel_size,stride=None,padding=0,ceil_mode=False,count_include_pad=True)\nParameter description: padding - implicit zero paddings on both sides of the input. Can be a single number or a tuple (padW,). Default: 0\nConstraints:\ndtype:bool;int\nstructure:tuple\nshape:Null\nndim:0;1\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.autograd.grad(outputs,inputs,grad_outputs=None,retain_graph=None,create_graph=False,only_inputs=True,allow_unused=False)\nParameter description: outputs(sequence of Tensor) - outputs of the differentiated function.\nConstraints:\ndtype:Null\nstructure:sequence;tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.repeat_interleave(input,repeats,dim=None)\nParameter description: dim(int, optional) - The dimension along which to repeat values.By default, use the flattened input array, and return a flat outputarray.\nConstraints:\ndtype:int\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.nn.ConvTranspose2d(in_channels,out_channels,kernel_size,stride=1,padding=0,output_padding=0,groups=1,bias=True,dilation=1,padding_mode=zeros)\nParameter description: output_padding(int or tuple, optional) - Additional size added to one side of each dimension in the output shape. Default: 0\nConstraints:",
            "constr": {
                "dtype": [
                    "int"
                ],
                "structure": [
                    "tuple"
                ],
                "shape": [],
                "ndim": [
                    "0",
                    "1"
                ],
                "enum": [],
                "range": [
                    "[0,inf)"
                ]
            },
            "signature": "torch.nn.ConvTranspose2d(in_channels,out_channels,kernel_size,stride=1,padding=0,output_padding=0,groups=1,bias=True,dilation=1,padding_mode=zeros)",
            "descp": "output_padding(int or tuple, optional) - Additional size added to one side of each dimension in the output shape. Default: 0"
        },
        {
            "fname": "torch.autograd.gradcheck.yaml",
            "arg": "rtol",
            "prompt": "Signature: torch.distributed.recv(tensor,src=None,group=<objectobject>,tag=0)\nParameter description: tensor(Tensor) - Tensor to fill with received data.\nConstraints:\ndtype:Null\nstructure:tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.div(input,other,out=None)\nParameter description: out(Tensor, optional) - the output tensor.\nConstraints:\ndtype:Null\nstructure:tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.nn.TransformerDecoder(decoder_layer,num_layers,norm=None)\nParameter description: num_layers - the number of sub-decoder-layers in the decoder (required).\nConstraints:\ndtype:int\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:[0,inf)\n\n###\n\nSignature: torch.nn.quantized.functional.interpolate(input,size=None,scale_factor=None,mode=nearest,align_corners=None)\nParameter description: align_corners(bool, optional) - Geometrically, we consider the pixels of the input and output as squares rather than points. If set to `True`, the input and output tensors are aligned by the center points of their corner pixels, preserving the values at the corner pixels. If set to `False`, the input and output tensors are aligned by the corner points of their corner pixels, and the interpolation uses edge value padding for out-of-boundary values, making this operation independent of input size when `scale_factor` is kept the same. This only has an effect when `mode` is `'bilinear'`. Default: `False`\nConstraints:\ndtype:bool\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.gt(input,other,out=None)\nParameter description: out(Tensor, optional) - the output tensor that must be a BoolTensor\nConstraints:\ndtype:booltensor\nstructure:tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.empty_like(input,dtype=None,layout=None,device=None,requires_grad=False,memory_format=torch.preserve_format)\nParameter description: dtype(`torch.dtype`, optional) - the desired data type of returned Tensor. Default: if `None`, defaults to the dtype of `input`.\nConstraints:\ndtype:torch.dtype\nstructure:Null\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.nn.RNN(*args,**kwargs,input_size=None,hidden_size=None,num_layers=None,nonlinearity=None,bias=None,batch_first=None,dropout=None,bidirectional=None)\nParameter description: hidden_size - The number of features in the hidden state h\nConstraints:\ndtype:int\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:[0,inf)\n\n###\n\nSignature: torch.nn.functional.avg_pool1d(input,kernel_size,stride=None,padding=0,ceil_mode=False,count_include_pad=True)\nParameter description: padding - implicit zero paddings on both sides of the input. Can be a single number or a tuple (padW,). Default: 0\nConstraints:\ndtype:bool;int\nstructure:tuple\nshape:Null\nndim:0;1\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.autograd.grad(outputs,inputs,grad_outputs=None,retain_graph=None,create_graph=False,only_inputs=True,allow_unused=False)\nParameter description: outputs(sequence of Tensor) - outputs of the differentiated function.\nConstraints:\ndtype:Null\nstructure:sequence;tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.repeat_interleave(input,repeats,dim=None)\nParameter description: dim(int, optional) - The dimension along which to repeat values.By default, use the flattened input array, and return a flat outputarray.\nConstraints:\ndtype:int\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.autograd.gradcheck(func,inputs,eps=1e-06,atol=1e-05,rtol=0.001,raise_exception=True,check_sparse_nnz=False,nondet_tol=0.0)\nParameter description: rtol(float, optional) - relative tolerance\nConstraints:",
            "constr": {
                "dtype": [
                    "float"
                ],
                "structure": [],
                "shape": [],
                "ndim": [
                    "0"
                ],
                "enum": [],
                "range": []
            },
            "signature": "torch.autograd.gradcheck(func,inputs,eps=1e-06,atol=1e-05,rtol=0.001,raise_exception=True,check_sparse_nnz=False,nondet_tol=0.0)",
            "descp": "rtol(float, optional) - relative tolerance"
        },
        {
            "fname": "torch.quantization.convert.yaml",
            "arg": "inplace",
            "prompt": "Signature: torch.distributed.recv(tensor,src=None,group=<objectobject>,tag=0)\nParameter description: tensor(Tensor) - Tensor to fill with received data.\nConstraints:\ndtype:Null\nstructure:tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.div(input,other,out=None)\nParameter description: out(Tensor, optional) - the output tensor.\nConstraints:\ndtype:Null\nstructure:tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.nn.TransformerDecoder(decoder_layer,num_layers,norm=None)\nParameter description: num_layers - the number of sub-decoder-layers in the decoder (required).\nConstraints:\ndtype:int\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:[0,inf)\n\n###\n\nSignature: torch.nn.quantized.functional.interpolate(input,size=None,scale_factor=None,mode=nearest,align_corners=None)\nParameter description: align_corners(bool, optional) - Geometrically, we consider the pixels of the input and output as squares rather than points. If set to `True`, the input and output tensors are aligned by the center points of their corner pixels, preserving the values at the corner pixels. If set to `False`, the input and output tensors are aligned by the corner points of their corner pixels, and the interpolation uses edge value padding for out-of-boundary values, making this operation independent of input size when `scale_factor` is kept the same. This only has an effect when `mode` is `'bilinear'`. Default: `False`\nConstraints:\ndtype:bool\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.gt(input,other,out=None)\nParameter description: out(Tensor, optional) - the output tensor that must be a BoolTensor\nConstraints:\ndtype:booltensor\nstructure:tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.empty_like(input,dtype=None,layout=None,device=None,requires_grad=False,memory_format=torch.preserve_format)\nParameter description: dtype(`torch.dtype`, optional) - the desired data type of returned Tensor. Default: if `None`, defaults to the dtype of `input`.\nConstraints:\ndtype:torch.dtype\nstructure:Null\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.nn.RNN(*args,**kwargs,input_size=None,hidden_size=None,num_layers=None,nonlinearity=None,bias=None,batch_first=None,dropout=None,bidirectional=None)\nParameter description: hidden_size - The number of features in the hidden state h\nConstraints:\ndtype:int\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:[0,inf)\n\n###\n\nSignature: torch.nn.functional.avg_pool1d(input,kernel_size,stride=None,padding=0,ceil_mode=False,count_include_pad=True)\nParameter description: padding - implicit zero paddings on both sides of the input. Can be a single number or a tuple (padW,). Default: 0\nConstraints:\ndtype:bool;int\nstructure:tuple\nshape:Null\nndim:0;1\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.autograd.grad(outputs,inputs,grad_outputs=None,retain_graph=None,create_graph=False,only_inputs=True,allow_unused=False)\nParameter description: outputs(sequence of Tensor) - outputs of the differentiated function.\nConstraints:\ndtype:Null\nstructure:sequence;tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.repeat_interleave(input,repeats,dim=None)\nParameter description: dim(int, optional) - The dimension along which to repeat values.By default, use the flattened input array, and return a flat outputarray.\nConstraints:\ndtype:int\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.quantization.convert(module,mapping=None,inplace=False)\nParameter description: inplace - carry out model transformations in-place, the original module is mutated\nConstraints:",
            "constr": {
                "dtype": [
                    "bool"
                ],
                "structure": [],
                "shape": [],
                "ndim": [
                    "0"
                ],
                "enum": [],
                "range": []
            },
            "signature": "torch.quantization.convert(module,mapping=None,inplace=False)",
            "descp": "inplace - carry out model transformations in-place, the original module is mutated"
        },
        {
            "fname": "torch.clamp.yaml",
            "arg": "min",
            "prompt": "Signature: torch.distributed.recv(tensor,src=None,group=<objectobject>,tag=0)\nParameter description: tensor(Tensor) - Tensor to fill with received data.\nConstraints:\ndtype:Null\nstructure:tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.div(input,other,out=None)\nParameter description: out(Tensor, optional) - the output tensor.\nConstraints:\ndtype:Null\nstructure:tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.nn.TransformerDecoder(decoder_layer,num_layers,norm=None)\nParameter description: num_layers - the number of sub-decoder-layers in the decoder (required).\nConstraints:\ndtype:int\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:[0,inf)\n\n###\n\nSignature: torch.nn.quantized.functional.interpolate(input,size=None,scale_factor=None,mode=nearest,align_corners=None)\nParameter description: align_corners(bool, optional) - Geometrically, we consider the pixels of the input and output as squares rather than points. If set to `True`, the input and output tensors are aligned by the center points of their corner pixels, preserving the values at the corner pixels. If set to `False`, the input and output tensors are aligned by the corner points of their corner pixels, and the interpolation uses edge value padding for out-of-boundary values, making this operation independent of input size when `scale_factor` is kept the same. This only has an effect when `mode` is `'bilinear'`. Default: `False`\nConstraints:\ndtype:bool\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.gt(input,other,out=None)\nParameter description: out(Tensor, optional) - the output tensor that must be a BoolTensor\nConstraints:\ndtype:booltensor\nstructure:tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.empty_like(input,dtype=None,layout=None,device=None,requires_grad=False,memory_format=torch.preserve_format)\nParameter description: dtype(`torch.dtype`, optional) - the desired data type of returned Tensor. Default: if `None`, defaults to the dtype of `input`.\nConstraints:\ndtype:torch.dtype\nstructure:Null\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.nn.RNN(*args,**kwargs,input_size=None,hidden_size=None,num_layers=None,nonlinearity=None,bias=None,batch_first=None,dropout=None,bidirectional=None)\nParameter description: hidden_size - The number of features in the hidden state h\nConstraints:\ndtype:int\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:[0,inf)\n\n###\n\nSignature: torch.nn.functional.avg_pool1d(input,kernel_size,stride=None,padding=0,ceil_mode=False,count_include_pad=True)\nParameter description: padding - implicit zero paddings on both sides of the input. Can be a single number or a tuple (padW,). Default: 0\nConstraints:\ndtype:bool;int\nstructure:tuple\nshape:Null\nndim:0;1\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.autograd.grad(outputs,inputs,grad_outputs=None,retain_graph=None,create_graph=False,only_inputs=True,allow_unused=False)\nParameter description: outputs(sequence of Tensor) - outputs of the differentiated function.\nConstraints:\ndtype:Null\nstructure:sequence;tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.repeat_interleave(input,repeats,dim=None)\nParameter description: dim(int, optional) - The dimension along which to repeat values.By default, use the flattened input array, and return a flat outputarray.\nConstraints:\ndtype:int\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.clamp(input,min,max,out=None)\nParameter description: min(Number) - lower-bound of the range to be clamped to\nConstraints:",
            "constr": {
                "dtype": [
                    "numeric"
                ],
                "structure": [],
                "shape": [],
                "ndim": [],
                "enum": [],
                "range": []
            },
            "signature": "torch.clamp(input,min,max,out=None)",
            "descp": "min(Number) - lower-bound of the range to be clamped to"
        },
        {
            "fname": "torch.nn.functional.avg_pool2d.yaml",
            "arg": "input",
            "prompt": "Signature: torch.distributed.recv(tensor,src=None,group=<objectobject>,tag=0)\nParameter description: tensor(Tensor) - Tensor to fill with received data.\nConstraints:\ndtype:Null\nstructure:tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.div(input,other,out=None)\nParameter description: out(Tensor, optional) - the output tensor.\nConstraints:\ndtype:Null\nstructure:tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.nn.TransformerDecoder(decoder_layer,num_layers,norm=None)\nParameter description: num_layers - the number of sub-decoder-layers in the decoder (required).\nConstraints:\ndtype:int\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:[0,inf)\n\n###\n\nSignature: torch.nn.quantized.functional.interpolate(input,size=None,scale_factor=None,mode=nearest,align_corners=None)\nParameter description: align_corners(bool, optional) - Geometrically, we consider the pixels of the input and output as squares rather than points. If set to `True`, the input and output tensors are aligned by the center points of their corner pixels, preserving the values at the corner pixels. If set to `False`, the input and output tensors are aligned by the corner points of their corner pixels, and the interpolation uses edge value padding for out-of-boundary values, making this operation independent of input size when `scale_factor` is kept the same. This only has an effect when `mode` is `'bilinear'`. Default: `False`\nConstraints:\ndtype:bool\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.gt(input,other,out=None)\nParameter description: out(Tensor, optional) - the output tensor that must be a BoolTensor\nConstraints:\ndtype:booltensor\nstructure:tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.empty_like(input,dtype=None,layout=None,device=None,requires_grad=False,memory_format=torch.preserve_format)\nParameter description: dtype(`torch.dtype`, optional) - the desired data type of returned Tensor. Default: if `None`, defaults to the dtype of `input`.\nConstraints:\ndtype:torch.dtype\nstructure:Null\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.nn.RNN(*args,**kwargs,input_size=None,hidden_size=None,num_layers=None,nonlinearity=None,bias=None,batch_first=None,dropout=None,bidirectional=None)\nParameter description: hidden_size - The number of features in the hidden state h\nConstraints:\ndtype:int\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:[0,inf)\n\n###\n\nSignature: torch.nn.functional.avg_pool1d(input,kernel_size,stride=None,padding=0,ceil_mode=False,count_include_pad=True)\nParameter description: padding - implicit zero paddings on both sides of the input. Can be a single number or a tuple (padW,). Default: 0\nConstraints:\ndtype:bool;int\nstructure:tuple\nshape:Null\nndim:0;1\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.autograd.grad(outputs,inputs,grad_outputs=None,retain_graph=None,create_graph=False,only_inputs=True,allow_unused=False)\nParameter description: outputs(sequence of Tensor) - outputs of the differentiated function.\nConstraints:\ndtype:Null\nstructure:sequence;tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.repeat_interleave(input,repeats,dim=None)\nParameter description: dim(int, optional) - The dimension along which to repeat values.By default, use the flattened input array, and return a flat outputarray.\nConstraints:\ndtype:int\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.nn.functional.avg_pool2d(input,kernel_size,stride=None,padding=0,ceil_mode=False,count_include_pad=True,divisor_override=None)\nParameter description: input - input tensor (minibatch , in _channels , iH , iW) \nConstraints:",
            "constr": {
                "dtype": [],
                "structure": [
                    "tensor"
                ],
                "shape": [
                    "(minibatch , in _channels , iH , iW)"
                ],
                "ndim": [],
                "enum": [],
                "range": []
            },
            "signature": "torch.nn.functional.avg_pool2d(input,kernel_size,stride=None,padding=0,ceil_mode=False,count_include_pad=True,divisor_override=None)",
            "descp": "input - input tensor (minibatch , in _channels , iH , iW) "
        },
        {
            "fname": "torch.div.yaml",
            "arg": "other",
            "prompt": "Signature: torch.distributed.recv(tensor,src=None,group=<objectobject>,tag=0)\nParameter description: tensor(Tensor) - Tensor to fill with received data.\nConstraints:\ndtype:Null\nstructure:tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.div(input,other,out=None)\nParameter description: out(Tensor, optional) - the output tensor.\nConstraints:\ndtype:Null\nstructure:tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.nn.TransformerDecoder(decoder_layer,num_layers,norm=None)\nParameter description: num_layers - the number of sub-decoder-layers in the decoder (required).\nConstraints:\ndtype:int\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:[0,inf)\n\n###\n\nSignature: torch.nn.quantized.functional.interpolate(input,size=None,scale_factor=None,mode=nearest,align_corners=None)\nParameter description: align_corners(bool, optional) - Geometrically, we consider the pixels of the input and output as squares rather than points. If set to `True`, the input and output tensors are aligned by the center points of their corner pixels, preserving the values at the corner pixels. If set to `False`, the input and output tensors are aligned by the corner points of their corner pixels, and the interpolation uses edge value padding for out-of-boundary values, making this operation independent of input size when `scale_factor` is kept the same. This only has an effect when `mode` is `'bilinear'`. Default: `False`\nConstraints:\ndtype:bool\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.gt(input,other,out=None)\nParameter description: out(Tensor, optional) - the output tensor that must be a BoolTensor\nConstraints:\ndtype:booltensor\nstructure:tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.empty_like(input,dtype=None,layout=None,device=None,requires_grad=False,memory_format=torch.preserve_format)\nParameter description: dtype(`torch.dtype`, optional) - the desired data type of returned Tensor. Default: if `None`, defaults to the dtype of `input`.\nConstraints:\ndtype:torch.dtype\nstructure:Null\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.nn.RNN(*args,**kwargs,input_size=None,hidden_size=None,num_layers=None,nonlinearity=None,bias=None,batch_first=None,dropout=None,bidirectional=None)\nParameter description: hidden_size - The number of features in the hidden state h\nConstraints:\ndtype:int\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:[0,inf)\n\n###\n\nSignature: torch.nn.functional.avg_pool1d(input,kernel_size,stride=None,padding=0,ceil_mode=False,count_include_pad=True)\nParameter description: padding - implicit zero paddings on both sides of the input. Can be a single number or a tuple (padW,). Default: 0\nConstraints:\ndtype:bool;int\nstructure:tuple\nshape:Null\nndim:0;1\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.autograd.grad(outputs,inputs,grad_outputs=None,retain_graph=None,create_graph=False,only_inputs=True,allow_unused=False)\nParameter description: outputs(sequence of Tensor) - outputs of the differentiated function.\nConstraints:\ndtype:Null\nstructure:sequence;tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.repeat_interleave(input,repeats,dim=None)\nParameter description: dim(int, optional) - The dimension along which to repeat values.By default, use the flattened input array, and return a flat outputarray.\nConstraints:\ndtype:int\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.div(input,other,out=None)\nParameter description: other(Number) - the number to be divided to each element of `input`\nConstraints:",
            "constr": {
                "dtype": [
                    "numeric",
                    "int"
                ],
                "structure": [],
                "shape": [],
                "ndim": [
                    "0"
                ],
                "enum": [],
                "range": [
                    "[0,inf)"
                ]
            },
            "signature": "torch.div(input,other,out=None)",
            "descp": "other(Number) - the number to be divided to each element of `input`"
        },
        {
            "fname": "torch.nonzero.yaml",
            "arg": "input",
            "prompt": "Signature: torch.distributed.recv(tensor,src=None,group=<objectobject>,tag=0)\nParameter description: tensor(Tensor) - Tensor to fill with received data.\nConstraints:\ndtype:Null\nstructure:tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.div(input,other,out=None)\nParameter description: out(Tensor, optional) - the output tensor.\nConstraints:\ndtype:Null\nstructure:tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.nn.TransformerDecoder(decoder_layer,num_layers,norm=None)\nParameter description: num_layers - the number of sub-decoder-layers in the decoder (required).\nConstraints:\ndtype:int\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:[0,inf)\n\n###\n\nSignature: torch.nn.quantized.functional.interpolate(input,size=None,scale_factor=None,mode=nearest,align_corners=None)\nParameter description: align_corners(bool, optional) - Geometrically, we consider the pixels of the input and output as squares rather than points. If set to `True`, the input and output tensors are aligned by the center points of their corner pixels, preserving the values at the corner pixels. If set to `False`, the input and output tensors are aligned by the corner points of their corner pixels, and the interpolation uses edge value padding for out-of-boundary values, making this operation independent of input size when `scale_factor` is kept the same. This only has an effect when `mode` is `'bilinear'`. Default: `False`\nConstraints:\ndtype:bool\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.gt(input,other,out=None)\nParameter description: out(Tensor, optional) - the output tensor that must be a BoolTensor\nConstraints:\ndtype:booltensor\nstructure:tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.empty_like(input,dtype=None,layout=None,device=None,requires_grad=False,memory_format=torch.preserve_format)\nParameter description: dtype(`torch.dtype`, optional) - the desired data type of returned Tensor. Default: if `None`, defaults to the dtype of `input`.\nConstraints:\ndtype:torch.dtype\nstructure:Null\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.nn.RNN(*args,**kwargs,input_size=None,hidden_size=None,num_layers=None,nonlinearity=None,bias=None,batch_first=None,dropout=None,bidirectional=None)\nParameter description: hidden_size - The number of features in the hidden state h\nConstraints:\ndtype:int\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:[0,inf)\n\n###\n\nSignature: torch.nn.functional.avg_pool1d(input,kernel_size,stride=None,padding=0,ceil_mode=False,count_include_pad=True)\nParameter description: padding - implicit zero paddings on both sides of the input. Can be a single number or a tuple (padW,). Default: 0\nConstraints:\ndtype:bool;int\nstructure:tuple\nshape:Null\nndim:0;1\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.autograd.grad(outputs,inputs,grad_outputs=None,retain_graph=None,create_graph=False,only_inputs=True,allow_unused=False)\nParameter description: outputs(sequence of Tensor) - outputs of the differentiated function.\nConstraints:\ndtype:Null\nstructure:sequence;tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.repeat_interleave(input,repeats,dim=None)\nParameter description: dim(int, optional) - The dimension along which to repeat values.By default, use the flattened input array, and return a flat outputarray.\nConstraints:\ndtype:int\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.nonzero(input,out=None,as_tuple=False)\nParameter description: input(Tensor) - the input tensor.\nConstraints:",
            "constr": {
                "dtype": [],
                "structure": [
                    "tensor"
                ],
                "shape": [],
                "ndim": [],
                "enum": [],
                "range": []
            },
            "signature": "torch.nonzero(input,out=None,as_tuple=False)",
            "descp": "input(Tensor) - the input tensor."
        },
        {
            "fname": "torch.nn.functional.conv_transpose2d.yaml",
            "arg": "dilation",
            "prompt": "Signature: torch.distributed.recv(tensor,src=None,group=<objectobject>,tag=0)\nParameter description: tensor(Tensor) - Tensor to fill with received data.\nConstraints:\ndtype:Null\nstructure:tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.div(input,other,out=None)\nParameter description: out(Tensor, optional) - the output tensor.\nConstraints:\ndtype:Null\nstructure:tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.nn.TransformerDecoder(decoder_layer,num_layers,norm=None)\nParameter description: num_layers - the number of sub-decoder-layers in the decoder (required).\nConstraints:\ndtype:int\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:[0,inf)\n\n###\n\nSignature: torch.nn.quantized.functional.interpolate(input,size=None,scale_factor=None,mode=nearest,align_corners=None)\nParameter description: align_corners(bool, optional) - Geometrically, we consider the pixels of the input and output as squares rather than points. If set to `True`, the input and output tensors are aligned by the center points of their corner pixels, preserving the values at the corner pixels. If set to `False`, the input and output tensors are aligned by the corner points of their corner pixels, and the interpolation uses edge value padding for out-of-boundary values, making this operation independent of input size when `scale_factor` is kept the same. This only has an effect when `mode` is `'bilinear'`. Default: `False`\nConstraints:\ndtype:bool\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.gt(input,other,out=None)\nParameter description: out(Tensor, optional) - the output tensor that must be a BoolTensor\nConstraints:\ndtype:booltensor\nstructure:tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.empty_like(input,dtype=None,layout=None,device=None,requires_grad=False,memory_format=torch.preserve_format)\nParameter description: dtype(`torch.dtype`, optional) - the desired data type of returned Tensor. Default: if `None`, defaults to the dtype of `input`.\nConstraints:\ndtype:torch.dtype\nstructure:Null\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.nn.RNN(*args,**kwargs,input_size=None,hidden_size=None,num_layers=None,nonlinearity=None,bias=None,batch_first=None,dropout=None,bidirectional=None)\nParameter description: hidden_size - The number of features in the hidden state h\nConstraints:\ndtype:int\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:[0,inf)\n\n###\n\nSignature: torch.nn.functional.avg_pool1d(input,kernel_size,stride=None,padding=0,ceil_mode=False,count_include_pad=True)\nParameter description: padding - implicit zero paddings on both sides of the input. Can be a single number or a tuple (padW,). Default: 0\nConstraints:\ndtype:bool;int\nstructure:tuple\nshape:Null\nndim:0;1\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.autograd.grad(outputs,inputs,grad_outputs=None,retain_graph=None,create_graph=False,only_inputs=True,allow_unused=False)\nParameter description: outputs(sequence of Tensor) - outputs of the differentiated function.\nConstraints:\ndtype:Null\nstructure:sequence;tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.repeat_interleave(input,repeats,dim=None)\nParameter description: dim(int, optional) - The dimension along which to repeat values.By default, use the flattened input array, and return a flat outputarray.\nConstraints:\ndtype:int\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.nn.functional.conv_transpose2d(input,weight,bias=None,stride=1,padding=0,output_padding=0,groups=1,dilation=1)\nParameter description: dilation - the spacing between kernel elements. Can be a single number or a tuple `(dH, dW)`. Default: 1\nConstraints:",
            "constr": {
                "dtype": [
                    "bool",
                    "int"
                ],
                "structure": [
                    "tuple"
                ],
                "shape": [],
                "ndim": [
                    "0",
                    "1"
                ],
                "enum": [],
                "range": []
            },
            "signature": "torch.nn.functional.conv_transpose2d(input,weight,bias=None,stride=1,padding=0,output_padding=0,groups=1,dilation=1)",
            "descp": "dilation - the spacing between kernel elements. Can be a single number or a tuple `(dH, dW)`. Default: 1"
        },
        {
            "fname": "torch.eye.yaml",
            "arg": "dtype",
            "prompt": "Signature: torch.distributed.recv(tensor,src=None,group=<objectobject>,tag=0)\nParameter description: tensor(Tensor) - Tensor to fill with received data.\nConstraints:\ndtype:Null\nstructure:tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.div(input,other,out=None)\nParameter description: out(Tensor, optional) - the output tensor.\nConstraints:\ndtype:Null\nstructure:tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.nn.TransformerDecoder(decoder_layer,num_layers,norm=None)\nParameter description: num_layers - the number of sub-decoder-layers in the decoder (required).\nConstraints:\ndtype:int\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:[0,inf)\n\n###\n\nSignature: torch.nn.quantized.functional.interpolate(input,size=None,scale_factor=None,mode=nearest,align_corners=None)\nParameter description: align_corners(bool, optional) - Geometrically, we consider the pixels of the input and output as squares rather than points. If set to `True`, the input and output tensors are aligned by the center points of their corner pixels, preserving the values at the corner pixels. If set to `False`, the input and output tensors are aligned by the corner points of their corner pixels, and the interpolation uses edge value padding for out-of-boundary values, making this operation independent of input size when `scale_factor` is kept the same. This only has an effect when `mode` is `'bilinear'`. Default: `False`\nConstraints:\ndtype:bool\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.gt(input,other,out=None)\nParameter description: out(Tensor, optional) - the output tensor that must be a BoolTensor\nConstraints:\ndtype:booltensor\nstructure:tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.empty_like(input,dtype=None,layout=None,device=None,requires_grad=False,memory_format=torch.preserve_format)\nParameter description: dtype(`torch.dtype`, optional) - the desired data type of returned Tensor. Default: if `None`, defaults to the dtype of `input`.\nConstraints:\ndtype:torch.dtype\nstructure:Null\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.nn.RNN(*args,**kwargs,input_size=None,hidden_size=None,num_layers=None,nonlinearity=None,bias=None,batch_first=None,dropout=None,bidirectional=None)\nParameter description: hidden_size - The number of features in the hidden state h\nConstraints:\ndtype:int\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:[0,inf)\n\n###\n\nSignature: torch.nn.functional.avg_pool1d(input,kernel_size,stride=None,padding=0,ceil_mode=False,count_include_pad=True)\nParameter description: padding - implicit zero paddings on both sides of the input. Can be a single number or a tuple (padW,). Default: 0\nConstraints:\ndtype:bool;int\nstructure:tuple\nshape:Null\nndim:0;1\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.autograd.grad(outputs,inputs,grad_outputs=None,retain_graph=None,create_graph=False,only_inputs=True,allow_unused=False)\nParameter description: outputs(sequence of Tensor) - outputs of the differentiated function.\nConstraints:\ndtype:Null\nstructure:sequence;tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.repeat_interleave(input,repeats,dim=None)\nParameter description: dim(int, optional) - The dimension along which to repeat values.By default, use the flattened input array, and return a flat outputarray.\nConstraints:\ndtype:int\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.eye(n,m=None,out=None,dtype=None,layout=torch.strided,device=None,requires_grad=False)\nParameter description: dtype(`torch.dtype`, optional) - the desired data type of returned tensor. Default: if `None`, uses a global default (see `torch.set_default_tensor_type()`).\nConstraints:",
            "constr": {
                "dtype": [
                    "torch.dtype"
                ],
                "structure": [],
                "shape": [],
                "ndim": [],
                "enum": [],
                "range": []
            },
            "signature": "torch.eye(n,m=None,out=None,dtype=None,layout=torch.strided,device=None,requires_grad=False)",
            "descp": "dtype(`torch.dtype`, optional) - the desired data type of returned tensor. Default: if `None`, uses a global default (see `torch.set_default_tensor_type()`)."
        },
        {
            "fname": "torch.narrow.yaml",
            "arg": "start",
            "prompt": "Signature: torch.distributed.recv(tensor,src=None,group=<objectobject>,tag=0)\nParameter description: tensor(Tensor) - Tensor to fill with received data.\nConstraints:\ndtype:Null\nstructure:tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.div(input,other,out=None)\nParameter description: out(Tensor, optional) - the output tensor.\nConstraints:\ndtype:Null\nstructure:tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.nn.TransformerDecoder(decoder_layer,num_layers,norm=None)\nParameter description: num_layers - the number of sub-decoder-layers in the decoder (required).\nConstraints:\ndtype:int\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:[0,inf)\n\n###\n\nSignature: torch.nn.quantized.functional.interpolate(input,size=None,scale_factor=None,mode=nearest,align_corners=None)\nParameter description: align_corners(bool, optional) - Geometrically, we consider the pixels of the input and output as squares rather than points. If set to `True`, the input and output tensors are aligned by the center points of their corner pixels, preserving the values at the corner pixels. If set to `False`, the input and output tensors are aligned by the corner points of their corner pixels, and the interpolation uses edge value padding for out-of-boundary values, making this operation independent of input size when `scale_factor` is kept the same. This only has an effect when `mode` is `'bilinear'`. Default: `False`\nConstraints:\ndtype:bool\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.gt(input,other,out=None)\nParameter description: out(Tensor, optional) - the output tensor that must be a BoolTensor\nConstraints:\ndtype:booltensor\nstructure:tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.empty_like(input,dtype=None,layout=None,device=None,requires_grad=False,memory_format=torch.preserve_format)\nParameter description: dtype(`torch.dtype`, optional) - the desired data type of returned Tensor. Default: if `None`, defaults to the dtype of `input`.\nConstraints:\ndtype:torch.dtype\nstructure:Null\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.nn.RNN(*args,**kwargs,input_size=None,hidden_size=None,num_layers=None,nonlinearity=None,bias=None,batch_first=None,dropout=None,bidirectional=None)\nParameter description: hidden_size - The number of features in the hidden state h\nConstraints:\ndtype:int\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:[0,inf)\n\n###\n\nSignature: torch.nn.functional.avg_pool1d(input,kernel_size,stride=None,padding=0,ceil_mode=False,count_include_pad=True)\nParameter description: padding - implicit zero paddings on both sides of the input. Can be a single number or a tuple (padW,). Default: 0\nConstraints:\ndtype:bool;int\nstructure:tuple\nshape:Null\nndim:0;1\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.autograd.grad(outputs,inputs,grad_outputs=None,retain_graph=None,create_graph=False,only_inputs=True,allow_unused=False)\nParameter description: outputs(sequence of Tensor) - outputs of the differentiated function.\nConstraints:\ndtype:Null\nstructure:sequence;tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.repeat_interleave(input,repeats,dim=None)\nParameter description: dim(int, optional) - The dimension along which to repeat values.By default, use the flattened input array, and return a flat outputarray.\nConstraints:\ndtype:int\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.narrow(input,dim,start,length)\nParameter description: start(int) - the starting dimension\nConstraints:",
            "constr": {
                "dtype": [
                    "int"
                ],
                "structure": [],
                "shape": [],
                "ndim": [
                    "0"
                ],
                "enum": [],
                "range": []
            },
            "signature": "torch.narrow(input,dim,start,length)",
            "descp": "start(int) - the starting dimension"
        },
        {
            "fname": "torch.lobpcg.yaml",
            "arg": "n",
            "prompt": "Signature: torch.distributed.recv(tensor,src=None,group=<objectobject>,tag=0)\nParameter description: tensor(Tensor) - Tensor to fill with received data.\nConstraints:\ndtype:Null\nstructure:tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.div(input,other,out=None)\nParameter description: out(Tensor, optional) - the output tensor.\nConstraints:\ndtype:Null\nstructure:tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.nn.TransformerDecoder(decoder_layer,num_layers,norm=None)\nParameter description: num_layers - the number of sub-decoder-layers in the decoder (required).\nConstraints:\ndtype:int\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:[0,inf)\n\n###\n\nSignature: torch.nn.quantized.functional.interpolate(input,size=None,scale_factor=None,mode=nearest,align_corners=None)\nParameter description: align_corners(bool, optional) - Geometrically, we consider the pixels of the input and output as squares rather than points. If set to `True`, the input and output tensors are aligned by the center points of their corner pixels, preserving the values at the corner pixels. If set to `False`, the input and output tensors are aligned by the corner points of their corner pixels, and the interpolation uses edge value padding for out-of-boundary values, making this operation independent of input size when `scale_factor` is kept the same. This only has an effect when `mode` is `'bilinear'`. Default: `False`\nConstraints:\ndtype:bool\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.gt(input,other,out=None)\nParameter description: out(Tensor, optional) - the output tensor that must be a BoolTensor\nConstraints:\ndtype:booltensor\nstructure:tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.empty_like(input,dtype=None,layout=None,device=None,requires_grad=False,memory_format=torch.preserve_format)\nParameter description: dtype(`torch.dtype`, optional) - the desired data type of returned Tensor. Default: if `None`, defaults to the dtype of `input`.\nConstraints:\ndtype:torch.dtype\nstructure:Null\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.nn.RNN(*args,**kwargs,input_size=None,hidden_size=None,num_layers=None,nonlinearity=None,bias=None,batch_first=None,dropout=None,bidirectional=None)\nParameter description: hidden_size - The number of features in the hidden state h\nConstraints:\ndtype:int\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:[0,inf)\n\n###\n\nSignature: torch.nn.functional.avg_pool1d(input,kernel_size,stride=None,padding=0,ceil_mode=False,count_include_pad=True)\nParameter description: padding - implicit zero paddings on both sides of the input. Can be a single number or a tuple (padW,). Default: 0\nConstraints:\ndtype:bool;int\nstructure:tuple\nshape:Null\nndim:0;1\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.autograd.grad(outputs,inputs,grad_outputs=None,retain_graph=None,create_graph=False,only_inputs=True,allow_unused=False)\nParameter description: outputs(sequence of Tensor) - outputs of the differentiated function.\nConstraints:\ndtype:Null\nstructure:sequence;tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.repeat_interleave(input,repeats,dim=None)\nParameter description: dim(int, optional) - The dimension along which to repeat values.By default, use the flattened input array, and return a flat outputarray.\nConstraints:\ndtype:int\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.lobpcg(A,k=None,B=None,X=None,n=None,iK=None,niter=None,tol=None,largest=None,method=None,tracker=None,ortho_iparams=None,ortho_fparams=None,ortho_bparams=None)\nParameter description: n(integer, optional) - if X  is not specified then n specifies the size of the generated random approximation of eigenvectors. Default value for n is k. If X  is specifed, the value of n (when specified) must be the number of X  columns.\nConstraints:",
            "constr": {
                "dtype": [
                    "integer"
                ],
                "structure": [],
                "shape": [],
                "ndim": [],
                "enum": [],
                "range": []
            },
            "signature": "torch.lobpcg(A,k=None,B=None,X=None,n=None,iK=None,niter=None,tol=None,largest=None,method=None,tracker=None,ortho_iparams=None,ortho_fparams=None,ortho_bparams=None)",
            "descp": "n(integer, optional) - if X  is not specified then n specifies the size of the generated random approximation of eigenvectors. Default value for n is k. If X  is specifed, the value of n (when specified) must be the number of X  columns."
        },
        {
            "fname": "torch.chunk.yaml",
            "arg": "dim",
            "prompt": "Signature: torch.distributed.recv(tensor,src=None,group=<objectobject>,tag=0)\nParameter description: tensor(Tensor) - Tensor to fill with received data.\nConstraints:\ndtype:Null\nstructure:tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.div(input,other,out=None)\nParameter description: out(Tensor, optional) - the output tensor.\nConstraints:\ndtype:Null\nstructure:tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.nn.TransformerDecoder(decoder_layer,num_layers,norm=None)\nParameter description: num_layers - the number of sub-decoder-layers in the decoder (required).\nConstraints:\ndtype:int\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:[0,inf)\n\n###\n\nSignature: torch.nn.quantized.functional.interpolate(input,size=None,scale_factor=None,mode=nearest,align_corners=None)\nParameter description: align_corners(bool, optional) - Geometrically, we consider the pixels of the input and output as squares rather than points. If set to `True`, the input and output tensors are aligned by the center points of their corner pixels, preserving the values at the corner pixels. If set to `False`, the input and output tensors are aligned by the corner points of their corner pixels, and the interpolation uses edge value padding for out-of-boundary values, making this operation independent of input size when `scale_factor` is kept the same. This only has an effect when `mode` is `'bilinear'`. Default: `False`\nConstraints:\ndtype:bool\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.gt(input,other,out=None)\nParameter description: out(Tensor, optional) - the output tensor that must be a BoolTensor\nConstraints:\ndtype:booltensor\nstructure:tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.empty_like(input,dtype=None,layout=None,device=None,requires_grad=False,memory_format=torch.preserve_format)\nParameter description: dtype(`torch.dtype`, optional) - the desired data type of returned Tensor. Default: if `None`, defaults to the dtype of `input`.\nConstraints:\ndtype:torch.dtype\nstructure:Null\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.nn.RNN(*args,**kwargs,input_size=None,hidden_size=None,num_layers=None,nonlinearity=None,bias=None,batch_first=None,dropout=None,bidirectional=None)\nParameter description: hidden_size - The number of features in the hidden state h\nConstraints:\ndtype:int\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:[0,inf)\n\n###\n\nSignature: torch.nn.functional.avg_pool1d(input,kernel_size,stride=None,padding=0,ceil_mode=False,count_include_pad=True)\nParameter description: padding - implicit zero paddings on both sides of the input. Can be a single number or a tuple (padW,). Default: 0\nConstraints:\ndtype:bool;int\nstructure:tuple\nshape:Null\nndim:0;1\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.autograd.grad(outputs,inputs,grad_outputs=None,retain_graph=None,create_graph=False,only_inputs=True,allow_unused=False)\nParameter description: outputs(sequence of Tensor) - outputs of the differentiated function.\nConstraints:\ndtype:Null\nstructure:sequence;tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.repeat_interleave(input,repeats,dim=None)\nParameter description: dim(int, optional) - The dimension along which to repeat values.By default, use the flattened input array, and return a flat outputarray.\nConstraints:\ndtype:int\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.chunk(input,chunks,dim=0)\nParameter description: dim(int) - dimension along which to split the tensor\nConstraints:",
            "constr": {
                "dtype": [
                    "int"
                ],
                "structure": [],
                "shape": [],
                "ndim": [
                    "0"
                ],
                "enum": [],
                "range": []
            },
            "signature": "torch.chunk(input,chunks,dim=0)",
            "descp": "dim(int) - dimension along which to split the tensor"
        },
        {
            "fname": "torch.autograd.functional.hessian.yaml",
            "arg": "inputs",
            "prompt": "Signature: torch.distributed.recv(tensor,src=None,group=<objectobject>,tag=0)\nParameter description: tensor(Tensor) - Tensor to fill with received data.\nConstraints:\ndtype:Null\nstructure:tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.div(input,other,out=None)\nParameter description: out(Tensor, optional) - the output tensor.\nConstraints:\ndtype:Null\nstructure:tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.nn.TransformerDecoder(decoder_layer,num_layers,norm=None)\nParameter description: num_layers - the number of sub-decoder-layers in the decoder (required).\nConstraints:\ndtype:int\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:[0,inf)\n\n###\n\nSignature: torch.nn.quantized.functional.interpolate(input,size=None,scale_factor=None,mode=nearest,align_corners=None)\nParameter description: align_corners(bool, optional) - Geometrically, we consider the pixels of the input and output as squares rather than points. If set to `True`, the input and output tensors are aligned by the center points of their corner pixels, preserving the values at the corner pixels. If set to `False`, the input and output tensors are aligned by the corner points of their corner pixels, and the interpolation uses edge value padding for out-of-boundary values, making this operation independent of input size when `scale_factor` is kept the same. This only has an effect when `mode` is `'bilinear'`. Default: `False`\nConstraints:\ndtype:bool\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.gt(input,other,out=None)\nParameter description: out(Tensor, optional) - the output tensor that must be a BoolTensor\nConstraints:\ndtype:booltensor\nstructure:tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.empty_like(input,dtype=None,layout=None,device=None,requires_grad=False,memory_format=torch.preserve_format)\nParameter description: dtype(`torch.dtype`, optional) - the desired data type of returned Tensor. Default: if `None`, defaults to the dtype of `input`.\nConstraints:\ndtype:torch.dtype\nstructure:Null\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.nn.RNN(*args,**kwargs,input_size=None,hidden_size=None,num_layers=None,nonlinearity=None,bias=None,batch_first=None,dropout=None,bidirectional=None)\nParameter description: hidden_size - The number of features in the hidden state h\nConstraints:\ndtype:int\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:[0,inf)\n\n###\n\nSignature: torch.nn.functional.avg_pool1d(input,kernel_size,stride=None,padding=0,ceil_mode=False,count_include_pad=True)\nParameter description: padding - implicit zero paddings on both sides of the input. Can be a single number or a tuple (padW,). Default: 0\nConstraints:\ndtype:bool;int\nstructure:tuple\nshape:Null\nndim:0;1\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.autograd.grad(outputs,inputs,grad_outputs=None,retain_graph=None,create_graph=False,only_inputs=True,allow_unused=False)\nParameter description: outputs(sequence of Tensor) - outputs of the differentiated function.\nConstraints:\ndtype:Null\nstructure:sequence;tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.repeat_interleave(input,repeats,dim=None)\nParameter description: dim(int, optional) - The dimension along which to repeat values.By default, use the flattened input array, and return a flat outputarray.\nConstraints:\ndtype:int\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.autograd.functional.hessian(func,inputs,create_graph=False,strict=False)\nParameter description: inputs(tuple of Tensors or Tensor) - inputs to the function `func`.\nConstraints:",
            "constr": {
                "dtype": [],
                "structure": [
                    "tensor"
                ],
                "shape": [],
                "ndim": [],
                "enum": [],
                "range": []
            },
            "signature": "torch.autograd.functional.hessian(func,inputs,create_graph=False,strict=False)",
            "descp": "inputs(tuple of Tensors or Tensor) - inputs to the function `func`."
        },
        {
            "fname": "torch.norm.yaml",
            "arg": "input",
            "prompt": "Signature: torch.distributed.recv(tensor,src=None,group=<objectobject>,tag=0)\nParameter description: tensor(Tensor) - Tensor to fill with received data.\nConstraints:\ndtype:Null\nstructure:tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.div(input,other,out=None)\nParameter description: out(Tensor, optional) - the output tensor.\nConstraints:\ndtype:Null\nstructure:tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.nn.TransformerDecoder(decoder_layer,num_layers,norm=None)\nParameter description: num_layers - the number of sub-decoder-layers in the decoder (required).\nConstraints:\ndtype:int\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:[0,inf)\n\n###\n\nSignature: torch.nn.quantized.functional.interpolate(input,size=None,scale_factor=None,mode=nearest,align_corners=None)\nParameter description: align_corners(bool, optional) - Geometrically, we consider the pixels of the input and output as squares rather than points. If set to `True`, the input and output tensors are aligned by the center points of their corner pixels, preserving the values at the corner pixels. If set to `False`, the input and output tensors are aligned by the corner points of their corner pixels, and the interpolation uses edge value padding for out-of-boundary values, making this operation independent of input size when `scale_factor` is kept the same. This only has an effect when `mode` is `'bilinear'`. Default: `False`\nConstraints:\ndtype:bool\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.gt(input,other,out=None)\nParameter description: out(Tensor, optional) - the output tensor that must be a BoolTensor\nConstraints:\ndtype:booltensor\nstructure:tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.empty_like(input,dtype=None,layout=None,device=None,requires_grad=False,memory_format=torch.preserve_format)\nParameter description: dtype(`torch.dtype`, optional) - the desired data type of returned Tensor. Default: if `None`, defaults to the dtype of `input`.\nConstraints:\ndtype:torch.dtype\nstructure:Null\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.nn.RNN(*args,**kwargs,input_size=None,hidden_size=None,num_layers=None,nonlinearity=None,bias=None,batch_first=None,dropout=None,bidirectional=None)\nParameter description: hidden_size - The number of features in the hidden state h\nConstraints:\ndtype:int\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:[0,inf)\n\n###\n\nSignature: torch.nn.functional.avg_pool1d(input,kernel_size,stride=None,padding=0,ceil_mode=False,count_include_pad=True)\nParameter description: padding - implicit zero paddings on both sides of the input. Can be a single number or a tuple (padW,). Default: 0\nConstraints:\ndtype:bool;int\nstructure:tuple\nshape:Null\nndim:0;1\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.autograd.grad(outputs,inputs,grad_outputs=None,retain_graph=None,create_graph=False,only_inputs=True,allow_unused=False)\nParameter description: outputs(sequence of Tensor) - outputs of the differentiated function.\nConstraints:\ndtype:Null\nstructure:sequence;tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.repeat_interleave(input,repeats,dim=None)\nParameter description: dim(int, optional) - The dimension along which to repeat values.By default, use the flattened input array, and return a flat outputarray.\nConstraints:\ndtype:int\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.norm(input,p=fro,dim=None,keepdim=False,out=None,dtype=None)\nParameter description: input(Tensor) - the input tensor\nConstraints:",
            "constr": {
                "dtype": [],
                "structure": [
                    "tensor"
                ],
                "shape": [],
                "ndim": [],
                "enum": [],
                "range": []
            },
            "signature": "torch.norm(input,p=fro,dim=None,keepdim=False,out=None,dtype=None)",
            "descp": "input(Tensor) - the input tensor"
        },
        {
            "fname": "torch.cdist.yaml",
            "arg": "x1",
            "prompt": "Signature: torch.distributed.recv(tensor,src=None,group=<objectobject>,tag=0)\nParameter description: tensor(Tensor) - Tensor to fill with received data.\nConstraints:\ndtype:Null\nstructure:tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.div(input,other,out=None)\nParameter description: out(Tensor, optional) - the output tensor.\nConstraints:\ndtype:Null\nstructure:tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.nn.TransformerDecoder(decoder_layer,num_layers,norm=None)\nParameter description: num_layers - the number of sub-decoder-layers in the decoder (required).\nConstraints:\ndtype:int\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:[0,inf)\n\n###\n\nSignature: torch.nn.quantized.functional.interpolate(input,size=None,scale_factor=None,mode=nearest,align_corners=None)\nParameter description: align_corners(bool, optional) - Geometrically, we consider the pixels of the input and output as squares rather than points. If set to `True`, the input and output tensors are aligned by the center points of their corner pixels, preserving the values at the corner pixels. If set to `False`, the input and output tensors are aligned by the corner points of their corner pixels, and the interpolation uses edge value padding for out-of-boundary values, making this operation independent of input size when `scale_factor` is kept the same. This only has an effect when `mode` is `'bilinear'`. Default: `False`\nConstraints:\ndtype:bool\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.gt(input,other,out=None)\nParameter description: out(Tensor, optional) - the output tensor that must be a BoolTensor\nConstraints:\ndtype:booltensor\nstructure:tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.empty_like(input,dtype=None,layout=None,device=None,requires_grad=False,memory_format=torch.preserve_format)\nParameter description: dtype(`torch.dtype`, optional) - the desired data type of returned Tensor. Default: if `None`, defaults to the dtype of `input`.\nConstraints:\ndtype:torch.dtype\nstructure:Null\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.nn.RNN(*args,**kwargs,input_size=None,hidden_size=None,num_layers=None,nonlinearity=None,bias=None,batch_first=None,dropout=None,bidirectional=None)\nParameter description: hidden_size - The number of features in the hidden state h\nConstraints:\ndtype:int\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:[0,inf)\n\n###\n\nSignature: torch.nn.functional.avg_pool1d(input,kernel_size,stride=None,padding=0,ceil_mode=False,count_include_pad=True)\nParameter description: padding - implicit zero paddings on both sides of the input. Can be a single number or a tuple (padW,). Default: 0\nConstraints:\ndtype:bool;int\nstructure:tuple\nshape:Null\nndim:0;1\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.autograd.grad(outputs,inputs,grad_outputs=None,retain_graph=None,create_graph=False,only_inputs=True,allow_unused=False)\nParameter description: outputs(sequence of Tensor) - outputs of the differentiated function.\nConstraints:\ndtype:Null\nstructure:sequence;tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.repeat_interleave(input,repeats,dim=None)\nParameter description: dim(int, optional) - The dimension along which to repeat values.By default, use the flattened input array, and return a flat outputarray.\nConstraints:\ndtype:int\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.cdist(x1,x2,p=2.0,compute_mode=use_mm_for_euclid_dist_if_necessary)\nParameter description: x1(Tensor) - input tensor of shape B  times P  times M .\nConstraints:",
            "constr": {
                "dtype": [],
                "structure": [
                    "tensor"
                ],
                "shape": [
                    "[B,P,M]"
                ],
                "ndim": [],
                "enum": [],
                "range": []
            },
            "signature": "torch.cdist(x1,x2,p=2.0,compute_mode=use_mm_for_euclid_dist_if_necessary)",
            "descp": "x1(Tensor) - input tensor of shape B  times P  times M ."
        },
        {
            "fname": "torch.randn_like.yaml",
            "arg": "input",
            "prompt": "Signature: torch.distributed.recv(tensor,src=None,group=<objectobject>,tag=0)\nParameter description: tensor(Tensor) - Tensor to fill with received data.\nConstraints:\ndtype:Null\nstructure:tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.div(input,other,out=None)\nParameter description: out(Tensor, optional) - the output tensor.\nConstraints:\ndtype:Null\nstructure:tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.nn.TransformerDecoder(decoder_layer,num_layers,norm=None)\nParameter description: num_layers - the number of sub-decoder-layers in the decoder (required).\nConstraints:\ndtype:int\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:[0,inf)\n\n###\n\nSignature: torch.nn.quantized.functional.interpolate(input,size=None,scale_factor=None,mode=nearest,align_corners=None)\nParameter description: align_corners(bool, optional) - Geometrically, we consider the pixels of the input and output as squares rather than points. If set to `True`, the input and output tensors are aligned by the center points of their corner pixels, preserving the values at the corner pixels. If set to `False`, the input and output tensors are aligned by the corner points of their corner pixels, and the interpolation uses edge value padding for out-of-boundary values, making this operation independent of input size when `scale_factor` is kept the same. This only has an effect when `mode` is `'bilinear'`. Default: `False`\nConstraints:\ndtype:bool\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.gt(input,other,out=None)\nParameter description: out(Tensor, optional) - the output tensor that must be a BoolTensor\nConstraints:\ndtype:booltensor\nstructure:tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.empty_like(input,dtype=None,layout=None,device=None,requires_grad=False,memory_format=torch.preserve_format)\nParameter description: dtype(`torch.dtype`, optional) - the desired data type of returned Tensor. Default: if `None`, defaults to the dtype of `input`.\nConstraints:\ndtype:torch.dtype\nstructure:Null\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.nn.RNN(*args,**kwargs,input_size=None,hidden_size=None,num_layers=None,nonlinearity=None,bias=None,batch_first=None,dropout=None,bidirectional=None)\nParameter description: hidden_size - The number of features in the hidden state h\nConstraints:\ndtype:int\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:[0,inf)\n\n###\n\nSignature: torch.nn.functional.avg_pool1d(input,kernel_size,stride=None,padding=0,ceil_mode=False,count_include_pad=True)\nParameter description: padding - implicit zero paddings on both sides of the input. Can be a single number or a tuple (padW,). Default: 0\nConstraints:\ndtype:bool;int\nstructure:tuple\nshape:Null\nndim:0;1\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.autograd.grad(outputs,inputs,grad_outputs=None,retain_graph=None,create_graph=False,only_inputs=True,allow_unused=False)\nParameter description: outputs(sequence of Tensor) - outputs of the differentiated function.\nConstraints:\ndtype:Null\nstructure:sequence;tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.repeat_interleave(input,repeats,dim=None)\nParameter description: dim(int, optional) - The dimension along which to repeat values.By default, use the flattened input array, and return a flat outputarray.\nConstraints:\ndtype:int\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.randn_like(input,dtype=None,layout=None,device=None,requires_grad=False,memory_format=torch.preserve_format)\nParameter description: input(Tensor) - the size of `input` will determine size of the output tensor.\nConstraints:",
            "constr": {
                "dtype": [
                    "int"
                ],
                "structure": [
                    "tensor"
                ],
                "shape": [],
                "ndim": [],
                "enum": [],
                "range": [
                    "[0,inf)"
                ]
            },
            "signature": "torch.randn_like(input,dtype=None,layout=None,device=None,requires_grad=False,memory_format=torch.preserve_format)",
            "descp": "input(Tensor) - the size of `input` will determine size of the output tensor."
        },
        {
            "fname": "torch.utils.cpp_extension.load_inline.yaml",
            "arg": "functions",
            "prompt": "Signature: torch.distributed.recv(tensor,src=None,group=<objectobject>,tag=0)\nParameter description: tensor(Tensor) - Tensor to fill with received data.\nConstraints:\ndtype:Null\nstructure:tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.div(input,other,out=None)\nParameter description: out(Tensor, optional) - the output tensor.\nConstraints:\ndtype:Null\nstructure:tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.nn.TransformerDecoder(decoder_layer,num_layers,norm=None)\nParameter description: num_layers - the number of sub-decoder-layers in the decoder (required).\nConstraints:\ndtype:int\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:[0,inf)\n\n###\n\nSignature: torch.nn.quantized.functional.interpolate(input,size=None,scale_factor=None,mode=nearest,align_corners=None)\nParameter description: align_corners(bool, optional) - Geometrically, we consider the pixels of the input and output as squares rather than points. If set to `True`, the input and output tensors are aligned by the center points of their corner pixels, preserving the values at the corner pixels. If set to `False`, the input and output tensors are aligned by the corner points of their corner pixels, and the interpolation uses edge value padding for out-of-boundary values, making this operation independent of input size when `scale_factor` is kept the same. This only has an effect when `mode` is `'bilinear'`. Default: `False`\nConstraints:\ndtype:bool\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.gt(input,other,out=None)\nParameter description: out(Tensor, optional) - the output tensor that must be a BoolTensor\nConstraints:\ndtype:booltensor\nstructure:tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.empty_like(input,dtype=None,layout=None,device=None,requires_grad=False,memory_format=torch.preserve_format)\nParameter description: dtype(`torch.dtype`, optional) - the desired data type of returned Tensor. Default: if `None`, defaults to the dtype of `input`.\nConstraints:\ndtype:torch.dtype\nstructure:Null\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.nn.RNN(*args,**kwargs,input_size=None,hidden_size=None,num_layers=None,nonlinearity=None,bias=None,batch_first=None,dropout=None,bidirectional=None)\nParameter description: hidden_size - The number of features in the hidden state h\nConstraints:\ndtype:int\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:[0,inf)\n\n###\n\nSignature: torch.nn.functional.avg_pool1d(input,kernel_size,stride=None,padding=0,ceil_mode=False,count_include_pad=True)\nParameter description: padding - implicit zero paddings on both sides of the input. Can be a single number or a tuple (padW,). Default: 0\nConstraints:\ndtype:bool;int\nstructure:tuple\nshape:Null\nndim:0;1\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.autograd.grad(outputs,inputs,grad_outputs=None,retain_graph=None,create_graph=False,only_inputs=True,allow_unused=False)\nParameter description: outputs(sequence of Tensor) - outputs of the differentiated function.\nConstraints:\ndtype:Null\nstructure:sequence;tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.repeat_interleave(input,repeats,dim=None)\nParameter description: dim(int, optional) - The dimension along which to repeat values.By default, use the flattened input array, and return a flat outputarray.\nConstraints:\ndtype:int\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.utils.cpp_extension.load_inline(name,cpp_sources,cuda_sources=None,functions=None,extra_cflags=None,extra_cuda_cflags=None,extra_ldflags=None,extra_include_paths=None,build_directory=None,verbose=False,with_cuda=None,is_python_module=True,with_pytorch_error_handling=True)\nParameter description: functions - A list of function names for which to generate function bindings. If a dictionary is given, it should map function names to docstrings (which are otherwise just the function names).\nConstraints:",
            "constr": {
                "dtype": [
                    "string"
                ],
                "structure": [
                    "dictionary"
                ],
                "shape": [],
                "ndim": [],
                "enum": [],
                "range": []
            },
            "signature": "torch.utils.cpp_extension.load_inline(name,cpp_sources,cuda_sources=None,functions=None,extra_cflags=None,extra_cuda_cflags=None,extra_ldflags=None,extra_include_paths=None,build_directory=None,verbose=False,with_cuda=None,is_python_module=True,with_pytorch_error_handling=True)",
            "descp": "functions - A list of function names for which to generate function bindings. If a dictionary is given, it should map function names to docstrings (which are otherwise just the function names)."
        },
        {
            "fname": "torch.div.yaml",
            "arg": "input",
            "prompt": "Signature: torch.distributed.recv(tensor,src=None,group=<objectobject>,tag=0)\nParameter description: tensor(Tensor) - Tensor to fill with received data.\nConstraints:\ndtype:Null\nstructure:tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.div(input,other,out=None)\nParameter description: out(Tensor, optional) - the output tensor.\nConstraints:\ndtype:Null\nstructure:tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.nn.TransformerDecoder(decoder_layer,num_layers,norm=None)\nParameter description: num_layers - the number of sub-decoder-layers in the decoder (required).\nConstraints:\ndtype:int\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:[0,inf)\n\n###\n\nSignature: torch.nn.quantized.functional.interpolate(input,size=None,scale_factor=None,mode=nearest,align_corners=None)\nParameter description: align_corners(bool, optional) - Geometrically, we consider the pixels of the input and output as squares rather than points. If set to `True`, the input and output tensors are aligned by the center points of their corner pixels, preserving the values at the corner pixels. If set to `False`, the input and output tensors are aligned by the corner points of their corner pixels, and the interpolation uses edge value padding for out-of-boundary values, making this operation independent of input size when `scale_factor` is kept the same. This only has an effect when `mode` is `'bilinear'`. Default: `False`\nConstraints:\ndtype:bool\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.gt(input,other,out=None)\nParameter description: out(Tensor, optional) - the output tensor that must be a BoolTensor\nConstraints:\ndtype:booltensor\nstructure:tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.empty_like(input,dtype=None,layout=None,device=None,requires_grad=False,memory_format=torch.preserve_format)\nParameter description: dtype(`torch.dtype`, optional) - the desired data type of returned Tensor. Default: if `None`, defaults to the dtype of `input`.\nConstraints:\ndtype:torch.dtype\nstructure:Null\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.nn.RNN(*args,**kwargs,input_size=None,hidden_size=None,num_layers=None,nonlinearity=None,bias=None,batch_first=None,dropout=None,bidirectional=None)\nParameter description: hidden_size - The number of features in the hidden state h\nConstraints:\ndtype:int\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:[0,inf)\n\n###\n\nSignature: torch.nn.functional.avg_pool1d(input,kernel_size,stride=None,padding=0,ceil_mode=False,count_include_pad=True)\nParameter description: padding - implicit zero paddings on both sides of the input. Can be a single number or a tuple (padW,). Default: 0\nConstraints:\ndtype:bool;int\nstructure:tuple\nshape:Null\nndim:0;1\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.autograd.grad(outputs,inputs,grad_outputs=None,retain_graph=None,create_graph=False,only_inputs=True,allow_unused=False)\nParameter description: outputs(sequence of Tensor) - outputs of the differentiated function.\nConstraints:\ndtype:Null\nstructure:sequence;tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.repeat_interleave(input,repeats,dim=None)\nParameter description: dim(int, optional) - The dimension along which to repeat values.By default, use the flattened input array, and return a flat outputarray.\nConstraints:\ndtype:int\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.div(input,other,out=None)\nParameter description: input(Tensor) - the input tensor.\nConstraints:",
            "constr": {
                "dtype": [],
                "structure": [
                    "tensor"
                ],
                "shape": [],
                "ndim": [],
                "enum": [],
                "range": []
            },
            "signature": "torch.div(input,other,out=None)",
            "descp": "input(Tensor) - the input tensor."
        },
        {
            "fname": "torch.distributed.send.yaml",
            "arg": "tensor",
            "prompt": "Signature: torch.distributed.recv(tensor,src=None,group=<objectobject>,tag=0)\nParameter description: tensor(Tensor) - Tensor to fill with received data.\nConstraints:\ndtype:Null\nstructure:tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.div(input,other,out=None)\nParameter description: out(Tensor, optional) - the output tensor.\nConstraints:\ndtype:Null\nstructure:tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.nn.TransformerDecoder(decoder_layer,num_layers,norm=None)\nParameter description: num_layers - the number of sub-decoder-layers in the decoder (required).\nConstraints:\ndtype:int\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:[0,inf)\n\n###\n\nSignature: torch.nn.quantized.functional.interpolate(input,size=None,scale_factor=None,mode=nearest,align_corners=None)\nParameter description: align_corners(bool, optional) - Geometrically, we consider the pixels of the input and output as squares rather than points. If set to `True`, the input and output tensors are aligned by the center points of their corner pixels, preserving the values at the corner pixels. If set to `False`, the input and output tensors are aligned by the corner points of their corner pixels, and the interpolation uses edge value padding for out-of-boundary values, making this operation independent of input size when `scale_factor` is kept the same. This only has an effect when `mode` is `'bilinear'`. Default: `False`\nConstraints:\ndtype:bool\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.gt(input,other,out=None)\nParameter description: out(Tensor, optional) - the output tensor that must be a BoolTensor\nConstraints:\ndtype:booltensor\nstructure:tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.empty_like(input,dtype=None,layout=None,device=None,requires_grad=False,memory_format=torch.preserve_format)\nParameter description: dtype(`torch.dtype`, optional) - the desired data type of returned Tensor. Default: if `None`, defaults to the dtype of `input`.\nConstraints:\ndtype:torch.dtype\nstructure:Null\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.nn.RNN(*args,**kwargs,input_size=None,hidden_size=None,num_layers=None,nonlinearity=None,bias=None,batch_first=None,dropout=None,bidirectional=None)\nParameter description: hidden_size - The number of features in the hidden state h\nConstraints:\ndtype:int\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:[0,inf)\n\n###\n\nSignature: torch.nn.functional.avg_pool1d(input,kernel_size,stride=None,padding=0,ceil_mode=False,count_include_pad=True)\nParameter description: padding - implicit zero paddings on both sides of the input. Can be a single number or a tuple (padW,). Default: 0\nConstraints:\ndtype:bool;int\nstructure:tuple\nshape:Null\nndim:0;1\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.autograd.grad(outputs,inputs,grad_outputs=None,retain_graph=None,create_graph=False,only_inputs=True,allow_unused=False)\nParameter description: outputs(sequence of Tensor) - outputs of the differentiated function.\nConstraints:\ndtype:Null\nstructure:sequence;tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.repeat_interleave(input,repeats,dim=None)\nParameter description: dim(int, optional) - The dimension along which to repeat values.By default, use the flattened input array, and return a flat outputarray.\nConstraints:\ndtype:int\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.distributed.send(tensor,dst,group=<objectobject>,tag=0)\nParameter description: tensor(Tensor) - Tensor to send.\nConstraints:",
            "constr": {
                "dtype": [],
                "structure": [
                    "tensor"
                ],
                "shape": [],
                "ndim": [],
                "enum": [],
                "range": []
            },
            "signature": "torch.distributed.send(tensor,dst,group=<objectobject>,tag=0)",
            "descp": "tensor(Tensor) - Tensor to send."
        },
        {
            "fname": "torch.nn.l1loss.yaml",
            "arg": "size_average",
            "prompt": "Signature: torch.distributed.recv(tensor,src=None,group=<objectobject>,tag=0)\nParameter description: tensor(Tensor) - Tensor to fill with received data.\nConstraints:\ndtype:Null\nstructure:tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.div(input,other,out=None)\nParameter description: out(Tensor, optional) - the output tensor.\nConstraints:\ndtype:Null\nstructure:tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.nn.TransformerDecoder(decoder_layer,num_layers,norm=None)\nParameter description: num_layers - the number of sub-decoder-layers in the decoder (required).\nConstraints:\ndtype:int\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:[0,inf)\n\n###\n\nSignature: torch.nn.quantized.functional.interpolate(input,size=None,scale_factor=None,mode=nearest,align_corners=None)\nParameter description: align_corners(bool, optional) - Geometrically, we consider the pixels of the input and output as squares rather than points. If set to `True`, the input and output tensors are aligned by the center points of their corner pixels, preserving the values at the corner pixels. If set to `False`, the input and output tensors are aligned by the corner points of their corner pixels, and the interpolation uses edge value padding for out-of-boundary values, making this operation independent of input size when `scale_factor` is kept the same. This only has an effect when `mode` is `'bilinear'`. Default: `False`\nConstraints:\ndtype:bool\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.gt(input,other,out=None)\nParameter description: out(Tensor, optional) - the output tensor that must be a BoolTensor\nConstraints:\ndtype:booltensor\nstructure:tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.empty_like(input,dtype=None,layout=None,device=None,requires_grad=False,memory_format=torch.preserve_format)\nParameter description: dtype(`torch.dtype`, optional) - the desired data type of returned Tensor. Default: if `None`, defaults to the dtype of `input`.\nConstraints:\ndtype:torch.dtype\nstructure:Null\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.nn.RNN(*args,**kwargs,input_size=None,hidden_size=None,num_layers=None,nonlinearity=None,bias=None,batch_first=None,dropout=None,bidirectional=None)\nParameter description: hidden_size - The number of features in the hidden state h\nConstraints:\ndtype:int\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:[0,inf)\n\n###\n\nSignature: torch.nn.functional.avg_pool1d(input,kernel_size,stride=None,padding=0,ceil_mode=False,count_include_pad=True)\nParameter description: padding - implicit zero paddings on both sides of the input. Can be a single number or a tuple (padW,). Default: 0\nConstraints:\ndtype:bool;int\nstructure:tuple\nshape:Null\nndim:0;1\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.autograd.grad(outputs,inputs,grad_outputs=None,retain_graph=None,create_graph=False,only_inputs=True,allow_unused=False)\nParameter description: outputs(sequence of Tensor) - outputs of the differentiated function.\nConstraints:\ndtype:Null\nstructure:sequence;tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.repeat_interleave(input,repeats,dim=None)\nParameter description: dim(int, optional) - The dimension along which to repeat values.By default, use the flattened input array, and return a flat outputarray.\nConstraints:\ndtype:int\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.nn.L1Loss(size_average=None,reduce=None,reduction=mean)\nParameter description: size_average(bool, optional) - Deprecated (see `reduction`). By default, the losses are averaged over each loss element in the batch. Note that for some losses, there are multiple elements per sample. If the field `size_average` is set to `False`, the losses are instead summed for each minibatch. Ignored when reduce is `False`. Default: `True`\nConstraints:",
            "constr": {
                "dtype": [
                    "bool"
                ],
                "structure": [],
                "shape": [],
                "ndim": [
                    "0"
                ],
                "enum": [],
                "range": []
            },
            "signature": "torch.nn.L1Loss(size_average=None,reduce=None,reduction=mean)",
            "descp": "size_average(bool, optional) - Deprecated (see `reduction`). By default, the losses are averaged over each loss element in the batch. Note that for some losses, there are multiple elements per sample. If the field `size_average` is set to `False`, the losses are instead summed for each minibatch. Ignored when reduce is `False`. Default: `True`"
        },
        {
            "fname": "torch.prod2.yaml",
            "arg": "keepdim",
            "prompt": "Signature: torch.distributed.recv(tensor,src=None,group=<objectobject>,tag=0)\nParameter description: tensor(Tensor) - Tensor to fill with received data.\nConstraints:\ndtype:Null\nstructure:tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.div(input,other,out=None)\nParameter description: out(Tensor, optional) - the output tensor.\nConstraints:\ndtype:Null\nstructure:tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.nn.TransformerDecoder(decoder_layer,num_layers,norm=None)\nParameter description: num_layers - the number of sub-decoder-layers in the decoder (required).\nConstraints:\ndtype:int\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:[0,inf)\n\n###\n\nSignature: torch.nn.quantized.functional.interpolate(input,size=None,scale_factor=None,mode=nearest,align_corners=None)\nParameter description: align_corners(bool, optional) - Geometrically, we consider the pixels of the input and output as squares rather than points. If set to `True`, the input and output tensors are aligned by the center points of their corner pixels, preserving the values at the corner pixels. If set to `False`, the input and output tensors are aligned by the corner points of their corner pixels, and the interpolation uses edge value padding for out-of-boundary values, making this operation independent of input size when `scale_factor` is kept the same. This only has an effect when `mode` is `'bilinear'`. Default: `False`\nConstraints:\ndtype:bool\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.gt(input,other,out=None)\nParameter description: out(Tensor, optional) - the output tensor that must be a BoolTensor\nConstraints:\ndtype:booltensor\nstructure:tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.empty_like(input,dtype=None,layout=None,device=None,requires_grad=False,memory_format=torch.preserve_format)\nParameter description: dtype(`torch.dtype`, optional) - the desired data type of returned Tensor. Default: if `None`, defaults to the dtype of `input`.\nConstraints:\ndtype:torch.dtype\nstructure:Null\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.nn.RNN(*args,**kwargs,input_size=None,hidden_size=None,num_layers=None,nonlinearity=None,bias=None,batch_first=None,dropout=None,bidirectional=None)\nParameter description: hidden_size - The number of features in the hidden state h\nConstraints:\ndtype:int\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:[0,inf)\n\n###\n\nSignature: torch.nn.functional.avg_pool1d(input,kernel_size,stride=None,padding=0,ceil_mode=False,count_include_pad=True)\nParameter description: padding - implicit zero paddings on both sides of the input. Can be a single number or a tuple (padW,). Default: 0\nConstraints:\ndtype:bool;int\nstructure:tuple\nshape:Null\nndim:0;1\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.autograd.grad(outputs,inputs,grad_outputs=None,retain_graph=None,create_graph=False,only_inputs=True,allow_unused=False)\nParameter description: outputs(sequence of Tensor) - outputs of the differentiated function.\nConstraints:\ndtype:Null\nstructure:sequence;tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.repeat_interleave(input,repeats,dim=None)\nParameter description: dim(int, optional) - The dimension along which to repeat values.By default, use the flattened input array, and return a flat outputarray.\nConstraints:\ndtype:int\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.prod(input,dim,keepdim=False,dtype=None)\nParameter description: keepdim(bool) - whether the output tensor has `dim` retained or not.\nConstraints:",
            "constr": {
                "dtype": [
                    "bool"
                ],
                "structure": [],
                "shape": [],
                "ndim": [
                    "0"
                ],
                "enum": [],
                "range": []
            },
            "signature": "torch.prod(input,dim,keepdim=False,dtype=None)",
            "descp": "keepdim(bool) - whether the output tensor has `dim` retained or not."
        },
        {
            "fname": "torch.nn.lppool1d.yaml",
            "arg": "kernel_size",
            "prompt": "Signature: torch.distributed.recv(tensor,src=None,group=<objectobject>,tag=0)\nParameter description: tensor(Tensor) - Tensor to fill with received data.\nConstraints:\ndtype:Null\nstructure:tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.div(input,other,out=None)\nParameter description: out(Tensor, optional) - the output tensor.\nConstraints:\ndtype:Null\nstructure:tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.nn.TransformerDecoder(decoder_layer,num_layers,norm=None)\nParameter description: num_layers - the number of sub-decoder-layers in the decoder (required).\nConstraints:\ndtype:int\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:[0,inf)\n\n###\n\nSignature: torch.nn.quantized.functional.interpolate(input,size=None,scale_factor=None,mode=nearest,align_corners=None)\nParameter description: align_corners(bool, optional) - Geometrically, we consider the pixels of the input and output as squares rather than points. If set to `True`, the input and output tensors are aligned by the center points of their corner pixels, preserving the values at the corner pixels. If set to `False`, the input and output tensors are aligned by the corner points of their corner pixels, and the interpolation uses edge value padding for out-of-boundary values, making this operation independent of input size when `scale_factor` is kept the same. This only has an effect when `mode` is `'bilinear'`. Default: `False`\nConstraints:\ndtype:bool\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.gt(input,other,out=None)\nParameter description: out(Tensor, optional) - the output tensor that must be a BoolTensor\nConstraints:\ndtype:booltensor\nstructure:tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.empty_like(input,dtype=None,layout=None,device=None,requires_grad=False,memory_format=torch.preserve_format)\nParameter description: dtype(`torch.dtype`, optional) - the desired data type of returned Tensor. Default: if `None`, defaults to the dtype of `input`.\nConstraints:\ndtype:torch.dtype\nstructure:Null\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.nn.RNN(*args,**kwargs,input_size=None,hidden_size=None,num_layers=None,nonlinearity=None,bias=None,batch_first=None,dropout=None,bidirectional=None)\nParameter description: hidden_size - The number of features in the hidden state h\nConstraints:\ndtype:int\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:[0,inf)\n\n###\n\nSignature: torch.nn.functional.avg_pool1d(input,kernel_size,stride=None,padding=0,ceil_mode=False,count_include_pad=True)\nParameter description: padding - implicit zero paddings on both sides of the input. Can be a single number or a tuple (padW,). Default: 0\nConstraints:\ndtype:bool;int\nstructure:tuple\nshape:Null\nndim:0;1\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.autograd.grad(outputs,inputs,grad_outputs=None,retain_graph=None,create_graph=False,only_inputs=True,allow_unused=False)\nParameter description: outputs(sequence of Tensor) - outputs of the differentiated function.\nConstraints:\ndtype:Null\nstructure:sequence;tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.repeat_interleave(input,repeats,dim=None)\nParameter description: dim(int, optional) - The dimension along which to repeat values.By default, use the flattened input array, and return a flat outputarray.\nConstraints:\ndtype:int\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.nn.LPPool1d(norm_type,kernel_size,stride=None,ceil_mode=False)\nParameter description: kernel_size - a single int, the size of the window\nConstraints:",
            "constr": {
                "dtype": [
                    "int"
                ],
                "structure": [],
                "shape": [],
                "ndim": [
                    "0"
                ],
                "enum": [],
                "range": [
                    "[0,inf)"
                ]
            },
            "signature": "torch.nn.LPPool1d(norm_type,kernel_size,stride=None,ceil_mode=False)",
            "descp": "kernel_size - a single int, the size of the window"
        },
        {
            "fname": "torch.zeros_like.yaml",
            "arg": "dtype",
            "prompt": "Signature: torch.distributed.recv(tensor,src=None,group=<objectobject>,tag=0)\nParameter description: tensor(Tensor) - Tensor to fill with received data.\nConstraints:\ndtype:Null\nstructure:tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.div(input,other,out=None)\nParameter description: out(Tensor, optional) - the output tensor.\nConstraints:\ndtype:Null\nstructure:tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.nn.TransformerDecoder(decoder_layer,num_layers,norm=None)\nParameter description: num_layers - the number of sub-decoder-layers in the decoder (required).\nConstraints:\ndtype:int\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:[0,inf)\n\n###\n\nSignature: torch.nn.quantized.functional.interpolate(input,size=None,scale_factor=None,mode=nearest,align_corners=None)\nParameter description: align_corners(bool, optional) - Geometrically, we consider the pixels of the input and output as squares rather than points. If set to `True`, the input and output tensors are aligned by the center points of their corner pixels, preserving the values at the corner pixels. If set to `False`, the input and output tensors are aligned by the corner points of their corner pixels, and the interpolation uses edge value padding for out-of-boundary values, making this operation independent of input size when `scale_factor` is kept the same. This only has an effect when `mode` is `'bilinear'`. Default: `False`\nConstraints:\ndtype:bool\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.gt(input,other,out=None)\nParameter description: out(Tensor, optional) - the output tensor that must be a BoolTensor\nConstraints:\ndtype:booltensor\nstructure:tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.empty_like(input,dtype=None,layout=None,device=None,requires_grad=False,memory_format=torch.preserve_format)\nParameter description: dtype(`torch.dtype`, optional) - the desired data type of returned Tensor. Default: if `None`, defaults to the dtype of `input`.\nConstraints:\ndtype:torch.dtype\nstructure:Null\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.nn.RNN(*args,**kwargs,input_size=None,hidden_size=None,num_layers=None,nonlinearity=None,bias=None,batch_first=None,dropout=None,bidirectional=None)\nParameter description: hidden_size - The number of features in the hidden state h\nConstraints:\ndtype:int\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:[0,inf)\n\n###\n\nSignature: torch.nn.functional.avg_pool1d(input,kernel_size,stride=None,padding=0,ceil_mode=False,count_include_pad=True)\nParameter description: padding - implicit zero paddings on both sides of the input. Can be a single number or a tuple (padW,). Default: 0\nConstraints:\ndtype:bool;int\nstructure:tuple\nshape:Null\nndim:0;1\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.autograd.grad(outputs,inputs,grad_outputs=None,retain_graph=None,create_graph=False,only_inputs=True,allow_unused=False)\nParameter description: outputs(sequence of Tensor) - outputs of the differentiated function.\nConstraints:\ndtype:Null\nstructure:sequence;tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.repeat_interleave(input,repeats,dim=None)\nParameter description: dim(int, optional) - The dimension along which to repeat values.By default, use the flattened input array, and return a flat outputarray.\nConstraints:\ndtype:int\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.zeros_like(input,dtype=None,layout=None,device=None,requires_grad=False,memory_format=torch.preserve_format)\nParameter description: dtype(`torch.dtype`, optional) - the desired data type of returned Tensor. Default: if `None`, defaults to the dtype of `input`.\nConstraints:",
            "constr": {
                "dtype": [
                    "torch.dtype"
                ],
                "structure": [],
                "shape": [],
                "ndim": [],
                "enum": [],
                "range": []
            },
            "signature": "torch.zeros_like(input,dtype=None,layout=None,device=None,requires_grad=False,memory_format=torch.preserve_format)",
            "descp": "dtype(`torch.dtype`, optional) - the desired data type of returned Tensor. Default: if `None`, defaults to the dtype of `input`."
        },
        {
            "fname": "torch.nn.upsamplingnearest2d.yaml",
            "arg": "size",
            "prompt": "Signature: torch.distributed.recv(tensor,src=None,group=<objectobject>,tag=0)\nParameter description: tensor(Tensor) - Tensor to fill with received data.\nConstraints:\ndtype:Null\nstructure:tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.div(input,other,out=None)\nParameter description: out(Tensor, optional) - the output tensor.\nConstraints:\ndtype:Null\nstructure:tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.nn.TransformerDecoder(decoder_layer,num_layers,norm=None)\nParameter description: num_layers - the number of sub-decoder-layers in the decoder (required).\nConstraints:\ndtype:int\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:[0,inf)\n\n###\n\nSignature: torch.nn.quantized.functional.interpolate(input,size=None,scale_factor=None,mode=nearest,align_corners=None)\nParameter description: align_corners(bool, optional) - Geometrically, we consider the pixels of the input and output as squares rather than points. If set to `True`, the input and output tensors are aligned by the center points of their corner pixels, preserving the values at the corner pixels. If set to `False`, the input and output tensors are aligned by the corner points of their corner pixels, and the interpolation uses edge value padding for out-of-boundary values, making this operation independent of input size when `scale_factor` is kept the same. This only has an effect when `mode` is `'bilinear'`. Default: `False`\nConstraints:\ndtype:bool\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.gt(input,other,out=None)\nParameter description: out(Tensor, optional) - the output tensor that must be a BoolTensor\nConstraints:\ndtype:booltensor\nstructure:tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.empty_like(input,dtype=None,layout=None,device=None,requires_grad=False,memory_format=torch.preserve_format)\nParameter description: dtype(`torch.dtype`, optional) - the desired data type of returned Tensor. Default: if `None`, defaults to the dtype of `input`.\nConstraints:\ndtype:torch.dtype\nstructure:Null\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.nn.RNN(*args,**kwargs,input_size=None,hidden_size=None,num_layers=None,nonlinearity=None,bias=None,batch_first=None,dropout=None,bidirectional=None)\nParameter description: hidden_size - The number of features in the hidden state h\nConstraints:\ndtype:int\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:[0,inf)\n\n###\n\nSignature: torch.nn.functional.avg_pool1d(input,kernel_size,stride=None,padding=0,ceil_mode=False,count_include_pad=True)\nParameter description: padding - implicit zero paddings on both sides of the input. Can be a single number or a tuple (padW,). Default: 0\nConstraints:\ndtype:bool;int\nstructure:tuple\nshape:Null\nndim:0;1\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.autograd.grad(outputs,inputs,grad_outputs=None,retain_graph=None,create_graph=False,only_inputs=True,allow_unused=False)\nParameter description: outputs(sequence of Tensor) - outputs of the differentiated function.\nConstraints:\ndtype:Null\nstructure:sequence;tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.repeat_interleave(input,repeats,dim=None)\nParameter description: dim(int, optional) - The dimension along which to repeat values.By default, use the flattened input array, and return a flat outputarray.\nConstraints:\ndtype:int\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.nn.UpsamplingNearest2d(size=None,scale_factor=None)\nParameter description: size(int or Tuple[int, int], optional) - output spatial sizes\nConstraints:",
            "constr": {
                "dtype": [
                    "int"
                ],
                "structure": [
                    "tuple"
                ],
                "shape": [],
                "ndim": [
                    "0",
                    "1"
                ],
                "enum": [],
                "range": [
                    "[0,inf)"
                ]
            },
            "signature": "torch.nn.UpsamplingNearest2d(size=None,scale_factor=None)",
            "descp": "size(int or Tuple[int, int], optional) - output spatial sizes"
        },
        {
            "fname": "torch.nn.quantized.functional.conv3d.yaml",
            "arg": "input",
            "prompt": "Signature: torch.distributed.recv(tensor,src=None,group=<objectobject>,tag=0)\nParameter description: tensor(Tensor) - Tensor to fill with received data.\nConstraints:\ndtype:Null\nstructure:tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.div(input,other,out=None)\nParameter description: out(Tensor, optional) - the output tensor.\nConstraints:\ndtype:Null\nstructure:tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.nn.TransformerDecoder(decoder_layer,num_layers,norm=None)\nParameter description: num_layers - the number of sub-decoder-layers in the decoder (required).\nConstraints:\ndtype:int\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:[0,inf)\n\n###\n\nSignature: torch.nn.quantized.functional.interpolate(input,size=None,scale_factor=None,mode=nearest,align_corners=None)\nParameter description: align_corners(bool, optional) - Geometrically, we consider the pixels of the input and output as squares rather than points. If set to `True`, the input and output tensors are aligned by the center points of their corner pixels, preserving the values at the corner pixels. If set to `False`, the input and output tensors are aligned by the corner points of their corner pixels, and the interpolation uses edge value padding for out-of-boundary values, making this operation independent of input size when `scale_factor` is kept the same. This only has an effect when `mode` is `'bilinear'`. Default: `False`\nConstraints:\ndtype:bool\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.gt(input,other,out=None)\nParameter description: out(Tensor, optional) - the output tensor that must be a BoolTensor\nConstraints:\ndtype:booltensor\nstructure:tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.empty_like(input,dtype=None,layout=None,device=None,requires_grad=False,memory_format=torch.preserve_format)\nParameter description: dtype(`torch.dtype`, optional) - the desired data type of returned Tensor. Default: if `None`, defaults to the dtype of `input`.\nConstraints:\ndtype:torch.dtype\nstructure:Null\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.nn.RNN(*args,**kwargs,input_size=None,hidden_size=None,num_layers=None,nonlinearity=None,bias=None,batch_first=None,dropout=None,bidirectional=None)\nParameter description: hidden_size - The number of features in the hidden state h\nConstraints:\ndtype:int\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:[0,inf)\n\n###\n\nSignature: torch.nn.functional.avg_pool1d(input,kernel_size,stride=None,padding=0,ceil_mode=False,count_include_pad=True)\nParameter description: padding - implicit zero paddings on both sides of the input. Can be a single number or a tuple (padW,). Default: 0\nConstraints:\ndtype:bool;int\nstructure:tuple\nshape:Null\nndim:0;1\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.autograd.grad(outputs,inputs,grad_outputs=None,retain_graph=None,create_graph=False,only_inputs=True,allow_unused=False)\nParameter description: outputs(sequence of Tensor) - outputs of the differentiated function.\nConstraints:\ndtype:Null\nstructure:sequence;tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.repeat_interleave(input,repeats,dim=None)\nParameter description: dim(int, optional) - The dimension along which to repeat values.By default, use the flattened input array, and return a flat outputarray.\nConstraints:\ndtype:int\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.nn.quantized.functional.conv3d(input,weight,bias,stride=1,padding=0,dilation=1,groups=1,padding_mode=zeros,scale=1.0,zero_point=0,dtype=torch.quint8)\nParameter description: input - quantized input tensor of shape (minibatch , in _channels , iD , iH , iW) \nConstraints:",
            "constr": {
                "dtype": [],
                "structure": [
                    "tensor"
                ],
                "shape": [
                    "(minibatch , in _channels , iD , iH , iW)"
                ],
                "ndim": [],
                "enum": [],
                "range": []
            },
            "signature": "torch.nn.quantized.functional.conv3d(input,weight,bias,stride=1,padding=0,dilation=1,groups=1,padding_mode=zeros,scale=1.0,zero_point=0,dtype=torch.quint8)",
            "descp": "input - quantized input tensor of shape (minibatch , in _channels , iD , iH , iW) "
        },
        {
            "fname": "torch.nn.transformer.yaml",
            "arg": "activation",
            "prompt": "Signature: torch.distributed.recv(tensor,src=None,group=<objectobject>,tag=0)\nParameter description: tensor(Tensor) - Tensor to fill with received data.\nConstraints:\ndtype:Null\nstructure:tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.div(input,other,out=None)\nParameter description: out(Tensor, optional) - the output tensor.\nConstraints:\ndtype:Null\nstructure:tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.nn.TransformerDecoder(decoder_layer,num_layers,norm=None)\nParameter description: num_layers - the number of sub-decoder-layers in the decoder (required).\nConstraints:\ndtype:int\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:[0,inf)\n\n###\n\nSignature: torch.nn.quantized.functional.interpolate(input,size=None,scale_factor=None,mode=nearest,align_corners=None)\nParameter description: align_corners(bool, optional) - Geometrically, we consider the pixels of the input and output as squares rather than points. If set to `True`, the input and output tensors are aligned by the center points of their corner pixels, preserving the values at the corner pixels. If set to `False`, the input and output tensors are aligned by the corner points of their corner pixels, and the interpolation uses edge value padding for out-of-boundary values, making this operation independent of input size when `scale_factor` is kept the same. This only has an effect when `mode` is `'bilinear'`. Default: `False`\nConstraints:\ndtype:bool\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.gt(input,other,out=None)\nParameter description: out(Tensor, optional) - the output tensor that must be a BoolTensor\nConstraints:\ndtype:booltensor\nstructure:tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.empty_like(input,dtype=None,layout=None,device=None,requires_grad=False,memory_format=torch.preserve_format)\nParameter description: dtype(`torch.dtype`, optional) - the desired data type of returned Tensor. Default: if `None`, defaults to the dtype of `input`.\nConstraints:\ndtype:torch.dtype\nstructure:Null\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.nn.RNN(*args,**kwargs,input_size=None,hidden_size=None,num_layers=None,nonlinearity=None,bias=None,batch_first=None,dropout=None,bidirectional=None)\nParameter description: hidden_size - The number of features in the hidden state h\nConstraints:\ndtype:int\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:[0,inf)\n\n###\n\nSignature: torch.nn.functional.avg_pool1d(input,kernel_size,stride=None,padding=0,ceil_mode=False,count_include_pad=True)\nParameter description: padding - implicit zero paddings on both sides of the input. Can be a single number or a tuple (padW,). Default: 0\nConstraints:\ndtype:bool;int\nstructure:tuple\nshape:Null\nndim:0;1\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.autograd.grad(outputs,inputs,grad_outputs=None,retain_graph=None,create_graph=False,only_inputs=True,allow_unused=False)\nParameter description: outputs(sequence of Tensor) - outputs of the differentiated function.\nConstraints:\ndtype:Null\nstructure:sequence;tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.repeat_interleave(input,repeats,dim=None)\nParameter description: dim(int, optional) - The dimension along which to repeat values.By default, use the flattened input array, and return a flat outputarray.\nConstraints:\ndtype:int\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.nn.Transformer(d_model=512,nhead=8,num_encoder_layers=6,num_decoder_layers=6,dim_feedforward=2048,dropout=0.1,activation=relu,custom_encoder=None,custom_decoder=None)\nParameter description: activation - the activation function of encoder/decoder intermediate layer, relu or gelu (default=relu).\nConstraints:",
            "constr": {
                "dtype": [
                    "string"
                ],
                "structure": [],
                "shape": [],
                "ndim": [],
                "enum": [],
                "range": []
            },
            "signature": "torch.nn.Transformer(d_model=512,nhead=8,num_encoder_layers=6,num_decoder_layers=6,dim_feedforward=2048,dropout=0.1,activation=relu,custom_encoder=None,custom_decoder=None)",
            "descp": "activation - the activation function of encoder/decoder intermediate layer, relu or gelu (default=relu)."
        },
        {
            "fname": "torch.lobpcg.yaml",
            "arg": "niter",
            "prompt": "Signature: torch.distributed.recv(tensor,src=None,group=<objectobject>,tag=0)\nParameter description: tensor(Tensor) - Tensor to fill with received data.\nConstraints:\ndtype:Null\nstructure:tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.div(input,other,out=None)\nParameter description: out(Tensor, optional) - the output tensor.\nConstraints:\ndtype:Null\nstructure:tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.nn.TransformerDecoder(decoder_layer,num_layers,norm=None)\nParameter description: num_layers - the number of sub-decoder-layers in the decoder (required).\nConstraints:\ndtype:int\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:[0,inf)\n\n###\n\nSignature: torch.nn.quantized.functional.interpolate(input,size=None,scale_factor=None,mode=nearest,align_corners=None)\nParameter description: align_corners(bool, optional) - Geometrically, we consider the pixels of the input and output as squares rather than points. If set to `True`, the input and output tensors are aligned by the center points of their corner pixels, preserving the values at the corner pixels. If set to `False`, the input and output tensors are aligned by the corner points of their corner pixels, and the interpolation uses edge value padding for out-of-boundary values, making this operation independent of input size when `scale_factor` is kept the same. This only has an effect when `mode` is `'bilinear'`. Default: `False`\nConstraints:\ndtype:bool\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.gt(input,other,out=None)\nParameter description: out(Tensor, optional) - the output tensor that must be a BoolTensor\nConstraints:\ndtype:booltensor\nstructure:tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.empty_like(input,dtype=None,layout=None,device=None,requires_grad=False,memory_format=torch.preserve_format)\nParameter description: dtype(`torch.dtype`, optional) - the desired data type of returned Tensor. Default: if `None`, defaults to the dtype of `input`.\nConstraints:\ndtype:torch.dtype\nstructure:Null\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.nn.RNN(*args,**kwargs,input_size=None,hidden_size=None,num_layers=None,nonlinearity=None,bias=None,batch_first=None,dropout=None,bidirectional=None)\nParameter description: hidden_size - The number of features in the hidden state h\nConstraints:\ndtype:int\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:[0,inf)\n\n###\n\nSignature: torch.nn.functional.avg_pool1d(input,kernel_size,stride=None,padding=0,ceil_mode=False,count_include_pad=True)\nParameter description: padding - implicit zero paddings on both sides of the input. Can be a single number or a tuple (padW,). Default: 0\nConstraints:\ndtype:bool;int\nstructure:tuple\nshape:Null\nndim:0;1\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.autograd.grad(outputs,inputs,grad_outputs=None,retain_graph=None,create_graph=False,only_inputs=True,allow_unused=False)\nParameter description: outputs(sequence of Tensor) - outputs of the differentiated function.\nConstraints:\ndtype:Null\nstructure:sequence;tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.repeat_interleave(input,repeats,dim=None)\nParameter description: dim(int, optional) - The dimension along which to repeat values.By default, use the flattened input array, and return a flat outputarray.\nConstraints:\ndtype:int\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.lobpcg(A,k=None,B=None,X=None,n=None,iK=None,niter=None,tol=None,largest=None,method=None,tracker=None,ortho_iparams=None,ortho_fparams=None,ortho_bparams=None)\nParameter description: niter(int, optional) - maximum number of iterations. When reached, the iteration process is hard-stopped and the current approximation of eigenpairs is returned. For infinite iteration but until convergence criteria is met, use -1.\nConstraints:",
            "constr": {
                "dtype": [
                    "int"
                ],
                "structure": [],
                "shape": [],
                "ndim": [
                    "0"
                ],
                "enum": [],
                "range": [
                    "[0,inf)"
                ]
            },
            "signature": "torch.lobpcg(A,k=None,B=None,X=None,n=None,iK=None,niter=None,tol=None,largest=None,method=None,tracker=None,ortho_iparams=None,ortho_fparams=None,ortho_bparams=None)",
            "descp": "niter(int, optional) - maximum number of iterations. When reached, the iteration process is hard-stopped and the current approximation of eigenpairs is returned. For infinite iteration but until convergence criteria is met, use -1."
        },
        {
            "fname": "torch.cholesky_solve.yaml",
            "arg": "input2",
            "prompt": "Signature: torch.distributed.recv(tensor,src=None,group=<objectobject>,tag=0)\nParameter description: tensor(Tensor) - Tensor to fill with received data.\nConstraints:\ndtype:Null\nstructure:tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.div(input,other,out=None)\nParameter description: out(Tensor, optional) - the output tensor.\nConstraints:\ndtype:Null\nstructure:tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.nn.TransformerDecoder(decoder_layer,num_layers,norm=None)\nParameter description: num_layers - the number of sub-decoder-layers in the decoder (required).\nConstraints:\ndtype:int\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:[0,inf)\n\n###\n\nSignature: torch.nn.quantized.functional.interpolate(input,size=None,scale_factor=None,mode=nearest,align_corners=None)\nParameter description: align_corners(bool, optional) - Geometrically, we consider the pixels of the input and output as squares rather than points. If set to `True`, the input and output tensors are aligned by the center points of their corner pixels, preserving the values at the corner pixels. If set to `False`, the input and output tensors are aligned by the corner points of their corner pixels, and the interpolation uses edge value padding for out-of-boundary values, making this operation independent of input size when `scale_factor` is kept the same. This only has an effect when `mode` is `'bilinear'`. Default: `False`\nConstraints:\ndtype:bool\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.gt(input,other,out=None)\nParameter description: out(Tensor, optional) - the output tensor that must be a BoolTensor\nConstraints:\ndtype:booltensor\nstructure:tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.empty_like(input,dtype=None,layout=None,device=None,requires_grad=False,memory_format=torch.preserve_format)\nParameter description: dtype(`torch.dtype`, optional) - the desired data type of returned Tensor. Default: if `None`, defaults to the dtype of `input`.\nConstraints:\ndtype:torch.dtype\nstructure:Null\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.nn.RNN(*args,**kwargs,input_size=None,hidden_size=None,num_layers=None,nonlinearity=None,bias=None,batch_first=None,dropout=None,bidirectional=None)\nParameter description: hidden_size - The number of features in the hidden state h\nConstraints:\ndtype:int\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:[0,inf)\n\n###\n\nSignature: torch.nn.functional.avg_pool1d(input,kernel_size,stride=None,padding=0,ceil_mode=False,count_include_pad=True)\nParameter description: padding - implicit zero paddings on both sides of the input. Can be a single number or a tuple (padW,). Default: 0\nConstraints:\ndtype:bool;int\nstructure:tuple\nshape:Null\nndim:0;1\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.autograd.grad(outputs,inputs,grad_outputs=None,retain_graph=None,create_graph=False,only_inputs=True,allow_unused=False)\nParameter description: outputs(sequence of Tensor) - outputs of the differentiated function.\nConstraints:\ndtype:Null\nstructure:sequence;tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.repeat_interleave(input,repeats,dim=None)\nParameter description: dim(int, optional) - The dimension along which to repeat values.By default, use the flattened input array, and return a flat outputarray.\nConstraints:\ndtype:int\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.cholesky_solve(input,input2,upper=False,out=None)\nParameter description: input2(Tensor) - input matrix u  of size (*, m, m) , where *  is zero of more batch dimensions composed of upper or lower triangular Cholesky factor\nConstraints:",
            "constr": {
                "dtype": [
                    "numeric"
                ],
                "structure": [
                    "tensor"
                ],
                "shape": [
                    "(*, m, m)"
                ],
                "ndim": [],
                "enum": [],
                "range": []
            },
            "signature": "torch.cholesky_solve(input,input2,upper=False,out=None)",
            "descp": "input2(Tensor) - input matrix u  of size (*, m, m) , where *  is zero of more batch dimensions composed of upper or lower triangular Cholesky factor"
        },
        {
            "fname": "torch.ifft.yaml",
            "arg": "input",
            "prompt": "Signature: torch.distributed.recv(tensor,src=None,group=<objectobject>,tag=0)\nParameter description: tensor(Tensor) - Tensor to fill with received data.\nConstraints:\ndtype:Null\nstructure:tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.div(input,other,out=None)\nParameter description: out(Tensor, optional) - the output tensor.\nConstraints:\ndtype:Null\nstructure:tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.nn.TransformerDecoder(decoder_layer,num_layers,norm=None)\nParameter description: num_layers - the number of sub-decoder-layers in the decoder (required).\nConstraints:\ndtype:int\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:[0,inf)\n\n###\n\nSignature: torch.nn.quantized.functional.interpolate(input,size=None,scale_factor=None,mode=nearest,align_corners=None)\nParameter description: align_corners(bool, optional) - Geometrically, we consider the pixels of the input and output as squares rather than points. If set to `True`, the input and output tensors are aligned by the center points of their corner pixels, preserving the values at the corner pixels. If set to `False`, the input and output tensors are aligned by the corner points of their corner pixels, and the interpolation uses edge value padding for out-of-boundary values, making this operation independent of input size when `scale_factor` is kept the same. This only has an effect when `mode` is `'bilinear'`. Default: `False`\nConstraints:\ndtype:bool\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.gt(input,other,out=None)\nParameter description: out(Tensor, optional) - the output tensor that must be a BoolTensor\nConstraints:\ndtype:booltensor\nstructure:tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.empty_like(input,dtype=None,layout=None,device=None,requires_grad=False,memory_format=torch.preserve_format)\nParameter description: dtype(`torch.dtype`, optional) - the desired data type of returned Tensor. Default: if `None`, defaults to the dtype of `input`.\nConstraints:\ndtype:torch.dtype\nstructure:Null\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.nn.RNN(*args,**kwargs,input_size=None,hidden_size=None,num_layers=None,nonlinearity=None,bias=None,batch_first=None,dropout=None,bidirectional=None)\nParameter description: hidden_size - The number of features in the hidden state h\nConstraints:\ndtype:int\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:[0,inf)\n\n###\n\nSignature: torch.nn.functional.avg_pool1d(input,kernel_size,stride=None,padding=0,ceil_mode=False,count_include_pad=True)\nParameter description: padding - implicit zero paddings on both sides of the input. Can be a single number or a tuple (padW,). Default: 0\nConstraints:\ndtype:bool;int\nstructure:tuple\nshape:Null\nndim:0;1\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.autograd.grad(outputs,inputs,grad_outputs=None,retain_graph=None,create_graph=False,only_inputs=True,allow_unused=False)\nParameter description: outputs(sequence of Tensor) - outputs of the differentiated function.\nConstraints:\ndtype:Null\nstructure:sequence;tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.repeat_interleave(input,repeats,dim=None)\nParameter description: dim(int, optional) - The dimension along which to repeat values.By default, use the flattened input array, and return a flat outputarray.\nConstraints:\ndtype:int\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.ifft(input,signal_ndim,normalized=False)\nParameter description: input(Tensor) - the input tensor of at least `signal_ndim` `+ 1` dimensions\nConstraints:",
            "constr": {
                "dtype": [],
                "structure": [
                    "tensor"
                ],
                "shape": [],
                "ndim": [
                    ">=signal_ndim+1"
                ],
                "enum": [],
                "range": []
            },
            "signature": "torch.ifft(input,signal_ndim,normalized=False)",
            "descp": "input(Tensor) - the input tensor of at least `signal_ndim` `+ 1` dimensions"
        },
        {
            "fname": "torch.argmax2.yaml",
            "arg": "dim",
            "prompt": "Signature: torch.distributed.recv(tensor,src=None,group=<objectobject>,tag=0)\nParameter description: tensor(Tensor) - Tensor to fill with received data.\nConstraints:\ndtype:Null\nstructure:tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.div(input,other,out=None)\nParameter description: out(Tensor, optional) - the output tensor.\nConstraints:\ndtype:Null\nstructure:tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.nn.TransformerDecoder(decoder_layer,num_layers,norm=None)\nParameter description: num_layers - the number of sub-decoder-layers in the decoder (required).\nConstraints:\ndtype:int\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:[0,inf)\n\n###\n\nSignature: torch.nn.quantized.functional.interpolate(input,size=None,scale_factor=None,mode=nearest,align_corners=None)\nParameter description: align_corners(bool, optional) - Geometrically, we consider the pixels of the input and output as squares rather than points. If set to `True`, the input and output tensors are aligned by the center points of their corner pixels, preserving the values at the corner pixels. If set to `False`, the input and output tensors are aligned by the corner points of their corner pixels, and the interpolation uses edge value padding for out-of-boundary values, making this operation independent of input size when `scale_factor` is kept the same. This only has an effect when `mode` is `'bilinear'`. Default: `False`\nConstraints:\ndtype:bool\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.gt(input,other,out=None)\nParameter description: out(Tensor, optional) - the output tensor that must be a BoolTensor\nConstraints:\ndtype:booltensor\nstructure:tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.empty_like(input,dtype=None,layout=None,device=None,requires_grad=False,memory_format=torch.preserve_format)\nParameter description: dtype(`torch.dtype`, optional) - the desired data type of returned Tensor. Default: if `None`, defaults to the dtype of `input`.\nConstraints:\ndtype:torch.dtype\nstructure:Null\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.nn.RNN(*args,**kwargs,input_size=None,hidden_size=None,num_layers=None,nonlinearity=None,bias=None,batch_first=None,dropout=None,bidirectional=None)\nParameter description: hidden_size - The number of features in the hidden state h\nConstraints:\ndtype:int\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:[0,inf)\n\n###\n\nSignature: torch.nn.functional.avg_pool1d(input,kernel_size,stride=None,padding=0,ceil_mode=False,count_include_pad=True)\nParameter description: padding - implicit zero paddings on both sides of the input. Can be a single number or a tuple (padW,). Default: 0\nConstraints:\ndtype:bool;int\nstructure:tuple\nshape:Null\nndim:0;1\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.autograd.grad(outputs,inputs,grad_outputs=None,retain_graph=None,create_graph=False,only_inputs=True,allow_unused=False)\nParameter description: outputs(sequence of Tensor) - outputs of the differentiated function.\nConstraints:\ndtype:Null\nstructure:sequence;tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.repeat_interleave(input,repeats,dim=None)\nParameter description: dim(int, optional) - The dimension along which to repeat values.By default, use the flattened input array, and return a flat outputarray.\nConstraints:\ndtype:int\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.argmax(input,dim,keepdim=False)\nParameter description: dim(int) - the dimension to reduce. If `None`, the argmax of the flattened input is returned.\nConstraints:",
            "constr": {
                "dtype": [
                    "int"
                ],
                "structure": [],
                "shape": [],
                "ndim": [
                    "0"
                ],
                "enum": [],
                "range": []
            },
            "signature": "torch.argmax(input,dim,keepdim=False)",
            "descp": "dim(int) - the dimension to reduce. If `None`, the argmax of the flattened input is returned."
        },
        {
            "fname": "torch.distributed.recv.yaml",
            "arg": "tensor",
            "prompt": "Signature: torch.mean(input)\nParameter description: input(Tensor) - the input tensor.\nConstraints:\ndtype:Null\nstructure:tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.nn.Linear(in_features,out_features,bias=True)\nParameter description: in_features - size of each input sample\nConstraints:\ndtype:int\nstructure:Null\nshape:Null\nndim:Null\nenum:Null\nrange:[0,inf)\n\n###\n\nSignature: torch.combinations(input,r=2,with_replacement=False)\nParameter description: with_replacement(boolean, optional) - whether to allow duplication in combination\nConstraints:\ndtype:boolean;bool\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.rand_like(input,dtype=None,layout=None,device=None,requires_grad=False,memory_format=torch.preserve_format)\nParameter description: input(Tensor) - the size of `input` will determine size of the output tensor.\nConstraints:\ndtype:int\nstructure:tensor\nshape:Null\nndim:Null\nenum:Null\nrange:[0,inf)\n\n###\n\nSignature: torch.median(input)\nParameter description: input(Tensor) - the input tensor.\nConstraints:\ndtype:Null\nstructure:tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.tensordot(a,b,dims=2)\nParameter description: b(Tensor) - Right tensor to contract\nConstraints:\ndtype:Null\nstructure:tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.quantization.quantize(model,run_fn,run_args,mapping=None,inplace=False)\nParameter description: inplace - carry out model transformations in-place, the original module is mutated\nConstraints:\ndtype:bool\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.nn.SyncBatchNorm(num_features,eps=1e-05,momentum=0.1,affine=True,track_running_stats=True,process_group=None)\nParameter description: track_running_stats - a boolean value that when set to `True`, this module tracks the running mean and variance, and when set to `False`, this module does not track such statistics and always uses batch statistics in both training and eval modes. Default: `True`\nConstraints:\ndtype:boolean;bool\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.floor_divide(input,other,out=None)\nParameter description: other(Tensor or Scalar) - the denominator\nConstraints:\ndtype:Null\nstructure:tensor\nshape:Null\nndim:0\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.nn.AvgPool3d(kernel_size,stride=None,padding=0,ceil_mode=False,count_include_pad=True,divisor_override=None)\nParameter description: count_include_pad - when True, will include the zero-padding in the averaging calculation\nConstraints:\ndtype:bool\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.distributed.recv(tensor,src=None,group=<objectobject>,tag=0)\nParameter description: tensor(Tensor) - Tensor to fill with received data.\nConstraints:",
            "constr": {
                "dtype": [],
                "structure": [
                    "tensor"
                ],
                "shape": [],
                "ndim": [],
                "enum": [],
                "range": []
            },
            "signature": "torch.distributed.recv(tensor,src=None,group=<objectobject>,tag=0)",
            "descp": "tensor(Tensor) - Tensor to fill with received data."
        },
        {
            "fname": "torch.div.yaml",
            "arg": "out",
            "prompt": "Signature: torch.mean(input)\nParameter description: input(Tensor) - the input tensor.\nConstraints:\ndtype:Null\nstructure:tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.nn.Linear(in_features,out_features,bias=True)\nParameter description: in_features - size of each input sample\nConstraints:\ndtype:int\nstructure:Null\nshape:Null\nndim:Null\nenum:Null\nrange:[0,inf)\n\n###\n\nSignature: torch.combinations(input,r=2,with_replacement=False)\nParameter description: with_replacement(boolean, optional) - whether to allow duplication in combination\nConstraints:\ndtype:boolean;bool\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.rand_like(input,dtype=None,layout=None,device=None,requires_grad=False,memory_format=torch.preserve_format)\nParameter description: input(Tensor) - the size of `input` will determine size of the output tensor.\nConstraints:\ndtype:int\nstructure:tensor\nshape:Null\nndim:Null\nenum:Null\nrange:[0,inf)\n\n###\n\nSignature: torch.median(input)\nParameter description: input(Tensor) - the input tensor.\nConstraints:\ndtype:Null\nstructure:tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.tensordot(a,b,dims=2)\nParameter description: b(Tensor) - Right tensor to contract\nConstraints:\ndtype:Null\nstructure:tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.quantization.quantize(model,run_fn,run_args,mapping=None,inplace=False)\nParameter description: inplace - carry out model transformations in-place, the original module is mutated\nConstraints:\ndtype:bool\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.nn.SyncBatchNorm(num_features,eps=1e-05,momentum=0.1,affine=True,track_running_stats=True,process_group=None)\nParameter description: track_running_stats - a boolean value that when set to `True`, this module tracks the running mean and variance, and when set to `False`, this module does not track such statistics and always uses batch statistics in both training and eval modes. Default: `True`\nConstraints:\ndtype:boolean;bool\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.floor_divide(input,other,out=None)\nParameter description: other(Tensor or Scalar) - the denominator\nConstraints:\ndtype:Null\nstructure:tensor\nshape:Null\nndim:0\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.nn.AvgPool3d(kernel_size,stride=None,padding=0,ceil_mode=False,count_include_pad=True,divisor_override=None)\nParameter description: count_include_pad - when True, will include the zero-padding in the averaging calculation\nConstraints:\ndtype:bool\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.div(input,other,out=None)\nParameter description: out(Tensor, optional) - the output tensor.\nConstraints:",
            "constr": {
                "dtype": [],
                "structure": [
                    "tensor"
                ],
                "shape": [],
                "ndim": [],
                "enum": [],
                "range": []
            },
            "signature": "torch.div(input,other,out=None)",
            "descp": "out(Tensor, optional) - the output tensor."
        },
        {
            "fname": "torch.nn.transformerdecoder.yaml",
            "arg": "num_layers",
            "prompt": "Signature: torch.mean(input)\nParameter description: input(Tensor) - the input tensor.\nConstraints:\ndtype:Null\nstructure:tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.nn.Linear(in_features,out_features,bias=True)\nParameter description: in_features - size of each input sample\nConstraints:\ndtype:int\nstructure:Null\nshape:Null\nndim:Null\nenum:Null\nrange:[0,inf)\n\n###\n\nSignature: torch.combinations(input,r=2,with_replacement=False)\nParameter description: with_replacement(boolean, optional) - whether to allow duplication in combination\nConstraints:\ndtype:boolean;bool\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.rand_like(input,dtype=None,layout=None,device=None,requires_grad=False,memory_format=torch.preserve_format)\nParameter description: input(Tensor) - the size of `input` will determine size of the output tensor.\nConstraints:\ndtype:int\nstructure:tensor\nshape:Null\nndim:Null\nenum:Null\nrange:[0,inf)\n\n###\n\nSignature: torch.median(input)\nParameter description: input(Tensor) - the input tensor.\nConstraints:\ndtype:Null\nstructure:tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.tensordot(a,b,dims=2)\nParameter description: b(Tensor) - Right tensor to contract\nConstraints:\ndtype:Null\nstructure:tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.quantization.quantize(model,run_fn,run_args,mapping=None,inplace=False)\nParameter description: inplace - carry out model transformations in-place, the original module is mutated\nConstraints:\ndtype:bool\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.nn.SyncBatchNorm(num_features,eps=1e-05,momentum=0.1,affine=True,track_running_stats=True,process_group=None)\nParameter description: track_running_stats - a boolean value that when set to `True`, this module tracks the running mean and variance, and when set to `False`, this module does not track such statistics and always uses batch statistics in both training and eval modes. Default: `True`\nConstraints:\ndtype:boolean;bool\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.floor_divide(input,other,out=None)\nParameter description: other(Tensor or Scalar) - the denominator\nConstraints:\ndtype:Null\nstructure:tensor\nshape:Null\nndim:0\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.nn.AvgPool3d(kernel_size,stride=None,padding=0,ceil_mode=False,count_include_pad=True,divisor_override=None)\nParameter description: count_include_pad - when True, will include the zero-padding in the averaging calculation\nConstraints:\ndtype:bool\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.nn.TransformerDecoder(decoder_layer,num_layers,norm=None)\nParameter description: num_layers - the number of sub-decoder-layers in the decoder (required).\nConstraints:",
            "constr": {
                "dtype": [
                    "int"
                ],
                "structure": [],
                "shape": [],
                "ndim": [
                    "0"
                ],
                "enum": [],
                "range": [
                    "[0,inf)"
                ]
            },
            "signature": "torch.nn.TransformerDecoder(decoder_layer,num_layers,norm=None)",
            "descp": "num_layers - the number of sub-decoder-layers in the decoder (required)."
        },
        {
            "fname": "torch.nn.quantized.functional.interpolate.yaml",
            "arg": "align_corners",
            "prompt": "Signature: torch.mean(input)\nParameter description: input(Tensor) - the input tensor.\nConstraints:\ndtype:Null\nstructure:tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.nn.Linear(in_features,out_features,bias=True)\nParameter description: in_features - size of each input sample\nConstraints:\ndtype:int\nstructure:Null\nshape:Null\nndim:Null\nenum:Null\nrange:[0,inf)\n\n###\n\nSignature: torch.combinations(input,r=2,with_replacement=False)\nParameter description: with_replacement(boolean, optional) - whether to allow duplication in combination\nConstraints:\ndtype:boolean;bool\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.rand_like(input,dtype=None,layout=None,device=None,requires_grad=False,memory_format=torch.preserve_format)\nParameter description: input(Tensor) - the size of `input` will determine size of the output tensor.\nConstraints:\ndtype:int\nstructure:tensor\nshape:Null\nndim:Null\nenum:Null\nrange:[0,inf)\n\n###\n\nSignature: torch.median(input)\nParameter description: input(Tensor) - the input tensor.\nConstraints:\ndtype:Null\nstructure:tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.tensordot(a,b,dims=2)\nParameter description: b(Tensor) - Right tensor to contract\nConstraints:\ndtype:Null\nstructure:tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.quantization.quantize(model,run_fn,run_args,mapping=None,inplace=False)\nParameter description: inplace - carry out model transformations in-place, the original module is mutated\nConstraints:\ndtype:bool\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.nn.SyncBatchNorm(num_features,eps=1e-05,momentum=0.1,affine=True,track_running_stats=True,process_group=None)\nParameter description: track_running_stats - a boolean value that when set to `True`, this module tracks the running mean and variance, and when set to `False`, this module does not track such statistics and always uses batch statistics in both training and eval modes. Default: `True`\nConstraints:\ndtype:boolean;bool\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.floor_divide(input,other,out=None)\nParameter description: other(Tensor or Scalar) - the denominator\nConstraints:\ndtype:Null\nstructure:tensor\nshape:Null\nndim:0\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.nn.AvgPool3d(kernel_size,stride=None,padding=0,ceil_mode=False,count_include_pad=True,divisor_override=None)\nParameter description: count_include_pad - when True, will include the zero-padding in the averaging calculation\nConstraints:\ndtype:bool\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.nn.quantized.functional.interpolate(input,size=None,scale_factor=None,mode=nearest,align_corners=None)\nParameter description: align_corners(bool, optional) - Geometrically, we consider the pixels of the input and output as squares rather than points. If set to `True`, the input and output tensors are aligned by the center points of their corner pixels, preserving the values at the corner pixels. If set to `False`, the input and output tensors are aligned by the corner points of their corner pixels, and the interpolation uses edge value padding for out-of-boundary values, making this operation independent of input size when `scale_factor` is kept the same. This only has an effect when `mode` is `'bilinear'`. Default: `False`\nConstraints:",
            "constr": {
                "dtype": [
                    "bool"
                ],
                "structure": [],
                "shape": [],
                "ndim": [
                    "0"
                ],
                "enum": [],
                "range": []
            },
            "signature": "torch.nn.quantized.functional.interpolate(input,size=None,scale_factor=None,mode=nearest,align_corners=None)",
            "descp": "align_corners(bool, optional) - Geometrically, we consider the pixels of the input and output as squares rather than points. If set to `True`, the input and output tensors are aligned by the center points of their corner pixels, preserving the values at the corner pixels. If set to `False`, the input and output tensors are aligned by the corner points of their corner pixels, and the interpolation uses edge value padding for out-of-boundary values, making this operation independent of input size when `scale_factor` is kept the same. This only has an effect when `mode` is `'bilinear'`. Default: `False`"
        },
        {
            "fname": "torch.gt.yaml",
            "arg": "out",
            "prompt": "Signature: torch.mean(input)\nParameter description: input(Tensor) - the input tensor.\nConstraints:\ndtype:Null\nstructure:tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.nn.Linear(in_features,out_features,bias=True)\nParameter description: in_features - size of each input sample\nConstraints:\ndtype:int\nstructure:Null\nshape:Null\nndim:Null\nenum:Null\nrange:[0,inf)\n\n###\n\nSignature: torch.combinations(input,r=2,with_replacement=False)\nParameter description: with_replacement(boolean, optional) - whether to allow duplication in combination\nConstraints:\ndtype:boolean;bool\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.rand_like(input,dtype=None,layout=None,device=None,requires_grad=False,memory_format=torch.preserve_format)\nParameter description: input(Tensor) - the size of `input` will determine size of the output tensor.\nConstraints:\ndtype:int\nstructure:tensor\nshape:Null\nndim:Null\nenum:Null\nrange:[0,inf)\n\n###\n\nSignature: torch.median(input)\nParameter description: input(Tensor) - the input tensor.\nConstraints:\ndtype:Null\nstructure:tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.tensordot(a,b,dims=2)\nParameter description: b(Tensor) - Right tensor to contract\nConstraints:\ndtype:Null\nstructure:tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.quantization.quantize(model,run_fn,run_args,mapping=None,inplace=False)\nParameter description: inplace - carry out model transformations in-place, the original module is mutated\nConstraints:\ndtype:bool\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.nn.SyncBatchNorm(num_features,eps=1e-05,momentum=0.1,affine=True,track_running_stats=True,process_group=None)\nParameter description: track_running_stats - a boolean value that when set to `True`, this module tracks the running mean and variance, and when set to `False`, this module does not track such statistics and always uses batch statistics in both training and eval modes. Default: `True`\nConstraints:\ndtype:boolean;bool\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.floor_divide(input,other,out=None)\nParameter description: other(Tensor or Scalar) - the denominator\nConstraints:\ndtype:Null\nstructure:tensor\nshape:Null\nndim:0\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.nn.AvgPool3d(kernel_size,stride=None,padding=0,ceil_mode=False,count_include_pad=True,divisor_override=None)\nParameter description: count_include_pad - when True, will include the zero-padding in the averaging calculation\nConstraints:\ndtype:bool\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.gt(input,other,out=None)\nParameter description: out(Tensor, optional) - the output tensor that must be a BoolTensor\nConstraints:",
            "constr": {
                "dtype": [
                    "booltensor"
                ],
                "structure": [
                    "tensor"
                ],
                "shape": [],
                "ndim": [],
                "enum": [],
                "range": []
            },
            "signature": "torch.gt(input,other,out=None)",
            "descp": "out(Tensor, optional) - the output tensor that must be a BoolTensor"
        },
        {
            "fname": "torch.empty_like.yaml",
            "arg": "dtype",
            "prompt": "Signature: torch.mean(input)\nParameter description: input(Tensor) - the input tensor.\nConstraints:\ndtype:Null\nstructure:tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.nn.Linear(in_features,out_features,bias=True)\nParameter description: in_features - size of each input sample\nConstraints:\ndtype:int\nstructure:Null\nshape:Null\nndim:Null\nenum:Null\nrange:[0,inf)\n\n###\n\nSignature: torch.combinations(input,r=2,with_replacement=False)\nParameter description: with_replacement(boolean, optional) - whether to allow duplication in combination\nConstraints:\ndtype:boolean;bool\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.rand_like(input,dtype=None,layout=None,device=None,requires_grad=False,memory_format=torch.preserve_format)\nParameter description: input(Tensor) - the size of `input` will determine size of the output tensor.\nConstraints:\ndtype:int\nstructure:tensor\nshape:Null\nndim:Null\nenum:Null\nrange:[0,inf)\n\n###\n\nSignature: torch.median(input)\nParameter description: input(Tensor) - the input tensor.\nConstraints:\ndtype:Null\nstructure:tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.tensordot(a,b,dims=2)\nParameter description: b(Tensor) - Right tensor to contract\nConstraints:\ndtype:Null\nstructure:tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.quantization.quantize(model,run_fn,run_args,mapping=None,inplace=False)\nParameter description: inplace - carry out model transformations in-place, the original module is mutated\nConstraints:\ndtype:bool\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.nn.SyncBatchNorm(num_features,eps=1e-05,momentum=0.1,affine=True,track_running_stats=True,process_group=None)\nParameter description: track_running_stats - a boolean value that when set to `True`, this module tracks the running mean and variance, and when set to `False`, this module does not track such statistics and always uses batch statistics in both training and eval modes. Default: `True`\nConstraints:\ndtype:boolean;bool\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.floor_divide(input,other,out=None)\nParameter description: other(Tensor or Scalar) - the denominator\nConstraints:\ndtype:Null\nstructure:tensor\nshape:Null\nndim:0\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.nn.AvgPool3d(kernel_size,stride=None,padding=0,ceil_mode=False,count_include_pad=True,divisor_override=None)\nParameter description: count_include_pad - when True, will include the zero-padding in the averaging calculation\nConstraints:\ndtype:bool\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.empty_like(input,dtype=None,layout=None,device=None,requires_grad=False,memory_format=torch.preserve_format)\nParameter description: dtype(`torch.dtype`, optional) - the desired data type of returned Tensor. Default: if `None`, defaults to the dtype of `input`.\nConstraints:",
            "constr": {
                "dtype": [
                    "torch.dtype"
                ],
                "structure": [],
                "shape": [],
                "ndim": [],
                "enum": [],
                "range": []
            },
            "signature": "torch.empty_like(input,dtype=None,layout=None,device=None,requires_grad=False,memory_format=torch.preserve_format)",
            "descp": "dtype(`torch.dtype`, optional) - the desired data type of returned Tensor. Default: if `None`, defaults to the dtype of `input`."
        },
        {
            "fname": "torch.nn.rnn.yaml",
            "arg": "hidden_size",
            "prompt": "Signature: torch.mean(input)\nParameter description: input(Tensor) - the input tensor.\nConstraints:\ndtype:Null\nstructure:tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.nn.Linear(in_features,out_features,bias=True)\nParameter description: in_features - size of each input sample\nConstraints:\ndtype:int\nstructure:Null\nshape:Null\nndim:Null\nenum:Null\nrange:[0,inf)\n\n###\n\nSignature: torch.combinations(input,r=2,with_replacement=False)\nParameter description: with_replacement(boolean, optional) - whether to allow duplication in combination\nConstraints:\ndtype:boolean;bool\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.rand_like(input,dtype=None,layout=None,device=None,requires_grad=False,memory_format=torch.preserve_format)\nParameter description: input(Tensor) - the size of `input` will determine size of the output tensor.\nConstraints:\ndtype:int\nstructure:tensor\nshape:Null\nndim:Null\nenum:Null\nrange:[0,inf)\n\n###\n\nSignature: torch.median(input)\nParameter description: input(Tensor) - the input tensor.\nConstraints:\ndtype:Null\nstructure:tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.tensordot(a,b,dims=2)\nParameter description: b(Tensor) - Right tensor to contract\nConstraints:\ndtype:Null\nstructure:tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.quantization.quantize(model,run_fn,run_args,mapping=None,inplace=False)\nParameter description: inplace - carry out model transformations in-place, the original module is mutated\nConstraints:\ndtype:bool\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.nn.SyncBatchNorm(num_features,eps=1e-05,momentum=0.1,affine=True,track_running_stats=True,process_group=None)\nParameter description: track_running_stats - a boolean value that when set to `True`, this module tracks the running mean and variance, and when set to `False`, this module does not track such statistics and always uses batch statistics in both training and eval modes. Default: `True`\nConstraints:\ndtype:boolean;bool\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.floor_divide(input,other,out=None)\nParameter description: other(Tensor or Scalar) - the denominator\nConstraints:\ndtype:Null\nstructure:tensor\nshape:Null\nndim:0\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.nn.AvgPool3d(kernel_size,stride=None,padding=0,ceil_mode=False,count_include_pad=True,divisor_override=None)\nParameter description: count_include_pad - when True, will include the zero-padding in the averaging calculation\nConstraints:\ndtype:bool\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.nn.RNN(*args,**kwargs,input_size=None,hidden_size=None,num_layers=None,nonlinearity=None,bias=None,batch_first=None,dropout=None,bidirectional=None)\nParameter description: hidden_size - The number of features in the hidden state h\nConstraints:",
            "constr": {
                "dtype": [
                    "int"
                ],
                "structure": [],
                "shape": [],
                "ndim": [
                    "0"
                ],
                "enum": [],
                "range": [
                    "[0,inf)"
                ]
            },
            "signature": "torch.nn.RNN(*args,**kwargs,input_size=None,hidden_size=None,num_layers=None,nonlinearity=None,bias=None,batch_first=None,dropout=None,bidirectional=None)",
            "descp": "hidden_size - The number of features in the hidden state h"
        },
        {
            "fname": "torch.nn.functional.avg_pool1d.yaml",
            "arg": "padding",
            "prompt": "Signature: torch.mean(input)\nParameter description: input(Tensor) - the input tensor.\nConstraints:\ndtype:Null\nstructure:tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.nn.Linear(in_features,out_features,bias=True)\nParameter description: in_features - size of each input sample\nConstraints:\ndtype:int\nstructure:Null\nshape:Null\nndim:Null\nenum:Null\nrange:[0,inf)\n\n###\n\nSignature: torch.combinations(input,r=2,with_replacement=False)\nParameter description: with_replacement(boolean, optional) - whether to allow duplication in combination\nConstraints:\ndtype:boolean;bool\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.rand_like(input,dtype=None,layout=None,device=None,requires_grad=False,memory_format=torch.preserve_format)\nParameter description: input(Tensor) - the size of `input` will determine size of the output tensor.\nConstraints:\ndtype:int\nstructure:tensor\nshape:Null\nndim:Null\nenum:Null\nrange:[0,inf)\n\n###\n\nSignature: torch.median(input)\nParameter description: input(Tensor) - the input tensor.\nConstraints:\ndtype:Null\nstructure:tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.tensordot(a,b,dims=2)\nParameter description: b(Tensor) - Right tensor to contract\nConstraints:\ndtype:Null\nstructure:tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.quantization.quantize(model,run_fn,run_args,mapping=None,inplace=False)\nParameter description: inplace - carry out model transformations in-place, the original module is mutated\nConstraints:\ndtype:bool\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.nn.SyncBatchNorm(num_features,eps=1e-05,momentum=0.1,affine=True,track_running_stats=True,process_group=None)\nParameter description: track_running_stats - a boolean value that when set to `True`, this module tracks the running mean and variance, and when set to `False`, this module does not track such statistics and always uses batch statistics in both training and eval modes. Default: `True`\nConstraints:\ndtype:boolean;bool\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.floor_divide(input,other,out=None)\nParameter description: other(Tensor or Scalar) - the denominator\nConstraints:\ndtype:Null\nstructure:tensor\nshape:Null\nndim:0\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.nn.AvgPool3d(kernel_size,stride=None,padding=0,ceil_mode=False,count_include_pad=True,divisor_override=None)\nParameter description: count_include_pad - when True, will include the zero-padding in the averaging calculation\nConstraints:\ndtype:bool\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.nn.functional.avg_pool1d(input,kernel_size,stride=None,padding=0,ceil_mode=False,count_include_pad=True)\nParameter description: padding - implicit zero paddings on both sides of the input. Can be a single number or a tuple (padW,). Default: 0\nConstraints:",
            "constr": {
                "dtype": [
                    "bool",
                    "int"
                ],
                "structure": [
                    "tuple"
                ],
                "shape": [],
                "ndim": [
                    "0",
                    "1"
                ],
                "enum": [],
                "range": []
            },
            "signature": "torch.nn.functional.avg_pool1d(input,kernel_size,stride=None,padding=0,ceil_mode=False,count_include_pad=True)",
            "descp": "padding - implicit zero paddings on both sides of the input. Can be a single number or a tuple (padW,). Default: 0"
        },
        {
            "fname": "torch.autograd.grad.yaml",
            "arg": "outputs",
            "prompt": "Signature: torch.mean(input)\nParameter description: input(Tensor) - the input tensor.\nConstraints:\ndtype:Null\nstructure:tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.nn.Linear(in_features,out_features,bias=True)\nParameter description: in_features - size of each input sample\nConstraints:\ndtype:int\nstructure:Null\nshape:Null\nndim:Null\nenum:Null\nrange:[0,inf)\n\n###\n\nSignature: torch.combinations(input,r=2,with_replacement=False)\nParameter description: with_replacement(boolean, optional) - whether to allow duplication in combination\nConstraints:\ndtype:boolean;bool\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.rand_like(input,dtype=None,layout=None,device=None,requires_grad=False,memory_format=torch.preserve_format)\nParameter description: input(Tensor) - the size of `input` will determine size of the output tensor.\nConstraints:\ndtype:int\nstructure:tensor\nshape:Null\nndim:Null\nenum:Null\nrange:[0,inf)\n\n###\n\nSignature: torch.median(input)\nParameter description: input(Tensor) - the input tensor.\nConstraints:\ndtype:Null\nstructure:tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.tensordot(a,b,dims=2)\nParameter description: b(Tensor) - Right tensor to contract\nConstraints:\ndtype:Null\nstructure:tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.quantization.quantize(model,run_fn,run_args,mapping=None,inplace=False)\nParameter description: inplace - carry out model transformations in-place, the original module is mutated\nConstraints:\ndtype:bool\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.nn.SyncBatchNorm(num_features,eps=1e-05,momentum=0.1,affine=True,track_running_stats=True,process_group=None)\nParameter description: track_running_stats - a boolean value that when set to `True`, this module tracks the running mean and variance, and when set to `False`, this module does not track such statistics and always uses batch statistics in both training and eval modes. Default: `True`\nConstraints:\ndtype:boolean;bool\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.floor_divide(input,other,out=None)\nParameter description: other(Tensor or Scalar) - the denominator\nConstraints:\ndtype:Null\nstructure:tensor\nshape:Null\nndim:0\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.nn.AvgPool3d(kernel_size,stride=None,padding=0,ceil_mode=False,count_include_pad=True,divisor_override=None)\nParameter description: count_include_pad - when True, will include the zero-padding in the averaging calculation\nConstraints:\ndtype:bool\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.autograd.grad(outputs,inputs,grad_outputs=None,retain_graph=None,create_graph=False,only_inputs=True,allow_unused=False)\nParameter description: outputs(sequence of Tensor) - outputs of the differentiated function.\nConstraints:",
            "constr": {
                "dtype": [],
                "structure": [
                    "sequence",
                    "tensor"
                ],
                "shape": [],
                "ndim": [],
                "enum": [],
                "range": []
            },
            "signature": "torch.autograd.grad(outputs,inputs,grad_outputs=None,retain_graph=None,create_graph=False,only_inputs=True,allow_unused=False)",
            "descp": "outputs(sequence of Tensor) - outputs of the differentiated function."
        },
        {
            "fname": "torch.repeat_interleave.yaml",
            "arg": "dim",
            "prompt": "Signature: torch.mean(input)\nParameter description: input(Tensor) - the input tensor.\nConstraints:\ndtype:Null\nstructure:tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.nn.Linear(in_features,out_features,bias=True)\nParameter description: in_features - size of each input sample\nConstraints:\ndtype:int\nstructure:Null\nshape:Null\nndim:Null\nenum:Null\nrange:[0,inf)\n\n###\n\nSignature: torch.combinations(input,r=2,with_replacement=False)\nParameter description: with_replacement(boolean, optional) - whether to allow duplication in combination\nConstraints:\ndtype:boolean;bool\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.rand_like(input,dtype=None,layout=None,device=None,requires_grad=False,memory_format=torch.preserve_format)\nParameter description: input(Tensor) - the size of `input` will determine size of the output tensor.\nConstraints:\ndtype:int\nstructure:tensor\nshape:Null\nndim:Null\nenum:Null\nrange:[0,inf)\n\n###\n\nSignature: torch.median(input)\nParameter description: input(Tensor) - the input tensor.\nConstraints:\ndtype:Null\nstructure:tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.tensordot(a,b,dims=2)\nParameter description: b(Tensor) - Right tensor to contract\nConstraints:\ndtype:Null\nstructure:tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.quantization.quantize(model,run_fn,run_args,mapping=None,inplace=False)\nParameter description: inplace - carry out model transformations in-place, the original module is mutated\nConstraints:\ndtype:bool\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.nn.SyncBatchNorm(num_features,eps=1e-05,momentum=0.1,affine=True,track_running_stats=True,process_group=None)\nParameter description: track_running_stats - a boolean value that when set to `True`, this module tracks the running mean and variance, and when set to `False`, this module does not track such statistics and always uses batch statistics in both training and eval modes. Default: `True`\nConstraints:\ndtype:boolean;bool\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.floor_divide(input,other,out=None)\nParameter description: other(Tensor or Scalar) - the denominator\nConstraints:\ndtype:Null\nstructure:tensor\nshape:Null\nndim:0\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.nn.AvgPool3d(kernel_size,stride=None,padding=0,ceil_mode=False,count_include_pad=True,divisor_override=None)\nParameter description: count_include_pad - when True, will include the zero-padding in the averaging calculation\nConstraints:\ndtype:bool\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.repeat_interleave(input,repeats,dim=None)\nParameter description: dim(int, optional) - The dimension along which to repeat values.By default, use the flattened input array, and return a flat outputarray.\nConstraints:",
            "constr": {
                "dtype": [
                    "int"
                ],
                "structure": [],
                "shape": [],
                "ndim": [
                    "0"
                ],
                "enum": [],
                "range": []
            },
            "signature": "torch.repeat_interleave(input,repeats,dim=None)",
            "descp": "dim(int, optional) - The dimension along which to repeat values.By default, use the flattened input array, and return a flat outputarray."
        },
        {
            "fname": "torch.roll.yaml",
            "arg": "shifts",
            "prompt": "Signature: torch.mean(input)\nParameter description: input(Tensor) - the input tensor.\nConstraints:\ndtype:Null\nstructure:tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.nn.Linear(in_features,out_features,bias=True)\nParameter description: in_features - size of each input sample\nConstraints:\ndtype:int\nstructure:Null\nshape:Null\nndim:Null\nenum:Null\nrange:[0,inf)\n\n###\n\nSignature: torch.combinations(input,r=2,with_replacement=False)\nParameter description: with_replacement(boolean, optional) - whether to allow duplication in combination\nConstraints:\ndtype:boolean;bool\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.rand_like(input,dtype=None,layout=None,device=None,requires_grad=False,memory_format=torch.preserve_format)\nParameter description: input(Tensor) - the size of `input` will determine size of the output tensor.\nConstraints:\ndtype:int\nstructure:tensor\nshape:Null\nndim:Null\nenum:Null\nrange:[0,inf)\n\n###\n\nSignature: torch.median(input)\nParameter description: input(Tensor) - the input tensor.\nConstraints:\ndtype:Null\nstructure:tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.tensordot(a,b,dims=2)\nParameter description: b(Tensor) - Right tensor to contract\nConstraints:\ndtype:Null\nstructure:tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.quantization.quantize(model,run_fn,run_args,mapping=None,inplace=False)\nParameter description: inplace - carry out model transformations in-place, the original module is mutated\nConstraints:\ndtype:bool\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.nn.SyncBatchNorm(num_features,eps=1e-05,momentum=0.1,affine=True,track_running_stats=True,process_group=None)\nParameter description: track_running_stats - a boolean value that when set to `True`, this module tracks the running mean and variance, and when set to `False`, this module does not track such statistics and always uses batch statistics in both training and eval modes. Default: `True`\nConstraints:\ndtype:boolean;bool\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.floor_divide(input,other,out=None)\nParameter description: other(Tensor or Scalar) - the denominator\nConstraints:\ndtype:Null\nstructure:tensor\nshape:Null\nndim:0\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.nn.AvgPool3d(kernel_size,stride=None,padding=0,ceil_mode=False,count_include_pad=True,divisor_override=None)\nParameter description: count_include_pad - when True, will include the zero-padding in the averaging calculation\nConstraints:\ndtype:bool\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.roll(input,shifts,dims=None)\nParameter description: shifts(int or tuple of python:ints) - The number of places by which the elements of the tensor are shifted. If shifts is a tuple, dims must be a tuple of the same size, and each dimension will be rolled by the corresponding value\nConstraints:",
            "constr": {
                "dtype": [
                    "int"
                ],
                "structure": [
                    "tuple"
                ],
                "shape": [],
                "ndim": [
                    "0",
                    "1"
                ],
                "enum": [],
                "range": [
                    "[0,inf)"
                ]
            },
            "signature": "torch.roll(input,shifts,dims=None)",
            "descp": "shifts(int or tuple of python:ints) - The number of places by which the elements of the tensor are shifted. If shifts is a tuple, dims must be a tuple of the same size, and each dimension will be rolled by the corresponding value"
        },
        {
            "fname": "torch.randperm.yaml",
            "arg": "dtype",
            "prompt": "Signature: torch.mean(input)\nParameter description: input(Tensor) - the input tensor.\nConstraints:\ndtype:Null\nstructure:tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.nn.Linear(in_features,out_features,bias=True)\nParameter description: in_features - size of each input sample\nConstraints:\ndtype:int\nstructure:Null\nshape:Null\nndim:Null\nenum:Null\nrange:[0,inf)\n\n###\n\nSignature: torch.combinations(input,r=2,with_replacement=False)\nParameter description: with_replacement(boolean, optional) - whether to allow duplication in combination\nConstraints:\ndtype:boolean;bool\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.rand_like(input,dtype=None,layout=None,device=None,requires_grad=False,memory_format=torch.preserve_format)\nParameter description: input(Tensor) - the size of `input` will determine size of the output tensor.\nConstraints:\ndtype:int\nstructure:tensor\nshape:Null\nndim:Null\nenum:Null\nrange:[0,inf)\n\n###\n\nSignature: torch.median(input)\nParameter description: input(Tensor) - the input tensor.\nConstraints:\ndtype:Null\nstructure:tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.tensordot(a,b,dims=2)\nParameter description: b(Tensor) - Right tensor to contract\nConstraints:\ndtype:Null\nstructure:tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.quantization.quantize(model,run_fn,run_args,mapping=None,inplace=False)\nParameter description: inplace - carry out model transformations in-place, the original module is mutated\nConstraints:\ndtype:bool\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.nn.SyncBatchNorm(num_features,eps=1e-05,momentum=0.1,affine=True,track_running_stats=True,process_group=None)\nParameter description: track_running_stats - a boolean value that when set to `True`, this module tracks the running mean and variance, and when set to `False`, this module does not track such statistics and always uses batch statistics in both training and eval modes. Default: `True`\nConstraints:\ndtype:boolean;bool\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.floor_divide(input,other,out=None)\nParameter description: other(Tensor or Scalar) - the denominator\nConstraints:\ndtype:Null\nstructure:tensor\nshape:Null\nndim:0\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.nn.AvgPool3d(kernel_size,stride=None,padding=0,ceil_mode=False,count_include_pad=True,divisor_override=None)\nParameter description: count_include_pad - when True, will include the zero-padding in the averaging calculation\nConstraints:\ndtype:bool\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.randperm(n,out=None,dtype=torch.int64,layout=torch.strided,device=None,requires_grad=False)\nParameter description: dtype(`torch.dtype`, optional) - the desired data type of returned tensor. Default: `torch.int64`.\nConstraints:",
            "constr": {
                "dtype": [
                    "torch.dtype"
                ],
                "structure": [],
                "shape": [],
                "ndim": [],
                "enum": [],
                "range": []
            },
            "signature": "torch.randperm(n,out=None,dtype=torch.int64,layout=torch.strided,device=None,requires_grad=False)",
            "descp": "dtype(`torch.dtype`, optional) - the desired data type of returned tensor. Default: `torch.int64`."
        },
        {
            "fname": "torch.nn.softmarginloss.yaml",
            "arg": "size_average",
            "prompt": "Signature: torch.mean(input)\nParameter description: input(Tensor) - the input tensor.\nConstraints:\ndtype:Null\nstructure:tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.nn.Linear(in_features,out_features,bias=True)\nParameter description: in_features - size of each input sample\nConstraints:\ndtype:int\nstructure:Null\nshape:Null\nndim:Null\nenum:Null\nrange:[0,inf)\n\n###\n\nSignature: torch.combinations(input,r=2,with_replacement=False)\nParameter description: with_replacement(boolean, optional) - whether to allow duplication in combination\nConstraints:\ndtype:boolean;bool\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.rand_like(input,dtype=None,layout=None,device=None,requires_grad=False,memory_format=torch.preserve_format)\nParameter description: input(Tensor) - the size of `input` will determine size of the output tensor.\nConstraints:\ndtype:int\nstructure:tensor\nshape:Null\nndim:Null\nenum:Null\nrange:[0,inf)\n\n###\n\nSignature: torch.median(input)\nParameter description: input(Tensor) - the input tensor.\nConstraints:\ndtype:Null\nstructure:tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.tensordot(a,b,dims=2)\nParameter description: b(Tensor) - Right tensor to contract\nConstraints:\ndtype:Null\nstructure:tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.quantization.quantize(model,run_fn,run_args,mapping=None,inplace=False)\nParameter description: inplace - carry out model transformations in-place, the original module is mutated\nConstraints:\ndtype:bool\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.nn.SyncBatchNorm(num_features,eps=1e-05,momentum=0.1,affine=True,track_running_stats=True,process_group=None)\nParameter description: track_running_stats - a boolean value that when set to `True`, this module tracks the running mean and variance, and when set to `False`, this module does not track such statistics and always uses batch statistics in both training and eval modes. Default: `True`\nConstraints:\ndtype:boolean;bool\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.floor_divide(input,other,out=None)\nParameter description: other(Tensor or Scalar) - the denominator\nConstraints:\ndtype:Null\nstructure:tensor\nshape:Null\nndim:0\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.nn.AvgPool3d(kernel_size,stride=None,padding=0,ceil_mode=False,count_include_pad=True,divisor_override=None)\nParameter description: count_include_pad - when True, will include the zero-padding in the averaging calculation\nConstraints:\ndtype:bool\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.nn.SoftMarginLoss(size_average=None,reduce=None,reduction=mean)\nParameter description: size_average(bool, optional) - Deprecated (see `reduction`). By default, the losses are averaged over each loss element in the batch. Note that for some losses, there are multiple elements per sample. If the field `size_average` is set to `False`, the losses are instead summed for each minibatch. Ignored when reduce is `False`. Default: `True`\nConstraints:",
            "constr": {
                "dtype": [
                    "bool"
                ],
                "structure": [],
                "shape": [],
                "ndim": [
                    "0"
                ],
                "enum": [],
                "range": []
            },
            "signature": "torch.nn.SoftMarginLoss(size_average=None,reduce=None,reduction=mean)",
            "descp": "size_average(bool, optional) - Deprecated (see `reduction`). By default, the losses are averaged over each loss element in the batch. Note that for some losses, there are multiple elements per sample. If the field `size_average` is set to `False`, the losses are instead summed for each minibatch. Ignored when reduce is `False`. Default: `True`"
        },
        {
            "fname": "torch.erf.yaml",
            "arg": "out",
            "prompt": "Signature: torch.mean(input)\nParameter description: input(Tensor) - the input tensor.\nConstraints:\ndtype:Null\nstructure:tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.nn.Linear(in_features,out_features,bias=True)\nParameter description: in_features - size of each input sample\nConstraints:\ndtype:int\nstructure:Null\nshape:Null\nndim:Null\nenum:Null\nrange:[0,inf)\n\n###\n\nSignature: torch.combinations(input,r=2,with_replacement=False)\nParameter description: with_replacement(boolean, optional) - whether to allow duplication in combination\nConstraints:\ndtype:boolean;bool\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.rand_like(input,dtype=None,layout=None,device=None,requires_grad=False,memory_format=torch.preserve_format)\nParameter description: input(Tensor) - the size of `input` will determine size of the output tensor.\nConstraints:\ndtype:int\nstructure:tensor\nshape:Null\nndim:Null\nenum:Null\nrange:[0,inf)\n\n###\n\nSignature: torch.median(input)\nParameter description: input(Tensor) - the input tensor.\nConstraints:\ndtype:Null\nstructure:tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.tensordot(a,b,dims=2)\nParameter description: b(Tensor) - Right tensor to contract\nConstraints:\ndtype:Null\nstructure:tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.quantization.quantize(model,run_fn,run_args,mapping=None,inplace=False)\nParameter description: inplace - carry out model transformations in-place, the original module is mutated\nConstraints:\ndtype:bool\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.nn.SyncBatchNorm(num_features,eps=1e-05,momentum=0.1,affine=True,track_running_stats=True,process_group=None)\nParameter description: track_running_stats - a boolean value that when set to `True`, this module tracks the running mean and variance, and when set to `False`, this module does not track such statistics and always uses batch statistics in both training and eval modes. Default: `True`\nConstraints:\ndtype:boolean;bool\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.floor_divide(input,other,out=None)\nParameter description: other(Tensor or Scalar) - the denominator\nConstraints:\ndtype:Null\nstructure:tensor\nshape:Null\nndim:0\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.nn.AvgPool3d(kernel_size,stride=None,padding=0,ceil_mode=False,count_include_pad=True,divisor_override=None)\nParameter description: count_include_pad - when True, will include the zero-padding in the averaging calculation\nConstraints:\ndtype:bool\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.erf(input,out=None)\nParameter description: out(Tensor, optional) - the output tensor.\nConstraints:",
            "constr": {
                "dtype": [],
                "structure": [
                    "tensor"
                ],
                "shape": [],
                "ndim": [],
                "enum": [],
                "range": []
            },
            "signature": "torch.erf(input,out=None)",
            "descp": "out(Tensor, optional) - the output tensor."
        },
        {
            "fname": "torch.triu_indices.yaml",
            "arg": "row",
            "prompt": "Signature: torch.mean(input)\nParameter description: input(Tensor) - the input tensor.\nConstraints:\ndtype:Null\nstructure:tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.nn.Linear(in_features,out_features,bias=True)\nParameter description: in_features - size of each input sample\nConstraints:\ndtype:int\nstructure:Null\nshape:Null\nndim:Null\nenum:Null\nrange:[0,inf)\n\n###\n\nSignature: torch.combinations(input,r=2,with_replacement=False)\nParameter description: with_replacement(boolean, optional) - whether to allow duplication in combination\nConstraints:\ndtype:boolean;bool\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.rand_like(input,dtype=None,layout=None,device=None,requires_grad=False,memory_format=torch.preserve_format)\nParameter description: input(Tensor) - the size of `input` will determine size of the output tensor.\nConstraints:\ndtype:int\nstructure:tensor\nshape:Null\nndim:Null\nenum:Null\nrange:[0,inf)\n\n###\n\nSignature: torch.median(input)\nParameter description: input(Tensor) - the input tensor.\nConstraints:\ndtype:Null\nstructure:tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.tensordot(a,b,dims=2)\nParameter description: b(Tensor) - Right tensor to contract\nConstraints:\ndtype:Null\nstructure:tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.quantization.quantize(model,run_fn,run_args,mapping=None,inplace=False)\nParameter description: inplace - carry out model transformations in-place, the original module is mutated\nConstraints:\ndtype:bool\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.nn.SyncBatchNorm(num_features,eps=1e-05,momentum=0.1,affine=True,track_running_stats=True,process_group=None)\nParameter description: track_running_stats - a boolean value that when set to `True`, this module tracks the running mean and variance, and when set to `False`, this module does not track such statistics and always uses batch statistics in both training and eval modes. Default: `True`\nConstraints:\ndtype:boolean;bool\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.floor_divide(input,other,out=None)\nParameter description: other(Tensor or Scalar) - the denominator\nConstraints:\ndtype:Null\nstructure:tensor\nshape:Null\nndim:0\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.nn.AvgPool3d(kernel_size,stride=None,padding=0,ceil_mode=False,count_include_pad=True,divisor_override=None)\nParameter description: count_include_pad - when True, will include the zero-padding in the averaging calculation\nConstraints:\ndtype:bool\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.triu_indices(row,col,offset=0,dtype=torch.long,device=cpu,layout=torch.strided)\nParameter description: row(`int`) - number of rows in the 2-D matrix.\nConstraints:",
            "constr": {
                "dtype": [
                    "int"
                ],
                "structure": [],
                "shape": [],
                "ndim": [
                    "0"
                ],
                "enum": [],
                "range": [
                    "[0,inf)"
                ]
            },
            "signature": "torch.triu_indices(row,col,offset=0,dtype=torch.long,device=cpu,layout=torch.strided)",
            "descp": "row(`int`) - number of rows in the 2-D matrix."
        },
        {
            "fname": "torch.unique.yaml",
            "arg": "input",
            "prompt": "Signature: torch.mean(input)\nParameter description: input(Tensor) - the input tensor.\nConstraints:\ndtype:Null\nstructure:tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.nn.Linear(in_features,out_features,bias=True)\nParameter description: in_features - size of each input sample\nConstraints:\ndtype:int\nstructure:Null\nshape:Null\nndim:Null\nenum:Null\nrange:[0,inf)\n\n###\n\nSignature: torch.combinations(input,r=2,with_replacement=False)\nParameter description: with_replacement(boolean, optional) - whether to allow duplication in combination\nConstraints:\ndtype:boolean;bool\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.rand_like(input,dtype=None,layout=None,device=None,requires_grad=False,memory_format=torch.preserve_format)\nParameter description: input(Tensor) - the size of `input` will determine size of the output tensor.\nConstraints:\ndtype:int\nstructure:tensor\nshape:Null\nndim:Null\nenum:Null\nrange:[0,inf)\n\n###\n\nSignature: torch.median(input)\nParameter description: input(Tensor) - the input tensor.\nConstraints:\ndtype:Null\nstructure:tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.tensordot(a,b,dims=2)\nParameter description: b(Tensor) - Right tensor to contract\nConstraints:\ndtype:Null\nstructure:tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.quantization.quantize(model,run_fn,run_args,mapping=None,inplace=False)\nParameter description: inplace - carry out model transformations in-place, the original module is mutated\nConstraints:\ndtype:bool\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.nn.SyncBatchNorm(num_features,eps=1e-05,momentum=0.1,affine=True,track_running_stats=True,process_group=None)\nParameter description: track_running_stats - a boolean value that when set to `True`, this module tracks the running mean and variance, and when set to `False`, this module does not track such statistics and always uses batch statistics in both training and eval modes. Default: `True`\nConstraints:\ndtype:boolean;bool\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.floor_divide(input,other,out=None)\nParameter description: other(Tensor or Scalar) - the denominator\nConstraints:\ndtype:Null\nstructure:tensor\nshape:Null\nndim:0\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.nn.AvgPool3d(kernel_size,stride=None,padding=0,ceil_mode=False,count_include_pad=True,divisor_override=None)\nParameter description: count_include_pad - when True, will include the zero-padding in the averaging calculation\nConstraints:\ndtype:bool\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.unique(input,sorted=True,return_inverse=False,return_counts=False,dim=None)\nParameter description: input(Tensor) - the input tensor\nConstraints:",
            "constr": {
                "dtype": [],
                "structure": [
                    "tensor"
                ],
                "shape": [],
                "ndim": [],
                "enum": [],
                "range": []
            },
            "signature": "torch.unique(input,sorted=True,return_inverse=False,return_counts=False,dim=None)",
            "descp": "input(Tensor) - the input tensor"
        },
        {
            "fname": "torch.nn.adaptivemaxpool3d.yaml",
            "arg": "return_indices",
            "prompt": "Signature: torch.mean(input)\nParameter description: input(Tensor) - the input tensor.\nConstraints:\ndtype:Null\nstructure:tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.nn.Linear(in_features,out_features,bias=True)\nParameter description: in_features - size of each input sample\nConstraints:\ndtype:int\nstructure:Null\nshape:Null\nndim:Null\nenum:Null\nrange:[0,inf)\n\n###\n\nSignature: torch.combinations(input,r=2,with_replacement=False)\nParameter description: with_replacement(boolean, optional) - whether to allow duplication in combination\nConstraints:\ndtype:boolean;bool\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.rand_like(input,dtype=None,layout=None,device=None,requires_grad=False,memory_format=torch.preserve_format)\nParameter description: input(Tensor) - the size of `input` will determine size of the output tensor.\nConstraints:\ndtype:int\nstructure:tensor\nshape:Null\nndim:Null\nenum:Null\nrange:[0,inf)\n\n###\n\nSignature: torch.median(input)\nParameter description: input(Tensor) - the input tensor.\nConstraints:\ndtype:Null\nstructure:tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.tensordot(a,b,dims=2)\nParameter description: b(Tensor) - Right tensor to contract\nConstraints:\ndtype:Null\nstructure:tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.quantization.quantize(model,run_fn,run_args,mapping=None,inplace=False)\nParameter description: inplace - carry out model transformations in-place, the original module is mutated\nConstraints:\ndtype:bool\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.nn.SyncBatchNorm(num_features,eps=1e-05,momentum=0.1,affine=True,track_running_stats=True,process_group=None)\nParameter description: track_running_stats - a boolean value that when set to `True`, this module tracks the running mean and variance, and when set to `False`, this module does not track such statistics and always uses batch statistics in both training and eval modes. Default: `True`\nConstraints:\ndtype:boolean;bool\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.floor_divide(input,other,out=None)\nParameter description: other(Tensor or Scalar) - the denominator\nConstraints:\ndtype:Null\nstructure:tensor\nshape:Null\nndim:0\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.nn.AvgPool3d(kernel_size,stride=None,padding=0,ceil_mode=False,count_include_pad=True,divisor_override=None)\nParameter description: count_include_pad - when True, will include the zero-padding in the averaging calculation\nConstraints:\ndtype:bool\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.nn.AdaptiveMaxPool3d(output_size,return_indices=False)\nParameter description: return_indices - if `True`, will return the indices along with the outputs. Useful to pass to nn.MaxUnpool3d. Default: `False`\nConstraints:",
            "constr": {
                "dtype": [
                    "bool"
                ],
                "structure": [],
                "shape": [],
                "ndim": [
                    "0"
                ],
                "enum": [],
                "range": []
            },
            "signature": "torch.nn.AdaptiveMaxPool3d(output_size,return_indices=False)",
            "descp": "return_indices - if `True`, will return the indices along with the outputs. Useful to pass to nn.MaxUnpool3d. Default: `False`"
        },
        {
            "fname": "torch.nn.gru.yaml",
            "arg": "hidden_size",
            "prompt": "Signature: torch.mean(input)\nParameter description: input(Tensor) - the input tensor.\nConstraints:\ndtype:Null\nstructure:tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.nn.Linear(in_features,out_features,bias=True)\nParameter description: in_features - size of each input sample\nConstraints:\ndtype:int\nstructure:Null\nshape:Null\nndim:Null\nenum:Null\nrange:[0,inf)\n\n###\n\nSignature: torch.combinations(input,r=2,with_replacement=False)\nParameter description: with_replacement(boolean, optional) - whether to allow duplication in combination\nConstraints:\ndtype:boolean;bool\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.rand_like(input,dtype=None,layout=None,device=None,requires_grad=False,memory_format=torch.preserve_format)\nParameter description: input(Tensor) - the size of `input` will determine size of the output tensor.\nConstraints:\ndtype:int\nstructure:tensor\nshape:Null\nndim:Null\nenum:Null\nrange:[0,inf)\n\n###\n\nSignature: torch.median(input)\nParameter description: input(Tensor) - the input tensor.\nConstraints:\ndtype:Null\nstructure:tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.tensordot(a,b,dims=2)\nParameter description: b(Tensor) - Right tensor to contract\nConstraints:\ndtype:Null\nstructure:tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.quantization.quantize(model,run_fn,run_args,mapping=None,inplace=False)\nParameter description: inplace - carry out model transformations in-place, the original module is mutated\nConstraints:\ndtype:bool\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.nn.SyncBatchNorm(num_features,eps=1e-05,momentum=0.1,affine=True,track_running_stats=True,process_group=None)\nParameter description: track_running_stats - a boolean value that when set to `True`, this module tracks the running mean and variance, and when set to `False`, this module does not track such statistics and always uses batch statistics in both training and eval modes. Default: `True`\nConstraints:\ndtype:boolean;bool\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.floor_divide(input,other,out=None)\nParameter description: other(Tensor or Scalar) - the denominator\nConstraints:\ndtype:Null\nstructure:tensor\nshape:Null\nndim:0\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.nn.AvgPool3d(kernel_size,stride=None,padding=0,ceil_mode=False,count_include_pad=True,divisor_override=None)\nParameter description: count_include_pad - when True, will include the zero-padding in the averaging calculation\nConstraints:\ndtype:bool\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.nn.GRU(*args,**kwargs,input_size=None,hidden_size=None,num_layers=None,bias=None,batch_first=None,dropout=None,bidirectional=None)\nParameter description: hidden_size - The number of features in the hidden state h\nConstraints:",
            "constr": {
                "dtype": [
                    "int"
                ],
                "structure": [],
                "shape": [],
                "ndim": [
                    "0"
                ],
                "enum": [],
                "range": [
                    "[0,inf)"
                ]
            },
            "signature": "torch.nn.GRU(*args,**kwargs,input_size=None,hidden_size=None,num_layers=None,bias=None,batch_first=None,dropout=None,bidirectional=None)",
            "descp": "hidden_size - The number of features in the hidden state h"
        },
        {
            "fname": "torch.trapz.yaml",
            "arg": "dim",
            "prompt": "Signature: torch.mean(input)\nParameter description: input(Tensor) - the input tensor.\nConstraints:\ndtype:Null\nstructure:tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.nn.Linear(in_features,out_features,bias=True)\nParameter description: in_features - size of each input sample\nConstraints:\ndtype:int\nstructure:Null\nshape:Null\nndim:Null\nenum:Null\nrange:[0,inf)\n\n###\n\nSignature: torch.combinations(input,r=2,with_replacement=False)\nParameter description: with_replacement(boolean, optional) - whether to allow duplication in combination\nConstraints:\ndtype:boolean;bool\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.rand_like(input,dtype=None,layout=None,device=None,requires_grad=False,memory_format=torch.preserve_format)\nParameter description: input(Tensor) - the size of `input` will determine size of the output tensor.\nConstraints:\ndtype:int\nstructure:tensor\nshape:Null\nndim:Null\nenum:Null\nrange:[0,inf)\n\n###\n\nSignature: torch.median(input)\nParameter description: input(Tensor) - the input tensor.\nConstraints:\ndtype:Null\nstructure:tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.tensordot(a,b,dims=2)\nParameter description: b(Tensor) - Right tensor to contract\nConstraints:\ndtype:Null\nstructure:tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.quantization.quantize(model,run_fn,run_args,mapping=None,inplace=False)\nParameter description: inplace - carry out model transformations in-place, the original module is mutated\nConstraints:\ndtype:bool\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.nn.SyncBatchNorm(num_features,eps=1e-05,momentum=0.1,affine=True,track_running_stats=True,process_group=None)\nParameter description: track_running_stats - a boolean value that when set to `True`, this module tracks the running mean and variance, and when set to `False`, this module does not track such statistics and always uses batch statistics in both training and eval modes. Default: `True`\nConstraints:\ndtype:boolean;bool\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.floor_divide(input,other,out=None)\nParameter description: other(Tensor or Scalar) - the denominator\nConstraints:\ndtype:Null\nstructure:tensor\nshape:Null\nndim:0\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.nn.AvgPool3d(kernel_size,stride=None,padding=0,ceil_mode=False,count_include_pad=True,divisor_override=None)\nParameter description: count_include_pad - when True, will include the zero-padding in the averaging calculation\nConstraints:\ndtype:bool\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.trapz(y,x,dim=-1)\nParameter description: dim(int) - The dimension along which to integrate.By default, use the last dimension.\nConstraints:",
            "constr": {
                "dtype": [
                    "int"
                ],
                "structure": [],
                "shape": [],
                "ndim": [
                    "0"
                ],
                "enum": [],
                "range": []
            },
            "signature": "torch.trapz(y,x,dim=-1)",
            "descp": "dim(int) - The dimension along which to integrate.By default, use the last dimension."
        },
        {
            "fname": "torch.nn.convtranspose3d.yaml",
            "arg": "groups",
            "prompt": "Signature: torch.mean(input)\nParameter description: input(Tensor) - the input tensor.\nConstraints:\ndtype:Null\nstructure:tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.nn.Linear(in_features,out_features,bias=True)\nParameter description: in_features - size of each input sample\nConstraints:\ndtype:int\nstructure:Null\nshape:Null\nndim:Null\nenum:Null\nrange:[0,inf)\n\n###\n\nSignature: torch.combinations(input,r=2,with_replacement=False)\nParameter description: with_replacement(boolean, optional) - whether to allow duplication in combination\nConstraints:\ndtype:boolean;bool\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.rand_like(input,dtype=None,layout=None,device=None,requires_grad=False,memory_format=torch.preserve_format)\nParameter description: input(Tensor) - the size of `input` will determine size of the output tensor.\nConstraints:\ndtype:int\nstructure:tensor\nshape:Null\nndim:Null\nenum:Null\nrange:[0,inf)\n\n###\n\nSignature: torch.median(input)\nParameter description: input(Tensor) - the input tensor.\nConstraints:\ndtype:Null\nstructure:tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.tensordot(a,b,dims=2)\nParameter description: b(Tensor) - Right tensor to contract\nConstraints:\ndtype:Null\nstructure:tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.quantization.quantize(model,run_fn,run_args,mapping=None,inplace=False)\nParameter description: inplace - carry out model transformations in-place, the original module is mutated\nConstraints:\ndtype:bool\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.nn.SyncBatchNorm(num_features,eps=1e-05,momentum=0.1,affine=True,track_running_stats=True,process_group=None)\nParameter description: track_running_stats - a boolean value that when set to `True`, this module tracks the running mean and variance, and when set to `False`, this module does not track such statistics and always uses batch statistics in both training and eval modes. Default: `True`\nConstraints:\ndtype:boolean;bool\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.floor_divide(input,other,out=None)\nParameter description: other(Tensor or Scalar) - the denominator\nConstraints:\ndtype:Null\nstructure:tensor\nshape:Null\nndim:0\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.nn.AvgPool3d(kernel_size,stride=None,padding=0,ceil_mode=False,count_include_pad=True,divisor_override=None)\nParameter description: count_include_pad - when True, will include the zero-padding in the averaging calculation\nConstraints:\ndtype:bool\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.nn.ConvTranspose3d(in_channels,out_channels,kernel_size,stride=1,padding=0,output_padding=0,groups=1,bias=True,dilation=1,padding_mode=zeros)\nParameter description: groups(int, optional) - Number of blocked connections from input channels to output channels. Default: 1\nConstraints:",
            "constr": {
                "dtype": [
                    "int"
                ],
                "structure": [],
                "shape": [],
                "ndim": [
                    "0"
                ],
                "enum": [],
                "range": [
                    "[0,inf)"
                ]
            },
            "signature": "torch.nn.ConvTranspose3d(in_channels,out_channels,kernel_size,stride=1,padding=0,output_padding=0,groups=1,bias=True,dilation=1,padding_mode=zeros)",
            "descp": "groups(int, optional) - Number of blocked connections from input channels to output channels. Default: 1"
        },
        {
            "fname": "torch.utils.cpp_extension.check_compiler_abi_compatibility.yaml",
            "arg": "compiler",
            "prompt": "Signature: torch.mean(input)\nParameter description: input(Tensor) - the input tensor.\nConstraints:\ndtype:Null\nstructure:tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.nn.Linear(in_features,out_features,bias=True)\nParameter description: in_features - size of each input sample\nConstraints:\ndtype:int\nstructure:Null\nshape:Null\nndim:Null\nenum:Null\nrange:[0,inf)\n\n###\n\nSignature: torch.combinations(input,r=2,with_replacement=False)\nParameter description: with_replacement(boolean, optional) - whether to allow duplication in combination\nConstraints:\ndtype:boolean;bool\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.rand_like(input,dtype=None,layout=None,device=None,requires_grad=False,memory_format=torch.preserve_format)\nParameter description: input(Tensor) - the size of `input` will determine size of the output tensor.\nConstraints:\ndtype:int\nstructure:tensor\nshape:Null\nndim:Null\nenum:Null\nrange:[0,inf)\n\n###\n\nSignature: torch.median(input)\nParameter description: input(Tensor) - the input tensor.\nConstraints:\ndtype:Null\nstructure:tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.tensordot(a,b,dims=2)\nParameter description: b(Tensor) - Right tensor to contract\nConstraints:\ndtype:Null\nstructure:tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.quantization.quantize(model,run_fn,run_args,mapping=None,inplace=False)\nParameter description: inplace - carry out model transformations in-place, the original module is mutated\nConstraints:\ndtype:bool\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.nn.SyncBatchNorm(num_features,eps=1e-05,momentum=0.1,affine=True,track_running_stats=True,process_group=None)\nParameter description: track_running_stats - a boolean value that when set to `True`, this module tracks the running mean and variance, and when set to `False`, this module does not track such statistics and always uses batch statistics in both training and eval modes. Default: `True`\nConstraints:\ndtype:boolean;bool\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.floor_divide(input,other,out=None)\nParameter description: other(Tensor or Scalar) - the denominator\nConstraints:\ndtype:Null\nstructure:tensor\nshape:Null\nndim:0\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.nn.AvgPool3d(kernel_size,stride=None,padding=0,ceil_mode=False,count_include_pad=True,divisor_override=None)\nParameter description: count_include_pad - when True, will include the zero-padding in the averaging calculation\nConstraints:\ndtype:bool\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.utils.cpp_extension.check_compiler_abi_compatibility(compiler)\nParameter description: compiler(str) - The compiler executable name to check (e.g. `g++`). Must be executable in a shell process.\nConstraints:",
            "constr": {
                "dtype": [
                    "string",
                    "str"
                ],
                "structure": [],
                "shape": [],
                "ndim": [],
                "enum": [],
                "range": []
            },
            "signature": "torch.utils.cpp_extension.check_compiler_abi_compatibility(compiler)",
            "descp": "compiler(str) - The compiler executable name to check (e.g. `g++`). Must be executable in a shell process."
        },
        {
            "fname": "torch.symeig.yaml",
            "arg": "out",
            "prompt": "Signature: torch.mean(input)\nParameter description: input(Tensor) - the input tensor.\nConstraints:\ndtype:Null\nstructure:tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.nn.Linear(in_features,out_features,bias=True)\nParameter description: in_features - size of each input sample\nConstraints:\ndtype:int\nstructure:Null\nshape:Null\nndim:Null\nenum:Null\nrange:[0,inf)\n\n###\n\nSignature: torch.combinations(input,r=2,with_replacement=False)\nParameter description: with_replacement(boolean, optional) - whether to allow duplication in combination\nConstraints:\ndtype:boolean;bool\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.rand_like(input,dtype=None,layout=None,device=None,requires_grad=False,memory_format=torch.preserve_format)\nParameter description: input(Tensor) - the size of `input` will determine size of the output tensor.\nConstraints:\ndtype:int\nstructure:tensor\nshape:Null\nndim:Null\nenum:Null\nrange:[0,inf)\n\n###\n\nSignature: torch.median(input)\nParameter description: input(Tensor) - the input tensor.\nConstraints:\ndtype:Null\nstructure:tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.tensordot(a,b,dims=2)\nParameter description: b(Tensor) - Right tensor to contract\nConstraints:\ndtype:Null\nstructure:tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.quantization.quantize(model,run_fn,run_args,mapping=None,inplace=False)\nParameter description: inplace - carry out model transformations in-place, the original module is mutated\nConstraints:\ndtype:bool\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.nn.SyncBatchNorm(num_features,eps=1e-05,momentum=0.1,affine=True,track_running_stats=True,process_group=None)\nParameter description: track_running_stats - a boolean value that when set to `True`, this module tracks the running mean and variance, and when set to `False`, this module does not track such statistics and always uses batch statistics in both training and eval modes. Default: `True`\nConstraints:\ndtype:boolean;bool\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.floor_divide(input,other,out=None)\nParameter description: other(Tensor or Scalar) - the denominator\nConstraints:\ndtype:Null\nstructure:tensor\nshape:Null\nndim:0\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.nn.AvgPool3d(kernel_size,stride=None,padding=0,ceil_mode=False,count_include_pad=True,divisor_override=None)\nParameter description: count_include_pad - when True, will include the zero-padding in the averaging calculation\nConstraints:\ndtype:bool\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.symeig(input,eigenvectors=False,upper=True,out=None)\nParameter description: out(tuple, optional) - the output tuple of (Tensor, Tensor)\nConstraints:",
            "constr": {
                "dtype": [],
                "structure": [
                    "tuple",
                    "tensor"
                ],
                "shape": [],
                "ndim": [],
                "enum": [],
                "range": []
            },
            "signature": "torch.symeig(input,eigenvectors=False,upper=True,out=None)",
            "descp": "out(tuple, optional) - the output tuple of (Tensor, Tensor)"
        },
        {
            "fname": "torch.lu_unpack.yaml",
            "arg": "LU_pivots",
            "prompt": "Signature: torch.mean(input)\nParameter description: input(Tensor) - the input tensor.\nConstraints:\ndtype:Null\nstructure:tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.nn.Linear(in_features,out_features,bias=True)\nParameter description: in_features - size of each input sample\nConstraints:\ndtype:int\nstructure:Null\nshape:Null\nndim:Null\nenum:Null\nrange:[0,inf)\n\n###\n\nSignature: torch.combinations(input,r=2,with_replacement=False)\nParameter description: with_replacement(boolean, optional) - whether to allow duplication in combination\nConstraints:\ndtype:boolean;bool\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.rand_like(input,dtype=None,layout=None,device=None,requires_grad=False,memory_format=torch.preserve_format)\nParameter description: input(Tensor) - the size of `input` will determine size of the output tensor.\nConstraints:\ndtype:int\nstructure:tensor\nshape:Null\nndim:Null\nenum:Null\nrange:[0,inf)\n\n###\n\nSignature: torch.median(input)\nParameter description: input(Tensor) - the input tensor.\nConstraints:\ndtype:Null\nstructure:tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.tensordot(a,b,dims=2)\nParameter description: b(Tensor) - Right tensor to contract\nConstraints:\ndtype:Null\nstructure:tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.quantization.quantize(model,run_fn,run_args,mapping=None,inplace=False)\nParameter description: inplace - carry out model transformations in-place, the original module is mutated\nConstraints:\ndtype:bool\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.nn.SyncBatchNorm(num_features,eps=1e-05,momentum=0.1,affine=True,track_running_stats=True,process_group=None)\nParameter description: track_running_stats - a boolean value that when set to `True`, this module tracks the running mean and variance, and when set to `False`, this module does not track such statistics and always uses batch statistics in both training and eval modes. Default: `True`\nConstraints:\ndtype:boolean;bool\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.floor_divide(input,other,out=None)\nParameter description: other(Tensor or Scalar) - the denominator\nConstraints:\ndtype:Null\nstructure:tensor\nshape:Null\nndim:0\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.nn.AvgPool3d(kernel_size,stride=None,padding=0,ceil_mode=False,count_include_pad=True,divisor_override=None)\nParameter description: count_include_pad - when True, will include the zero-padding in the averaging calculation\nConstraints:\ndtype:bool\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.lu_unpack(LU_data,LU_pivots,unpack_data=True,unpack_pivots=True)\nParameter description: LU_pivots(Tensor) - the packed LU factorization pivots\nConstraints:",
            "constr": {
                "dtype": [],
                "structure": [
                    "tensor"
                ],
                "shape": [],
                "ndim": [],
                "enum": [],
                "range": []
            },
            "signature": "torch.lu_unpack(LU_data,LU_pivots,unpack_data=True,unpack_pivots=True)",
            "descp": "LU_pivots(Tensor) - the packed LU factorization pivots"
        },
        {
            "fname": "torch.bernoulli.yaml",
            "arg": "out",
            "prompt": "Signature: torch.mean(input)\nParameter description: input(Tensor) - the input tensor.\nConstraints:\ndtype:Null\nstructure:tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.nn.Linear(in_features,out_features,bias=True)\nParameter description: in_features - size of each input sample\nConstraints:\ndtype:int\nstructure:Null\nshape:Null\nndim:Null\nenum:Null\nrange:[0,inf)\n\n###\n\nSignature: torch.combinations(input,r=2,with_replacement=False)\nParameter description: with_replacement(boolean, optional) - whether to allow duplication in combination\nConstraints:\ndtype:boolean;bool\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.rand_like(input,dtype=None,layout=None,device=None,requires_grad=False,memory_format=torch.preserve_format)\nParameter description: input(Tensor) - the size of `input` will determine size of the output tensor.\nConstraints:\ndtype:int\nstructure:tensor\nshape:Null\nndim:Null\nenum:Null\nrange:[0,inf)\n\n###\n\nSignature: torch.median(input)\nParameter description: input(Tensor) - the input tensor.\nConstraints:\ndtype:Null\nstructure:tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.tensordot(a,b,dims=2)\nParameter description: b(Tensor) - Right tensor to contract\nConstraints:\ndtype:Null\nstructure:tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.quantization.quantize(model,run_fn,run_args,mapping=None,inplace=False)\nParameter description: inplace - carry out model transformations in-place, the original module is mutated\nConstraints:\ndtype:bool\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.nn.SyncBatchNorm(num_features,eps=1e-05,momentum=0.1,affine=True,track_running_stats=True,process_group=None)\nParameter description: track_running_stats - a boolean value that when set to `True`, this module tracks the running mean and variance, and when set to `False`, this module does not track such statistics and always uses batch statistics in both training and eval modes. Default: `True`\nConstraints:\ndtype:boolean;bool\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.floor_divide(input,other,out=None)\nParameter description: other(Tensor or Scalar) - the denominator\nConstraints:\ndtype:Null\nstructure:tensor\nshape:Null\nndim:0\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.nn.AvgPool3d(kernel_size,stride=None,padding=0,ceil_mode=False,count_include_pad=True,divisor_override=None)\nParameter description: count_include_pad - when True, will include the zero-padding in the averaging calculation\nConstraints:\ndtype:bool\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.bernoulli(input,generator=None,out=None)\nParameter description: out(Tensor, optional) - the output tensor.\nConstraints:",
            "constr": {
                "dtype": [],
                "structure": [
                    "tensor"
                ],
                "shape": [],
                "ndim": [],
                "enum": [],
                "range": []
            },
            "signature": "torch.bernoulli(input,generator=None,out=None)",
            "descp": "out(Tensor, optional) - the output tensor."
        },
        {
            "fname": "torch.nn.init.eye_.yaml",
            "arg": "tensor",
            "prompt": "Signature: torch.mean(input)\nParameter description: input(Tensor) - the input tensor.\nConstraints:\ndtype:Null\nstructure:tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.nn.Linear(in_features,out_features,bias=True)\nParameter description: in_features - size of each input sample\nConstraints:\ndtype:int\nstructure:Null\nshape:Null\nndim:Null\nenum:Null\nrange:[0,inf)\n\n###\n\nSignature: torch.combinations(input,r=2,with_replacement=False)\nParameter description: with_replacement(boolean, optional) - whether to allow duplication in combination\nConstraints:\ndtype:boolean;bool\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.rand_like(input,dtype=None,layout=None,device=None,requires_grad=False,memory_format=torch.preserve_format)\nParameter description: input(Tensor) - the size of `input` will determine size of the output tensor.\nConstraints:\ndtype:int\nstructure:tensor\nshape:Null\nndim:Null\nenum:Null\nrange:[0,inf)\n\n###\n\nSignature: torch.median(input)\nParameter description: input(Tensor) - the input tensor.\nConstraints:\ndtype:Null\nstructure:tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.tensordot(a,b,dims=2)\nParameter description: b(Tensor) - Right tensor to contract\nConstraints:\ndtype:Null\nstructure:tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.quantization.quantize(model,run_fn,run_args,mapping=None,inplace=False)\nParameter description: inplace - carry out model transformations in-place, the original module is mutated\nConstraints:\ndtype:bool\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.nn.SyncBatchNorm(num_features,eps=1e-05,momentum=0.1,affine=True,track_running_stats=True,process_group=None)\nParameter description: track_running_stats - a boolean value that when set to `True`, this module tracks the running mean and variance, and when set to `False`, this module does not track such statistics and always uses batch statistics in both training and eval modes. Default: `True`\nConstraints:\ndtype:boolean;bool\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.floor_divide(input,other,out=None)\nParameter description: other(Tensor or Scalar) - the denominator\nConstraints:\ndtype:Null\nstructure:tensor\nshape:Null\nndim:0\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.nn.AvgPool3d(kernel_size,stride=None,padding=0,ceil_mode=False,count_include_pad=True,divisor_override=None)\nParameter description: count_include_pad - when True, will include the zero-padding in the averaging calculation\nConstraints:\ndtype:bool\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.nn.init.eye_(tensor)\nParameter description: tensor - a 2-dimensional torch.Tensor\nConstraints:",
            "constr": {
                "dtype": [],
                "structure": [
                    "torch.tensor"
                ],
                "shape": [],
                "ndim": [
                    "2"
                ],
                "enum": [],
                "range": []
            },
            "signature": "torch.nn.init.eye_(tensor)",
            "descp": "tensor - a 2-dimensional torch.Tensor"
        },
        {
            "fname": "torch.as_strided.yaml",
            "arg": "storage_offset",
            "prompt": "Signature: torch.mean(input)\nParameter description: input(Tensor) - the input tensor.\nConstraints:\ndtype:Null\nstructure:tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.nn.Linear(in_features,out_features,bias=True)\nParameter description: in_features - size of each input sample\nConstraints:\ndtype:int\nstructure:Null\nshape:Null\nndim:Null\nenum:Null\nrange:[0,inf)\n\n###\n\nSignature: torch.combinations(input,r=2,with_replacement=False)\nParameter description: with_replacement(boolean, optional) - whether to allow duplication in combination\nConstraints:\ndtype:boolean;bool\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.rand_like(input,dtype=None,layout=None,device=None,requires_grad=False,memory_format=torch.preserve_format)\nParameter description: input(Tensor) - the size of `input` will determine size of the output tensor.\nConstraints:\ndtype:int\nstructure:tensor\nshape:Null\nndim:Null\nenum:Null\nrange:[0,inf)\n\n###\n\nSignature: torch.median(input)\nParameter description: input(Tensor) - the input tensor.\nConstraints:\ndtype:Null\nstructure:tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.tensordot(a,b,dims=2)\nParameter description: b(Tensor) - Right tensor to contract\nConstraints:\ndtype:Null\nstructure:tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.quantization.quantize(model,run_fn,run_args,mapping=None,inplace=False)\nParameter description: inplace - carry out model transformations in-place, the original module is mutated\nConstraints:\ndtype:bool\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.nn.SyncBatchNorm(num_features,eps=1e-05,momentum=0.1,affine=True,track_running_stats=True,process_group=None)\nParameter description: track_running_stats - a boolean value that when set to `True`, this module tracks the running mean and variance, and when set to `False`, this module does not track such statistics and always uses batch statistics in both training and eval modes. Default: `True`\nConstraints:\ndtype:boolean;bool\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.floor_divide(input,other,out=None)\nParameter description: other(Tensor or Scalar) - the denominator\nConstraints:\ndtype:Null\nstructure:tensor\nshape:Null\nndim:0\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.nn.AvgPool3d(kernel_size,stride=None,padding=0,ceil_mode=False,count_include_pad=True,divisor_override=None)\nParameter description: count_include_pad - when True, will include the zero-padding in the averaging calculation\nConstraints:\ndtype:bool\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.as_strided(input,size,stride,storage_offset=0)\nParameter description: storage_offset(int, optional) - the offset in the underlying storage of the output tensor\nConstraints:",
            "constr": {
                "dtype": [
                    "int"
                ],
                "structure": [],
                "shape": [],
                "ndim": [
                    "0"
                ],
                "enum": [],
                "range": []
            },
            "signature": "torch.as_strided(input,size,stride,storage_offset=0)",
            "descp": "storage_offset(int, optional) - the offset in the underlying storage of the output tensor"
        },
        {
            "fname": "torch.nn.maxunpool3d.yaml",
            "arg": "padding",
            "prompt": "Signature: torch.mean(input)\nParameter description: input(Tensor) - the input tensor.\nConstraints:\ndtype:Null\nstructure:tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.nn.Linear(in_features,out_features,bias=True)\nParameter description: in_features - size of each input sample\nConstraints:\ndtype:int\nstructure:Null\nshape:Null\nndim:Null\nenum:Null\nrange:[0,inf)\n\n###\n\nSignature: torch.combinations(input,r=2,with_replacement=False)\nParameter description: with_replacement(boolean, optional) - whether to allow duplication in combination\nConstraints:\ndtype:boolean;bool\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.rand_like(input,dtype=None,layout=None,device=None,requires_grad=False,memory_format=torch.preserve_format)\nParameter description: input(Tensor) - the size of `input` will determine size of the output tensor.\nConstraints:\ndtype:int\nstructure:tensor\nshape:Null\nndim:Null\nenum:Null\nrange:[0,inf)\n\n###\n\nSignature: torch.median(input)\nParameter description: input(Tensor) - the input tensor.\nConstraints:\ndtype:Null\nstructure:tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.tensordot(a,b,dims=2)\nParameter description: b(Tensor) - Right tensor to contract\nConstraints:\ndtype:Null\nstructure:tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.quantization.quantize(model,run_fn,run_args,mapping=None,inplace=False)\nParameter description: inplace - carry out model transformations in-place, the original module is mutated\nConstraints:\ndtype:bool\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.nn.SyncBatchNorm(num_features,eps=1e-05,momentum=0.1,affine=True,track_running_stats=True,process_group=None)\nParameter description: track_running_stats - a boolean value that when set to `True`, this module tracks the running mean and variance, and when set to `False`, this module does not track such statistics and always uses batch statistics in both training and eval modes. Default: `True`\nConstraints:\ndtype:boolean;bool\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.floor_divide(input,other,out=None)\nParameter description: other(Tensor or Scalar) - the denominator\nConstraints:\ndtype:Null\nstructure:tensor\nshape:Null\nndim:0\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.nn.AvgPool3d(kernel_size,stride=None,padding=0,ceil_mode=False,count_include_pad=True,divisor_override=None)\nParameter description: count_include_pad - when True, will include the zero-padding in the averaging calculation\nConstraints:\ndtype:bool\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.nn.MaxUnpool3d(kernel_size,stride=None,padding=0)\nParameter description: padding(int or tuple) - Padding that was added to the input\nConstraints:",
            "constr": {
                "dtype": [
                    "int"
                ],
                "structure": [
                    "tuple"
                ],
                "shape": [],
                "ndim": [
                    "0"
                ],
                "enum": [],
                "range": []
            },
            "signature": "torch.nn.MaxUnpool3d(kernel_size,stride=None,padding=0)",
            "descp": "padding(int or tuple) - Padding that was added to the input"
        },
        {
            "fname": "torch.ne.yaml",
            "arg": "other",
            "prompt": "Signature: torch.mean(input)\nParameter description: input(Tensor) - the input tensor.\nConstraints:\ndtype:Null\nstructure:tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.nn.Linear(in_features,out_features,bias=True)\nParameter description: in_features - size of each input sample\nConstraints:\ndtype:int\nstructure:Null\nshape:Null\nndim:Null\nenum:Null\nrange:[0,inf)\n\n###\n\nSignature: torch.combinations(input,r=2,with_replacement=False)\nParameter description: with_replacement(boolean, optional) - whether to allow duplication in combination\nConstraints:\ndtype:boolean;bool\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.rand_like(input,dtype=None,layout=None,device=None,requires_grad=False,memory_format=torch.preserve_format)\nParameter description: input(Tensor) - the size of `input` will determine size of the output tensor.\nConstraints:\ndtype:int\nstructure:tensor\nshape:Null\nndim:Null\nenum:Null\nrange:[0,inf)\n\n###\n\nSignature: torch.median(input)\nParameter description: input(Tensor) - the input tensor.\nConstraints:\ndtype:Null\nstructure:tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.tensordot(a,b,dims=2)\nParameter description: b(Tensor) - Right tensor to contract\nConstraints:\ndtype:Null\nstructure:tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.quantization.quantize(model,run_fn,run_args,mapping=None,inplace=False)\nParameter description: inplace - carry out model transformations in-place, the original module is mutated\nConstraints:\ndtype:bool\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.nn.SyncBatchNorm(num_features,eps=1e-05,momentum=0.1,affine=True,track_running_stats=True,process_group=None)\nParameter description: track_running_stats - a boolean value that when set to `True`, this module tracks the running mean and variance, and when set to `False`, this module does not track such statistics and always uses batch statistics in both training and eval modes. Default: `True`\nConstraints:\ndtype:boolean;bool\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.floor_divide(input,other,out=None)\nParameter description: other(Tensor or Scalar) - the denominator\nConstraints:\ndtype:Null\nstructure:tensor\nshape:Null\nndim:0\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.nn.AvgPool3d(kernel_size,stride=None,padding=0,ceil_mode=False,count_include_pad=True,divisor_override=None)\nParameter description: count_include_pad - when True, will include the zero-padding in the averaging calculation\nConstraints:\ndtype:bool\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.ne(input,other,out=None)\nParameter description: other(Tensor or float) - the tensor or value to compare\nConstraints:",
            "constr": {
                "dtype": [
                    "float"
                ],
                "structure": [
                    "tensor"
                ],
                "shape": [],
                "ndim": [],
                "enum": [],
                "range": []
            },
            "signature": "torch.ne(input,other,out=None)",
            "descp": "other(Tensor or float) - the tensor or value to compare"
        },
        {
            "fname": "torch.lu.yaml",
            "arg": "out",
            "prompt": "Signature: torch.mean(input)\nParameter description: input(Tensor) - the input tensor.\nConstraints:\ndtype:Null\nstructure:tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.nn.Linear(in_features,out_features,bias=True)\nParameter description: in_features - size of each input sample\nConstraints:\ndtype:int\nstructure:Null\nshape:Null\nndim:Null\nenum:Null\nrange:[0,inf)\n\n###\n\nSignature: torch.combinations(input,r=2,with_replacement=False)\nParameter description: with_replacement(boolean, optional) - whether to allow duplication in combination\nConstraints:\ndtype:boolean;bool\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.rand_like(input,dtype=None,layout=None,device=None,requires_grad=False,memory_format=torch.preserve_format)\nParameter description: input(Tensor) - the size of `input` will determine size of the output tensor.\nConstraints:\ndtype:int\nstructure:tensor\nshape:Null\nndim:Null\nenum:Null\nrange:[0,inf)\n\n###\n\nSignature: torch.median(input)\nParameter description: input(Tensor) - the input tensor.\nConstraints:\ndtype:Null\nstructure:tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.tensordot(a,b,dims=2)\nParameter description: b(Tensor) - Right tensor to contract\nConstraints:\ndtype:Null\nstructure:tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.quantization.quantize(model,run_fn,run_args,mapping=None,inplace=False)\nParameter description: inplace - carry out model transformations in-place, the original module is mutated\nConstraints:\ndtype:bool\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.nn.SyncBatchNorm(num_features,eps=1e-05,momentum=0.1,affine=True,track_running_stats=True,process_group=None)\nParameter description: track_running_stats - a boolean value that when set to `True`, this module tracks the running mean and variance, and when set to `False`, this module does not track such statistics and always uses batch statistics in both training and eval modes. Default: `True`\nConstraints:\ndtype:boolean;bool\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.floor_divide(input,other,out=None)\nParameter description: other(Tensor or Scalar) - the denominator\nConstraints:\ndtype:Null\nstructure:tensor\nshape:Null\nndim:0\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.nn.AvgPool3d(kernel_size,stride=None,padding=0,ceil_mode=False,count_include_pad=True,divisor_override=None)\nParameter description: count_include_pad - when True, will include the zero-padding in the averaging calculation\nConstraints:\ndtype:bool\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.lu(*args,**kwargs,A=None,pivot=None,get_infos=None,out=None)\nParameter description: out(tuple, optional) - optional output tuple. If `get_infos` is `True`, then the elements in the tuple are Tensor, IntTensor, and IntTensor. If `get_infos` is `False`, then the elements in the tuple are Tensor, IntTensor. Default: `None`\nConstraints:",
            "constr": {
                "dtype": [
                    "int"
                ],
                "structure": [
                    "tuple",
                    "tensor"
                ],
                "shape": [],
                "ndim": [],
                "enum": [],
                "range": []
            },
            "signature": "torch.lu(*args,**kwargs,A=None,pivot=None,get_infos=None,out=None)",
            "descp": "out(tuple, optional) - optional output tuple. If `get_infos` is `True`, then the elements in the tuple are Tensor, IntTensor, and IntTensor. If `get_infos` is `False`, then the elements in the tuple are Tensor, IntTensor. Default: `None`"
        },
        {
            "fname": "torch.multinomial.yaml",
            "arg": "replacement",
            "prompt": "Signature: torch.mean(input)\nParameter description: input(Tensor) - the input tensor.\nConstraints:\ndtype:Null\nstructure:tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.nn.Linear(in_features,out_features,bias=True)\nParameter description: in_features - size of each input sample\nConstraints:\ndtype:int\nstructure:Null\nshape:Null\nndim:Null\nenum:Null\nrange:[0,inf)\n\n###\n\nSignature: torch.combinations(input,r=2,with_replacement=False)\nParameter description: with_replacement(boolean, optional) - whether to allow duplication in combination\nConstraints:\ndtype:boolean;bool\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.rand_like(input,dtype=None,layout=None,device=None,requires_grad=False,memory_format=torch.preserve_format)\nParameter description: input(Tensor) - the size of `input` will determine size of the output tensor.\nConstraints:\ndtype:int\nstructure:tensor\nshape:Null\nndim:Null\nenum:Null\nrange:[0,inf)\n\n###\n\nSignature: torch.median(input)\nParameter description: input(Tensor) - the input tensor.\nConstraints:\ndtype:Null\nstructure:tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.tensordot(a,b,dims=2)\nParameter description: b(Tensor) - Right tensor to contract\nConstraints:\ndtype:Null\nstructure:tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.quantization.quantize(model,run_fn,run_args,mapping=None,inplace=False)\nParameter description: inplace - carry out model transformations in-place, the original module is mutated\nConstraints:\ndtype:bool\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.nn.SyncBatchNorm(num_features,eps=1e-05,momentum=0.1,affine=True,track_running_stats=True,process_group=None)\nParameter description: track_running_stats - a boolean value that when set to `True`, this module tracks the running mean and variance, and when set to `False`, this module does not track such statistics and always uses batch statistics in both training and eval modes. Default: `True`\nConstraints:\ndtype:boolean;bool\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.floor_divide(input,other,out=None)\nParameter description: other(Tensor or Scalar) - the denominator\nConstraints:\ndtype:Null\nstructure:tensor\nshape:Null\nndim:0\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.nn.AvgPool3d(kernel_size,stride=None,padding=0,ceil_mode=False,count_include_pad=True,divisor_override=None)\nParameter description: count_include_pad - when True, will include the zero-padding in the averaging calculation\nConstraints:\ndtype:bool\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.multinomial(input,num_samples,replacement=False,generator=None,out=None)\nParameter description: replacement(bool, optional) - whether to draw with replacement or not\nConstraints:",
            "constr": {
                "dtype": [
                    "bool"
                ],
                "structure": [],
                "shape": [],
                "ndim": [
                    "0"
                ],
                "enum": [],
                "range": []
            },
            "signature": "torch.multinomial(input,num_samples,replacement=False,generator=None,out=None)",
            "descp": "replacement(bool, optional) - whether to draw with replacement or not"
        },
        {
            "fname": "torch.autograd.grad.yaml",
            "arg": "allow_unused",
            "prompt": "Signature: torch.mean(input)\nParameter description: input(Tensor) - the input tensor.\nConstraints:\ndtype:Null\nstructure:tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.nn.Linear(in_features,out_features,bias=True)\nParameter description: in_features - size of each input sample\nConstraints:\ndtype:int\nstructure:Null\nshape:Null\nndim:Null\nenum:Null\nrange:[0,inf)\n\n###\n\nSignature: torch.combinations(input,r=2,with_replacement=False)\nParameter description: with_replacement(boolean, optional) - whether to allow duplication in combination\nConstraints:\ndtype:boolean;bool\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.rand_like(input,dtype=None,layout=None,device=None,requires_grad=False,memory_format=torch.preserve_format)\nParameter description: input(Tensor) - the size of `input` will determine size of the output tensor.\nConstraints:\ndtype:int\nstructure:tensor\nshape:Null\nndim:Null\nenum:Null\nrange:[0,inf)\n\n###\n\nSignature: torch.median(input)\nParameter description: input(Tensor) - the input tensor.\nConstraints:\ndtype:Null\nstructure:tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.tensordot(a,b,dims=2)\nParameter description: b(Tensor) - Right tensor to contract\nConstraints:\ndtype:Null\nstructure:tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.quantization.quantize(model,run_fn,run_args,mapping=None,inplace=False)\nParameter description: inplace - carry out model transformations in-place, the original module is mutated\nConstraints:\ndtype:bool\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.nn.SyncBatchNorm(num_features,eps=1e-05,momentum=0.1,affine=True,track_running_stats=True,process_group=None)\nParameter description: track_running_stats - a boolean value that when set to `True`, this module tracks the running mean and variance, and when set to `False`, this module does not track such statistics and always uses batch statistics in both training and eval modes. Default: `True`\nConstraints:\ndtype:boolean;bool\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.floor_divide(input,other,out=None)\nParameter description: other(Tensor or Scalar) - the denominator\nConstraints:\ndtype:Null\nstructure:tensor\nshape:Null\nndim:0\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.nn.AvgPool3d(kernel_size,stride=None,padding=0,ceil_mode=False,count_include_pad=True,divisor_override=None)\nParameter description: count_include_pad - when True, will include the zero-padding in the averaging calculation\nConstraints:\ndtype:bool\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.autograd.grad(outputs,inputs,grad_outputs=None,retain_graph=None,create_graph=False,only_inputs=True,allow_unused=False)\nParameter description: allow_unused(bool, optional) - If `False`, specifying inputs that were not used when computing outputs (and therefore their grad is always zero) is an error. Defaults to `False`.\nConstraints:",
            "constr": {
                "dtype": [
                    "bool"
                ],
                "structure": [],
                "shape": [],
                "ndim": [
                    "0"
                ],
                "enum": [],
                "range": []
            },
            "signature": "torch.autograd.grad(outputs,inputs,grad_outputs=None,retain_graph=None,create_graph=False,only_inputs=True,allow_unused=False)",
            "descp": "allow_unused(bool, optional) - If `False`, specifying inputs that were not used when computing outputs (and therefore their grad is always zero) is an error. Defaults to `False`."
        },
        {
            "fname": "torch.nn.upsample.yaml",
            "arg": "scale_factor",
            "prompt": "Signature: torch.mean(input)\nParameter description: input(Tensor) - the input tensor.\nConstraints:\ndtype:Null\nstructure:tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.nn.Linear(in_features,out_features,bias=True)\nParameter description: in_features - size of each input sample\nConstraints:\ndtype:int\nstructure:Null\nshape:Null\nndim:Null\nenum:Null\nrange:[0,inf)\n\n###\n\nSignature: torch.combinations(input,r=2,with_replacement=False)\nParameter description: with_replacement(boolean, optional) - whether to allow duplication in combination\nConstraints:\ndtype:boolean;bool\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.rand_like(input,dtype=None,layout=None,device=None,requires_grad=False,memory_format=torch.preserve_format)\nParameter description: input(Tensor) - the size of `input` will determine size of the output tensor.\nConstraints:\ndtype:int\nstructure:tensor\nshape:Null\nndim:Null\nenum:Null\nrange:[0,inf)\n\n###\n\nSignature: torch.median(input)\nParameter description: input(Tensor) - the input tensor.\nConstraints:\ndtype:Null\nstructure:tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.tensordot(a,b,dims=2)\nParameter description: b(Tensor) - Right tensor to contract\nConstraints:\ndtype:Null\nstructure:tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.quantization.quantize(model,run_fn,run_args,mapping=None,inplace=False)\nParameter description: inplace - carry out model transformations in-place, the original module is mutated\nConstraints:\ndtype:bool\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.nn.SyncBatchNorm(num_features,eps=1e-05,momentum=0.1,affine=True,track_running_stats=True,process_group=None)\nParameter description: track_running_stats - a boolean value that when set to `True`, this module tracks the running mean and variance, and when set to `False`, this module does not track such statistics and always uses batch statistics in both training and eval modes. Default: `True`\nConstraints:\ndtype:boolean;bool\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.floor_divide(input,other,out=None)\nParameter description: other(Tensor or Scalar) - the denominator\nConstraints:\ndtype:Null\nstructure:tensor\nshape:Null\nndim:0\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.nn.AvgPool3d(kernel_size,stride=None,padding=0,ceil_mode=False,count_include_pad=True,divisor_override=None)\nParameter description: count_include_pad - when True, will include the zero-padding in the averaging calculation\nConstraints:\ndtype:bool\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.nn.Upsample(size=None,scale_factor=None,mode=nearest,align_corners=None)\nParameter description: scale_factor(float or Tuple[float] or Tuple[float, float] or Tuple[float, float, float], optional) - multiplier for spatial size. Has to match input size if it is a tuple.\nConstraints:",
            "constr": {
                "dtype": [
                    "float"
                ],
                "structure": [
                    "tuple"
                ],
                "shape": [],
                "ndim": [
                    "0",
                    "1"
                ],
                "enum": [],
                "range": []
            },
            "signature": "torch.nn.Upsample(size=None,scale_factor=None,mode=nearest,align_corners=None)",
            "descp": "scale_factor(float or Tuple[float] or Tuple[float, float] or Tuple[float, float, float], optional) - multiplier for spatial size. Has to match input size if it is a tuple."
        },
        {
            "fname": "torch.neg.yaml",
            "arg": "out",
            "prompt": "Signature: torch.mean(input)\nParameter description: input(Tensor) - the input tensor.\nConstraints:\ndtype:Null\nstructure:tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.nn.Linear(in_features,out_features,bias=True)\nParameter description: in_features - size of each input sample\nConstraints:\ndtype:int\nstructure:Null\nshape:Null\nndim:Null\nenum:Null\nrange:[0,inf)\n\n###\n\nSignature: torch.combinations(input,r=2,with_replacement=False)\nParameter description: with_replacement(boolean, optional) - whether to allow duplication in combination\nConstraints:\ndtype:boolean;bool\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.rand_like(input,dtype=None,layout=None,device=None,requires_grad=False,memory_format=torch.preserve_format)\nParameter description: input(Tensor) - the size of `input` will determine size of the output tensor.\nConstraints:\ndtype:int\nstructure:tensor\nshape:Null\nndim:Null\nenum:Null\nrange:[0,inf)\n\n###\n\nSignature: torch.median(input)\nParameter description: input(Tensor) - the input tensor.\nConstraints:\ndtype:Null\nstructure:tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.tensordot(a,b,dims=2)\nParameter description: b(Tensor) - Right tensor to contract\nConstraints:\ndtype:Null\nstructure:tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.quantization.quantize(model,run_fn,run_args,mapping=None,inplace=False)\nParameter description: inplace - carry out model transformations in-place, the original module is mutated\nConstraints:\ndtype:bool\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.nn.SyncBatchNorm(num_features,eps=1e-05,momentum=0.1,affine=True,track_running_stats=True,process_group=None)\nParameter description: track_running_stats - a boolean value that when set to `True`, this module tracks the running mean and variance, and when set to `False`, this module does not track such statistics and always uses batch statistics in both training and eval modes. Default: `True`\nConstraints:\ndtype:boolean;bool\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.floor_divide(input,other,out=None)\nParameter description: other(Tensor or Scalar) - the denominator\nConstraints:\ndtype:Null\nstructure:tensor\nshape:Null\nndim:0\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.nn.AvgPool3d(kernel_size,stride=None,padding=0,ceil_mode=False,count_include_pad=True,divisor_override=None)\nParameter description: count_include_pad - when True, will include the zero-padding in the averaging calculation\nConstraints:\ndtype:bool\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.neg(input,out=None)\nParameter description: out(Tensor, optional) - the output tensor.\nConstraints:",
            "constr": {
                "dtype": [],
                "structure": [
                    "tensor"
                ],
                "shape": [],
                "ndim": [],
                "enum": [],
                "range": []
            },
            "signature": "torch.neg(input,out=None)",
            "descp": "out(Tensor, optional) - the output tensor."
        },
        {
            "fname": "torch.nn.functional.avg_pool3d.yaml",
            "arg": "ceil_mode",
            "prompt": "Signature: torch.mean(input)\nParameter description: input(Tensor) - the input tensor.\nConstraints:\ndtype:Null\nstructure:tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.nn.Linear(in_features,out_features,bias=True)\nParameter description: in_features - size of each input sample\nConstraints:\ndtype:int\nstructure:Null\nshape:Null\nndim:Null\nenum:Null\nrange:[0,inf)\n\n###\n\nSignature: torch.combinations(input,r=2,with_replacement=False)\nParameter description: with_replacement(boolean, optional) - whether to allow duplication in combination\nConstraints:\ndtype:boolean;bool\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.rand_like(input,dtype=None,layout=None,device=None,requires_grad=False,memory_format=torch.preserve_format)\nParameter description: input(Tensor) - the size of `input` will determine size of the output tensor.\nConstraints:\ndtype:int\nstructure:tensor\nshape:Null\nndim:Null\nenum:Null\nrange:[0,inf)\n\n###\n\nSignature: torch.median(input)\nParameter description: input(Tensor) - the input tensor.\nConstraints:\ndtype:Null\nstructure:tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.tensordot(a,b,dims=2)\nParameter description: b(Tensor) - Right tensor to contract\nConstraints:\ndtype:Null\nstructure:tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.quantization.quantize(model,run_fn,run_args,mapping=None,inplace=False)\nParameter description: inplace - carry out model transformations in-place, the original module is mutated\nConstraints:\ndtype:bool\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.nn.SyncBatchNorm(num_features,eps=1e-05,momentum=0.1,affine=True,track_running_stats=True,process_group=None)\nParameter description: track_running_stats - a boolean value that when set to `True`, this module tracks the running mean and variance, and when set to `False`, this module does not track such statistics and always uses batch statistics in both training and eval modes. Default: `True`\nConstraints:\ndtype:boolean;bool\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.floor_divide(input,other,out=None)\nParameter description: other(Tensor or Scalar) - the denominator\nConstraints:\ndtype:Null\nstructure:tensor\nshape:Null\nndim:0\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.nn.AvgPool3d(kernel_size,stride=None,padding=0,ceil_mode=False,count_include_pad=True,divisor_override=None)\nParameter description: count_include_pad - when True, will include the zero-padding in the averaging calculation\nConstraints:\ndtype:bool\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.nn.functional.avg_pool3d(input,kernel_size,stride=None,padding=0,ceil_mode=False,count_include_pad=True,divisor_override=None)\nParameter description: ceil_mode - when True, will use ceil instead of floor in the formula to compute the output shape\nConstraints:",
            "constr": {
                "dtype": [
                    "bool"
                ],
                "structure": [],
                "shape": [],
                "ndim": [
                    "0"
                ],
                "enum": [],
                "range": []
            },
            "signature": "torch.nn.functional.avg_pool3d(input,kernel_size,stride=None,padding=0,ceil_mode=False,count_include_pad=True,divisor_override=None)",
            "descp": "ceil_mode - when True, will use ceil instead of floor in the formula to compute the output shape"
        },
        {
            "fname": "torch.sparse.addmm.yaml",
            "arg": "mat1",
            "prompt": "Signature: torch.mean(input)\nParameter description: input(Tensor) - the input tensor.\nConstraints:\ndtype:Null\nstructure:tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.nn.Linear(in_features,out_features,bias=True)\nParameter description: in_features - size of each input sample\nConstraints:\ndtype:int\nstructure:Null\nshape:Null\nndim:Null\nenum:Null\nrange:[0,inf)\n\n###\n\nSignature: torch.combinations(input,r=2,with_replacement=False)\nParameter description: with_replacement(boolean, optional) - whether to allow duplication in combination\nConstraints:\ndtype:boolean;bool\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.rand_like(input,dtype=None,layout=None,device=None,requires_grad=False,memory_format=torch.preserve_format)\nParameter description: input(Tensor) - the size of `input` will determine size of the output tensor.\nConstraints:\ndtype:int\nstructure:tensor\nshape:Null\nndim:Null\nenum:Null\nrange:[0,inf)\n\n###\n\nSignature: torch.median(input)\nParameter description: input(Tensor) - the input tensor.\nConstraints:\ndtype:Null\nstructure:tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.tensordot(a,b,dims=2)\nParameter description: b(Tensor) - Right tensor to contract\nConstraints:\ndtype:Null\nstructure:tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.quantization.quantize(model,run_fn,run_args,mapping=None,inplace=False)\nParameter description: inplace - carry out model transformations in-place, the original module is mutated\nConstraints:\ndtype:bool\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.nn.SyncBatchNorm(num_features,eps=1e-05,momentum=0.1,affine=True,track_running_stats=True,process_group=None)\nParameter description: track_running_stats - a boolean value that when set to `True`, this module tracks the running mean and variance, and when set to `False`, this module does not track such statistics and always uses batch statistics in both training and eval modes. Default: `True`\nConstraints:\ndtype:boolean;bool\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.floor_divide(input,other,out=None)\nParameter description: other(Tensor or Scalar) - the denominator\nConstraints:\ndtype:Null\nstructure:tensor\nshape:Null\nndim:0\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.nn.AvgPool3d(kernel_size,stride=None,padding=0,ceil_mode=False,count_include_pad=True,divisor_override=None)\nParameter description: count_include_pad - when True, will include the zero-padding in the averaging calculation\nConstraints:\ndtype:bool\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.sparse.addmm(mat,mat1,mat2,beta=1,alpha=1)\nParameter description: mat1(SparseTensor) - a sparse matrix to be multiplied\nConstraints:",
            "constr": {
                "dtype": [
                    "numeric"
                ],
                "structure": [
                    "sparsetensor"
                ],
                "shape": [],
                "ndim": [],
                "enum": [],
                "range": []
            },
            "signature": "torch.sparse.addmm(mat,mat1,mat2,beta=1,alpha=1)",
            "descp": "mat1(SparseTensor) - a sparse matrix to be multiplied"
        },
        {
            "fname": "torch.nn.functional.binary_cross_entropy_with_logits.yaml",
            "arg": "target",
            "prompt": "Signature: torch.mean(input)\nParameter description: input(Tensor) - the input tensor.\nConstraints:\ndtype:Null\nstructure:tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.nn.Linear(in_features,out_features,bias=True)\nParameter description: in_features - size of each input sample\nConstraints:\ndtype:int\nstructure:Null\nshape:Null\nndim:Null\nenum:Null\nrange:[0,inf)\n\n###\n\nSignature: torch.combinations(input,r=2,with_replacement=False)\nParameter description: with_replacement(boolean, optional) - whether to allow duplication in combination\nConstraints:\ndtype:boolean;bool\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.rand_like(input,dtype=None,layout=None,device=None,requires_grad=False,memory_format=torch.preserve_format)\nParameter description: input(Tensor) - the size of `input` will determine size of the output tensor.\nConstraints:\ndtype:int\nstructure:tensor\nshape:Null\nndim:Null\nenum:Null\nrange:[0,inf)\n\n###\n\nSignature: torch.median(input)\nParameter description: input(Tensor) - the input tensor.\nConstraints:\ndtype:Null\nstructure:tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.tensordot(a,b,dims=2)\nParameter description: b(Tensor) - Right tensor to contract\nConstraints:\ndtype:Null\nstructure:tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.quantization.quantize(model,run_fn,run_args,mapping=None,inplace=False)\nParameter description: inplace - carry out model transformations in-place, the original module is mutated\nConstraints:\ndtype:bool\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.nn.SyncBatchNorm(num_features,eps=1e-05,momentum=0.1,affine=True,track_running_stats=True,process_group=None)\nParameter description: track_running_stats - a boolean value that when set to `True`, this module tracks the running mean and variance, and when set to `False`, this module does not track such statistics and always uses batch statistics in both training and eval modes. Default: `True`\nConstraints:\ndtype:boolean;bool\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.floor_divide(input,other,out=None)\nParameter description: other(Tensor or Scalar) - the denominator\nConstraints:\ndtype:Null\nstructure:tensor\nshape:Null\nndim:0\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.nn.AvgPool3d(kernel_size,stride=None,padding=0,ceil_mode=False,count_include_pad=True,divisor_override=None)\nParameter description: count_include_pad - when True, will include the zero-padding in the averaging calculation\nConstraints:\ndtype:bool\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.nn.functional.binary_cross_entropy_with_logits(input,target,weight=None,size_average=None,reduce=None,reduction=mean,pos_weight=None)\nParameter description: target - Tensor of the same shape as input\nConstraints:",
            "constr": {
                "dtype": [],
                "structure": [
                    "tensor"
                ],
                "shape": [
                    "&input"
                ],
                "ndim": [],
                "enum": [],
                "range": []
            },
            "signature": "torch.nn.functional.binary_cross_entropy_with_logits(input,target,weight=None,size_average=None,reduce=None,reduction=mean,pos_weight=None)",
            "descp": "target - Tensor of the same shape as input"
        },
        {
            "fname": "torch.nn.gru.yaml",
            "arg": "bias",
            "prompt": "Signature: torch.mean(input)\nParameter description: input(Tensor) - the input tensor.\nConstraints:\ndtype:Null\nstructure:tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.nn.Linear(in_features,out_features,bias=True)\nParameter description: in_features - size of each input sample\nConstraints:\ndtype:int\nstructure:Null\nshape:Null\nndim:Null\nenum:Null\nrange:[0,inf)\n\n###\n\nSignature: torch.combinations(input,r=2,with_replacement=False)\nParameter description: with_replacement(boolean, optional) - whether to allow duplication in combination\nConstraints:\ndtype:boolean;bool\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.rand_like(input,dtype=None,layout=None,device=None,requires_grad=False,memory_format=torch.preserve_format)\nParameter description: input(Tensor) - the size of `input` will determine size of the output tensor.\nConstraints:\ndtype:int\nstructure:tensor\nshape:Null\nndim:Null\nenum:Null\nrange:[0,inf)\n\n###\n\nSignature: torch.median(input)\nParameter description: input(Tensor) - the input tensor.\nConstraints:\ndtype:Null\nstructure:tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.tensordot(a,b,dims=2)\nParameter description: b(Tensor) - Right tensor to contract\nConstraints:\ndtype:Null\nstructure:tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.quantization.quantize(model,run_fn,run_args,mapping=None,inplace=False)\nParameter description: inplace - carry out model transformations in-place, the original module is mutated\nConstraints:\ndtype:bool\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.nn.SyncBatchNorm(num_features,eps=1e-05,momentum=0.1,affine=True,track_running_stats=True,process_group=None)\nParameter description: track_running_stats - a boolean value that when set to `True`, this module tracks the running mean and variance, and when set to `False`, this module does not track such statistics and always uses batch statistics in both training and eval modes. Default: `True`\nConstraints:\ndtype:boolean;bool\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.floor_divide(input,other,out=None)\nParameter description: other(Tensor or Scalar) - the denominator\nConstraints:\ndtype:Null\nstructure:tensor\nshape:Null\nndim:0\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.nn.AvgPool3d(kernel_size,stride=None,padding=0,ceil_mode=False,count_include_pad=True,divisor_override=None)\nParameter description: count_include_pad - when True, will include the zero-padding in the averaging calculation\nConstraints:\ndtype:bool\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.nn.GRU(*args,**kwargs,input_size=None,hidden_size=None,num_layers=None,bias=None,batch_first=None,dropout=None,bidirectional=None)\nParameter description: bias - If `False`, then the layer does not use bias weights b_ih and b_hh. Default: `True`\nConstraints:",
            "constr": {
                "dtype": [
                    "bool"
                ],
                "structure": [],
                "shape": [],
                "ndim": [
                    "0"
                ],
                "enum": [],
                "range": []
            },
            "signature": "torch.nn.GRU(*args,**kwargs,input_size=None,hidden_size=None,num_layers=None,bias=None,batch_first=None,dropout=None,bidirectional=None)",
            "descp": "bias - If `False`, then the layer does not use bias weights b_ih and b_hh. Default: `True`"
        },
        {
            "fname": "torch.repeat_interleave.yaml",
            "arg": "input",
            "prompt": "Signature: torch.mean(input)\nParameter description: input(Tensor) - the input tensor.\nConstraints:\ndtype:Null\nstructure:tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.nn.Linear(in_features,out_features,bias=True)\nParameter description: in_features - size of each input sample\nConstraints:\ndtype:int\nstructure:Null\nshape:Null\nndim:Null\nenum:Null\nrange:[0,inf)\n\n###\n\nSignature: torch.combinations(input,r=2,with_replacement=False)\nParameter description: with_replacement(boolean, optional) - whether to allow duplication in combination\nConstraints:\ndtype:boolean;bool\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.rand_like(input,dtype=None,layout=None,device=None,requires_grad=False,memory_format=torch.preserve_format)\nParameter description: input(Tensor) - the size of `input` will determine size of the output tensor.\nConstraints:\ndtype:int\nstructure:tensor\nshape:Null\nndim:Null\nenum:Null\nrange:[0,inf)\n\n###\n\nSignature: torch.median(input)\nParameter description: input(Tensor) - the input tensor.\nConstraints:\ndtype:Null\nstructure:tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.tensordot(a,b,dims=2)\nParameter description: b(Tensor) - Right tensor to contract\nConstraints:\ndtype:Null\nstructure:tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.quantization.quantize(model,run_fn,run_args,mapping=None,inplace=False)\nParameter description: inplace - carry out model transformations in-place, the original module is mutated\nConstraints:\ndtype:bool\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.nn.SyncBatchNorm(num_features,eps=1e-05,momentum=0.1,affine=True,track_running_stats=True,process_group=None)\nParameter description: track_running_stats - a boolean value that when set to `True`, this module tracks the running mean and variance, and when set to `False`, this module does not track such statistics and always uses batch statistics in both training and eval modes. Default: `True`\nConstraints:\ndtype:boolean;bool\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.floor_divide(input,other,out=None)\nParameter description: other(Tensor or Scalar) - the denominator\nConstraints:\ndtype:Null\nstructure:tensor\nshape:Null\nndim:0\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.nn.AvgPool3d(kernel_size,stride=None,padding=0,ceil_mode=False,count_include_pad=True,divisor_override=None)\nParameter description: count_include_pad - when True, will include the zero-padding in the averaging calculation\nConstraints:\ndtype:bool\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.repeat_interleave(input,repeats,dim=None)\nParameter description: input(Tensor) - the input tensor.\nConstraints:",
            "constr": {
                "dtype": [],
                "structure": [
                    "tensor"
                ],
                "shape": [],
                "ndim": [],
                "enum": [],
                "range": []
            },
            "signature": "torch.repeat_interleave(input,repeats,dim=None)",
            "descp": "input(Tensor) - the input tensor."
        },
        {
            "fname": "torch.nn.functional.normalize.yaml",
            "arg": "out",
            "prompt": "Signature: torch.mean(input)\nParameter description: input(Tensor) - the input tensor.\nConstraints:\ndtype:Null\nstructure:tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.nn.Linear(in_features,out_features,bias=True)\nParameter description: in_features - size of each input sample\nConstraints:\ndtype:int\nstructure:Null\nshape:Null\nndim:Null\nenum:Null\nrange:[0,inf)\n\n###\n\nSignature: torch.combinations(input,r=2,with_replacement=False)\nParameter description: with_replacement(boolean, optional) - whether to allow duplication in combination\nConstraints:\ndtype:boolean;bool\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.rand_like(input,dtype=None,layout=None,device=None,requires_grad=False,memory_format=torch.preserve_format)\nParameter description: input(Tensor) - the size of `input` will determine size of the output tensor.\nConstraints:\ndtype:int\nstructure:tensor\nshape:Null\nndim:Null\nenum:Null\nrange:[0,inf)\n\n###\n\nSignature: torch.median(input)\nParameter description: input(Tensor) - the input tensor.\nConstraints:\ndtype:Null\nstructure:tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.tensordot(a,b,dims=2)\nParameter description: b(Tensor) - Right tensor to contract\nConstraints:\ndtype:Null\nstructure:tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.quantization.quantize(model,run_fn,run_args,mapping=None,inplace=False)\nParameter description: inplace - carry out model transformations in-place, the original module is mutated\nConstraints:\ndtype:bool\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.nn.SyncBatchNorm(num_features,eps=1e-05,momentum=0.1,affine=True,track_running_stats=True,process_group=None)\nParameter description: track_running_stats - a boolean value that when set to `True`, this module tracks the running mean and variance, and when set to `False`, this module does not track such statistics and always uses batch statistics in both training and eval modes. Default: `True`\nConstraints:\ndtype:boolean;bool\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.floor_divide(input,other,out=None)\nParameter description: other(Tensor or Scalar) - the denominator\nConstraints:\ndtype:Null\nstructure:tensor\nshape:Null\nndim:0\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.nn.AvgPool3d(kernel_size,stride=None,padding=0,ceil_mode=False,count_include_pad=True,divisor_override=None)\nParameter description: count_include_pad - when True, will include the zero-padding in the averaging calculation\nConstraints:\ndtype:bool\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.nn.functional.normalize(input,p=2,dim=1,eps=1e-12,out=None)\nParameter description: out(Tensor, optional) - the output tensor. If `out` is used, this operation won't be differentiable.\nConstraints:",
            "constr": {
                "dtype": [],
                "structure": [
                    "tensor"
                ],
                "shape": [],
                "ndim": [],
                "enum": [],
                "range": []
            },
            "signature": "torch.nn.functional.normalize(input,p=2,dim=1,eps=1e-12,out=None)",
            "descp": "out(Tensor, optional) - the output tensor. If `out` is used, this operation won't be differentiable."
        },
        {
            "fname": "torch.nn.functional.affine_grid.yaml",
            "arg": "theta",
            "prompt": "Signature: torch.mean(input)\nParameter description: input(Tensor) - the input tensor.\nConstraints:\ndtype:Null\nstructure:tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.nn.Linear(in_features,out_features,bias=True)\nParameter description: in_features - size of each input sample\nConstraints:\ndtype:int\nstructure:Null\nshape:Null\nndim:Null\nenum:Null\nrange:[0,inf)\n\n###\n\nSignature: torch.combinations(input,r=2,with_replacement=False)\nParameter description: with_replacement(boolean, optional) - whether to allow duplication in combination\nConstraints:\ndtype:boolean;bool\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.rand_like(input,dtype=None,layout=None,device=None,requires_grad=False,memory_format=torch.preserve_format)\nParameter description: input(Tensor) - the size of `input` will determine size of the output tensor.\nConstraints:\ndtype:int\nstructure:tensor\nshape:Null\nndim:Null\nenum:Null\nrange:[0,inf)\n\n###\n\nSignature: torch.median(input)\nParameter description: input(Tensor) - the input tensor.\nConstraints:\ndtype:Null\nstructure:tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.tensordot(a,b,dims=2)\nParameter description: b(Tensor) - Right tensor to contract\nConstraints:\ndtype:Null\nstructure:tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.quantization.quantize(model,run_fn,run_args,mapping=None,inplace=False)\nParameter description: inplace - carry out model transformations in-place, the original module is mutated\nConstraints:\ndtype:bool\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.nn.SyncBatchNorm(num_features,eps=1e-05,momentum=0.1,affine=True,track_running_stats=True,process_group=None)\nParameter description: track_running_stats - a boolean value that when set to `True`, this module tracks the running mean and variance, and when set to `False`, this module does not track such statistics and always uses batch statistics in both training and eval modes. Default: `True`\nConstraints:\ndtype:boolean;bool\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.floor_divide(input,other,out=None)\nParameter description: other(Tensor or Scalar) - the denominator\nConstraints:\ndtype:Null\nstructure:tensor\nshape:Null\nndim:0\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.nn.AvgPool3d(kernel_size,stride=None,padding=0,ceil_mode=False,count_include_pad=True,divisor_override=None)\nParameter description: count_include_pad - when True, will include the zero-padding in the averaging calculation\nConstraints:\ndtype:bool\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.nn.functional.affine_grid(theta,size,align_corners=None)\nParameter description: theta(Tensor) - input batch of affine matrices with shape (N  times 2  times 3 ) for 2D or (N  times 3  times 4 ) for 3D\nConstraints:",
            "constr": {
                "dtype": [
                    "numeric"
                ],
                "structure": [
                    "tensor"
                ],
                "shape": [
                    "(N times 2 times 3 )",
                    "(N times 3 times 4 )"
                ],
                "ndim": [],
                "enum": [],
                "range": []
            },
            "signature": "torch.nn.functional.affine_grid(theta,size,align_corners=None)",
            "descp": "theta(Tensor) - input batch of affine matrices with shape (N  times 2  times 3 ) for 2D or (N  times 3  times 4 ) for 3D"
        },
        {
            "fname": "torch.autograd.functional.hvp.yaml",
            "arg": "strict",
            "prompt": "Signature: torch.mean(input)\nParameter description: input(Tensor) - the input tensor.\nConstraints:\ndtype:Null\nstructure:tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.nn.Linear(in_features,out_features,bias=True)\nParameter description: in_features - size of each input sample\nConstraints:\ndtype:int\nstructure:Null\nshape:Null\nndim:Null\nenum:Null\nrange:[0,inf)\n\n###\n\nSignature: torch.combinations(input,r=2,with_replacement=False)\nParameter description: with_replacement(boolean, optional) - whether to allow duplication in combination\nConstraints:\ndtype:boolean;bool\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.rand_like(input,dtype=None,layout=None,device=None,requires_grad=False,memory_format=torch.preserve_format)\nParameter description: input(Tensor) - the size of `input` will determine size of the output tensor.\nConstraints:\ndtype:int\nstructure:tensor\nshape:Null\nndim:Null\nenum:Null\nrange:[0,inf)\n\n###\n\nSignature: torch.median(input)\nParameter description: input(Tensor) - the input tensor.\nConstraints:\ndtype:Null\nstructure:tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.tensordot(a,b,dims=2)\nParameter description: b(Tensor) - Right tensor to contract\nConstraints:\ndtype:Null\nstructure:tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.quantization.quantize(model,run_fn,run_args,mapping=None,inplace=False)\nParameter description: inplace - carry out model transformations in-place, the original module is mutated\nConstraints:\ndtype:bool\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.nn.SyncBatchNorm(num_features,eps=1e-05,momentum=0.1,affine=True,track_running_stats=True,process_group=None)\nParameter description: track_running_stats - a boolean value that when set to `True`, this module tracks the running mean and variance, and when set to `False`, this module does not track such statistics and always uses batch statistics in both training and eval modes. Default: `True`\nConstraints:\ndtype:boolean;bool\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.floor_divide(input,other,out=None)\nParameter description: other(Tensor or Scalar) - the denominator\nConstraints:\ndtype:Null\nstructure:tensor\nshape:Null\nndim:0\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.nn.AvgPool3d(kernel_size,stride=None,padding=0,ceil_mode=False,count_include_pad=True,divisor_override=None)\nParameter description: count_include_pad - when True, will include the zero-padding in the averaging calculation\nConstraints:\ndtype:bool\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.autograd.functional.hvp(func,inputs,v=None,create_graph=False,strict=False)\nParameter description: strict(bool, optional) - If `True`, an error will be raised when we detect that there exists an input such that all the outputs are independent of it. If `False`, we return a Tensor of zeros as the hvp for said inputs, which is the expected mathematical value. Defaults to `False`.\nConstraints:",
            "constr": {
                "dtype": [
                    "bool"
                ],
                "structure": [],
                "shape": [],
                "ndim": [
                    "0"
                ],
                "enum": [],
                "range": []
            },
            "signature": "torch.autograd.functional.hvp(func,inputs,v=None,create_graph=False,strict=False)",
            "descp": "strict(bool, optional) - If `True`, an error will be raised when we detect that there exists an input such that all the outputs are independent of it. If `False`, we return a Tensor of zeros as the hvp for said inputs, which is the expected mathematical value. Defaults to `False`."
        },
        {
            "fname": "torch.var2.yaml",
            "arg": "out",
            "prompt": "Signature: torch.mean(input)\nParameter description: input(Tensor) - the input tensor.\nConstraints:\ndtype:Null\nstructure:tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.nn.Linear(in_features,out_features,bias=True)\nParameter description: in_features - size of each input sample\nConstraints:\ndtype:int\nstructure:Null\nshape:Null\nndim:Null\nenum:Null\nrange:[0,inf)\n\n###\n\nSignature: torch.combinations(input,r=2,with_replacement=False)\nParameter description: with_replacement(boolean, optional) - whether to allow duplication in combination\nConstraints:\ndtype:boolean;bool\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.rand_like(input,dtype=None,layout=None,device=None,requires_grad=False,memory_format=torch.preserve_format)\nParameter description: input(Tensor) - the size of `input` will determine size of the output tensor.\nConstraints:\ndtype:int\nstructure:tensor\nshape:Null\nndim:Null\nenum:Null\nrange:[0,inf)\n\n###\n\nSignature: torch.median(input)\nParameter description: input(Tensor) - the input tensor.\nConstraints:\ndtype:Null\nstructure:tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.tensordot(a,b,dims=2)\nParameter description: b(Tensor) - Right tensor to contract\nConstraints:\ndtype:Null\nstructure:tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.quantization.quantize(model,run_fn,run_args,mapping=None,inplace=False)\nParameter description: inplace - carry out model transformations in-place, the original module is mutated\nConstraints:\ndtype:bool\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.nn.SyncBatchNorm(num_features,eps=1e-05,momentum=0.1,affine=True,track_running_stats=True,process_group=None)\nParameter description: track_running_stats - a boolean value that when set to `True`, this module tracks the running mean and variance, and when set to `False`, this module does not track such statistics and always uses batch statistics in both training and eval modes. Default: `True`\nConstraints:\ndtype:boolean;bool\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.floor_divide(input,other,out=None)\nParameter description: other(Tensor or Scalar) - the denominator\nConstraints:\ndtype:Null\nstructure:tensor\nshape:Null\nndim:0\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.nn.AvgPool3d(kernel_size,stride=None,padding=0,ceil_mode=False,count_include_pad=True,divisor_override=None)\nParameter description: count_include_pad - when True, will include the zero-padding in the averaging calculation\nConstraints:\ndtype:bool\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.var(input,dim,keepdim=False,unbiased=True,out=None)\nParameter description: out(Tensor, optional) - the output tensor.\nConstraints:",
            "constr": {
                "dtype": [],
                "structure": [
                    "tensor"
                ],
                "shape": [],
                "ndim": [],
                "enum": [],
                "range": []
            },
            "signature": "torch.var(input,dim,keepdim=False,unbiased=True,out=None)",
            "descp": "out(Tensor, optional) - the output tensor."
        },
        {
            "fname": "torch.remainder.yaml",
            "arg": "other",
            "prompt": "Signature: torch.mean(input)\nParameter description: input(Tensor) - the input tensor.\nConstraints:\ndtype:Null\nstructure:tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.nn.Linear(in_features,out_features,bias=True)\nParameter description: in_features - size of each input sample\nConstraints:\ndtype:int\nstructure:Null\nshape:Null\nndim:Null\nenum:Null\nrange:[0,inf)\n\n###\n\nSignature: torch.combinations(input,r=2,with_replacement=False)\nParameter description: with_replacement(boolean, optional) - whether to allow duplication in combination\nConstraints:\ndtype:boolean;bool\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.rand_like(input,dtype=None,layout=None,device=None,requires_grad=False,memory_format=torch.preserve_format)\nParameter description: input(Tensor) - the size of `input` will determine size of the output tensor.\nConstraints:\ndtype:int\nstructure:tensor\nshape:Null\nndim:Null\nenum:Null\nrange:[0,inf)\n\n###\n\nSignature: torch.median(input)\nParameter description: input(Tensor) - the input tensor.\nConstraints:\ndtype:Null\nstructure:tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.tensordot(a,b,dims=2)\nParameter description: b(Tensor) - Right tensor to contract\nConstraints:\ndtype:Null\nstructure:tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.quantization.quantize(model,run_fn,run_args,mapping=None,inplace=False)\nParameter description: inplace - carry out model transformations in-place, the original module is mutated\nConstraints:\ndtype:bool\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.nn.SyncBatchNorm(num_features,eps=1e-05,momentum=0.1,affine=True,track_running_stats=True,process_group=None)\nParameter description: track_running_stats - a boolean value that when set to `True`, this module tracks the running mean and variance, and when set to `False`, this module does not track such statistics and always uses batch statistics in both training and eval modes. Default: `True`\nConstraints:\ndtype:boolean;bool\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.floor_divide(input,other,out=None)\nParameter description: other(Tensor or Scalar) - the denominator\nConstraints:\ndtype:Null\nstructure:tensor\nshape:Null\nndim:0\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.nn.AvgPool3d(kernel_size,stride=None,padding=0,ceil_mode=False,count_include_pad=True,divisor_override=None)\nParameter description: count_include_pad - when True, will include the zero-padding in the averaging calculation\nConstraints:\ndtype:bool\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.remainder(input,other,out=None)\nParameter description: other(Tensor or float) - the divisor that may be either a number or a Tensor of the same shape as the dividend\nConstraints:",
            "constr": {
                "dtype": [
                    "float",
                    "int"
                ],
                "structure": [
                    "tensor"
                ],
                "shape": [],
                "ndim": [],
                "enum": [],
                "range": []
            },
            "signature": "torch.remainder(input,other,out=None)",
            "descp": "other(Tensor or float) - the divisor that may be either a number or a Tensor of the same shape as the dividend"
        },
        {
            "fname": "torch.lu.yaml",
            "arg": "get_infos",
            "prompt": "Signature: torch.mean(input)\nParameter description: input(Tensor) - the input tensor.\nConstraints:\ndtype:Null\nstructure:tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.nn.Linear(in_features,out_features,bias=True)\nParameter description: in_features - size of each input sample\nConstraints:\ndtype:int\nstructure:Null\nshape:Null\nndim:Null\nenum:Null\nrange:[0,inf)\n\n###\n\nSignature: torch.combinations(input,r=2,with_replacement=False)\nParameter description: with_replacement(boolean, optional) - whether to allow duplication in combination\nConstraints:\ndtype:boolean;bool\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.rand_like(input,dtype=None,layout=None,device=None,requires_grad=False,memory_format=torch.preserve_format)\nParameter description: input(Tensor) - the size of `input` will determine size of the output tensor.\nConstraints:\ndtype:int\nstructure:tensor\nshape:Null\nndim:Null\nenum:Null\nrange:[0,inf)\n\n###\n\nSignature: torch.median(input)\nParameter description: input(Tensor) - the input tensor.\nConstraints:\ndtype:Null\nstructure:tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.tensordot(a,b,dims=2)\nParameter description: b(Tensor) - Right tensor to contract\nConstraints:\ndtype:Null\nstructure:tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.quantization.quantize(model,run_fn,run_args,mapping=None,inplace=False)\nParameter description: inplace - carry out model transformations in-place, the original module is mutated\nConstraints:\ndtype:bool\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.nn.SyncBatchNorm(num_features,eps=1e-05,momentum=0.1,affine=True,track_running_stats=True,process_group=None)\nParameter description: track_running_stats - a boolean value that when set to `True`, this module tracks the running mean and variance, and when set to `False`, this module does not track such statistics and always uses batch statistics in both training and eval modes. Default: `True`\nConstraints:\ndtype:boolean;bool\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.floor_divide(input,other,out=None)\nParameter description: other(Tensor or Scalar) - the denominator\nConstraints:\ndtype:Null\nstructure:tensor\nshape:Null\nndim:0\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.nn.AvgPool3d(kernel_size,stride=None,padding=0,ceil_mode=False,count_include_pad=True,divisor_override=None)\nParameter description: count_include_pad - when True, will include the zero-padding in the averaging calculation\nConstraints:\ndtype:bool\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.lu(*args,**kwargs,A=None,pivot=None,get_infos=None,out=None)\nParameter description: get_infos(bool, optional) - if set to `True`, returns an info IntTensor. Default: `False`\nConstraints:",
            "constr": {
                "dtype": [
                    "bool"
                ],
                "structure": [],
                "shape": [],
                "ndim": [
                    "0"
                ],
                "enum": [],
                "range": []
            },
            "signature": "torch.lu(*args,**kwargs,A=None,pivot=None,get_infos=None,out=None)",
            "descp": "get_infos(bool, optional) - if set to `True`, returns an info IntTensor. Default: `False`"
        },
        {
            "fname": "torch.distributed.recv.yaml",
            "arg": "src",
            "prompt": "Signature: torch.mean(input)\nParameter description: input(Tensor) - the input tensor.\nConstraints:\ndtype:Null\nstructure:tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.nn.Linear(in_features,out_features,bias=True)\nParameter description: in_features - size of each input sample\nConstraints:\ndtype:int\nstructure:Null\nshape:Null\nndim:Null\nenum:Null\nrange:[0,inf)\n\n###\n\nSignature: torch.combinations(input,r=2,with_replacement=False)\nParameter description: with_replacement(boolean, optional) - whether to allow duplication in combination\nConstraints:\ndtype:boolean;bool\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.rand_like(input,dtype=None,layout=None,device=None,requires_grad=False,memory_format=torch.preserve_format)\nParameter description: input(Tensor) - the size of `input` will determine size of the output tensor.\nConstraints:\ndtype:int\nstructure:tensor\nshape:Null\nndim:Null\nenum:Null\nrange:[0,inf)\n\n###\n\nSignature: torch.median(input)\nParameter description: input(Tensor) - the input tensor.\nConstraints:\ndtype:Null\nstructure:tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.tensordot(a,b,dims=2)\nParameter description: b(Tensor) - Right tensor to contract\nConstraints:\ndtype:Null\nstructure:tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.quantization.quantize(model,run_fn,run_args,mapping=None,inplace=False)\nParameter description: inplace - carry out model transformations in-place, the original module is mutated\nConstraints:\ndtype:bool\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.nn.SyncBatchNorm(num_features,eps=1e-05,momentum=0.1,affine=True,track_running_stats=True,process_group=None)\nParameter description: track_running_stats - a boolean value that when set to `True`, this module tracks the running mean and variance, and when set to `False`, this module does not track such statistics and always uses batch statistics in both training and eval modes. Default: `True`\nConstraints:\ndtype:boolean;bool\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.floor_divide(input,other,out=None)\nParameter description: other(Tensor or Scalar) - the denominator\nConstraints:\ndtype:Null\nstructure:tensor\nshape:Null\nndim:0\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.nn.AvgPool3d(kernel_size,stride=None,padding=0,ceil_mode=False,count_include_pad=True,divisor_override=None)\nParameter description: count_include_pad - when True, will include the zero-padding in the averaging calculation\nConstraints:\ndtype:bool\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.distributed.recv(tensor,src=None,group=<objectobject>,tag=0)\nParameter description: src(int, optional) - Source rank. Will receive from any process if unspecified.\nConstraints:",
            "constr": {
                "dtype": [
                    "int"
                ],
                "structure": [],
                "shape": [],
                "ndim": [],
                "enum": [],
                "range": []
            },
            "signature": "torch.distributed.recv(tensor,src=None,group=<objectobject>,tag=0)",
            "descp": "src(int, optional) - Source rank. Will receive from any process if unspecified."
        },
        {
            "fname": "torch.nn.maxpool1d.yaml",
            "arg": "ceil_mode",
            "prompt": "Signature: torch.mean(input)\nParameter description: input(Tensor) - the input tensor.\nConstraints:\ndtype:Null\nstructure:tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.nn.Linear(in_features,out_features,bias=True)\nParameter description: in_features - size of each input sample\nConstraints:\ndtype:int\nstructure:Null\nshape:Null\nndim:Null\nenum:Null\nrange:[0,inf)\n\n###\n\nSignature: torch.combinations(input,r=2,with_replacement=False)\nParameter description: with_replacement(boolean, optional) - whether to allow duplication in combination\nConstraints:\ndtype:boolean;bool\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.rand_like(input,dtype=None,layout=None,device=None,requires_grad=False,memory_format=torch.preserve_format)\nParameter description: input(Tensor) - the size of `input` will determine size of the output tensor.\nConstraints:\ndtype:int\nstructure:tensor\nshape:Null\nndim:Null\nenum:Null\nrange:[0,inf)\n\n###\n\nSignature: torch.median(input)\nParameter description: input(Tensor) - the input tensor.\nConstraints:\ndtype:Null\nstructure:tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.tensordot(a,b,dims=2)\nParameter description: b(Tensor) - Right tensor to contract\nConstraints:\ndtype:Null\nstructure:tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.quantization.quantize(model,run_fn,run_args,mapping=None,inplace=False)\nParameter description: inplace - carry out model transformations in-place, the original module is mutated\nConstraints:\ndtype:bool\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.nn.SyncBatchNorm(num_features,eps=1e-05,momentum=0.1,affine=True,track_running_stats=True,process_group=None)\nParameter description: track_running_stats - a boolean value that when set to `True`, this module tracks the running mean and variance, and when set to `False`, this module does not track such statistics and always uses batch statistics in both training and eval modes. Default: `True`\nConstraints:\ndtype:boolean;bool\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.floor_divide(input,other,out=None)\nParameter description: other(Tensor or Scalar) - the denominator\nConstraints:\ndtype:Null\nstructure:tensor\nshape:Null\nndim:0\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.nn.AvgPool3d(kernel_size,stride=None,padding=0,ceil_mode=False,count_include_pad=True,divisor_override=None)\nParameter description: count_include_pad - when True, will include the zero-padding in the averaging calculation\nConstraints:\ndtype:bool\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.nn.MaxPool1d(kernel_size,stride=None,padding=0,dilation=1,return_indices=False,ceil_mode=False)\nParameter description: ceil_mode - when True, will use ceil instead of floor to compute the output shape\nConstraints:",
            "constr": {
                "dtype": [
                    "bool"
                ],
                "structure": [],
                "shape": [],
                "ndim": [
                    "0"
                ],
                "enum": [],
                "range": []
            },
            "signature": "torch.nn.MaxPool1d(kernel_size,stride=None,padding=0,dilation=1,return_indices=False,ceil_mode=False)",
            "descp": "ceil_mode - when True, will use ceil instead of floor to compute the output shape"
        },
        {
            "fname": "torch.sparse_coo_tensor.yaml",
            "arg": "requires_grad",
            "prompt": "Signature: torch.mean(input)\nParameter description: input(Tensor) - the input tensor.\nConstraints:\ndtype:Null\nstructure:tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.nn.Linear(in_features,out_features,bias=True)\nParameter description: in_features - size of each input sample\nConstraints:\ndtype:int\nstructure:Null\nshape:Null\nndim:Null\nenum:Null\nrange:[0,inf)\n\n###\n\nSignature: torch.combinations(input,r=2,with_replacement=False)\nParameter description: with_replacement(boolean, optional) - whether to allow duplication in combination\nConstraints:\ndtype:boolean;bool\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.rand_like(input,dtype=None,layout=None,device=None,requires_grad=False,memory_format=torch.preserve_format)\nParameter description: input(Tensor) - the size of `input` will determine size of the output tensor.\nConstraints:\ndtype:int\nstructure:tensor\nshape:Null\nndim:Null\nenum:Null\nrange:[0,inf)\n\n###\n\nSignature: torch.median(input)\nParameter description: input(Tensor) - the input tensor.\nConstraints:\ndtype:Null\nstructure:tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.tensordot(a,b,dims=2)\nParameter description: b(Tensor) - Right tensor to contract\nConstraints:\ndtype:Null\nstructure:tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.quantization.quantize(model,run_fn,run_args,mapping=None,inplace=False)\nParameter description: inplace - carry out model transformations in-place, the original module is mutated\nConstraints:\ndtype:bool\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.nn.SyncBatchNorm(num_features,eps=1e-05,momentum=0.1,affine=True,track_running_stats=True,process_group=None)\nParameter description: track_running_stats - a boolean value that when set to `True`, this module tracks the running mean and variance, and when set to `False`, this module does not track such statistics and always uses batch statistics in both training and eval modes. Default: `True`\nConstraints:\ndtype:boolean;bool\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.floor_divide(input,other,out=None)\nParameter description: other(Tensor or Scalar) - the denominator\nConstraints:\ndtype:Null\nstructure:tensor\nshape:Null\nndim:0\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.nn.AvgPool3d(kernel_size,stride=None,padding=0,ceil_mode=False,count_include_pad=True,divisor_override=None)\nParameter description: count_include_pad - when True, will include the zero-padding in the averaging calculation\nConstraints:\ndtype:bool\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.sparse_coo_tensor(indices,values,size=None,dtype=None,device=None,requires_grad=False)\nParameter description: requires_grad(bool, optional) - If autograd should record operations on the returned tensor. Default: `False`.\nConstraints:",
            "constr": {
                "dtype": [
                    "bool"
                ],
                "structure": [],
                "shape": [],
                "ndim": [
                    "0"
                ],
                "enum": [],
                "range": []
            },
            "signature": "torch.sparse_coo_tensor(indices,values,size=None,dtype=None,device=None,requires_grad=False)",
            "descp": "requires_grad(bool, optional) - If autograd should record operations on the returned tensor. Default: `False`."
        },
        {
            "fname": "torch.nn.instancenorm1d.yaml",
            "arg": "momentum",
            "prompt": "Signature: torch.mean(input)\nParameter description: input(Tensor) - the input tensor.\nConstraints:\ndtype:Null\nstructure:tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.nn.Linear(in_features,out_features,bias=True)\nParameter description: in_features - size of each input sample\nConstraints:\ndtype:int\nstructure:Null\nshape:Null\nndim:Null\nenum:Null\nrange:[0,inf)\n\n###\n\nSignature: torch.combinations(input,r=2,with_replacement=False)\nParameter description: with_replacement(boolean, optional) - whether to allow duplication in combination\nConstraints:\ndtype:boolean;bool\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.rand_like(input,dtype=None,layout=None,device=None,requires_grad=False,memory_format=torch.preserve_format)\nParameter description: input(Tensor) - the size of `input` will determine size of the output tensor.\nConstraints:\ndtype:int\nstructure:tensor\nshape:Null\nndim:Null\nenum:Null\nrange:[0,inf)\n\n###\n\nSignature: torch.median(input)\nParameter description: input(Tensor) - the input tensor.\nConstraints:\ndtype:Null\nstructure:tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.tensordot(a,b,dims=2)\nParameter description: b(Tensor) - Right tensor to contract\nConstraints:\ndtype:Null\nstructure:tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.quantization.quantize(model,run_fn,run_args,mapping=None,inplace=False)\nParameter description: inplace - carry out model transformations in-place, the original module is mutated\nConstraints:\ndtype:bool\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.nn.SyncBatchNorm(num_features,eps=1e-05,momentum=0.1,affine=True,track_running_stats=True,process_group=None)\nParameter description: track_running_stats - a boolean value that when set to `True`, this module tracks the running mean and variance, and when set to `False`, this module does not track such statistics and always uses batch statistics in both training and eval modes. Default: `True`\nConstraints:\ndtype:boolean;bool\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.floor_divide(input,other,out=None)\nParameter description: other(Tensor or Scalar) - the denominator\nConstraints:\ndtype:Null\nstructure:tensor\nshape:Null\nndim:0\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.nn.AvgPool3d(kernel_size,stride=None,padding=0,ceil_mode=False,count_include_pad=True,divisor_override=None)\nParameter description: count_include_pad - when True, will include the zero-padding in the averaging calculation\nConstraints:\ndtype:bool\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.nn.InstanceNorm1d(num_features,eps=1e-05,momentum=0.1,affine=False,track_running_stats=False)\nParameter description: momentum - the value used for the running_mean and running_var computation. Default: 0.1\nConstraints:",
            "constr": {
                "dtype": [
                    "bool",
                    "float"
                ],
                "structure": [],
                "shape": [],
                "ndim": [
                    "0"
                ],
                "enum": [],
                "range": []
            },
            "signature": "torch.nn.InstanceNorm1d(num_features,eps=1e-05,momentum=0.1,affine=False,track_running_stats=False)",
            "descp": "momentum - the value used for the running_mean and running_var computation. Default: 0.1"
        },
        {
            "fname": "torch.random.fork_rng.yaml",
            "arg": "devices",
            "prompt": "Signature: torch.mean(input)\nParameter description: input(Tensor) - the input tensor.\nConstraints:\ndtype:Null\nstructure:tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.nn.Linear(in_features,out_features,bias=True)\nParameter description: in_features - size of each input sample\nConstraints:\ndtype:int\nstructure:Null\nshape:Null\nndim:Null\nenum:Null\nrange:[0,inf)\n\n###\n\nSignature: torch.combinations(input,r=2,with_replacement=False)\nParameter description: with_replacement(boolean, optional) - whether to allow duplication in combination\nConstraints:\ndtype:boolean;bool\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.rand_like(input,dtype=None,layout=None,device=None,requires_grad=False,memory_format=torch.preserve_format)\nParameter description: input(Tensor) - the size of `input` will determine size of the output tensor.\nConstraints:\ndtype:int\nstructure:tensor\nshape:Null\nndim:Null\nenum:Null\nrange:[0,inf)\n\n###\n\nSignature: torch.median(input)\nParameter description: input(Tensor) - the input tensor.\nConstraints:\ndtype:Null\nstructure:tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.tensordot(a,b,dims=2)\nParameter description: b(Tensor) - Right tensor to contract\nConstraints:\ndtype:Null\nstructure:tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.quantization.quantize(model,run_fn,run_args,mapping=None,inplace=False)\nParameter description: inplace - carry out model transformations in-place, the original module is mutated\nConstraints:\ndtype:bool\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.nn.SyncBatchNorm(num_features,eps=1e-05,momentum=0.1,affine=True,track_running_stats=True,process_group=None)\nParameter description: track_running_stats - a boolean value that when set to `True`, this module tracks the running mean and variance, and when set to `False`, this module does not track such statistics and always uses batch statistics in both training and eval modes. Default: `True`\nConstraints:\ndtype:boolean;bool\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.floor_divide(input,other,out=None)\nParameter description: other(Tensor or Scalar) - the denominator\nConstraints:\ndtype:Null\nstructure:tensor\nshape:Null\nndim:0\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.nn.AvgPool3d(kernel_size,stride=None,padding=0,ceil_mode=False,count_include_pad=True,divisor_override=None)\nParameter description: count_include_pad - when True, will include the zero-padding in the averaging calculation\nConstraints:\ndtype:bool\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.random.fork_rng(devices=None,enabled=True,_caller=fork_rng,_devices_kw=devices)\nParameter description: devices(iterable of CUDA IDs) - CUDA devices for which to fork the RNG.  CPU RNG state is always forked.  By default, `fork_rng()` operates on all devices, but will emit a warning if your machine has a lot of devices, since this function will run very slowly in that case. If you explicitly specify devices, this warning will be suppressed\nConstraints:",
            "constr": {
                "dtype": [
                    "int"
                ],
                "structure": [
                    "iterable"
                ],
                "shape": [],
                "ndim": [],
                "enum": [],
                "range": [
                    "[0,inf)"
                ]
            },
            "signature": "torch.random.fork_rng(devices=None,enabled=True,_caller=fork_rng,_devices_kw=devices)",
            "descp": "devices(iterable of CUDA IDs) - CUDA devices for which to fork the RNG.  CPU RNG state is always forked.  By default, `fork_rng()` operates on all devices, but will emit a warning if your machine has a lot of devices, since this function will run very slowly in that case. If you explicitly specify devices, this warning will be suppressed"
        },
        {
            "fname": "torch.utils.data.random_split.yaml",
            "arg": "lengths",
            "prompt": "Signature: torch.mean(input)\nParameter description: input(Tensor) - the input tensor.\nConstraints:\ndtype:Null\nstructure:tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.nn.Linear(in_features,out_features,bias=True)\nParameter description: in_features - size of each input sample\nConstraints:\ndtype:int\nstructure:Null\nshape:Null\nndim:Null\nenum:Null\nrange:[0,inf)\n\n###\n\nSignature: torch.combinations(input,r=2,with_replacement=False)\nParameter description: with_replacement(boolean, optional) - whether to allow duplication in combination\nConstraints:\ndtype:boolean;bool\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.rand_like(input,dtype=None,layout=None,device=None,requires_grad=False,memory_format=torch.preserve_format)\nParameter description: input(Tensor) - the size of `input` will determine size of the output tensor.\nConstraints:\ndtype:int\nstructure:tensor\nshape:Null\nndim:Null\nenum:Null\nrange:[0,inf)\n\n###\n\nSignature: torch.median(input)\nParameter description: input(Tensor) - the input tensor.\nConstraints:\ndtype:Null\nstructure:tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.tensordot(a,b,dims=2)\nParameter description: b(Tensor) - Right tensor to contract\nConstraints:\ndtype:Null\nstructure:tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.quantization.quantize(model,run_fn,run_args,mapping=None,inplace=False)\nParameter description: inplace - carry out model transformations in-place, the original module is mutated\nConstraints:\ndtype:bool\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.nn.SyncBatchNorm(num_features,eps=1e-05,momentum=0.1,affine=True,track_running_stats=True,process_group=None)\nParameter description: track_running_stats - a boolean value that when set to `True`, this module tracks the running mean and variance, and when set to `False`, this module does not track such statistics and always uses batch statistics in both training and eval modes. Default: `True`\nConstraints:\ndtype:boolean;bool\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.floor_divide(input,other,out=None)\nParameter description: other(Tensor or Scalar) - the denominator\nConstraints:\ndtype:Null\nstructure:tensor\nshape:Null\nndim:0\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.nn.AvgPool3d(kernel_size,stride=None,padding=0,ceil_mode=False,count_include_pad=True,divisor_override=None)\nParameter description: count_include_pad - when True, will include the zero-padding in the averaging calculation\nConstraints:\ndtype:bool\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.utils.data.random_split(dataset,lengths)\nParameter description: lengths(sequence) - lengths of splits to be produced\nConstraints:",
            "constr": {
                "dtype": [
                    "int"
                ],
                "structure": [
                    "sequence"
                ],
                "shape": [],
                "ndim": [
                    "1"
                ],
                "enum": [],
                "range": [
                    "[0,inf)"
                ]
            },
            "signature": "torch.utils.data.random_split(dataset,lengths)",
            "descp": "lengths(sequence) - lengths of splits to be produced"
        },
        {
            "fname": "torch.nn.quantized.functional.avg_pool2d.yaml",
            "arg": "stride",
            "prompt": "Signature: torch.mean(input)\nParameter description: input(Tensor) - the input tensor.\nConstraints:\ndtype:Null\nstructure:tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.nn.Linear(in_features,out_features,bias=True)\nParameter description: in_features - size of each input sample\nConstraints:\ndtype:int\nstructure:Null\nshape:Null\nndim:Null\nenum:Null\nrange:[0,inf)\n\n###\n\nSignature: torch.combinations(input,r=2,with_replacement=False)\nParameter description: with_replacement(boolean, optional) - whether to allow duplication in combination\nConstraints:\ndtype:boolean;bool\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.rand_like(input,dtype=None,layout=None,device=None,requires_grad=False,memory_format=torch.preserve_format)\nParameter description: input(Tensor) - the size of `input` will determine size of the output tensor.\nConstraints:\ndtype:int\nstructure:tensor\nshape:Null\nndim:Null\nenum:Null\nrange:[0,inf)\n\n###\n\nSignature: torch.median(input)\nParameter description: input(Tensor) - the input tensor.\nConstraints:\ndtype:Null\nstructure:tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.tensordot(a,b,dims=2)\nParameter description: b(Tensor) - Right tensor to contract\nConstraints:\ndtype:Null\nstructure:tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.quantization.quantize(model,run_fn,run_args,mapping=None,inplace=False)\nParameter description: inplace - carry out model transformations in-place, the original module is mutated\nConstraints:\ndtype:bool\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.nn.SyncBatchNorm(num_features,eps=1e-05,momentum=0.1,affine=True,track_running_stats=True,process_group=None)\nParameter description: track_running_stats - a boolean value that when set to `True`, this module tracks the running mean and variance, and when set to `False`, this module does not track such statistics and always uses batch statistics in both training and eval modes. Default: `True`\nConstraints:\ndtype:boolean;bool\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.floor_divide(input,other,out=None)\nParameter description: other(Tensor or Scalar) - the denominator\nConstraints:\ndtype:Null\nstructure:tensor\nshape:Null\nndim:0\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.nn.AvgPool3d(kernel_size,stride=None,padding=0,ceil_mode=False,count_include_pad=True,divisor_override=None)\nParameter description: count_include_pad - when True, will include the zero-padding in the averaging calculation\nConstraints:\ndtype:bool\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.nn.quantized.functional.avg_pool2d(input,kernel_size,stride=None,padding=0,ceil_mode=False,count_include_pad=True,divisor_override=None)\nParameter description: stride - stride of the pooling operation. Can be a single number or a tuple (sH, sW). Default: `kernel_size`\nConstraints:",
            "constr": {
                "dtype": [
                    "int"
                ],
                "structure": [
                    "tuple"
                ],
                "shape": [],
                "ndim": [
                    "0",
                    "1"
                ],
                "enum": [],
                "range": [
                    "[0,inf)"
                ]
            },
            "signature": "torch.nn.quantized.functional.avg_pool2d(input,kernel_size,stride=None,padding=0,ceil_mode=False,count_include_pad=True,divisor_override=None)",
            "descp": "stride - stride of the pooling operation. Can be a single number or a tuple (sH, sW). Default: `kernel_size`"
        },
        {
            "fname": "torch.nn.tripletmarginloss.yaml",
            "arg": "reduction",
            "prompt": "Signature: torch.mean(input)\nParameter description: input(Tensor) - the input tensor.\nConstraints:\ndtype:Null\nstructure:tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.nn.Linear(in_features,out_features,bias=True)\nParameter description: in_features - size of each input sample\nConstraints:\ndtype:int\nstructure:Null\nshape:Null\nndim:Null\nenum:Null\nrange:[0,inf)\n\n###\n\nSignature: torch.combinations(input,r=2,with_replacement=False)\nParameter description: with_replacement(boolean, optional) - whether to allow duplication in combination\nConstraints:\ndtype:boolean;bool\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.rand_like(input,dtype=None,layout=None,device=None,requires_grad=False,memory_format=torch.preserve_format)\nParameter description: input(Tensor) - the size of `input` will determine size of the output tensor.\nConstraints:\ndtype:int\nstructure:tensor\nshape:Null\nndim:Null\nenum:Null\nrange:[0,inf)\n\n###\n\nSignature: torch.median(input)\nParameter description: input(Tensor) - the input tensor.\nConstraints:\ndtype:Null\nstructure:tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.tensordot(a,b,dims=2)\nParameter description: b(Tensor) - Right tensor to contract\nConstraints:\ndtype:Null\nstructure:tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.quantization.quantize(model,run_fn,run_args,mapping=None,inplace=False)\nParameter description: inplace - carry out model transformations in-place, the original module is mutated\nConstraints:\ndtype:bool\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.nn.SyncBatchNorm(num_features,eps=1e-05,momentum=0.1,affine=True,track_running_stats=True,process_group=None)\nParameter description: track_running_stats - a boolean value that when set to `True`, this module tracks the running mean and variance, and when set to `False`, this module does not track such statistics and always uses batch statistics in both training and eval modes. Default: `True`\nConstraints:\ndtype:boolean;bool\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.floor_divide(input,other,out=None)\nParameter description: other(Tensor or Scalar) - the denominator\nConstraints:\ndtype:Null\nstructure:tensor\nshape:Null\nndim:0\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.nn.AvgPool3d(kernel_size,stride=None,padding=0,ceil_mode=False,count_include_pad=True,divisor_override=None)\nParameter description: count_include_pad - when True, will include the zero-padding in the averaging calculation\nConstraints:\ndtype:bool\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.nn.TripletMarginLoss(margin=1.0,p=2.0,eps=1e-06,swap=False,size_average=None,reduce=None,reduction=mean)\nParameter description: reduction(string, optional) - Specifies the reduction to apply to the output: `'none'` | `'mean'` | `'sum'`. `'none'`: no reduction will be applied, `'mean'`: the sum of the output will be divided by the number of elements in the output, `'sum'`: the output will be summed. Note: `size_average` and `reduce` are in the process of being deprecated, and in the meantime, specifying either of those two args will override `reduction`. Default: `'mean'`\nConstraints:",
            "constr": {
                "dtype": [
                    "string"
                ],
                "structure": [],
                "shape": [],
                "ndim": [],
                "enum": [
                    "none",
                    "mean",
                    "sum"
                ],
                "range": []
            },
            "signature": "torch.nn.TripletMarginLoss(margin=1.0,p=2.0,eps=1e-06,swap=False,size_average=None,reduce=None,reduction=mean)",
            "descp": "reduction(string, optional) - Specifies the reduction to apply to the output: `'none'` | `'mean'` | `'sum'`. `'none'`: no reduction will be applied, `'mean'`: the sum of the output will be divided by the number of elements in the output, `'sum'`: the output will be summed. Note: `size_average` and `reduce` are in the process of being deprecated, and in the meantime, specifying either of those two args will override `reduction`. Default: `'mean'`"
        },
        {
            "fname": "torch.nn.mseloss.yaml",
            "arg": "reduction",
            "prompt": "Signature: torch.mean(input)\nParameter description: input(Tensor) - the input tensor.\nConstraints:\ndtype:Null\nstructure:tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.nn.Linear(in_features,out_features,bias=True)\nParameter description: in_features - size of each input sample\nConstraints:\ndtype:int\nstructure:Null\nshape:Null\nndim:Null\nenum:Null\nrange:[0,inf)\n\n###\n\nSignature: torch.combinations(input,r=2,with_replacement=False)\nParameter description: with_replacement(boolean, optional) - whether to allow duplication in combination\nConstraints:\ndtype:boolean;bool\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.rand_like(input,dtype=None,layout=None,device=None,requires_grad=False,memory_format=torch.preserve_format)\nParameter description: input(Tensor) - the size of `input` will determine size of the output tensor.\nConstraints:\ndtype:int\nstructure:tensor\nshape:Null\nndim:Null\nenum:Null\nrange:[0,inf)\n\n###\n\nSignature: torch.median(input)\nParameter description: input(Tensor) - the input tensor.\nConstraints:\ndtype:Null\nstructure:tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.tensordot(a,b,dims=2)\nParameter description: b(Tensor) - Right tensor to contract\nConstraints:\ndtype:Null\nstructure:tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.quantization.quantize(model,run_fn,run_args,mapping=None,inplace=False)\nParameter description: inplace - carry out model transformations in-place, the original module is mutated\nConstraints:\ndtype:bool\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.nn.SyncBatchNorm(num_features,eps=1e-05,momentum=0.1,affine=True,track_running_stats=True,process_group=None)\nParameter description: track_running_stats - a boolean value that when set to `True`, this module tracks the running mean and variance, and when set to `False`, this module does not track such statistics and always uses batch statistics in both training and eval modes. Default: `True`\nConstraints:\ndtype:boolean;bool\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.floor_divide(input,other,out=None)\nParameter description: other(Tensor or Scalar) - the denominator\nConstraints:\ndtype:Null\nstructure:tensor\nshape:Null\nndim:0\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.nn.AvgPool3d(kernel_size,stride=None,padding=0,ceil_mode=False,count_include_pad=True,divisor_override=None)\nParameter description: count_include_pad - when True, will include the zero-padding in the averaging calculation\nConstraints:\ndtype:bool\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.nn.MSELoss(size_average=None,reduce=None,reduction=mean)\nParameter description: reduction(string, optional) - Specifies the reduction to apply to the output: `'none'` | `'mean'` | `'sum'`. `'none'`: no reduction will be applied, `'mean'`: the sum of the output will be divided by the number of elements in the output, `'sum'`: the output will be summed. Note: `size_average` and `reduce` are in the process of being deprecated, and in the meantime, specifying either of those two args will override `reduction`. Default: `'mean'`\nConstraints:",
            "constr": {
                "dtype": [
                    "string"
                ],
                "structure": [],
                "shape": [],
                "ndim": [],
                "enum": [
                    "sum",
                    "mean",
                    "none"
                ],
                "range": []
            },
            "signature": "torch.nn.MSELoss(size_average=None,reduce=None,reduction=mean)",
            "descp": "reduction(string, optional) - Specifies the reduction to apply to the output: `'none'` | `'mean'` | `'sum'`. `'none'`: no reduction will be applied, `'mean'`: the sum of the output will be divided by the number of elements in the output, `'sum'`: the output will be summed. Note: `size_average` and `reduce` are in the process of being deprecated, and in the meantime, specifying either of those two args will override `reduction`. Default: `'mean'`"
        },
        {
            "fname": "torch.nn.transformer.yaml",
            "arg": "num_decoder_layers",
            "prompt": "Signature: torch.mean(input)\nParameter description: input(Tensor) - the input tensor.\nConstraints:\ndtype:Null\nstructure:tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.nn.Linear(in_features,out_features,bias=True)\nParameter description: in_features - size of each input sample\nConstraints:\ndtype:int\nstructure:Null\nshape:Null\nndim:Null\nenum:Null\nrange:[0,inf)\n\n###\n\nSignature: torch.combinations(input,r=2,with_replacement=False)\nParameter description: with_replacement(boolean, optional) - whether to allow duplication in combination\nConstraints:\ndtype:boolean;bool\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.rand_like(input,dtype=None,layout=None,device=None,requires_grad=False,memory_format=torch.preserve_format)\nParameter description: input(Tensor) - the size of `input` will determine size of the output tensor.\nConstraints:\ndtype:int\nstructure:tensor\nshape:Null\nndim:Null\nenum:Null\nrange:[0,inf)\n\n###\n\nSignature: torch.median(input)\nParameter description: input(Tensor) - the input tensor.\nConstraints:\ndtype:Null\nstructure:tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.tensordot(a,b,dims=2)\nParameter description: b(Tensor) - Right tensor to contract\nConstraints:\ndtype:Null\nstructure:tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.quantization.quantize(model,run_fn,run_args,mapping=None,inplace=False)\nParameter description: inplace - carry out model transformations in-place, the original module is mutated\nConstraints:\ndtype:bool\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.nn.SyncBatchNorm(num_features,eps=1e-05,momentum=0.1,affine=True,track_running_stats=True,process_group=None)\nParameter description: track_running_stats - a boolean value that when set to `True`, this module tracks the running mean and variance, and when set to `False`, this module does not track such statistics and always uses batch statistics in both training and eval modes. Default: `True`\nConstraints:\ndtype:boolean;bool\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.floor_divide(input,other,out=None)\nParameter description: other(Tensor or Scalar) - the denominator\nConstraints:\ndtype:Null\nstructure:tensor\nshape:Null\nndim:0\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.nn.AvgPool3d(kernel_size,stride=None,padding=0,ceil_mode=False,count_include_pad=True,divisor_override=None)\nParameter description: count_include_pad - when True, will include the zero-padding in the averaging calculation\nConstraints:\ndtype:bool\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.nn.Transformer(d_model=512,nhead=8,num_encoder_layers=6,num_decoder_layers=6,dim_feedforward=2048,dropout=0.1,activation=relu,custom_encoder=None,custom_decoder=None)\nParameter description: num_decoder_layers - the number of sub-decoder-layers in the decoder (default=6).\nConstraints:",
            "constr": {
                "dtype": [
                    "int"
                ],
                "structure": [],
                "shape": [],
                "ndim": [
                    "0"
                ],
                "enum": [],
                "range": [
                    "[0,inf)"
                ]
            },
            "signature": "torch.nn.Transformer(d_model=512,nhead=8,num_encoder_layers=6,num_decoder_layers=6,dim_feedforward=2048,dropout=0.1,activation=relu,custom_encoder=None,custom_decoder=None)",
            "descp": "num_decoder_layers - the number of sub-decoder-layers in the decoder (default=6)."
        },
        {
            "fname": "torch.log1p.yaml",
            "arg": "input",
            "prompt": "Signature: torch.mean(input)\nParameter description: input(Tensor) - the input tensor.\nConstraints:\ndtype:Null\nstructure:tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.nn.Linear(in_features,out_features,bias=True)\nParameter description: in_features - size of each input sample\nConstraints:\ndtype:int\nstructure:Null\nshape:Null\nndim:Null\nenum:Null\nrange:[0,inf)\n\n###\n\nSignature: torch.combinations(input,r=2,with_replacement=False)\nParameter description: with_replacement(boolean, optional) - whether to allow duplication in combination\nConstraints:\ndtype:boolean;bool\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.rand_like(input,dtype=None,layout=None,device=None,requires_grad=False,memory_format=torch.preserve_format)\nParameter description: input(Tensor) - the size of `input` will determine size of the output tensor.\nConstraints:\ndtype:int\nstructure:tensor\nshape:Null\nndim:Null\nenum:Null\nrange:[0,inf)\n\n###\n\nSignature: torch.median(input)\nParameter description: input(Tensor) - the input tensor.\nConstraints:\ndtype:Null\nstructure:tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.tensordot(a,b,dims=2)\nParameter description: b(Tensor) - Right tensor to contract\nConstraints:\ndtype:Null\nstructure:tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.quantization.quantize(model,run_fn,run_args,mapping=None,inplace=False)\nParameter description: inplace - carry out model transformations in-place, the original module is mutated\nConstraints:\ndtype:bool\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.nn.SyncBatchNorm(num_features,eps=1e-05,momentum=0.1,affine=True,track_running_stats=True,process_group=None)\nParameter description: track_running_stats - a boolean value that when set to `True`, this module tracks the running mean and variance, and when set to `False`, this module does not track such statistics and always uses batch statistics in both training and eval modes. Default: `True`\nConstraints:\ndtype:boolean;bool\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.floor_divide(input,other,out=None)\nParameter description: other(Tensor or Scalar) - the denominator\nConstraints:\ndtype:Null\nstructure:tensor\nshape:Null\nndim:0\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.nn.AvgPool3d(kernel_size,stride=None,padding=0,ceil_mode=False,count_include_pad=True,divisor_override=None)\nParameter description: count_include_pad - when True, will include the zero-padding in the averaging calculation\nConstraints:\ndtype:bool\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.log1p(input,out=None)\nParameter description: input(Tensor) - the input tensor.\nConstraints:",
            "constr": {
                "dtype": [],
                "structure": [
                    "tensor"
                ],
                "shape": [],
                "ndim": [],
                "enum": [],
                "range": []
            },
            "signature": "torch.log1p(input,out=None)",
            "descp": "input(Tensor) - the input tensor."
        },
        {
            "fname": "torch.nn.init.orthogonal_.yaml",
            "arg": "gain",
            "prompt": "Signature: torch.mean(input)\nParameter description: input(Tensor) - the input tensor.\nConstraints:\ndtype:Null\nstructure:tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.nn.Linear(in_features,out_features,bias=True)\nParameter description: in_features - size of each input sample\nConstraints:\ndtype:int\nstructure:Null\nshape:Null\nndim:Null\nenum:Null\nrange:[0,inf)\n\n###\n\nSignature: torch.combinations(input,r=2,with_replacement=False)\nParameter description: with_replacement(boolean, optional) - whether to allow duplication in combination\nConstraints:\ndtype:boolean;bool\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.rand_like(input,dtype=None,layout=None,device=None,requires_grad=False,memory_format=torch.preserve_format)\nParameter description: input(Tensor) - the size of `input` will determine size of the output tensor.\nConstraints:\ndtype:int\nstructure:tensor\nshape:Null\nndim:Null\nenum:Null\nrange:[0,inf)\n\n###\n\nSignature: torch.median(input)\nParameter description: input(Tensor) - the input tensor.\nConstraints:\ndtype:Null\nstructure:tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.tensordot(a,b,dims=2)\nParameter description: b(Tensor) - Right tensor to contract\nConstraints:\ndtype:Null\nstructure:tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.quantization.quantize(model,run_fn,run_args,mapping=None,inplace=False)\nParameter description: inplace - carry out model transformations in-place, the original module is mutated\nConstraints:\ndtype:bool\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.nn.SyncBatchNorm(num_features,eps=1e-05,momentum=0.1,affine=True,track_running_stats=True,process_group=None)\nParameter description: track_running_stats - a boolean value that when set to `True`, this module tracks the running mean and variance, and when set to `False`, this module does not track such statistics and always uses batch statistics in both training and eval modes. Default: `True`\nConstraints:\ndtype:boolean;bool\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.floor_divide(input,other,out=None)\nParameter description: other(Tensor or Scalar) - the denominator\nConstraints:\ndtype:Null\nstructure:tensor\nshape:Null\nndim:0\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.nn.AvgPool3d(kernel_size,stride=None,padding=0,ceil_mode=False,count_include_pad=True,divisor_override=None)\nParameter description: count_include_pad - when True, will include the zero-padding in the averaging calculation\nConstraints:\ndtype:bool\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.nn.init.orthogonal_(tensor,gain=1)\nParameter description: gain - optional scaling factor\nConstraints:",
            "constr": {
                "dtype": [
                    "int"
                ],
                "structure": [],
                "shape": [],
                "ndim": [
                    "0"
                ],
                "enum": [],
                "range": []
            },
            "signature": "torch.nn.init.orthogonal_(tensor,gain=1)",
            "descp": "gain - optional scaling factor"
        },
        {
            "fname": "torch.nn.multilabelmarginloss.yaml",
            "arg": "reduce",
            "prompt": "Signature: torch.mean(input)\nParameter description: input(Tensor) - the input tensor.\nConstraints:\ndtype:Null\nstructure:tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.nn.Linear(in_features,out_features,bias=True)\nParameter description: in_features - size of each input sample\nConstraints:\ndtype:int\nstructure:Null\nshape:Null\nndim:Null\nenum:Null\nrange:[0,inf)\n\n###\n\nSignature: torch.combinations(input,r=2,with_replacement=False)\nParameter description: with_replacement(boolean, optional) - whether to allow duplication in combination\nConstraints:\ndtype:boolean;bool\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.rand_like(input,dtype=None,layout=None,device=None,requires_grad=False,memory_format=torch.preserve_format)\nParameter description: input(Tensor) - the size of `input` will determine size of the output tensor.\nConstraints:\ndtype:int\nstructure:tensor\nshape:Null\nndim:Null\nenum:Null\nrange:[0,inf)\n\n###\n\nSignature: torch.median(input)\nParameter description: input(Tensor) - the input tensor.\nConstraints:\ndtype:Null\nstructure:tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.tensordot(a,b,dims=2)\nParameter description: b(Tensor) - Right tensor to contract\nConstraints:\ndtype:Null\nstructure:tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.quantization.quantize(model,run_fn,run_args,mapping=None,inplace=False)\nParameter description: inplace - carry out model transformations in-place, the original module is mutated\nConstraints:\ndtype:bool\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.nn.SyncBatchNorm(num_features,eps=1e-05,momentum=0.1,affine=True,track_running_stats=True,process_group=None)\nParameter description: track_running_stats - a boolean value that when set to `True`, this module tracks the running mean and variance, and when set to `False`, this module does not track such statistics and always uses batch statistics in both training and eval modes. Default: `True`\nConstraints:\ndtype:boolean;bool\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.floor_divide(input,other,out=None)\nParameter description: other(Tensor or Scalar) - the denominator\nConstraints:\ndtype:Null\nstructure:tensor\nshape:Null\nndim:0\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.nn.AvgPool3d(kernel_size,stride=None,padding=0,ceil_mode=False,count_include_pad=True,divisor_override=None)\nParameter description: count_include_pad - when True, will include the zero-padding in the averaging calculation\nConstraints:\ndtype:bool\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.nn.MultiLabelMarginLoss(size_average=None,reduce=None,reduction=mean)\nParameter description: reduce(bool, optional) - Deprecated (see `reduction`). By default, the losses are averaged or summed over observations for each minibatch depending on `size_average`. When `reduce` is `False`, returns a loss per batch element instead and ignores `size_average`. Default: `True`\nConstraints:",
            "constr": {
                "dtype": [
                    "bool"
                ],
                "structure": [],
                "shape": [],
                "ndim": [
                    "0"
                ],
                "enum": [],
                "range": []
            },
            "signature": "torch.nn.MultiLabelMarginLoss(size_average=None,reduce=None,reduction=mean)",
            "descp": "reduce(bool, optional) - Deprecated (see `reduction`). By default, the losses are averaged or summed over observations for each minibatch depending on `size_average`. When `reduce` is `False`, returns a loss per batch element instead and ignores `size_average`. Default: `True`"
        },
        {
            "fname": "torch.matrix_rank.yaml",
            "arg": "input",
            "prompt": "Signature: torch.mean(input)\nParameter description: input(Tensor) - the input tensor.\nConstraints:\ndtype:Null\nstructure:tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.nn.Linear(in_features,out_features,bias=True)\nParameter description: in_features - size of each input sample\nConstraints:\ndtype:int\nstructure:Null\nshape:Null\nndim:Null\nenum:Null\nrange:[0,inf)\n\n###\n\nSignature: torch.combinations(input,r=2,with_replacement=False)\nParameter description: with_replacement(boolean, optional) - whether to allow duplication in combination\nConstraints:\ndtype:boolean;bool\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.rand_like(input,dtype=None,layout=None,device=None,requires_grad=False,memory_format=torch.preserve_format)\nParameter description: input(Tensor) - the size of `input` will determine size of the output tensor.\nConstraints:\ndtype:int\nstructure:tensor\nshape:Null\nndim:Null\nenum:Null\nrange:[0,inf)\n\n###\n\nSignature: torch.median(input)\nParameter description: input(Tensor) - the input tensor.\nConstraints:\ndtype:Null\nstructure:tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.tensordot(a,b,dims=2)\nParameter description: b(Tensor) - Right tensor to contract\nConstraints:\ndtype:Null\nstructure:tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.quantization.quantize(model,run_fn,run_args,mapping=None,inplace=False)\nParameter description: inplace - carry out model transformations in-place, the original module is mutated\nConstraints:\ndtype:bool\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.nn.SyncBatchNorm(num_features,eps=1e-05,momentum=0.1,affine=True,track_running_stats=True,process_group=None)\nParameter description: track_running_stats - a boolean value that when set to `True`, this module tracks the running mean and variance, and when set to `False`, this module does not track such statistics and always uses batch statistics in both training and eval modes. Default: `True`\nConstraints:\ndtype:boolean;bool\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.floor_divide(input,other,out=None)\nParameter description: other(Tensor or Scalar) - the denominator\nConstraints:\ndtype:Null\nstructure:tensor\nshape:Null\nndim:0\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.nn.AvgPool3d(kernel_size,stride=None,padding=0,ceil_mode=False,count_include_pad=True,divisor_override=None)\nParameter description: count_include_pad - when True, will include the zero-padding in the averaging calculation\nConstraints:\ndtype:bool\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.matrix_rank(input,tol=None,symmetric=False)\nParameter description: input(Tensor) - the input 2-D tensor\nConstraints:",
            "constr": {
                "dtype": [],
                "structure": [
                    "tensor"
                ],
                "shape": [],
                "ndim": [
                    "2"
                ],
                "enum": [],
                "range": []
            },
            "signature": "torch.matrix_rank(input,tol=None,symmetric=False)",
            "descp": "input(Tensor) - the input 2-D tensor"
        },
        {
            "fname": "torch.add.yaml",
            "arg": "input",
            "prompt": "Signature: torch.mean(input)\nParameter description: input(Tensor) - the input tensor.\nConstraints:\ndtype:Null\nstructure:tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.nn.Linear(in_features,out_features,bias=True)\nParameter description: in_features - size of each input sample\nConstraints:\ndtype:int\nstructure:Null\nshape:Null\nndim:Null\nenum:Null\nrange:[0,inf)\n\n###\n\nSignature: torch.combinations(input,r=2,with_replacement=False)\nParameter description: with_replacement(boolean, optional) - whether to allow duplication in combination\nConstraints:\ndtype:boolean;bool\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.rand_like(input,dtype=None,layout=None,device=None,requires_grad=False,memory_format=torch.preserve_format)\nParameter description: input(Tensor) - the size of `input` will determine size of the output tensor.\nConstraints:\ndtype:int\nstructure:tensor\nshape:Null\nndim:Null\nenum:Null\nrange:[0,inf)\n\n###\n\nSignature: torch.median(input)\nParameter description: input(Tensor) - the input tensor.\nConstraints:\ndtype:Null\nstructure:tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.tensordot(a,b,dims=2)\nParameter description: b(Tensor) - Right tensor to contract\nConstraints:\ndtype:Null\nstructure:tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.quantization.quantize(model,run_fn,run_args,mapping=None,inplace=False)\nParameter description: inplace - carry out model transformations in-place, the original module is mutated\nConstraints:\ndtype:bool\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.nn.SyncBatchNorm(num_features,eps=1e-05,momentum=0.1,affine=True,track_running_stats=True,process_group=None)\nParameter description: track_running_stats - a boolean value that when set to `True`, this module tracks the running mean and variance, and when set to `False`, this module does not track such statistics and always uses batch statistics in both training and eval modes. Default: `True`\nConstraints:\ndtype:boolean;bool\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.floor_divide(input,other,out=None)\nParameter description: other(Tensor or Scalar) - the denominator\nConstraints:\ndtype:Null\nstructure:tensor\nshape:Null\nndim:0\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.nn.AvgPool3d(kernel_size,stride=None,padding=0,ceil_mode=False,count_include_pad=True,divisor_override=None)\nParameter description: count_include_pad - when True, will include the zero-padding in the averaging calculation\nConstraints:\ndtype:bool\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.add(input,other,alpha=1,out=None)\nParameter description: input(Tensor) - the first input tensor\nConstraints:",
            "constr": {
                "dtype": [],
                "structure": [
                    "tensor"
                ],
                "shape": [],
                "ndim": [],
                "enum": [],
                "range": []
            },
            "signature": "torch.add(input,other,alpha=1,out=None)",
            "descp": "input(Tensor) - the first input tensor"
        },
        {
            "fname": "torch.nn.functional.binary_cross_entropy_with_logits.yaml",
            "arg": "weight",
            "prompt": "Signature: torch.mean(input)\nParameter description: input(Tensor) - the input tensor.\nConstraints:\ndtype:Null\nstructure:tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.nn.Linear(in_features,out_features,bias=True)\nParameter description: in_features - size of each input sample\nConstraints:\ndtype:int\nstructure:Null\nshape:Null\nndim:Null\nenum:Null\nrange:[0,inf)\n\n###\n\nSignature: torch.combinations(input,r=2,with_replacement=False)\nParameter description: with_replacement(boolean, optional) - whether to allow duplication in combination\nConstraints:\ndtype:boolean;bool\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.rand_like(input,dtype=None,layout=None,device=None,requires_grad=False,memory_format=torch.preserve_format)\nParameter description: input(Tensor) - the size of `input` will determine size of the output tensor.\nConstraints:\ndtype:int\nstructure:tensor\nshape:Null\nndim:Null\nenum:Null\nrange:[0,inf)\n\n###\n\nSignature: torch.median(input)\nParameter description: input(Tensor) - the input tensor.\nConstraints:\ndtype:Null\nstructure:tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.tensordot(a,b,dims=2)\nParameter description: b(Tensor) - Right tensor to contract\nConstraints:\ndtype:Null\nstructure:tensor\nshape:Null\nndim:Null\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.quantization.quantize(model,run_fn,run_args,mapping=None,inplace=False)\nParameter description: inplace - carry out model transformations in-place, the original module is mutated\nConstraints:\ndtype:bool\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.nn.SyncBatchNorm(num_features,eps=1e-05,momentum=0.1,affine=True,track_running_stats=True,process_group=None)\nParameter description: track_running_stats - a boolean value that when set to `True`, this module tracks the running mean and variance, and when set to `False`, this module does not track such statistics and always uses batch statistics in both training and eval modes. Default: `True`\nConstraints:\ndtype:boolean;bool\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.floor_divide(input,other,out=None)\nParameter description: other(Tensor or Scalar) - the denominator\nConstraints:\ndtype:Null\nstructure:tensor\nshape:Null\nndim:0\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.nn.AvgPool3d(kernel_size,stride=None,padding=0,ceil_mode=False,count_include_pad=True,divisor_override=None)\nParameter description: count_include_pad - when True, will include the zero-padding in the averaging calculation\nConstraints:\ndtype:bool\nstructure:Null\nshape:Null\nndim:0\nenum:Null\nrange:Null\n\n###\n\nSignature: torch.nn.functional.binary_cross_entropy_with_logits(input,target,weight=None,size_average=None,reduce=None,reduction=mean,pos_weight=None)\nParameter description: weight(Tensor, optional) - a manual rescaling weight if provided it's repeated to match input tensor shape\nConstraints:",
            "constr": {
                "dtype": [
                    "numeric"
                ],
                "structure": [
                    "tensor"
                ],
                "shape": [],
                "ndim": [],
                "enum": [],
                "range": []
            },
            "signature": "torch.nn.functional.binary_cross_entropy_with_logits(input,target,weight=None,size_average=None,reduce=None,reduction=mean,pos_weight=None)",
            "descp": "weight(Tensor, optional) - a manual rescaling weight if provided it's repeated to match input tensor shape"
        }
    ]
}